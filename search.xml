<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[火之意志]]></title>
      <url>https://stanxia.github.io/2022/10/27/%E7%81%AB%E4%B9%8B%E6%84%8F%E5%BF%97/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><!--请开始装逼-->
<div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="http://oliji9s3j.bkt.clouddn.com/15120957305825.jpg" alt=""></div><div class="group-picture-column" style="width: 50%;"><img src="http://oliji9s3j.bkt.clouddn.com/15120957546525.jpg" alt=""></div></div><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="http://oliji9s3j.bkt.clouddn.com/15120957661673.jpg" alt=""></div><div class="group-picture-column" style="width: 50%;"><img src="http://oliji9s3j.bkt.clouddn.com/15120957755072.jpg" alt=""></div></div></div></div>
<!--对不起，到时间了，请停止装逼-->
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hive事务管理]]></title>
      <url>https://stanxia.github.io/2017/12/01/Hive%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Hive作为Hadoop家族历史最悠久的组件之一，一直以其优秀的兼容性支持和稳定性而著称，越来越多的企业将业务数据从传统数据库迁移至Hadoop平台，并通过Hive来进行数据分析。但是我们在迁移的过程中难免会碰到如何将传统数据库的功能也迁移到Hadoop的问题，比如说事务。事务作为传统数据库很重要的一个功能，在Hive中是如何实现的呢？Hive的实现有什么不一样的地方呢？我们将传统数据库的应用迁移到Hive如果有事务相关的场景我们该如何去转换并要注意什么问题呢？</p>
<p>本文会通过很多真实测试案例来比较Hive与传统数据库事务的区别，并在文末给出一些在Hive平台上使用事务相关的功能时的指导和建议。</p>
<h2 id="ACID与实现原理"><a href="#ACID与实现原理" class="headerlink" title="ACID与实现原理"></a>ACID与实现原理</h2><p>为了方便解释和说明后面的一些问题，这里重提传统数据库事务相关的概念，以下内容来源于网络。</p>
<a id="more"></a>
<h3 id="ACID说明"><a href="#ACID说明" class="headerlink" title="ACID说明"></a>ACID说明</h3><p>何为事务？就是一组单元化操作，这些操作要么都执行，要么都不执行，是一个不可分割的工作单位。</p>
<p>事务（transaction）所应该具有的四个要素：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。这四个基本要素通常称为ACID特性。</p>
<ol>
<li>原子性（Atomicity）:一个事务是一个不可再分割的工作单位，事务中的所有操作要么都发生，要么都不发生。</li>
<li>一致性（Consistency）:事务开始之前和事务结束以后，数据库的完整性约束没有被破坏。这是说数据库事务不能破坏关系数据的完整性以及业务逻辑上的一致性。</li>
<li>隔离性（Isolation）:多个事务并发访问，事务之间是隔离的，一个事务不影响其它事务运行效果。这指的是在并发环境中，当不同的事务同时操作相同的数据时，每个事务都有各自完整的数据空间。事务查看数据更新时，数据所处的状态要么是另一事务修改它之前的状态，要么是另一事务修改后的状态，事务不会查看到中间状态的数据。事务之间的相应影响，分别为：脏读、不可重复读、幻读、丢失更新。</li>
<li>持久性（Durability）:意味着在事务完成以后，该事务锁对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。</li>
</ol>
<h3 id="ACID的实现原理"><a href="#ACID的实现原理" class="headerlink" title="ACID的实现原理"></a>ACID的实现原理</h3><p>事务可以保证ACID原则的操作，那么事务是如何保证这些原则的？解决ACID问题的两大技术点是：</p>
<ol>
<li>预写日志（Write-ahead logging）保证原子性和持久性</li>
<li>锁（locking）保证隔离性</li>
</ol>
<p>这里并没有提到一致性，是因为一致性是应用相关的话题，它的定义一个由业务系统来定义，什么样的状态才是一致？而实现一致性的代码通常在业务逻辑的代码中得以体现。</p>
<p>注：锁是指在并发环境中通过读写锁来保证操作的互斥性。根据隔离程度不同，锁的运用也不同。</p>
<h2 id="测试环境"><a href="#测试环境" class="headerlink" title="测试环境"></a>测试环境</h2><table>
<thead>
<tr>
<th>操作系统</th>
<th>CentOS 6.5</th>
</tr>
</thead>
<tbody>
<tr>
<td>JDK</td>
<td>jdk1.7.0_67</td>
</tr>
<tr>
<td>CDH</td>
<td>5.9</td>
</tr>
<tr>
<td>Hadoop</td>
<td>2.6.0</td>
</tr>
<tr>
<td>Hive</td>
<td>1.1.0</td>
</tr>
</tbody>
</table>
<h2 id="Hive的ACID测试"><a href="#Hive的ACID测试" class="headerlink" title="Hive的ACID测试"></a>Hive的ACID测试</h2><h3 id="Hive中的锁（不开启事务）"><a href="#Hive中的锁（不开启事务）" class="headerlink" title="Hive中的锁（不开启事务）"></a>Hive中的锁（不开启事务）</h3><p>Hive中定义了两种锁的模式：共享锁（S）和排它锁（X），顾名思义，多个共享锁(S)可以同时获取，但是排它锁(X)会阻塞其它所有锁。在本次测试中，CDH5.9的Concurrency参数是默认开启的（hive.support.concurrency=true），以下分别对开启Concurrency和关闭进行相关测试。</p>
<p>首先在测试之前，创建一个普通的hive表：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_notransaction(user_id <span class="built_in">Int</span>,<span class="keyword">name</span> <span class="keyword">String</span>);</div></pre></td></tr></table></figure>
<p>向test_transaction表中插入测试数据：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_notransaction <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'peach1'</span>),(<span class="number">2</span>,<span class="string">'peach2'</span>),(<span class="number">3</span>, <span class="string">'peach3'</span>),(<span class="number">4</span>, <span class="string">'peach4'</span>);</div></pre></td></tr></table></figure>
<p>查看插入的数据：</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121055789375.jpg" alt=""></p>
<h4 id="开启Concurrency"><a href="#开启Concurrency" class="headerlink" title="开启Concurrency"></a>开启Concurrency</h4><p>1、对catalog_sales表进行并发select操作</p>
<p>执行的sql语句：select count(*) from catalog_sales;</p>
<p>执行单条sql查询时，获取一个共享锁（S），sql语句正常执行</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121056449113.jpg" alt=""></p>
<p>同时执行两条sql查询是，获取两个共享锁，并且sql语句均正常执行</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121056652164.jpg" alt=""></p>
<p><strong>分析</strong>：由此对比可得出hive在执行sql查询时获取Share锁，在并发的情况下可获取多个共享锁。</p>
<p>2、对test表进行并发Insert操作</p>
<p>创建表：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">test</span>(<span class="keyword">name</span> <span class="keyword">string</span>, <span class="keyword">id</span> <span class="built_in">int</span>);</div></pre></td></tr></table></figure>
<p>执行sql语句：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">test</span> <span class="keyword">values</span>(<span class="string">'test11aaa1'</span>,<span class="number">1252</span>); </div><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">test</span> <span class="keyword">values</span>(<span class="string">'test1'</span>,<span class="number">52</span>);</div></pre></td></tr></table></figure>
<p>执行单条insert语句时，获取一个X锁，sql语句正常执行</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121057731335.jpg" alt=""></p>
<p>同时执行两条insert语句时，只能获取一个test表X锁，第一条insert语句正常执行，第二条insert语句处于等待状态，在第一条insert语句释放test表的X锁，第二条sql语句正常执行.</p>
<p><strong>分析</strong>：由此对比可得出hive在执行insert操作时，只能获取一个X锁且锁不能共享，只能在sql执行完成释放锁后，后续sql方可继续执行。</p>
<p>3、对test表执行select的同时执行insert操作</p>
<p>执行sql语句：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">select count(*) from test; </div><div class="line">insert into test values(&quot;test123&quot;,123);</div></pre></td></tr></table></figure>
<p>步骤：</p>
<p>1) 执行select语句，在select未运行完时，在新的窗口同时执行insert语句观察两条sql执行情况，select语句正常执行，insert语句处于等待状态。</p>
<p>2) 此时查看test表锁状态</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121058445186.jpg" alt=""></p>
<p>在步骤1的执行过程中，获取到test表的锁为共享锁（S）</p>
<p>3) 在select语句执行完成后，观察insert语句开始正常执行，此时获取test表锁为排它锁（X）。注意：在select语句执行完成后，大概过40s左右insert语句才正常执行，这是由hive.lock.sleep.between.retries参数控制，默认60</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121058724922.jpg" alt=""></p>
<p><strong>分析</strong>： 由上述操作可得出，hive中一个表只能有一个排它锁(X)且锁不能共享，在获取排它锁时，表上不能有其它锁包括共享锁(S)，只有在表上所有的锁都释放后，insert操作才能继续，否则处于等待状态。</p>
<p>对注意部分进行参数调整，将hive.lock.sleep.between.retries设置为10s，再次进行测试发现，在select语句执行完成后，大概过6s左右insert语句开始执行,通过两次测试发现，等待时间均在10s以内，由此可以得出此参数影响sql操作获取锁的间隔（在未获取到锁的情况下），如果此时未到获取锁触发周期，执行其它sql则，该sql会优于等待的sql执行。</p>
<p>4、对test表执行insert的同时执行select操作</p>
<p>执行sql语句：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">insert into test values(&quot;test123&quot;,123); </div><div class="line">select count(*) from test;</div></pre></td></tr></table></figure>
<p>操作步骤：</p>
<p>1) 在命令窗口执行insert语句，在insert操作未执行完成时，在新的命令窗口执行select语句，观察两个窗口的sql执行情况，insert语句正常执行，select语句处于等待状态。</p>
<p>2) 此时查看test表锁状态，只有insert操作获取的排它锁（X）</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121059151954.jpg" alt=""></p>
<p>3) 在insert语句执行完成后，观察select语句开始正常执行，此时查看test表锁状态为共享锁（S），之前的insert操作获取的排它锁（X）已被释放</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121059320973.jpg" alt=""></p>
<p><strong>分析</strong>：在test表锁状态为排它锁(X)时，所有的操作均被阻塞处于等待状态，只有在排它锁(X)释放其它操作可继续进行。</p>
<p>5、测试update和delete修改test表数据</p>
<p>sql语句：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">update test set name=&apos;aaaa&apos; where id=1252; </div><div class="line">delete test set name=&apos;bbbb&apos; where id=123;</div></pre></td></tr></table></figure>
<p>1) 表中数据，更新前</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121059787723.jpg" alt=""></p>
<p>2) 在beeline窗口执行update操作</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121059930373.jpg" alt=""></p>
<p>执行update操作报错，异常提示“Attempt to do update or delete using transaction manager that does not support these operations”，在非事务模式下不支持update 和 delete。</p>
<h4 id="关闭Concurrency"><a href="#关闭Concurrency" class="headerlink" title="关闭Concurrency"></a>关闭Concurrency</h4><p>1、执行insert操作的同时执行select操作</p>
<p>sql语句：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">insert into test_notransaction values(1,&apos;peach1&apos;),(2,&apos;peach2&apos;),(3, &apos;peach3&apos;),(4, &apos;peach4&apos;); </div><div class="line">select count(*) from test_notransaction;</div></pre></td></tr></table></figure>
<p>操作sql前，查看表数据</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121060752594.jpg" alt=""></p>
<p>查看test_notransaction表获取情况，show locks;</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121060893595.jpg" alt=""></p>
<p>hive在未开启concurrency 的情况下,show locks不能正常获取表的锁，同时对同一张表执行insert和select操作时并发执行，获取数据取决于sql执行速度，因此在select 的时候未获取到插入数据。</p>
<p>2、执行select操作的同时执行insert操作</p>
<p>sql语句：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">select count(*) from test_notransaction; </div><div class="line">insert into test_notransaction values(1,&apos;peach1&apos;),(2,&apos;peach2&apos;),(3, &apos;peach3&apos;),(4, &apos;peach4&apos;);</div></pre></td></tr></table></figure>
<p>在执行select的同时执行insert操作，操作可以同时并行操作，未产生阻塞等待的过程。</p>
<p>3、同时执行多条insert操作</p>
<p>sql语句：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">insert into test_notransaction values(1,&apos;peach1&apos;),(2,&apos;peach2&apos;),(3, &apos;peach3&apos;),(4, &apos;peach4&apos;); </div><div class="line">insert into test_notransaction values(1,&apos;peach1&apos;),(2,&apos;peach2&apos;),(3, &apos;peach3&apos;),(4, &apos;peach4&apos;);</div></pre></td></tr></table></figure>
<p>同时执行insert操作时，可同时执行未产生阻塞等待的过程。</p>
<p>4、执行update操作，将表中user_id为2的用户名修改为peach22</p>
<p>sql语句：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">update test_notransaction set name=&apos;peach22&apos; where user_id=2;</div></pre></td></tr></table></figure>
<p>执行update操作，执行结果如下：</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121061484874.jpg" alt=""></p>
<p>在未配置hive的Transaction和ACID时，不支持update操作。</p>
<p>5、执行delete操作，将表中user_id为1信息删除</p>
<p>sql语句：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">delete</span> <span class="keyword">from</span> test_notransaction <span class="keyword">where</span> user_id=<span class="number">1</span>;</div></pre></td></tr></table></figure>
<p>执行delete操作，执行结果如下：</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121061713652.jpg" alt=""></p>
<p>hive未配置Transaction和ACID，不支持delete操作。</p>
<p>6、查看表获取锁类型</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">show</span> locks;</div></pre></td></tr></table></figure>
<p>无法正常执行；</p>
<h3 id="Hive的事务"><a href="#Hive的事务" class="headerlink" title="Hive的事务"></a>Hive的事务</h3><h4 id="Hive的事务配置"><a href="#Hive的事务配置" class="headerlink" title="Hive的事务配置"></a>Hive的事务配置</h4><p>Hive从0.13开始加入了事务支持，在行级别提供完整的ACID特性，Hive在0.14时加入了对INSERT…VALUES,UPDATE,and DELETE的支持。对于在Hive中使用ACID和Transactions，主要有以下限制：</p>
<ul>
<li>不支持BEGIN,COMMIT和ROLLBACK</li>
<li>只支持ORC文件格式</li>
<li>表必须分桶</li>
<li>不允许从一个非ACID连接写入/读取ACID表</li>
</ul>
<p>为了使Hive支持事务操作，需将以下参数加入到hive-site.xml文件中。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.support.concurrency<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.enforce.bucketing<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.dynamic.partition.mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>nonstrict<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.txn.manager<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.compactor.initiator.on<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.compactor.worker.threads <span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<p>可以在Cloudera Manager进行以下配置：</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121063102376.jpg" alt=""></p>
<p>为了让beeline支持还需要配置：</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121063263012.jpg" alt=""></p>
<h4 id="Hive事务测试"><a href="#Hive事务测试" class="headerlink" title="Hive事务测试"></a>Hive事务测试</h4><h5 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h5><p>1、创建一个支持ACID的表</p>
<p>建表语句：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_trancaction </div><div class="line">(user_id <span class="built_in">Int</span>,<span class="keyword">name</span> <span class="keyword">String</span>) </div><div class="line">clustered <span class="keyword">by</span> (user_id) <span class="keyword">into</span> <span class="number">3</span> buckets <span class="keyword">stored</span> <span class="keyword">as</span> orc TBLPROPERTIES (<span class="string">'transactional'</span>=<span class="string">'true'</span>);</div></pre></td></tr></table></figure>
<p>将表名修改为test_transaction</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">alter</span> <span class="keyword">table</span> test_trancaction <span class="keyword">rename</span> <span class="keyword">to</span> test_transaction;</div></pre></td></tr></table></figure>
<p>2、准备测试数据，向数据库中插入数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_transaction <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'peach'</span>),(<span class="number">2</span>,<span class="string">'peach2'</span>),(<span class="number">3</span>,<span class="string">'peach3'</span>),(<span class="number">4</span>,<span class="string">'peach4'</span>),(<span class="number">5</span>,<span class="string">'peach5'</span>);</div></pre></td></tr></table></figure>
<h5 id="用例测试"><a href="#用例测试" class="headerlink" title="用例测试"></a>用例测试</h5><p>1、执行update操作，将user_id的name修改为peach_update</p>
<p>sql语句：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">update</span> test_transaction <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">'peach_update'</span> <span class="keyword">where</span> user_id=<span class="number">1</span>;</div></pre></td></tr></table></figure>
<p>执行修改操作，查看表获取锁类型</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121064139828.jpg" alt=""></p>
<p>数据修改成功，且不影响其它数据。</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121064271722.jpg" alt=""></p>
<p>2、同时修改同一条数据，将user<em>id为1的用户名字修改为peach，另一条sql将名字修改为peach</em></p>
<p>sql语句：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">update</span> test_transaction <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">'peach'</span> <span class="keyword">where</span> user_id=<span class="number">1</span>;</div><div class="line"><span class="keyword">update</span> test_transaction <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">'peach_'</span> <span class="keyword">where</span> user_id=<span class="number">1</span>;</div></pre></td></tr></table></figure>
<p>sql执行顺序为peach，其次为peach_</p>
<p>此时查看表获取到的锁</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121065743030.jpg" alt=""></p>
<p>通过获取到锁分析，在同时修改同一条数据时，优先执行的sql获取到了SHARED_WRITE，而后执行的sql获取锁的状态为WAITING状态，表示还未获取到SHARED_WRITE锁，等待第一条sql执行结束后方可获取到锁对数据进行操作。</p>
<p>通过上不执行操作分析，数据user<em>id为1的用户名字应被修改为peach</em></p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121065916525.jpg" alt=""></p>
<p>3、同时修改不同数据，修改id为2的name为peachtest，修改id为3的name为peach_test</p>
<p>sql语句：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">update</span> test_transaction <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">'peachtest'</span> <span class="keyword">where</span> user_id=<span class="number">2</span>; </div><div class="line"><span class="keyword">update</span> test_transaction <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">'peach_test'</span> <span class="keyword">where</span> user_id=<span class="number">3</span>;</div></pre></td></tr></table></figure>
<p>sql执行顺序为peachtest，其次为peach_test</p>
<p>此时查看表获取到的锁</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121066198138.jpg" alt=""></p>
<p>通过sql操作获取锁分析，在同时修改不同数据时，优先执行的sql获取到了SHARED_WRITE，而后执行的sql获取锁的状态为WAITING状态，表示还未获取到SHARED_WRITE锁，等待第一条sql执行结束后方可获取到锁对数据进行操作。</p>
<p>4、执行select操作的同时执行insert操作</p>
<p>sql语句：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> test_transaction; </div><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_transaction <span class="keyword">values</span>(<span class="number">3</span>,<span class="string">'peach3'</span>);</div></pre></td></tr></table></figure>
<p>步骤：</p>
<p>先执行select操作，再执行insert操作，执行完成后查看表获取到的锁</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121066537536.jpg" alt=""></p>
<p>由于select和insert操作均获取的是SHARED_READ锁，读锁为并行，所以select查询和insert同时执行，互不影响。</p>
<p>5、update同一条数据的同时select该条数据</p>
<p>sql语句：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">update</span> test_transaction <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">'peach_update'</span> <span class="keyword">where</span> user_id=<span class="number">1</span>; <span class="keyword">select</span> * <span class="keyword">from</span></div><div class="line"> test_transaction <span class="keyword">where</span> user_id=<span class="number">1</span>;</div></pre></td></tr></table></figure>
<p>步骤：</p>
<p>先执行update操作，再执行select操作，获取此时表获取到的锁</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121066850801.jpg" alt=""></p>
<p>通过获取锁的情况分析， 在update操作时，获取到SHARED_WRITE锁，执行select操作时获取到SHARED_READ锁，在进行修改数据时未阻塞select查询操作，update未执行完成时，select查询到的数据为未修改的数据。</p>
<p>6、执行delete操作，将user_id为3的数据删除</p>
<p>sql语句：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">delete</span> <span class="keyword">from</span> test_transaction <span class="keyword">where</span> user_id=<span class="number">3</span>;</div></pre></td></tr></table></figure>
<p>步骤：</p>
<p>执行delete操作，获取此时表获取到的锁</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121067134496.jpg" alt=""></p>
<p>删除操作获取到的是SHARED_WRITE锁</p>
<p>执行成功后数据</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121067318880.jpg" alt=""></p>
<p>7、同时delete同一条数据</p>
<p>sql语句：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">delete</span> <span class="keyword">from</span> test_transaction <span class="keyword">where</span> user_id=<span class="number">3</span>;</div><div class="line"><span class="keyword">delete</span> <span class="keyword">from</span> test_transaction <span class="keyword">where</span> user_id=<span class="number">3</span>;</div></pre></td></tr></table></figure>
<p>步骤：</p>
<p>按顺序执行两条delete操作，查看此时表获取到的锁：</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121067593291.jpg" alt=""></p>
<p>通过查看delete操作获取到的锁，优先执行的操作获取到SHARED_WRITE锁，后执行的delete操作未获取到SHARED_WRITE锁，处于WAITING状态。</p>
<p>执行删除后结果</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121067832545.jpg" alt=""></p>
<p>8、同时delete两条不同的数据</p>
<p>sql语句：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">delete</span> <span class="keyword">from</span> test_transaction <span class="keyword">where</span> user_id=<span class="number">1</span>; </div><div class="line"><span class="keyword">delete</span> <span class="keyword">from</span> test_transaction <span class="keyword">where</span> user_id=<span class="number">5</span>;</div></pre></td></tr></table></figure>
<p>步骤：</p>
<p>按顺序执行两条delete操作，查看此时表获取到的锁：</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121068192695.jpg" alt=""></p>
<p>通过查看delete操作获取到的锁，优先执行的操作获取到SHARED_WRITE锁，后执行的delete操作未获取到SHARED_WRITE锁，处于WAITING状态。</p>
<p>执行删除后结果</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121068364921.jpg" alt=""></p>
<p>9、执行delete的同时对删除的数据进行update操作</p>
<p>sql语句：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">delete</span> <span class="keyword">from</span> test_transaction <span class="keyword">where</span> user_id=<span class="number">3</span>; </div><div class="line"><span class="keyword">update</span> test_transaction <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">'test'</span> <span class="keyword">where</span> user_id=<span class="number">3</span>;</div></pre></td></tr></table></figure>
<p>步骤：</p>
<p>按顺序执行两条sql，查看此时获取到表的锁：</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121068639301.jpg" alt=""></p>
<p>通过查看delete和update操作获取到的锁，优先执行的操作获取到SHARED_WRITE锁，后执行的操作未获取到SHARED_WRITE锁，处于WAITING状态。</p>
<p>执行delete和update后结果</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121068852336.jpg" alt=""></p>
<p><strong>注意</strong>：此处在delete优先于update执行，但执行结果为update的结果，执行异常。</p>
<p>10、执行delete的同时对不同的数据进行update操作</p>
<p>sql语句：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">delete</span> <span class="keyword">from</span> test_transaction <span class="keyword">where</span> user_id=<span class="number">2</span>; </div><div class="line"><span class="keyword">update</span> test_transaction <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">'test'</span> <span class="keyword">where</span> user_id=<span class="number">4</span>;</div></pre></td></tr></table></figure>
<p>步骤：</p>
<p>按顺序执行上面两条sql，查看表锁获取情况</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121069199763.jpg" alt=""></p>
<p>通过查看delete和update操作获取到的锁，优先执行的操作获取到SHARED_WRITE锁，后执行的操作未获取到SHARED_WRITE锁，处于WAITING状态。</p>
<p>执行delete和update后结果,执行结果正常</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121069373565.jpg" alt=""></p>
<p>11、执行delete的同时执行select操作</p>
<p>sql语句：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">delete</span> <span class="keyword">from</span> test_transaction <span class="keyword">where</span> user_id=<span class="number">4</span>; </div><div class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> test_transaction;</div></pre></td></tr></table></figure>
<p>步骤：</p>
<p>按顺序执行上面两条sql，查看表锁获取情况</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15121069645501.jpg" alt=""></p>
<p>在操作delete的同时执行select操作，两个操作均同时获取到SHARED_RED和SHARED_WRITE锁，操作并行进行未出现阻塞。</p>
<h2 id="总结对比"><a href="#总结对比" class="headerlink" title="总结对比"></a>总结对比</h2><p><img src="http://oliji9s3j.bkt.clouddn.com/15121077401844.jpg" alt=""></p>
<h2 id="Hive事务使用建议"><a href="#Hive事务使用建议" class="headerlink" title="Hive事务使用建议"></a>Hive事务使用建议</h2><ol>
<li>传统数据库中有三种模型隐式事务、显示事务和自动事务。在目前Hive对事务仅支持自动事务，因此Hive无法通过显示事务的方式对一个操作序列进行事务控制。</li>
<li>传统数据库事务在遇到异常情况可自动进行回滚，目前Hive无法支持ROLLBACK。</li>
<li>传统数据库中支持事务并发，而Hive对事务无法做到完全并发控制,多个操作均需要获取WRITE的时候则这些操作为串行模式执行（在测试用例中”delete同一条数据的同时update该数据”，操作是串行的且操作完成后数据未被删除且数据被修改）未保证数据一致性。</li>
<li>Hive的事务功能尚属于实验室功能，并不建议用户直接上生产系统，因为目前它还有诸多的限制，如只支持ORC文件格式，建表必须分桶等，使用起来没有那么方便，另外该功能的稳定性还有待进一步验证。</li>
<li>CDH默认开启了Hive的Concurrency功能，主要是对并发读写的的时候通过锁进行了控制。所以为了防止用户在使用Hive的时候，报错提示该表已经被lock，对于用户来说不友好，建议在业务侧控制一下写入和读取，比如写入同一个table或者partition的时候保证是单任务写入，其他写入需控制写完第一个任务了，后面才继续写，并且控制在写的时候不让用户进行查询。另外需要控制在查询的时候不要允许有写入操作。</li>
<li>如果对于数据一致性不在乎，可以完全关闭Hive的Concurrency功能关闭，即设置hive.support.concurrency为false，这样Hive的并发读写将没有任何限制。</li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[SparkSQL-Catalyst优化理解]]></title>
      <url>https://stanxia.github.io/2017/11/30/SparkSQL-Catalyst%E4%BC%98%E5%8C%96%E7%90%86%E8%A7%A3/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文主要介绍SparkSQL的优化器系统Catalyst，其设计思路基本都来自于传统型数据库，而且和大多数当前的大数据SQL处理引擎设计基本相同（Impala、Presto、Hive（Calcite）等）。</p>
<p>SQL优化器核心执行策略主要分为两个大的方向：</p>
<ol>
<li>基于规则优化（RBO）：是一种经验式、启发式地优化思路，更多地依靠前辈总结出来的优化规则，简单易行且能够覆盖到大部分优化逻辑，但是对于核心优化算子Join却显得有点力不从心。</li>
<li>基于代价优化 (CBO)：根据代价估算确定一种代价最小的方案。</li>
</ol>
<p>举个简单的例子，两个表执行Join到底应该使用BroadcastHashJoin还是SortMergeJoin？当前SparkSQL的方式是通过手工设定参数来确定，如果一个表的数据量小于这个值就使用BroadcastHashJoin，但是这种方案显得很不优雅，很不灵活。基于代价优化就是为了解决这类问题，它会针对每个Join评估当前两张表使用每种Join策略的代价，根据代价估算确定一种代价最小的方案。</p>
<a id="more"></a>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15120193767483.jpg" alt=""></p>
<h2 id="Tree-amp-Rule"><a href="#Tree-amp-Rule" class="headerlink" title="Tree&amp;Rule"></a>Tree&amp;Rule</h2><p>在介绍SQL优化器工作原理之前，有必要首先介绍两个重要的数据结构：Tree和Rule。SQL语法树就是SQL语句通过编译器之后会被解析成一棵树状结构。这棵树会包含很多节点对象，每个节点都拥有特定的数据类型，同时会有0个或多个孩子节点（节点对象在代码中定义为TreeNode对象），下图是个简单的示例：<br><img src="http://oliji9s3j.bkt.clouddn.com/15120194433988.jpg" alt=""><br>如上图所示，箭头左边表达式有3种数据类型（Literal表示常量、Attribute表示变量、Add表示动作），表示x+(1+2)。映射到右边树状结构后，每一种数据类型就会变成一个节点。另外，Tree还有一个非常重要的特性，可以通过一定的规则进行等价变换，如下图：<br><img src="http://oliji9s3j.bkt.clouddn.com/15120194660866.jpg" alt=""></p>
<p>上图定义了一个等价变换规则(Rule)：两个Integer类型的常量相加可以等价转换为一个Integer常量，这个规则其实很简单，对于上文中提到的表达式x+(1+2)来说就可以转变为x+3。对于程序来讲，如何找到两个Integer常量呢？其实就是简单的二叉树遍历算法，每遍历到一个节点，就模式匹配当前节点为Add、左右子节点是Integer常量的结构，定位到之后将此三个节点替换为一个Literal类型的节点。</p>
<p>上面用一个最简单的示例来说明等价变换规则以及如何将规则应用于语法树。在任何一个SQL优化器中，通常会定义大量的Rule（后面会讲到），SQL优化器会遍历语法树中每个节点，针对遍历到的节点模式匹配所有给定规则（Rule），如果有匹配成功的，就进行相应转换，如果所有规则都匹配失败，就继续遍历下一个节点。</p>
<h2 id="Catalyst工作流程"><a href="#Catalyst工作流程" class="headerlink" title="Catalyst工作流程"></a>Catalyst工作流程</h2><p>任何一个优化器工作原理都大同小异：SQL语句首先通过Parser模块被解析为语法树，此棵树称为Unresolved Logical Plan；Unresolved Logical Plan通过Analyzer模块借助于数据元数据解析为Logical Plan；此时再通过各种基于规则的优化策略进行深入优化，得到Optimized Logical Plan；优化后的逻辑执行计划依然是逻辑的，并不能被Spark系统理解，此时需要将此逻辑执行计划转换为Physical Plan；为了更好的对整个过程进行理解，下文通过一个简单示例进行解释。</p>
<h3 id="Parser"><a href="#Parser" class="headerlink" title="Parser"></a>Parser</h3><p>Parser简单来说是将SQL字符串切分成一个一个Token，再根据一定语义规则解析为一棵语法树。Parser模块目前基本都使用第三方类库ANTLR进行实现，比如Hive、 Presto、SparkSQL等。下图是一个示例性的SQL语句（有两张表，其中people表主要存储用户基本信息，score表存储用户的各种成绩），通过Parser解析后的AST语法树如图所示：<br><img src="http://oliji9s3j.bkt.clouddn.com/15120196105832.jpg" alt=""></p>
<h3 id="Analyzer"><a href="#Analyzer" class="headerlink" title="Analyzer"></a>Analyzer</h3><p>通过解析后的逻辑执行计划基本有了骨架，但是系统并不知道score、sum这些都是些什么鬼，此时需要基本的元数据信息来表达这些词素，最重要的元数据信息主要包括两部分：表的Scheme和基本函数信息，表的scheme主要包括表的基本定义（列名、数据类型）、表的数据格式（Json、Text）、表的物理位置等，基本函数信息主要指类信息。</p>
<p>Analyzer会再次遍历整个语法树，对树上的每个节点进行数据类型绑定以及函数绑定，比如people词素会根据元数据表信息解析为包含age、id以及name三列的表，people.age会被解析为数据类型为int的变量，sum会被解析为特定的聚合函数，如下图所示：<br><img src="http://oliji9s3j.bkt.clouddn.com/15120196532182.jpg" alt=""></p>
<p>SparkSQL中Analyzer定义了各种解析规则，可以查看Analyzer类，其中定义了基本的解析规则，如下：</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15120196892326.jpg" alt=""></p>
<h3 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h3><p>优化器是整个Catalyst的核心，上文提到优化器分为基于规则优化和基于代价优化两种，当前SparkSQL 2.1依然没有很好的支持基于代价优化，此处只介绍基于规则的优化策略，基于规则的优化策略实际上就是对语法树进行一次遍历，模式匹配能够满足特定规则的节点，再进行相应的等价转换。因此，基于规则优化说到底就是一棵树等价地转换为另一棵树。SQL中经典的优化规则有很多，下文结合示例介绍三种比较常见的规则：</p>
<ol>
<li>谓词下推（Predicate Pushdown）</li>
<li>常量累加（Constant Folding）</li>
<li>列值裁剪（Column Pruning）</li>
</ol>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15120197859386.jpg" alt=""></p>
<p>上图左边是经过Analyzer解析后的语法树，语法树中两个表先做join，之后再使用age&gt;10对结果进行过滤。大家知道join算子通常是一个非常耗时的算子，耗时多少一般取决于参与join的两个表的大小，如果能够减少参与join两表的大小，就可以大大降低join算子所需时间。谓词下推就是这样一种功能，它会将过滤操作下推到join之前进行，上图中过滤条件age&gt;0以及id!=null两个条件就分别下推到了join之前。这样，系统在扫描数据的时候就对数据进行了过滤，参与join的数据量将会得到显著的减少，join耗时必然也会降低。</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15120198070875.jpg" alt=""></p>
<p>常量累加其实很简单，就是上文中提到的规则  x+(1+2)  -&gt; x+3，虽然是一个很小的改动，但是意义巨大。示例如果没有进行优化的话，每一条结果都需要执行一次100+80的操作，然后再与变量math_score以及english_score相加，而优化后就不需要再执行100+80操作。</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15120198279383.jpg" alt=""></p>
<p>列值裁剪是另一个经典的规则，示例中对于people表来说，并不需要扫描它的所有列值，而只需要列值id，所以在扫描people之后需要将其他列进行裁剪，只留下列id。这个优化一方面大幅度减少了网络、内存数据量消耗，另一方面对于列存数据库（Parquet）来说大大提高了扫描效率。</p>
<p>除此之外，Catalyst还定义了很多其他优化规则，可以查看Optimizer类，下图简单的截取一部分规则：</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15120198495604.jpg" alt=""></p>
<p>至此，逻辑执行计划已经得到了比较完善的优化，然而，逻辑执行计划依然没办法真正执行，他们只是逻辑上可行，实际上Spark并不知道如何去执行这个东西。比如Join只是一个抽象概念，代表两个表根据相同的id进行合并，然而具体怎么实现这个合并，逻辑执行计划并没有说明。</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15120198739726.jpg" alt=""></p>
<p>此时就需要将逻辑执行计划转换为物理执行计划，将逻辑上可行的执行计划变为Spark可以真正执行的计划。比如Join算子，Spark根据不同场景为该算子制定了不同的算法策略，有BroadcastHashJoin、ShuffleHashJoin以及SortMergeJoin等（可以将Join理解为一个接口，BroadcastHashJoin是其中一个具体实现），物理执行计划实际上就是在这些具体实现中挑选一个耗时最小的算法实现，这个过程涉及到基于代价优化策略。</p>
<h2 id="查看SparkSQL执行计划"><a href="#查看SparkSQL执行计划" class="headerlink" title="查看SparkSQL执行计划"></a>查看SparkSQL执行计划</h2><p>至此，通过一个简单的示例完整的介绍了Catalyst的整个工作流程，包括Parser阶段、Analyzer阶段、Optimize阶段以及Physical Planning阶段。有同学可能会比较感兴趣Spark环境下如何查看一条具体的SQL的整个过程，在此介绍两种方法：</p>
<h3 id="查看逻辑执行计划"><a href="#查看逻辑执行计划" class="headerlink" title="查看逻辑执行计划"></a>查看逻辑执行计划</h3><p>使用queryExecution方法查看逻辑执行计划，如下所示：</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15120199915140.jpg" alt=""></p>
<h3 id="查看物理执行计划"><a href="#查看物理执行计划" class="headerlink" title="查看物理执行计划"></a>查看物理执行计划</h3><p>使用explain方法查看物理执行计划：</p>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15120200455737.jpg" alt=""></p>
<h3 id="Spark-WebUI进行查看"><a href="#Spark-WebUI进行查看" class="headerlink" title="Spark WebUI进行查看"></a>Spark WebUI进行查看</h3><p><img src="http://oliji9s3j.bkt.clouddn.com/15120200683962.jpg" alt=""></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[spark容错机制]]></title>
      <url>https://stanxia.github.io/2017/11/29/spark%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>一般来说，分布式数据集的容错性有两种方式： 数据检查点 和 记录数据的更新 。面向大规模数据分析，数据检查点操作成本很高，需要通过数据中心的网络连接在机器之间复制庞大的数据集，而网络带宽往往比内存带宽低得多，同时还需要消耗更多的存储资源。因此，Spark选择记录更新的方式。</p>
<p>但是，如果更新粒度太细太多，那么记录更新成本也不低。因此，RDD只支持粗粒度转换，即只记录单个块上执行的单个操作，然后将创建RDD的一系列变换序列（每个RDD都包含了他是如何由其他RDD变换过来的以及如何重建某一块数据的信息。因此RDD的容错机制又称“血统(Lineage)”容错）记录下来，以便恢复丢失的分区。 </p>
<p>Lineage本质上很类似于数据库中的重做日志（Redo Log），只不过这个重做日志粒度很大，是对全局数据做同样的重做进而恢复数据。</p>
<h2 id="Lineage机制"><a href="#Lineage机制" class="headerlink" title="Lineage机制"></a>Lineage机制</h2><h3 id="Lineage简介"><a href="#Lineage简介" class="headerlink" title="Lineage简介"></a>Lineage简介</h3><p>相比其他系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的特定数据Transformation操作（如filter、map、join等）行为。当这个RDD的部分分区数据丢失时，它可以通过Lineage获取足够的信息来重新运算和恢复丢失的数据分区。因为这种粗颗粒的数据模型，限制了Spark的运用场合，所以Spark并不适用于所有高性能要求的场景，但同时相比细颗粒度的数据模型，也带来了性能的提升。</p>
<a id="more"></a>
<h3 id="两种依赖关系"><a href="#两种依赖关系" class="headerlink" title="两种依赖关系"></a>两种依赖关系</h3><p>RDD在Lineage依赖方面分为两种：窄依赖(Narrow Dependencies)与宽依赖(Wide Dependencies,源码中称为Shuffle Dependencies)，用来解决数据容错的高效性。</p>
<ul>
<li>窄依赖：是指父RDD的每一个分区最多被一个子RDD的分区所用，表现为一个父RDD的分区对应于一个子RDD的分区，或多个父RDD的分区对应于一个子RDD的分区，也就是说一个父RDD的一个分区不可能对应一个子RDD的多个分区。 1个父RDD分区对应1个子RDD分区，这其中又分两种情况：1个子RDD分区对应1个父RDD分区（如map、filter等算子），1个子RDD分区对应N个父RDD分区（如co-paritioned（协同划分）过的Join）。</li>
<li>宽依赖：是指子RDD的分区依赖于父RDD的多个分区或所有分区，即存在一个父RDD的一个分区对应一个子RDD的多个分区。 1个父RDD分区对应多个子RDD分区，这其中又分两种情况：1个父RDD对应所有子RDD分区（未经协同划分的Join）或者1个父RDD对应非全部的多个RDD分区（如groupByKey）。 </li>
</ul>
<h3 id="本质理解"><a href="#本质理解" class="headerlink" title="本质理解"></a>本质理解</h3><p>根据父RDD分区是对应1个还是多个子RDD分区来区分窄依赖（父分区对应一个子分区，多个父分区对应一个子分区）和宽依赖（父分区对应多个子分区）。如果对应多个，则当容错重算分区时，因为父分区数据只有一部分是需要重算子分区的，其余数据重算就造成了冗余计算。</p>
<p>对于宽依赖，Stage计算的输入和输出在不同的节点上，对于输入节点完好，而输出节点死机的情况，通过重新计算恢复数据这种情况下，这种方法容错是有效的，否则无效，因为无法重试，需要向上追溯其祖先看是否可以重试（这就是lineage，血统的意思），窄依赖对于数据的重算开销要远小于宽依赖的数据重算开销。</p>
<p>窄依赖和宽依赖的概念主要用在两个地方：一个是容错中相当于Redo日志的功能；另一个是在调度中构建DAG作为不同Stage的划分点。</p>
<h3 id="依赖关系的特性"><a href="#依赖关系的特性" class="headerlink" title="依赖关系的特性"></a>依赖关系的特性</h3><p>第一，窄依赖可以在某个计算节点上直接通过计算父RDD的某块数据计算得到子RDD对应的某块数据；宽依赖则要等到父RDD所有数据都计算完成之后，并且父RDD的计算结果进行hash并传到对应节点上之后才能计算子RDD。 </p>
<p>第二，数据丢失时，对于窄依赖只需要重新计算丢失的那一块数据来恢复；对于宽依赖则要将祖先RDD中的所有数据块全部重新计算来恢复。所以在长“血统”链特别是有宽依赖的时候，需要在适当的时机设置数据检查点。也是这两个特性要求对于不同依赖关系要采取不同的任务调度机制和容错恢复机制。</p>
<h3 id="容错原理"><a href="#容错原理" class="headerlink" title="容错原理"></a>容错原理</h3><p>在容错机制中，如果一个节点死机了，而且运算窄依赖，则只要把丢失的父RDD分区重算即可，不依赖于其他节点。而宽依赖需要父RDD的所有分区都存在，重算就很昂贵了。</p>
<p>可以这样理解开销的经济与否：在窄依赖中，在子RDD的分区丢失、重算父RDD分区时，父RDD相应分区的所有数据都是子RDD分区的数据，并不存在冗余计算。在宽依赖情况下，丢失一个子RDD分区重算的每个父RDD的每个分区的所有数据并不是都给丢失的子RDD分区用的，会有一部分数据相当于对应的是未丢失的子RDD分区中需要的数据，这样就会产生冗余计算开销，这也是宽依赖开销更大的原因。</p>
<p>因此如果使用Checkpoint算子来做检查点，不仅要考虑Lineage是否足够长，也要考虑是否有宽依赖，对宽依赖加Checkpoint是最物有所值的。</p>
<h2 id="Checkpoint机制"><a href="#Checkpoint机制" class="headerlink" title="Checkpoint机制"></a>Checkpoint机制</h2><p>通过上述分析可以看出在以下两种情况下，RDD需要加检查点。</p>
<ol>
<li>DAG中的Lineage过长，如果重算，则开销太大（如在PageRank中）。</li>
<li>在宽依赖上做Checkpoint获得的收益更大。</li>
</ol>
<p>由于RDD是只读的，所以Spark的RDD计算中一致性不是主要关心的内容，内存相对容易管理，这也是设计者很有远见的地方，这样减少了框架的复杂性，提升了性能和可扩展性，为以后上层框架的丰富奠定了强有力的基础。 </p>
<p>在RDD计算中，通过检查点机制进行容错，传统做检查点有两种方式：通过冗余数据和日志记录更新操作。在RDD中的doCheckPoint方法相当于通过冗余数据来缓存数据，而之前介绍的血统就是通过相当粗粒度的记录更新操作来实现容错的。</p>
<p>检查点（本质是通过将RDD写入Disk做检查点）是为了通过lineage做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[ORC与PARQUET文件类型的比较]]></title>
      <url>https://stanxia.github.io/2017/11/29/ORC%E4%B8%8EPARQUET%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%AF%94%E8%BE%83/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><h2 id="列式存储"><a href="#列式存储" class="headerlink" title="列式存储"></a>列式存储</h2><p>由于OLAP查询的特点，列式存储可以提升其查询性能，但是它是如何做到的呢？这就要从列式存储的原理说起，从图1中可以看到，相对于关系数据库中通常使用的行式存储，在使用列式存储时每一列的所有元素都是顺序存储的。由此特点可以给查询带来如下的优化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">查询的时候不需要扫描全部的数据，而只需要读取每次查询涉及的列，这样可以将I/O消耗降低N倍，另外可以保存每一列的统计信息(min、max、sum等)，实现部分的谓词下推。</div><div class="line">由于每一列的成员都是同构的，可以针对不同的数据类型使用更高效的数据压缩算法，进一步减小I/O。</div><div class="line">由于每一列的成员的同构性，可以使用更加适合CPU pipeline的编码方式，减小CPU的缓存失效。</div></pre></td></tr></table></figure>
<center><img src="http://oliji9s3j.bkt.clouddn.com/15119228873171.jpg" alt=""></center><br><center>图1 行式存储VS列式存储</center>

<a id="more"></a>
<h2 id="嵌套数据格式"><a href="#嵌套数据格式" class="headerlink" title="嵌套数据格式"></a>嵌套数据格式</h2><p>通常我们使用关系数据库存储结构化数据，而关系数据库支持的数据模型都是扁平式的，而遇到诸如List、Map和自定义Struct的时候就需要用户自己解析，但是在大数据环境下，数据的来源多种多样，例如埋点数据，很可能需要把程序中的某些对象内容作为输出的一部分，而每一个对象都可能是嵌套的，所以如果能够原生的支持这种数据，查询的时候就不需要额外的解析便能获得想要的结果。例如在Twitter，他们一个典型的日志对象（一条记录）有87个字段，其中嵌套了7层，如下图。</p>
<center><img src="http://oliji9s3j.bkt.clouddn.com/15119230522913.jpg" alt=""><br>图2 嵌套数据模型</center>

<p>随着嵌套格式的数据的需求日益增加，目前Hadoop生态圈中主流的查询引擎都支持更丰富的数据类型，例如Hive、SparkSQL、Impala等都原生的支持诸如struct、map、array这样的复杂数据类型，这样促使各种存储格式都需要支持嵌套数据格式。</p>
<h2 id="Parquet存储格式"><a href="#Parquet存储格式" class="headerlink" title="Parquet存储格式"></a>Parquet存储格式</h2><p>Apache Parquet是Hadoop生态圈中一种新型列式存储格式，它可以兼容Hadoop生态圈中大多数计算框架(Mapreduce、Spark等)，被多种查询引擎支持（Hive、Impala、Drill等），并且它是语言和平台无关的。Parquet最初是由Twitter和Cloudera合作开发完成并开源，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。</p>
<p>Parquet最初的灵感来自Google于2010年发表的Dremel论文，文中介绍了一种支持嵌套结构的存储格式，并且使用了列式存储的方式提升查询性能，在Dremel论文中还介绍了Google如何使用这种存储格式实现并行查询的，如果对此感兴趣可以参考论文和开源实现Drill。</p>
<h3 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h3><p>Parquet支持嵌套的数据模型，类似于Protocol Buffers，每一个数据模型的schema包含多个字段，每一个字段有三个属性：重复次数、数据类型和字段名，重复次数可以是以下三种：required(只出现1次)，repeated(出现0次或多次)，optional(出现0次或1次)。每一个字段的数据类型可以分成两种：group(复杂类型)和primitive(基本类型)。例如Dremel中提供的Document的schema示例，它的定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">message Document &#123;</div><div class="line">  required int64 DocId;</div><div class="line">  optional group Links &#123;</div><div class="line">    repeated int64 Backward;</div><div class="line">    repeated int64 Forward; </div><div class="line">  &#125;</div><div class="line">  repeated group Name &#123;</div><div class="line">    repeated group Language &#123;</div><div class="line">      required string Code;</div><div class="line">      optional string Country; </div><div class="line">     &#125;</div><div class="line">    optional string Url; </div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>可以把这个Schema转换成树状结构，根节点可以理解为repeated类型，如图3。</p>
<center><img src="http://oliji9s3j.bkt.clouddn.com/15119233145314.jpg" alt=""><br>图3 Parquet的schema结构</center>

<p>可以看出在Schema中所有的基本类型字段都是叶子节点，在这个Schema中一共存在6个叶子节点，如果把这样的Schema转换成扁平式的关系模型，就可以理解为该表包含六个列。Parquet中没有Map、Array这样的复杂数据结构，但是可以通过repeated和group组合来实现的。由于一条记录中某一列可能出现零次或者多次，需要标示出哪些列的值构成一条完整的记录。这是由Striping/Assembly算法实现的。</p>
<p>由于Parquet支持的数据模型比较松散，可能一条记录中存在比较深的嵌套关系，如果为每一条记录都维护一个类似的树状结可能会占用较大的存储空间，因此Dremel论文中提出了一种高效的对于嵌套数据格式的压缩算法：Striping/Assembly算法。它的原理是每一个记录中的每一个成员值有三部分组成：Value、Repetition level和Definition level。value记录了该成员的原始值，可以根据特定类型的压缩算法进行压缩，两个level值用于记录该值在整个记录中的位置。对于repeated类型的列，Repetition level值记录了当前值属于哪一条记录以及它处于该记录的什么位置；对于repeated和optional类型的列，可能一条记录中某一列是没有值的，假设我们不记录这样的值就会导致本该属于下一条记录的值被当做当前记录的一部分，从而造成数据的错误，因此对于这种情况需要一个占位符标示这种情况。</p>
<p>通过Striping/Assembly算法，parquet可以使用较少的存储空间表示复杂的嵌套格式，并且通常Repetition level和Definition level都是较小的整数值，可以通过RLE算法对其进行压缩，进一步降低存储空间。</p>
<h3 id="文件结构"><a href="#文件结构" class="headerlink" title="文件结构"></a>文件结构</h3><p>Parquet文件是以二进制方式存储的，是不可以直接读取和修改的，Parquet文件是自解析的，文件中包括该文件的数据和元数据。在HDFS文件系统和Parquet文件中存在如下几个概念：</p>
<ol>
<li>HDFS块(Block)：它是HDFS上的最小的副本单位，HDFS会把一个Block存储在本地的一个文件并且维护分散在不同的机器上的多个副本，通常情况下一个Block的大小为256M、512M等。</li>
<li>HDFS文件(File)：一个HDFS的文件，包括数据和元数据，数据分散存储在多个Block中。</li>
<li>行组(Row Group)：按照行将数据物理上划分为多个单元，每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，Parquet读写的时候会将整个行组缓存在内存中，所以如果每一个行组的大小是由内存大的小决定的。</li>
<li>列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。不同的列块可能使用不同的算法进行压缩。</li>
<li>页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。</li>
</ol>
<p>通常情况下，在存储Parquet数据的时候会按照HDFS的Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如下图所示。</p>
<center><img src="http://oliji9s3j.bkt.clouddn.com/15119234450484.jpg" alt=""><br>图4 Parquet文件结构</center>

<p>上图展示了一个Parquet文件的结构，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length存储了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和当前文件的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页，但是在后面的版本中增加。</p>
<h3 id="数据访问"><a href="#数据访问" class="headerlink" title="数据访问"></a>数据访问</h3><p>说到列式存储的优势，Project下推是无疑最突出的，它意味着在获取表中原始数据时只需要扫描查询中需要的列，由于每一列的所有值都是连续存储的，避免扫描整个表文件内容。</p>
<p>在Parquet中原生就支持Project下推，执行查询的时候可以通过Configuration传递需要读取的列的信息，这些列必须是Schema的子集，Parquet每次会扫描一个Row Group的数据，然后一次性得将该Row Group里所有需要的列的Cloumn Chunk都读取到内存中，每次读取一个Row Group的数据能够大大降低随机读的次数，除此之外，Parquet在读取的时候会考虑列是否连续，如果某些需要的列是存储位置是连续的，那么一次读操作就可以把多个列的数据读取到内存。</p>
<p>在数据访问的过程中，Parquet还可以利用每一个row group生成的统计信息进行谓词下推，这部分信息包括该Column Chunk的最大值、最小值和空值个数。通过这些统计值和该列的过滤条件可以判断该Row Group是否需要扫描。另外Parquet未来还会增加诸如Bloom Filter和Index等优化数据，更加有效的完成谓词下推。</p>
<h2 id="ORC文件格式"><a href="#ORC文件格式" class="headerlink" title="ORC文件格式"></a>ORC文件格式</h2><p>ORC文件格式是一种Hadoop生态圈中的列式存储格式，它的产生早在2013年初，最初产生自Apache Hive，用于降低Hadoop数据存储空间和加速Hive查询速度。和Parquet类似，它并不是一个单纯的列式存储格式，仍然是首先根据行组分割整个表，在每一个行组内进行按列存储。ORC文件是自描述的，它的元数据使用Protocol Buffers序列化，并且文件中的数据尽可能的压缩以降低存储空间的消耗，目前也被Spark SQL、Presto等查询引擎支持，但是Impala对于ORC目前没有支持，仍然使用Parquet作为主要的列式存储格式。2015年ORC项目被Apache项目基金会提升为Apache顶级项目。</p>
<h3 id="数据模型-1"><a href="#数据模型-1" class="headerlink" title="数据模型"></a>数据模型</h3><p>和Parquet不同，ORC原生是不支持嵌套数据格式的，而是通过对复杂数据类型特殊处理的方式实现嵌套格式的支持，例如对于如下的hive表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE `orcStructTable`(</div><div class="line">  `name` string,</div><div class="line">  `course` struct&lt;course:string,score:int&gt;,</div><div class="line">  `score` map&lt;string,int&gt;,</div><div class="line">  `work_locations` array&lt;string&gt;)</div></pre></td></tr></table></figure>
<center><img src="http://oliji9s3j.bkt.clouddn.com/15119238618712.jpg" alt=""><br>图5 ORC的schema结构</center>

<p>在ORC的结构中这个schema包含10个column，其中包含了复杂类型列和原始类型的列，前者包括LIST、STRUCT、MAP和UNION类型，后者包括BOOLEAN、整数、浮点数、字符串类型等，其中STRUCT的孩子节点包括它的成员变量，可能有多个孩子节点，MAP有两个孩子节点，分别为key和value，LIST包含一个孩子节点，类型为该LIST的成员类型，UNION一般不怎么用得到。每一个Schema树的根节点为一个Struct类型，所有的column按照树的中序遍历顺序编号。</p>
<p>ORC只需要存储schema树中叶子节点的值，而中间的非叶子节点只是做一层代理，它们只需要负责孩子节点值得读取，只有真正的叶子节点才会读取数据，然后交由父节点封装成对应的数据结构返回。</p>
<h3 id="文件结构-1"><a href="#文件结构-1" class="headerlink" title="文件结构"></a>文件结构</h3><p>和Parquet类似，ORC文件也是以二进制方式存储的，所以是不可以直接读取，ORC文件也是自解析的，它包含许多的元数据，这些元数据都是同构ProtoBuffer进行序列化的。ORC的文件结构入图6，其中涉及到如下的概念：</p>
<ol>
<li>ORC文件：保存在文件系统上的普通二进制文件，一个ORC文件中可以包含多个stripe，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。</li>
<li>文件级元数据：包括文件的描述信息PostScript、文件meta信息（包括整个文件的统计信息）、所有stripe的信息和文件schema信息。</li>
<li>stripe：一组行形成一个stripe，每次读取文件是以行组为单位的，一般为HDFS的块大小，保存了每一列的索引和数据。</li>
<li>stripe元数据：保存stripe的位置、每一个列的在该stripe的统计信息以及所有的stream类型和位置。</li>
<li>row group：索引的最小单位，一个stripe中包含多个row group，默认为10000个值组成。</li>
<li>stream：一个stream表示文件中一段有效的数据，包括索引和数据两类。索引stream保存每一个row group的位置和统计信息，数据stream包括多种类型的数据，具体需要哪几种是由该列类型和编码方式决定。</li>
</ol>
<center><img src="http://oliji9s3j.bkt.clouddn.com/15119239944085.jpg" alt=""><br><br>图6 ORC文件结构</center>

<p>在ORC文件中保存了三个层级的统计信息，分别为文件级别、stripe级别和row group级别的，他们都可以用来根据Search ARGuments（谓词下推条件）判断是否可以跳过某些数据，在统计信息中都包含成员数和是否有null值，并且对于不同类型的数据设置一些特定的统计信息。</p>
<h3 id="数据访问-1"><a href="#数据访问-1" class="headerlink" title="数据访问"></a>数据访问</h3><p>读取ORC文件是从尾部开始的，第一次读取16KB的大小，尽可能的将Postscript和Footer数据都读入内存。文件的最后一个字节保存着PostScript的长度，它的长度不会超过256字节，PostScript中保存着整个文件的元数据信息，它包括文件的压缩格式、文件内部每一个压缩块的最大长度(每次分配内存的大小)、Footer长度，以及一些版本信息。在Postscript和Footer之间存储着整个文件的统计信息(上图中未画出)，这部分的统计信息包括每一个stripe中每一列的信息，主要统计成员数、最大值、最小值、是否有空值等。</p>
<p>接下来读取文件的Footer信息，它包含了每一个stripe的长度和偏移量，该文件的schema信息(将schema树按照schema中的编号保存在数组中)、整个文件的统计信息以及每一个row group的行数。</p>
<p>处理stripe时首先从Footer中获取每一个stripe的其实位置和长度、每一个stripe的Footer数据(元数据，记录了index和data的的长度)，整个striper被分为index和data两部分，stripe内部是按照row group进行分块的(每一个row group中多少条记录在文件的Footer中存储)，row group内部按列存储。每一个row group由多个stream保存数据和索引信息。每一个stream的数据会根据该列的类型使用特定的压缩算法保存。在ORC中存在如下几种stream类型：</p>
<ol>
<li>PRESENT：每一个成员值在这个stream中保持一位(bit)用于标示该值是否为NULL，通过它可以只记录部位NULL的值</li>
<li>DATA：该列的中属于当前stripe的成员值。</li>
<li>LENGTH：每一个成员的长度，这个是针对string类型的列才有的。</li>
<li>DICTIONARY_DATA：对string类型数据编码之后字典的内容。</li>
<li>SECONDARY：存储Decimal、timestamp类型的小数或者纳秒数等。</li>
<li>ROW_INDEX：保存stripe中每一个row group的统计信息和每一个row group起始位置信息。</li>
</ol>
<p>在初始化阶段获取全部的元数据之后，可以通过includes数组指定需要读取的列编号，它是一个boolean数组，如果不指定则读取全部的列，还可以通过传递SearchArgument参数指定过滤条件，根据元数据首先读取每一个stripe中的index信息，然后根据index中统计信息以及SearchArgument参数确定需要读取的row group编号，再根据includes数据决定需要从这些row group中读取的列，通过这两层的过滤需要读取的数据只是整个stripe多个小段的区间，然后ORC会尽可能合并多个离散的区间尽可能的减少I/O次数。然后再根据index中保存的下一个row group的位置信息调至该stripe中第一个需要读取的row group中。</p>
<p>由于ORC中使用了更加精确的索引信息，使得在读取数据时可以指定从任意一行开始读取，更细粒度的统计信息使得读取ORC文件跳过整个row group，ORC默认会对任何一块数据和索引信息使用ZLIB压缩，因此ORC文件占用的存储空间也更小，这点在后面的测试对比中也有所印证。</p>
<p>在新版本的ORC中也加入了对Bloom Filter的支持，它可以进一步提升谓词下推的效率，在Hive 1.2.0版本以后也加入了对此的支持。</p>
<h2 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h2><p>为了对比测试两种存储格式，我选择使用TPC-DS数据集并且对它进行改造以生成宽表、嵌套和多层嵌套的数据。使用最常用的Hive作为SQL引擎进行测试。</p>
<h3 id="测试环境"><a href="#测试环境" class="headerlink" title="测试环境"></a>测试环境</h3><ul>
<li>Hadoop集群：物理测试集群，四台DataNode/NodeManager机器，每个机器32core+128GB，测试时使用整个集群的资源。</li>
<li>Hive：Hive 1.2.1版本，使用hiveserver2启动，本机MySql作为元数据库，jdbc方式提交查询SQL</li>
<li>数据集：100GB TPC-DS数据集，选取其中的Store_Sales为事实表的模型作为测试数据</li>
<li>查询SQL：选择TPC-DS中涉及到上述模型的10条SQL并对其进行改造。</li>
</ul>
<h3 id="测试场景和结果"><a href="#测试场景和结果" class="headerlink" title="测试场景和结果"></a>测试场景和结果</h3><p>整个测试设置了四种场景，每一种场景下对比测试数据占用的存储空间的大小和相同查询执行消耗的时间对比，除了场景一基于原始的TPC-DS数据集外，其余的数据都需要进行数据导入，同时对比这几个场景的数据导入时间。</p>
<h4 id="场景一：一个事实表、多个维度表，复杂的join查询。"><a href="#场景一：一个事实表、多个维度表，复杂的join查询。" class="headerlink" title="场景一：一个事实表、多个维度表，复杂的join查询。"></a>场景一：一个事实表、多个维度表，复杂的join查询。</h4><p>基于原始的TPC-DS数据集。</p>
<p>Store_Sales表记录数：287,997,024，表大小为：</p>
<ul>
<li>原始Text格式，未压缩 : 38.1 G</li>
<li>ORC格式，默认压缩（ZLIB）,一共1800+个分区 : 11.5 G</li>
<li>Parquet格式，默认压缩（Snappy），一共1800+个分区 ： 14.8 G</li>
</ul>
<p>查询测试结果：</p>
<center><img src="http://oliji9s3j.bkt.clouddn.com/15119243155587.jpg" alt=""></center>

<h4 id="场景二：维度表和事实表join之后生成的宽表，只在一个表上做查询。"><a href="#场景二：维度表和事实表join之后生成的宽表，只在一个表上做查询。" class="headerlink" title="场景二：维度表和事实表join之后生成的宽表，只在一个表上做查询。"></a>场景二：维度表和事实表join之后生成的宽表，只在一个表上做查询。</h4><p>整个测试设置了四种场景，每一种场景下对比测试数据占用的存储空间的大小和相同查询执行消耗的时间对比，除了场景一基于原始的TPC-DS数据集外，其余的数据都需要进行数据导入，同时对比这几个场景的数据导入时间。选取数据模型中的store_sales, household_demographics, customer_address, date_dim, store表生成一个扁平式宽表(store_sales_wide_table)，基于这个表执行查询，由于场景一种选择的query大多数不能完全match到这个宽表，所以对场景1中的SQL进行部分改造。</p>
<p>store_sales_wide_table表记录数：263,704,266，表大小为：</p>
<ul>
<li>原始Text格式，未压缩 ： 149.0 G</li>
<li>ORC格式，默认压缩 ： 10.6 G</li>
<li>PARQUET格式，默认压缩 ： 12.5 G</li>
</ul>
<p>查询测试结果：</p>
<center><img src="http://oliji9s3j.bkt.clouddn.com/15119243950348.jpg" alt=""><br></center>

<h4 id="场景三：复杂的数据结构组成的宽表，struct、list、map等（1层）"><a href="#场景三：复杂的数据结构组成的宽表，struct、list、map等（1层）" class="headerlink" title="场景三：复杂的数据结构组成的宽表，struct、list、map等（1层）"></a>场景三：复杂的数据结构组成的宽表，struct、list、map等（1层）</h4><p>整个测试设置了四种场景，每一种场景下对比测试数据占用的存储空间的大小和相同查询执行消耗的时间对比，除了场景一基于原始的TPC-DS数据集外，其余的数据都需要进行数据导入，同时对比这几个场景的数据导入时间。在场景二的基础上，将维度表（除了store_sales表）转换成一个struct或者map对象，源store_sales表中的字段保持不变。生成有一层嵌套的新表（store_sales_wide_table_one_nested），使用的查询逻辑相同。</p>
<p>store_sales_wide_table_one_nested表记录数：263,704,266，表大小为：</p>
<ul>
<li>原始Text格式，未压缩 ： 245.3 G</li>
<li>ORC格式，默认压缩 ： 10.9 G </li>
<li>PARQUET格式，默认压缩 ： 29.8 G</li>
</ul>
<p>查询测试结果：</p>
<center><img src="http://oliji9s3j.bkt.clouddn.com/15119244638287.jpg" alt=""><br></center>

<h4 id="场景四：复杂的数据结构，多层嵌套。（3层）"><a href="#场景四：复杂的数据结构，多层嵌套。（3层）" class="headerlink" title="场景四：复杂的数据结构，多层嵌套。（3层）"></a>场景四：复杂的数据结构，多层嵌套。（3层）</h4><p>整个测试设置了四种场景，每一种场景下对比测试数据占用的存储空间的大小和相同查询执行消耗的时间对比，除了场景一基于原始的TPC-DS数据集外，其余的数据都需要进行数据导入，同时对比这几个场景的数据导入时间。在场景三的基础上，将部分维度表的struct内的字段再转换成struct或者map对象，只存在struct中嵌套map的情况，最深的嵌套为三层。生成一个多层嵌套的新表（store_sales_wide_table_more_nested），使用的查询逻辑相同。</p>
<p>该场景中只涉及一个多层嵌套的宽表，没有任何分区字段，store_sales_wide_table_more_nested表记录数：263,704,266，表大小为：</p>
<ul>
<li>原始Text格式，未压缩 ： 222.7 G</li>
<li>ORC格式，默认压缩 ： 10.9 G </li>
<li>PARQUET格式，默认压缩 ： 23.1 G 比一层嵌套表store_sales_wide_table_one_nested要小？</li>
</ul>
<p>查询测试结果：</p>
<center><img src="http://oliji9s3j.bkt.clouddn.com/15119245211902.jpg" alt=""></center>

<h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><p>从上述测试结果来看，星状模型对于数据分析场景并不是很合适，多个表的join会大大拖慢查询速度，并且不能很好的利用列式存储带来的性能提升，在使用宽表的情况下，列式存储的性能提升明显，ORC文件格式在存储空间上要远优于Text格式，较之于PARQUET格式有一倍的存储空间提升，在导数据（insert into table select 这样的方式）方面ORC格式也要优于PARQUET，在最终的查询性能上可以看到，无论是无嵌套的扁平式宽表，或是一层嵌套表，还是多层嵌套的宽表，两者的查询性能相差不多，较之于Text格式有2到3倍左右的提升。</p>
<p>另外，通过对比场景二和场景三的测试结果，可以发现扁平式的表结构要比嵌套式结构的查询性能有所提升，所以如果选择使用大宽表，则设计宽表的时候尽可能的将表设计的扁平化，减少嵌套数据。</p>
<p>通过这三种文件存储格式的测试对比，ORC文件存储格式无论是在空间存储、导数据速度还是查询速度上表现的都较好一些，并且ORC可以一定程度上支持ACID操作，社区的发展目前也是Hive中比较提倡使用的一种列式存储格式，另外，本次测试主要针对的是Hive引擎，所以不排除存在Hive与ORC的敏感度比PARQUET要高的可能性。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要从数据模型、文件格式和数据访问流程等几个方面详细介绍了Hadoop生态圈中的两种列式存储格式——Parquet和ORC，并通过大数据量的测试对两者的存储和查询性能进行了对比。对于大数据场景下的数据分析需求，使用这两种存储格式总会带来存储和性能上的提升，但是在实际使用时还需要针对实际的数据进行选择。另外由于不同开源产品可能对不同的存储格式有特定的优化，所以选择时还需要考虑查询引擎的因素。</p>
<p>拓展阅读：<a href="https://stanxia.github.io/2017/11/01/spark%E5%85%B3%E4%BA%8Eparquet%E7%9A%84%E4%BC%98%E5%8C%96/">spark关于parquet的优化</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[spark分层取样]]></title>
      <url>https://stanxia.github.io/2017/11/27/spark%E5%88%86%E5%B1%82%E5%8F%96%E6%A0%B7/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><p>先将总体的单位按某种特征分为若干次级总体（层），然后再从每一层内进行单纯随机抽样，组成一个样本的统计学计算方法叫做分层抽样。在<code>spark.mllib</code>中，用<code>key</code>来分层。</p>
<p>与存在于<code>spark.mllib</code>中的其它统计函数不同，分层采样方法<code>sampleByKey</code>和<code>sampleByKeyExact</code>可以在<code>key-value</code>对的<code>RDD</code>上执行。在分层采样中，可以认为<code>key</code>是一个标签，<br><code>value</code>是特定的属性。例如，<code>key</code>可以是男人或者女人或者文档<code>id</code>,它相应的<code>value</code>可能是一组年龄或者是文档中的词。<code>sampleByKey</code>方法通过掷硬币的方式决定是否采样一个观察数据，<br>因此它需要我们传递（<code>pass over</code>）数据并且提供期望的数据大小(<code>size</code>)。<code>sampleByKeyExact</code>比每层使用<code>sampleByKey</code>随机抽样需要更多的有意义的资源，但是它能使样本大小的准确性达到了<code>99.99%</code>。</p>
<p><a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="external">sampleByKeyExact()</a>允许用户准确抽取<code>f_k * n_k</code>个样本，<br>这里<code>f_k</code>表示期望获取键为<code>k</code>的样本的比例，<code>n_k</code>表示键为<code>k</code>的键值对的数量。下面是一个使用的例子：</p>
<a id="more"></a>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</div><div class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">PairRDDFunctions</span></div><div class="line"><span class="keyword">val</span> sc: <span class="type">SparkContext</span> = ...</div><div class="line"><span class="keyword">val</span> data = ... <span class="comment">// an RDD[(K, V)] of any key value pairs</span></div><div class="line"><span class="keyword">val</span> fractions: <span class="type">Map</span>[<span class="type">K</span>, <span class="type">Double</span>] = ... <span class="comment">// specify the exact fraction desired from each key</span></div><div class="line"><span class="comment">// Get an exact sample from each stratum</span></div><div class="line"><span class="keyword">val</span> approxSample = data.sampleByKey(withReplacement = <span class="literal">false</span>, fractions)</div><div class="line"><span class="keyword">val</span> exactSample = data.sampleByKeyExact(withReplacement = <span class="literal">false</span>, fractions)</div></pre></td></tr></table></figure>
<p>当<code>withReplacement</code>为<code>true</code>时，采用<code>PoissonSampler</code>取样器，当<code>withReplacement</code>为<code>false</code>使，采用<code>BernoulliSampler</code>取样器。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sampleByKey</span></span>(withReplacement: <span class="type">Boolean</span>,</div><div class="line">      fractions: <span class="type">Map</span>[<span class="type">K</span>, <span class="type">Double</span>],</div><div class="line">      seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</div><div class="line">    <span class="keyword">val</span> samplingFunc = <span class="keyword">if</span> (withReplacement) &#123;</div><div class="line">      <span class="type">StratifiedSamplingUtils</span>.getPoissonSamplingFunction(self, fractions, <span class="literal">false</span>, seed)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="type">StratifiedSamplingUtils</span>.getBernoulliSamplingFunction(self, fractions, <span class="literal">false</span>, seed)</div><div class="line">    &#125;</div><div class="line">    self.mapPartitionsWithIndex(samplingFunc, preservesPartitioning = <span class="literal">true</span>)</div><div class="line">  &#125;</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sampleByKeyExact</span></span>(</div><div class="line">      withReplacement: <span class="type">Boolean</span>,</div><div class="line">      fractions: <span class="type">Map</span>[<span class="type">K</span>, <span class="type">Double</span>],</div><div class="line">      seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</div><div class="line">    <span class="keyword">val</span> samplingFunc = <span class="keyword">if</span> (withReplacement) &#123;</div><div class="line">      <span class="type">StratifiedSamplingUtils</span>.getPoissonSamplingFunction(self, fractions, <span class="literal">true</span>, seed)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="type">StratifiedSamplingUtils</span>.getBernoulliSamplingFunction(self, fractions, <span class="literal">true</span>, seed)</div><div class="line">    &#125;</div><div class="line">    self.mapPartitionsWithIndex(samplingFunc, preservesPartitioning = <span class="literal">true</span>)</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>下面我们分别来看<code>sampleByKey</code>和<code>sampleByKeyExact</code>的实现。</p>
<h2 id="sampleByKey的实现"><a href="#sampleByKey的实现" class="headerlink" title="sampleByKey的实现"></a><code>sampleByKey</code>的实现</h2><p>当我们需要不重复抽样时，我们需要用泊松抽样器来抽样。当需要重复抽样时，用伯努利抽样器抽样。<code>sampleByKey</code>的实现比较简单，它就是统一的随机抽样。</p>
<h3 id="泊松抽样器"><a href="#泊松抽样器" class="headerlink" title="泊松抽样器"></a>泊松抽样器</h3><p>我们首先看泊松抽样器的实现。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPoissonSamplingFunction</span></span>[<span class="type">K</span>: <span class="type">ClassTag</span>, <span class="type">V</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)],</div><div class="line">      fractions: <span class="type">Map</span>[<span class="type">K</span>, <span class="type">Double</span>],</div><div class="line">      exact: <span class="type">Boolean</span>,</div><div class="line">      seed: <span class="type">Long</span>): (<span class="type">Int</span>, <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)]) =&gt; <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)] = &#123;</div><div class="line">      (idx: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)]) =&gt; &#123;</div><div class="line">              <span class="comment">//初始化随机生成器</span></div><div class="line">              <span class="keyword">val</span> rng = <span class="keyword">new</span> <span class="type">RandomDataGenerator</span>()</div><div class="line">              rng.reSeed(seed + idx)</div><div class="line">              iter.flatMap &#123; item =&gt;</div><div class="line">                <span class="comment">//获得下一个泊松值</span></div><div class="line">                <span class="keyword">val</span> count = rng.nextPoisson(fractions(item._1))</div><div class="line">                <span class="keyword">if</span> (count == <span class="number">0</span>) &#123;</div><div class="line">                  <span class="type">Iterator</span>.empty</div><div class="line">                &#125; <span class="keyword">else</span> &#123;</div><div class="line">                  <span class="type">Iterator</span>.fill(count)(item)</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>getPoissonSamplingFunction</code>返回的是一个函数，传递给<code>mapPartitionsWithIndex</code>处理每个分区的数据。这里<code>RandomDataGenerator</code>是一个随机生成器，它用于同时生成均匀值(<code>uniform values</code>)和泊松值(<code>Poisson values</code>)。</p>
<h3 id="伯努利抽样器"><a href="#伯努利抽样器" class="headerlink" title="伯努利抽样器"></a>伯努利抽样器</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getBernoulliSamplingFunction</span></span>[<span class="type">K</span>, <span class="type">V</span>](rdd: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)],</div><div class="line">      fractions: <span class="type">Map</span>[<span class="type">K</span>, <span class="type">Double</span>],</div><div class="line">      exact: <span class="type">Boolean</span>,</div><div class="line">      seed: <span class="type">Long</span>): (<span class="type">Int</span>, <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)]) =&gt; <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)] = &#123;</div><div class="line">    <span class="keyword">var</span> samplingRateByKey = fractions</div><div class="line">    (idx: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)]) =&gt; &#123;</div><div class="line">      <span class="comment">//初始化随机生成器</span></div><div class="line">      <span class="keyword">val</span> rng = <span class="keyword">new</span> <span class="type">RandomDataGenerator</span>()</div><div class="line">      rng.reSeed(seed + idx)</div><div class="line">      <span class="comment">// Must use the same invoke pattern on the rng as in getSeqOp for without replacement</span></div><div class="line">      <span class="comment">// in order to generate the same sequence of random numbers when creating the sample</span></div><div class="line">      iter.filter(t =&gt; rng.nextUniform() &lt; samplingRateByKey(t._1))</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h2 id="sampleByKeyExact的实现"><a href="#sampleByKeyExact的实现" class="headerlink" title="sampleByKeyExact的实现"></a><code>sampleByKeyExact</code>的实现</h2><p><code>sampleByKeyExact</code>获取更准确的抽样结果，它的实现也分为两种情况，重复抽样和不重复抽样。前者使用泊松抽样器，后者使用伯努利抽样器。</p>
<h3 id="泊松抽样器-1"><a href="#泊松抽样器-1" class="headerlink" title="泊松抽样器"></a>泊松抽样器</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> counts = <span class="type">Some</span>(rdd.countByKey())</div><div class="line"><span class="comment">//计算立即接受的样本数量，并且为每层生成候选名单</span></div><div class="line"><span class="keyword">val</span> finalResult = getAcceptanceResults(rdd, <span class="literal">true</span>, fractions, counts, seed)</div><div class="line"><span class="comment">//决定接受样本的阈值，生成准确的样本大小</span></div><div class="line"><span class="keyword">val</span> thresholdByKey = computeThresholdByKey(finalResult, fractions)</div><div class="line">(idx: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)]) =&gt; &#123;</div><div class="line">     <span class="keyword">val</span> rng = <span class="keyword">new</span> <span class="type">RandomDataGenerator</span>()</div><div class="line">     rng.reSeed(seed + idx)</div><div class="line">     iter.flatMap &#123; item =&gt;</div><div class="line">          <span class="keyword">val</span> key = item._1</div><div class="line">          <span class="keyword">val</span> acceptBound = finalResult(key).acceptBound</div><div class="line">          <span class="comment">// Must use the same invoke pattern on the rng as in getSeqOp for with replacement</span></div><div class="line">          <span class="comment">// in order to generate the same sequence of random numbers when creating the sample</span></div><div class="line">          <span class="keyword">val</span> copiesAccepted = <span class="keyword">if</span> (acceptBound == <span class="number">0</span>) <span class="number">0</span>L <span class="keyword">else</span> rng.nextPoisson(acceptBound)</div><div class="line">          <span class="comment">//候选名单</span></div><div class="line">          <span class="keyword">val</span> copiesWaitlisted = rng.nextPoisson(finalResult(key).waitListBound)</div><div class="line">          <span class="keyword">val</span> copiesInSample = copiesAccepted +</div><div class="line">            (<span class="number">0</span> until copiesWaitlisted).count(i =&gt; rng.nextUniform() &lt; thresholdByKey(key))</div><div class="line">          <span class="keyword">if</span> (copiesInSample &gt; <span class="number">0</span>) &#123;</div><div class="line">            <span class="type">Iterator</span>.fill(copiesInSample.toInt)(item)</div><div class="line">          &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="type">Iterator</span>.empty</div><div class="line">          &#125;</div><div class="line">     &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="伯努利抽样"><a href="#伯努利抽样" class="headerlink" title="伯努利抽样"></a>伯努利抽样</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getBernoulliSamplingFunction</span></span>[<span class="type">K</span>, <span class="type">V</span>](rdd: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)],</div><div class="line">      fractions: <span class="type">Map</span>[<span class="type">K</span>, <span class="type">Double</span>],</div><div class="line">      exact: <span class="type">Boolean</span>,</div><div class="line">      seed: <span class="type">Long</span>): (<span class="type">Int</span>, <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)]) =&gt; <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)] = &#123;</div><div class="line">    <span class="keyword">var</span> samplingRateByKey = fractions</div><div class="line">    <span class="comment">//计算立即接受的样本数量，并且为每层生成候选名单</span></div><div class="line">    <span class="keyword">val</span> finalResult = getAcceptanceResults(rdd, <span class="literal">false</span>, fractions, <span class="type">None</span>, seed)</div><div class="line">    <span class="comment">//决定接受样本的阈值，生成准确的样本大小</span></div><div class="line">    samplingRateByKey = computeThresholdByKey(finalResult, fractions)</div><div class="line">    (idx: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)]) =&gt; &#123;</div><div class="line">      <span class="keyword">val</span> rng = <span class="keyword">new</span> <span class="type">RandomDataGenerator</span>()</div><div class="line">      rng.reSeed(seed + idx)</div><div class="line">      <span class="comment">// Must use the same invoke pattern on the rng as in getSeqOp for without replacement</span></div><div class="line">      <span class="comment">// in order to generate the same sequence of random numbers when creating the sample</span></div><div class="line">      iter.filter(t =&gt; rng.nextUniform() &lt; samplingRateByKey(t._1))</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<!--视频end-->
<!--对不起，到时间了，请停止装逼-->
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Dataset.scala]]></title>
      <url>https://stanxia.github.io/2017/11/23/Dataset-scala/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Dataset 是一种强类型的领域特定对象集合，可以在使用功能或关系操作的同时进行转换。每个 Dataset 也有一个名为 “DataFrame” 的无类型视图，它是 [[Row]] 的 Dataset。<br>Dataset 上可用的操作分为转换和动作:</p>
<blockquote>
<p>转换：产生新的 Dataset ；包括 map, filter, select, and aggregate (<code>groupBy</code>).<br>动作：触发计算并返回结果 ；包括 count, show, or 写数据到文件系统。</p>
</blockquote>
<p>Dataset是懒加载的，例如：只有提交动作的时候才会触发计算。在内部，Datasets表示一个逻辑计划，它描述生成数据所需的计算。当提交动作时，Spark的查询优化器会优化逻辑计划，并以并行和分布式的方式生成有效执行的物理计划。请使用<code>explain</code> 功能，探索逻辑计划和优化的物理计划。</p>
<p>为了有效地支持特定于领域的对象，需要[[Encoder]]。编码器将特定类型的“T”映射到Spark的内部类型系统。例如：给一个 <code>Person</code> 类，并带有两个属性：<code>name</code> (string) and <code>age</code> (int),编码器告诉Spark在运行时生成代码，序列化 <code>Person</code> 对象为二进制结构。</p>
<p>通常有两种创建Dataset的方法:</p>
<blockquote>
<p>使用 <code>SparkSession</code> 上可用的 <code>read</code> 方法读取 Spark 指向的存储系统上的文件。<br>用现存的 Datasets 转换而来。</p>
</blockquote>
<p>Dataset操作也可以是无类型的，通过多种领域专用语言（DSL）方法定义：这些操作非常类似于 R或Python语言中的 数据框架抽象中可用的操作。<br><a id="more"></a></p>
<!--请开始装逼-->
<h2 id="basic-基础方法"><a href="#basic-基础方法" class="headerlink" title="basic-基础方法"></a>basic-基础方法</h2><h3 id="toDF"><a href="#toDF" class="headerlink" title="toDF"></a>toDF</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Converts this strongly typed collection of data to generic Dataframe. In contrast to the</div><div class="line">  * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]]</div><div class="line">  * objects that allow fields to be accessed by ordinal or name.</div><div class="line">  * 将这种强类型的数据集合转换为一般的Dataframe。</div><div class="line">  * 与Dataset操作所使用的强类型对象相反，</div><div class="line">  * Dataframe返回泛型[[Row]]对象，这些对象允许通过序号或名称访问字段</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="comment">// This is declared with parentheses to prevent the Scala compiler from treating</span></div><div class="line"><span class="comment">// `ds.toDF("1")` as invoking this toDF and then apply on the returned DataFrame.</span></div><div class="line"><span class="comment">// 这是用括号声明的，以防止Scala编译器处理ds.toDF(“1”)调用这个toDF，然后在返回的DataFrame上应用。</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">toDF</span></span>(): <span class="type">DataFrame</span> = <span class="keyword">new</span> <span class="type">Dataset</span>[<span class="type">Row</span>](sparkSession, queryExecution, <span class="type">RowEncoder</span>(schema))</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">  * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.</div><div class="line">  * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with</div><div class="line">  * meaningful names. For example:</div><div class="line">  *</div><div class="line">  * 将这种强类型的数据集合转换为通用的“DataFrame”，并将列重命名。</div><div class="line">  * 在将tuple的RDD转换为富有含义的名称的“DataFrame”时，这是非常方便的，如：</div><div class="line">  *</div><div class="line">  * &#123;&#123;&#123;</div><div class="line">  *   val rdd: RDD[(Int, String)] = ...</div><div class="line">  *   rdd.toDF()  // 隐式转换创建了 DataFrame ，列名为： `_1` and `_2`</div><div class="line">  *   rdd.toDF("id", "name")  // 创建了 DataFrame ，列名为： "id" and "name"</div><div class="line">  * &#125;&#125;&#125;</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="meta">@scala</span>.annotation.varargs</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">toDF</span></span>(colNames: <span class="type">String</span>*): <span class="type">DataFrame</span> = &#123;</div><div class="line">  require(schema.size == colNames.size,</div><div class="line">    <span class="string">"The number of columns doesn't match.\n"</span> +</div><div class="line">      <span class="string">s"Old column names (<span class="subst">$&#123;schema.size&#125;</span>): "</span> + schema.fields.map(_.name).mkString(<span class="string">", "</span>) + <span class="string">"\n"</span> +</div><div class="line">      <span class="string">s"New column names (<span class="subst">$&#123;colNames.size&#125;</span>): "</span> + colNames.mkString(<span class="string">", "</span>))</div><div class="line"></div><div class="line">  <span class="keyword">val</span> newCols = logicalPlan.output.zip(colNames).map &#123; <span class="keyword">case</span> (oldAttribute, newName) =&gt;</div><div class="line">    <span class="type">Column</span>(oldAttribute).as(newName)</div><div class="line">  &#125;</div><div class="line">  select(newCols: _*)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="as"><a href="#as" class="headerlink" title="as"></a>as</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">    * :: Experimental ::</div><div class="line">    * Returns a new Dataset where each record has been mapped on to the specified type. The</div><div class="line">    * method used to map columns depend on the type of `U`:</div><div class="line">    * </div><div class="line">    * 返回一个新的Dataset，其中每个记录都被映射到指定的类型。用于映射列的方法取决于“U”的类型:</div><div class="line">    * </div><div class="line">    *  - When `U` is a class, fields for the class will be mapped to columns of the same name</div><div class="line">    * (case sensitivity is determined by `spark.sql.caseSensitive`).</div><div class="line">    * </div><div class="line">    * 当“U”是类时：类的属性将映射到相同名称的列</div><div class="line">    * </div><div class="line">    *  - When `U` is a tuple, the columns will be be mapped by ordinal (i.e. the first column will</div><div class="line">    * be assigned to `_1`).</div><div class="line">    * </div><div class="line">    * 当“U”是元组时：列将由序数映射 （例如，第一列将为 "_1"）</div><div class="line">    * </div><div class="line">    *  - When `U` is a primitive type (i.e. String, Int, etc), then the first column of the</div><div class="line">    * `DataFrame` will be used.</div><div class="line">    * </div><div class="line">    * 当“U”是 基本类型（如 String，Int等）：然后将使用“DataFrame”的第一列。</div><div class="line">    *</div><div class="line">    * If the schema of the Dataset does not match the desired `U` type, you can use `select`</div><div class="line">    * along with `alias` or `as` to rearrange or rename as required.</div><div class="line">    * </div><div class="line">    * 如果数据集的模式与所需的“U”类型不匹配，您可以使用“select”和“alias”或“as”来重新排列或重命名。</div><div class="line">    *</div><div class="line">    * @group basic</div><div class="line">    * @since 1.6.0</div><div class="line">    */</div><div class="line">  <span class="meta">@Experimental</span></div><div class="line">  <span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">as</span></span>[<span class="type">U</span>: <span class="type">Encoder</span>]: <span class="type">Dataset</span>[<span class="type">U</span>] = <span class="type">Dataset</span>[<span class="type">U</span>](sparkSession, logicalPlan)</div></pre></td></tr></table></figure>
<h3 id="schema"><a href="#schema" class="headerlink" title="schema"></a>schema</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns the schema of this Dataset.</div><div class="line">  * 返回该Dataset的模版</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">schema</span></span>: <span class="type">StructType</span> = queryExecution.analyzed.schema</div></pre></td></tr></table></figure>
<h3 id="printSchema"><a href="#printSchema" class="headerlink" title="printSchema"></a>printSchema</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Prints the schema to the console in a nice tree format.</div><div class="line">  * </div><div class="line">  * 以一种漂亮的树格式将模式打印到控制台。</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="comment">// scalastyle:off println</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">printSchema</span></span>(): <span class="type">Unit</span> = println(schema.treeString)</div></pre></td></tr></table></figure>
<h3 id="explain"><a href="#explain" class="headerlink" title="explain"></a>explain</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Prints the plans (logical and physical) to the console for debugging purposes.</div><div class="line">  * </div><div class="line">  * 将计划(逻辑和物理)打印到控制台以进行调试。</div><div class="line">  * 参数：extended = false 为物理计划</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">explain</span></span>(extended: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">val</span> explain = <span class="type">ExplainCommand</span>(queryExecution.logical, extended = extended)</div><div class="line">  sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach &#123;</div><div class="line">    <span class="comment">// scalastyle:off println</span></div><div class="line">    r =&gt; println(r.getString(<span class="number">0</span>))</div><div class="line">    <span class="comment">// scalastyle:on println</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">  * Prints the physical plan to the console for debugging purposes.</div><div class="line">  * 将物理计划打印到控制台以进行调试。</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">explain</span></span>(): <span class="type">Unit</span> = explain(extended = <span class="literal">false</span>)</div></pre></td></tr></table></figure>
<h3 id="dtypes"><a href="#dtypes" class="headerlink" title="dtypes"></a>dtypes</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns all column names and their data types as an array.</div><div class="line">  * 以数组的形式返回所有列名称和它们的数据类型</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dtypes</span></span>: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">String</span>)] = schema.fields.map &#123; field =&gt;</div><div class="line">  (field.name, field.dataType.toString)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="columns"><a href="#columns" class="headerlink" title="columns"></a>columns</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns all column names as an array.</div><div class="line">  * 以数组的形式返回 所有列名</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">columns</span></span>: <span class="type">Array</span>[<span class="type">String</span>] = schema.fields.map(_.name)</div></pre></td></tr></table></figure>
<h3 id="isLocal"><a href="#isLocal" class="headerlink" title="isLocal"></a>isLocal</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns true if the `collect` and `take` methods can be run locally</div><div class="line">  * (without any Spark executors).</div><div class="line">  * 如果`collect` and `take` 方法能在本地运行，则返回true</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">isLocal</span></span>: <span class="type">Boolean</span> = logicalPlan.isInstanceOf[<span class="type">LocalRelation</span>]</div></pre></td></tr></table></figure>
<h3 id="checkpoint"><a href="#checkpoint" class="headerlink" title="checkpoint"></a>checkpoint</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate</div><div class="line">  * the logical plan of this Dataset, which is especially useful in iterative algorithms where the</div><div class="line">  * plan may grow exponentially. It will be saved to files inside the checkpoint</div><div class="line">  * directory set with `SparkContext#setCheckpointDir`.</div><div class="line">  * </div><div class="line">  * 急切地检查一个数据集并返回新的数据集。</div><div class="line">  * 检查点能用来清除Dataset的逻辑计划，尤其是在可能生成指数级别的迭代算法中尤其有用。</div><div class="line">  * 将会在检查点目录中保存检查文件。可以在`SparkContext#setCheckpointDir`中设置。</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 2.1.0</div><div class="line">  */</div><div class="line"><span class="meta">@Experimental</span></div><div class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkpoint</span></span>(): <span class="type">Dataset</span>[<span class="type">T</span>] = checkpoint(eager = <span class="literal">true</span>)</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">  * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the</div><div class="line">  * logical plan of this Dataset, which is especially useful in iterative algorithms where the</div><div class="line">  * plan may grow exponentially. It will be saved to files inside the checkpoint</div><div class="line">  * directory set with `SparkContext#setCheckpointDir`.</div><div class="line">  * 返回Dataset 之前检查过的版本。</div><div class="line">  * 检查点能用来清除Dataset的逻辑计划，尤其是在可能生成指数级别的迭代算法中尤其有用。</div><div class="line">  * 将会在检查点目录中保存检查文件。可以在`SparkContext#setCheckpointDir`中设置。</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 2.1.0</div><div class="line">  */</div><div class="line"><span class="meta">@Experimental</span></div><div class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkpoint</span></span>(eager: <span class="type">Boolean</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</div><div class="line">  <span class="keyword">val</span> internalRdd = queryExecution.toRdd.map(_.copy())</div><div class="line">  internalRdd.checkpoint()</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (eager) &#123;</div><div class="line">    internalRdd.count()</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">val</span> physicalPlan = queryExecution.executedPlan</div><div class="line"></div><div class="line">  <span class="comment">// Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the</span></div><div class="line">  <span class="comment">// size of `PartitioningCollection` may grow exponentially for queries involving deep inner</span></div><div class="line">  <span class="comment">// joins.</span></div><div class="line">  <span class="comment">// 每当我们看到“PartitioningCollection”时，就采用第一个叶子分区</span></div><div class="line">  <span class="comment">// 否则，用于涉及深度内连接的查询，“PartitioningCollection”的大小可能会以指数形式增长。</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">firstLeafPartitioning</span></span>(partitioning: <span class="type">Partitioning</span>): <span class="type">Partitioning</span> = &#123;</div><div class="line">    partitioning <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> p: <span class="type">PartitioningCollection</span> =&gt; firstLeafPartitioning(p.partitionings.head)</div><div class="line">      <span class="keyword">case</span> p =&gt; p</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">val</span> outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)</div><div class="line"></div><div class="line">  <span class="type">Dataset</span>.ofRows(</div><div class="line">    sparkSession,</div><div class="line">    <span class="type">LogicalRDD</span>(</div><div class="line">      logicalPlan.output,</div><div class="line">      internalRdd,</div><div class="line">      outputPartitioning,</div><div class="line">      physicalPlan.outputOrdering</div><div class="line">    )(sparkSession)).as[<span class="type">T</span>]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="persist"><a href="#persist" class="headerlink" title="persist"></a>persist</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).</div><div class="line">  *</div><div class="line">  * 持久化。</div><div class="line">  * 根据默认的 存储级别 (`MEMORY_AND_DISK`)  持久化Dataset。</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</div><div class="line">  sparkSession.sharedState.cacheManager.cacheQuery(<span class="keyword">this</span>)</div><div class="line">  <span class="keyword">this</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">  * Persist this Dataset with the given storage level.</div><div class="line">  *</div><div class="line">  * 根据指定的 存储级别 持久化 Dataset。</div><div class="line">  *</div><div class="line">  * @param newLevel One of:</div><div class="line">  *                 `MEMORY_ONLY`,</div><div class="line">  *                 `MEMORY_AND_DISK`,</div><div class="line">  *                 `MEMORY_ONLY_SER`,</div><div class="line">  *                 `MEMORY_AND_DISK_SER`,</div><div class="line">  *                 `DISK_ONLY`,</div><div class="line">  *                 `MEMORY_ONLY_2`, 与MEMORY_ONLY的区别是会备份数据到其他节点上</div><div class="line">  *                 `MEMORY_AND_DISK_2`, 与MEMORY_AND_DISK的区别是会备份数据到其他节点上</div><div class="line">  *                 etc.</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</div><div class="line">  sparkSession.sharedState.cacheManager.cacheQuery(<span class="keyword">this</span>, <span class="type">None</span>, newLevel)</div><div class="line">  <span class="keyword">this</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).</div><div class="line">  *</div><div class="line">  * 持久化。</div><div class="line">  * 根据默认的 存储级别 (`MEMORY_AND_DISK`)  持久化Dataset。</div><div class="line">  * 和 persist 一致。</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist()</div></pre></td></tr></table></figure>
<h3 id="storageLevel"><a href="#storageLevel" class="headerlink" title="storageLevel"></a>storageLevel</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.</div><div class="line">  *</div><div class="line">  * 获取当前Dataset的当前存储级别。如果没有缓存则 StorageLevel.NONE。</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 2.1.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">storageLevel</span></span>: <span class="type">StorageLevel</span> = &#123;</div><div class="line">  sparkSession.sharedState.cacheManager.lookupCachedData(<span class="keyword">this</span>).map &#123; cachedData =&gt;</div><div class="line">    cachedData.cachedRepresentation.storageLevel</div><div class="line">  &#125;.getOrElse(<span class="type">StorageLevel</span>.<span class="type">NONE</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="unpersist"><a href="#unpersist" class="headerlink" title="unpersist"></a>unpersist</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.</div><div class="line">  *</div><div class="line">  * 解除持久化。</div><div class="line">  * 将Dataset标记为非持久化，并从内存和磁盘中移除所有的块。</div><div class="line">  *</div><div class="line">  * @param blocking Whether to block until all blocks are deleted.</div><div class="line">  *                 是否阻塞，直到删除所有的块。</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">unpersist</span></span>(blocking: <span class="type">Boolean</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</div><div class="line">  sparkSession.sharedState.cacheManager.uncacheQuery(<span class="keyword">this</span>, blocking)</div><div class="line">  <span class="keyword">this</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">  * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.</div><div class="line">  *</div><div class="line">  * 解除持久化。</div><div class="line">  * 将Dataset标记为非持久化，并从内存和磁盘中移除所有的块。</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">unpersist</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = unpersist(blocking = <span class="literal">false</span>)</div></pre></td></tr></table></figure>
<h3 id="rdd"><a href="#rdd" class="headerlink" title="rdd"></a>rdd</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Represents the content of the Dataset as an `RDD` of [[T]].</div><div class="line">  *</div><div class="line">  * 转换为[[T]]的“RDD”，表示Dataset的内容</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">T</span>] = &#123;</div><div class="line">  <span class="keyword">val</span> objectType = exprEnc.deserializer.dataType</div><div class="line">  <span class="keyword">val</span> deserialized = <span class="type">CatalystSerde</span>.deserialize[<span class="type">T</span>](logicalPlan)</div><div class="line">  sparkSession.sessionState.executePlan(deserialized).toRdd.mapPartitions &#123; rows =&gt;</div><div class="line">    rows.map(_.get(<span class="number">0</span>, objectType).asInstanceOf[<span class="type">T</span>])</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="toJavaRDD"><a href="#toJavaRDD" class="headerlink" title="toJavaRDD"></a>toJavaRDD</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns the content of the Dataset as a `JavaRDD` of [[T]]s.</div><div class="line">  * </div><div class="line">  * 转换为JavaRDD</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">toJavaRDD</span></span>: <span class="type">JavaRDD</span>[<span class="type">T</span>] = rdd.toJavaRDD()</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">  * Returns the content of the Dataset as a `JavaRDD` of [[T]]s.</div><div class="line">  *</div><div class="line">  * 转换为JavaRDD</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">javaRDD</span></span>: <span class="type">JavaRDD</span>[<span class="type">T</span>] = toJavaRDD</div></pre></td></tr></table></figure>
<h3 id="registerTempTable"><a href="#registerTempTable" class="headerlink" title="registerTempTable"></a>registerTempTable</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Registers this Dataset as a temporary table using the given name. The lifetime of this</div><div class="line">  * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.</div><div class="line">  *</div><div class="line">  * 根据指定的表名，注册临时表。</div><div class="line">  * 生命周期为[[SparkSession]]的生命周期。</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="meta">@deprecated</span>(<span class="string">"Use createOrReplaceTempView(viewName) instead."</span>, <span class="string">"2.0.0"</span>)</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">registerTempTable</span></span>(tableName: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  createOrReplaceTempView(tableName)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="createTempView"><a href="#createTempView" class="headerlink" title="createTempView"></a>createTempView</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Creates a local temporary view using the given name. The lifetime of this</div><div class="line">  * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.</div><div class="line">  *</div><div class="line">  * 用指定的名字创建本地临时表。</div><div class="line">  * 与[[SparkSession]] 同生命周期。</div><div class="line">  *</div><div class="line">  * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that</div><div class="line">  * created it, i.e. it will be automatically dropped when the session terminates. It's not</div><div class="line">  * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view.</div><div class="line">  *</div><div class="line">  * 本地临时表是 session范围内的。当创建它的session停止的时候，该表也随之停止。</div><div class="line">  *</div><div class="line">  * @throws AnalysisException if the view name already exists</div><div class="line">  * @group basic</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="meta">@throws</span>[<span class="type">AnalysisException</span>]</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTempView</span></span>(viewName: <span class="type">String</span>): <span class="type">Unit</span> = withPlan &#123;</div><div class="line">  createTempViewCommand(viewName, replace = <span class="literal">false</span>, global = <span class="literal">false</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="createOrReplaceTempView"><a href="#createOrReplaceTempView" class="headerlink" title="createOrReplaceTempView"></a>createOrReplaceTempView</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Creates a local temporary view using the given name. The lifetime of this</div><div class="line">  * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.</div><div class="line">  *</div><div class="line">  * 用指定的名字创建本地临时表。如果已经有了则替换。</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createOrReplaceTempView</span></span>(viewName: <span class="type">String</span>): <span class="type">Unit</span> = withPlan &#123;</div><div class="line">  createTempViewCommand(viewName, replace = <span class="literal">true</span>, global = <span class="literal">false</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="createGlobalTempView"><a href="#createGlobalTempView" class="headerlink" title="createGlobalTempView"></a>createGlobalTempView</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Creates a global temporary view using the given name. The lifetime of this</div><div class="line">  * temporary view is tied to this Spark application.</div><div class="line">  *</div><div class="line">  * 创建全局临时表。</div><div class="line">  * 生命周期为整个Spark application.</div><div class="line">  *</div><div class="line">  * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,</div><div class="line">  * i.e. it will be automatically dropped when the application terminates. It's tied to a system</div><div class="line">  * preserved database `_global_temp`, and we must use the qualified name to refer a global temp</div><div class="line">  * view, e.g. `SELECT * FROM _global_temp.view1`.</div><div class="line">  *</div><div class="line">  * 全局临时表是跨session的。属于 _global_temp 数据库。e.g. `SELECT * FROM _global_temp.view1`.</div><div class="line">  *</div><div class="line">  * @throws AnalysisException if the view name already exists</div><div class="line">  *                           如果表已经存在，则报错。</div><div class="line">  * @group basic</div><div class="line">  * @since 2.1.0</div><div class="line">  */</div><div class="line"><span class="meta">@throws</span>[<span class="type">AnalysisException</span>]</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createGlobalTempView</span></span>(viewName: <span class="type">String</span>): <span class="type">Unit</span> = withPlan &#123;</div><div class="line">  createTempViewCommand(viewName, replace = <span class="literal">false</span>, global = <span class="literal">true</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="write"><a href="#write" class="headerlink" title="write"></a>write</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Interface for saving the content of the non-streaming Dataset out into external storage.</div><div class="line">  * </div><div class="line">  * 将非流Dataset的内容保存到外部存储中的接口。</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">write</span></span>: <span class="type">DataFrameWriter</span>[<span class="type">T</span>] = &#123;</div><div class="line">  <span class="keyword">if</span> (isStreaming) &#123;</div><div class="line">    logicalPlan.failAnalysis(</div><div class="line">      <span class="string">"'write' can not be called on streaming Dataset/DataFrame"</span>)</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">new</span> <span class="type">DataFrameWriter</span>[<span class="type">T</span>](<span class="keyword">this</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="writeStream"><a href="#writeStream" class="headerlink" title="writeStream"></a>writeStream</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * :: Experimental ::</div><div class="line">  * Interface for saving the content of the streaming Dataset out into external storage.</div><div class="line">  *</div><div class="line">  * 将流Dataset保存在外部存储。</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="meta">@Experimental</span></div><div class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">writeStream</span></span>: <span class="type">DataStreamWriter</span>[<span class="type">T</span>] = &#123;</div><div class="line">  <span class="keyword">if</span> (!isStreaming) &#123;</div><div class="line">    logicalPlan.failAnalysis(</div><div class="line">      <span class="string">"'writeStream' can be called only on streaming Dataset/DataFrame"</span>)</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">new</span> <span class="type">DataStreamWriter</span>[<span class="type">T</span>](<span class="keyword">this</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="toJSON"><a href="#toJSON" class="headerlink" title="toJSON"></a>toJSON</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns the content of the Dataset as a Dataset of JSON strings.</div><div class="line">  *</div><div class="line">  * 将Dataset转换为JSON。</div><div class="line">  *</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">toJSON</span></span>: <span class="type">Dataset</span>[<span class="type">String</span>] = &#123;</div><div class="line">  <span class="keyword">val</span> rowSchema = <span class="keyword">this</span>.schema</div><div class="line">  <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = queryExecution.toRdd.mapPartitions &#123; iter =&gt;</div><div class="line">    <span class="keyword">val</span> writer = <span class="keyword">new</span> <span class="type">CharArrayWriter</span>()</div><div class="line">    <span class="comment">// create the Generator without separator inserted between 2 records</span></div><div class="line">    <span class="keyword">val</span> gen = <span class="keyword">new</span> <span class="type">JacksonGenerator</span>(rowSchema, writer)</div><div class="line"></div><div class="line">    <span class="keyword">new</span> <span class="type">Iterator</span>[<span class="type">String</span>] &#123;</div><div class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = iter.hasNext</div><div class="line"></div><div class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): <span class="type">String</span> = &#123;</div><div class="line">        gen.write(iter.next())</div><div class="line">        gen.flush()</div><div class="line"></div><div class="line">        <span class="keyword">val</span> json = writer.toString</div><div class="line">        <span class="keyword">if</span> (hasNext) &#123;</div><div class="line">          writer.reset()</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          gen.close()</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        json</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">import</span> sparkSession.implicits.newStringEncoder</div><div class="line">  sparkSession.createDataset(rdd)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="inputFiles"><a href="#inputFiles" class="headerlink" title="inputFiles"></a>inputFiles</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a best-effort snapshot of the files that compose this Dataset. This method simply</div><div class="line">  * asks each constituent BaseRelation for its respective files and takes the union of all results.</div><div class="line">  * Depending on the source relations, this may not find all input files. Duplicates are removed.</div><div class="line">  *</div><div class="line">  * 返回组成这个Dataset的所有文件的最佳快照。</div><div class="line">  * 该方法简单地要求每个组件BaseRelation对其各自的文件进行处理，并联合所有结果。</div><div class="line">  * 基于源关系，应该可以找到所有的输入文件。</div><div class="line">  * 重复的也会被移除。</div><div class="line">  *</div><div class="line">  * @group basic</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputFiles</span></span>: <span class="type">Array</span>[<span class="type">String</span>] = &#123;</div><div class="line">  <span class="keyword">val</span> files: <span class="type">Seq</span>[<span class="type">String</span>] = queryExecution.optimizedPlan.collect &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">LogicalRelation</span>(fsBasedRelation: <span class="type">FileRelation</span>, _, _) =&gt;</div><div class="line">      fsBasedRelation.inputFiles</div><div class="line">    <span class="keyword">case</span> fr: <span class="type">FileRelation</span> =&gt;</div><div class="line">      fr.inputFiles</div><div class="line">  &#125;.flatten</div><div class="line">  files.toSet.toArray</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="streaming"><a href="#streaming" class="headerlink" title="streaming"></a>streaming</h2><h3 id="isStreaming"><a href="#isStreaming" class="headerlink" title="isStreaming"></a>isStreaming</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns true if this Dataset contains one or more sources that continuously</div><div class="line">  * return data as it arrives. A Dataset that reads data from a streaming source</div><div class="line">  * must be executed as a `StreamingQuery` using the `start()` method in</div><div class="line">  * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or</div><div class="line">  * `collect()`, will throw an [[AnalysisException]] when there is a streaming</div><div class="line">  * source present.</div><div class="line">  * </div><div class="line">  * 如果Dataset包含一个或多个持续返回数据的源，则返回true；</div><div class="line">  * 如果Dataset从streaming源读取数据，则必须像 `StreamingQuery` 一样执行：使用 `DataStreamWriter` 中的 `start()`方法。</div><div class="line">  * 返回单个值的方法，例如： `count()` or `collect()`，当存在streaming源时，将会抛出[[AnalysisException]]。</div><div class="line">  *</div><div class="line">  * @group streaming</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="meta">@Experimental</span></div><div class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">isStreaming</span></span>: <span class="type">Boolean</span> = logicalPlan.isStreaming</div></pre></td></tr></table></figure>
<h3 id="withWatermark"><a href="#withWatermark" class="headerlink" title="withWatermark"></a>withWatermark</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * :: Experimental :: 实验性的</div><div class="line">  * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time</div><div class="line">  * before which we assume no more late data is going to arrive.</div><div class="line">  * </div><div class="line">  * 为这个[[Dataset]]定义事件时间水印。</div><div class="line">  * 我们假设没有更多的晚期数据将到达之前，一个水印跟踪一个时间点。</div><div class="line">  *</div><div class="line">  * Spark will use this watermark for several purposes:</div><div class="line">  * Spark用水印有几个目的：</div><div class="line">  *  - To know when a given time window aggregation can be finalized and thus can be emitted when</div><div class="line">  * using output modes that do not allow updates.</div><div class="line">  * </div><div class="line">  * 可以知道何时完成给定的时间窗口聚合能够完成，因此当使用不允许更新的输出模式时能够被放出。</div><div class="line">  *  - To minimize the amount of state that we need to keep for on-going aggregations.</div><div class="line">  * 为了最小化我们需要持续不断的聚合的状态数量。</div><div class="line">  *</div><div class="line">  *</div><div class="line">  * The current watermark is computed by looking at the `MAX(eventTime)` seen across</div><div class="line">  * all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost</div><div class="line">  * of coordinating this value across partitions, the actual watermark used is only guaranteed</div><div class="line">  * to be at least `delayThreshold` behind the actual event time.  In some cases we may still</div><div class="line">  * process records that arrive more than `delayThreshold` late.</div><div class="line">  * </div><div class="line">  * 当前的水印 = 查看查询中所有分区上看到的`MAX(eventTime)` - 用户指定的`delayThreshold`</div><div class="line">  * 由于在分区之间协调这个值的花销，实际使用的水印只保证在实际事件时间后至少是“delayThreshold”。</div><div class="line">  * 在某些情况下，我们可能还会处理比“delayThreshold”晚些时候到达的记录。</div><div class="line">  *</div><div class="line">  * @param eventTime      the name of the column that contains the event time of the row.</div><div class="line">  *                       包含行的事件时间的列名</div><div class="line">  * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest</div><div class="line">  *                       record that has been processed in the form of an interval</div><div class="line">  *                       (e.g. "1 minute" or "5 hours").</div><div class="line">  *                       等待晚到数据的最少延迟，相对于以间隔形式处理的最新记录</div><div class="line">  * @group streaming</div><div class="line">  * @since 2.1.0</div><div class="line">  */</div><div class="line"><span class="meta">@Experimental</span></div><div class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line"><span class="comment">// We only accept an existing column name, not a derived column here as a watermark that is</span></div><div class="line"><span class="comment">// defined on a derived column cannot referenced elsewhere in the plan.</span></div><div class="line"><span class="comment">// 我们只接受一个现有的列名，而不是作为一个在派生列上定义的水印的派生列，而不能在该计划的其他地方引用。</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">withWatermark</span></span>(eventTime: <span class="type">String</span>, delayThreshold: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</div><div class="line">  <span class="keyword">val</span> parsedDelay =</div><div class="line">    <span class="type">Option</span>(<span class="type">CalendarInterval</span>.fromString(<span class="string">"interval "</span> + delayThreshold))</div><div class="line">      .getOrElse(<span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AnalysisException</span>(<span class="string">s"Unable to parse time delay '<span class="subst">$delayThreshold</span>'"</span>))</div><div class="line">  <span class="type">EventTimeWatermark</span>(<span class="type">UnresolvedAttribute</span>(eventTime), parsedDelay, logicalPlan)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="action"><a href="#action" class="headerlink" title="action"></a>action</h2><h3 id="show"><a href="#show" class="headerlink" title="show"></a>show</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,</div><div class="line">  * and all cells will be aligned right. For example:</div><div class="line">  * </div><div class="line">  * 以表格形式显示数据集。</div><div class="line">  * 字符串超过20个字符将被截断，</div><div class="line">  * 所有单元格将被对齐。</div><div class="line">  * &#123;&#123;&#123;</div><div class="line">  *   year  month AVG('Adj Close) MAX('Adj Close)</div><div class="line">  *   1980  12    0.503218        0.595103</div><div class="line">  *   1981  01    0.523289        0.570307</div><div class="line">  *   1982  02    0.436504        0.475256</div><div class="line">  *   1983  03    0.410516        0.442194</div><div class="line">  *   1984  04    0.450090        0.483521</div><div class="line">  * &#125;&#125;&#125;</div><div class="line">  *</div><div class="line">  * @param numRows Number of rows to show 要显示的行数</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span></span>(numRows: <span class="type">Int</span>): <span class="type">Unit</span> = show(numRows, truncate = <span class="literal">true</span>)</div><div class="line">  <span class="comment">/**</span></div><div class="line">  * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters</div><div class="line">  * will be truncated, and all cells will be aligned right.</div><div class="line">  * 显示头20行</div><div class="line">  *</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span></span>(): <span class="type">Unit</span> = show(<span class="number">20</span>)</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Displays the top 20 rows of Dataset in a tabular form.</div><div class="line">  * 显示头20行</div><div class="line">  *</div><div class="line">  * @param truncate Whether truncate long strings. If true, strings more than 20 characters will</div><div class="line">  *                 be truncated and all cells will be aligned right</div><div class="line">  *                 是否截断长字符串。如果 true：超过20个字符就会被截断</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span></span>(truncate: <span class="type">Boolean</span>): <span class="type">Unit</span> = show(<span class="number">20</span>, truncate)</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Displays the Dataset in a tabular form. For example:</div><div class="line">  * &#123;&#123;&#123;</div><div class="line">  *   year  month AVG('Adj Close) MAX('Adj Close)</div><div class="line">  *   1980  12    0.503218        0.595103</div><div class="line">  *   1981  01    0.523289        0.570307</div><div class="line">  *   1982  02    0.436504        0.475256</div><div class="line">  *   1983  03    0.410516        0.442194</div><div class="line">  *   1984  04    0.450090        0.483521</div><div class="line">  * &#125;&#125;&#125;</div><div class="line">  *</div><div class="line">  * @param numRows  Number of rows to show 显示的行数</div><div class="line">  * @param truncate Whether truncate long strings. If true, strings more than 20 characters will</div><div class="line">  *                 be truncated and all cells will be aligned right</div><div class="line">  *                 是否截断长字符串</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="comment">// scalastyle:off println</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span></span>(numRows: <span class="type">Int</span>, truncate: <span class="type">Boolean</span>): <span class="type">Unit</span> = <span class="keyword">if</span> (truncate) &#123;</div><div class="line">  println(showString(numRows, truncate = <span class="number">20</span>))</div><div class="line">&#125; <span class="keyword">else</span> &#123;</div><div class="line">  println(showString(numRows, truncate = <span class="number">0</span>))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// scalastyle:on println</span></div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Displays the Dataset in a tabular form. For example:</div><div class="line">  * &#123;&#123;&#123;</div><div class="line">  *   year  month AVG('Adj Close) MAX('Adj Close)</div><div class="line">  *   1980  12    0.503218        0.595103</div><div class="line">  *   1981  01    0.523289        0.570307</div><div class="line">  *   1982  02    0.436504        0.475256</div><div class="line">  *   1983  03    0.410516        0.442194</div><div class="line">  *   1984  04    0.450090        0.483521</div><div class="line">  * &#125;&#125;&#125;</div><div class="line">  *</div><div class="line">  * @param numRows  Number of rows to show</div><div class="line">  * @param truncate If set to more than 0, truncates strings to `truncate` characters and</div><div class="line">  *                 all cells will be aligned right.</div><div class="line">  *                 设置 触发截断字符串的阈值</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="comment">// scalastyle:off println</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span></span>(numRows: <span class="type">Int</span>, truncate: <span class="type">Int</span>): <span class="type">Unit</span> = println(showString(numRows, truncate))</div></pre></td></tr></table></figure>
<h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * :: Experimental ::</div><div class="line">  * (Scala-specific)</div><div class="line">  * Reduces the elements of this Dataset using the specified binary function. The given `func`</div><div class="line">  * must be commutative and associative or the result may be non-deterministic.</div><div class="line">  *</div><div class="line">  * 使用指定的二进制函数减少这个数据集的元素。给定的“func”必须是可交换的和关联的，否则结果可能是不确定性的。</div><div class="line">  *</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="meta">@Experimental</span></div><div class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(func: (<span class="type">T</span>, <span class="type">T</span>) =&gt; <span class="type">T</span>): <span class="type">T</span> = rdd.reduce(func)</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * :: Experimental ::</div><div class="line">  * (Java-specific)</div><div class="line">  * Reduces the elements of this Dataset using the specified binary function. The given `func`</div><div class="line">  * must be commutative and associative or the result may be non-deterministic.</div><div class="line">  * </div><div class="line">  * 使用指定的二进制函数减少这个数据集的元素。给定的“func”必须是可交换的和关联的，否则结果可能是不确定性的。</div><div class="line">  *</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="meta">@Experimental</span></div><div class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(func: <span class="type">ReduceFunction</span>[<span class="type">T</span>]): <span class="type">T</span> = reduce(func.call(_, _))</div></pre></td></tr></table></figure>
<h3 id="describe"><a href="#describe" class="headerlink" title="describe"></a>describe</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">   * Computes statistics for numeric and string columns, including count, mean, stddev, min, and</div><div class="line">   * max. If no columns are given, this function computes statistics for all numerical or string</div><div class="line">   * columns.</div><div class="line">   *</div><div class="line">   * 计算数字和字符串列的统计数据，包括count、mean、stddev、min和max。</div><div class="line">   * 如果没有给出任何列，该函数计算所有数值或字符串列的统计信息。</div><div class="line">   *</div><div class="line">   * This function is meant for exploratory data analysis, as we make no guarantee about the</div><div class="line">   * backward compatibility of the schema of the resulting Dataset. If you want to</div><div class="line">   * programmatically compute summary statistics, use the `agg` function instead.</div><div class="line">   *</div><div class="line">   * 这个函数用于探索性的数据分析，因为我们不能保证生成数据集的模式的向后兼容性。</div><div class="line">   * 如果您想通过编程计算汇总统计信息，可以使用“agg”函数。</div><div class="line">   *</div><div class="line">   * &#123;&#123;&#123;</div><div class="line">   *   ds.describe("age", "height").show()</div><div class="line">   *</div><div class="line">   *   // output:</div><div class="line">   *   // summary age   height</div><div class="line">   *   // count   10.0  10.0</div><div class="line">   *   // mean    53.3  178.05</div><div class="line">   *   // stddev  11.6  15.7</div><div class="line">   *   // min     18.0  163.0</div><div class="line">   *   // max     92.0  192.0</div><div class="line">   * &#125;&#125;&#125;</div><div class="line">   *</div><div class="line">   * @group action</div><div class="line">   * @since 1.6.0</div><div class="line">   */</div><div class="line"> <span class="meta">@scala</span>.annotation.varargs</div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">describe</span></span>(cols: <span class="type">String</span>*): <span class="type">DataFrame</span> = withPlan &#123;</div><div class="line"></div><div class="line">   <span class="comment">// The list of summary statistics to compute, in the form of expressions.</span></div><div class="line">   <span class="keyword">val</span> statistics = <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Expression</span> =&gt; <span class="type">Expression</span>)](</div><div class="line">     <span class="string">"count"</span> -&gt; ((child: <span class="type">Expression</span>) =&gt; <span class="type">Count</span>(child).toAggregateExpression()),</div><div class="line">     <span class="string">"mean"</span> -&gt; ((child: <span class="type">Expression</span>) =&gt; <span class="type">Average</span>(child).toAggregateExpression()),</div><div class="line">     <span class="string">"stddev"</span> -&gt; ((child: <span class="type">Expression</span>) =&gt; <span class="type">StddevSamp</span>(child).toAggregateExpression()),</div><div class="line">     <span class="string">"min"</span> -&gt; ((child: <span class="type">Expression</span>) =&gt; <span class="type">Min</span>(child).toAggregateExpression()),</div><div class="line">     <span class="string">"max"</span> -&gt; ((child: <span class="type">Expression</span>) =&gt; <span class="type">Max</span>(child).toAggregateExpression()))</div><div class="line"></div><div class="line">   <span class="keyword">val</span> outputCols =</div><div class="line">     (<span class="keyword">if</span> (cols.isEmpty) aggregatableColumns.map(usePrettyExpression(_).sql) <span class="keyword">else</span> cols).toList</div><div class="line"></div><div class="line">   <span class="keyword">val</span> ret: <span class="type">Seq</span>[<span class="type">Row</span>] = <span class="keyword">if</span> (outputCols.nonEmpty) &#123;</div><div class="line">     <span class="keyword">val</span> aggExprs = statistics.flatMap &#123; <span class="keyword">case</span> (_, colToAgg) =&gt;</div><div class="line">       outputCols.map(c =&gt; <span class="type">Column</span>(<span class="type">Cast</span>(colToAgg(<span class="type">Column</span>(c).expr), <span class="type">StringType</span>)).as(c))</div><div class="line">     &#125;</div><div class="line"></div><div class="line">     <span class="keyword">val</span> row = groupBy().agg(aggExprs.head, aggExprs.tail: _*).head().toSeq</div><div class="line"></div><div class="line">     <span class="comment">// Pivot the data so each summary is one row</span></div><div class="line">     row.grouped(outputCols.size).toSeq.zip(statistics).map &#123; <span class="keyword">case</span> (aggregation, (statistic, _)) =&gt;</div><div class="line">       <span class="type">Row</span>(statistic :: aggregation.toList: _*)</div><div class="line">     &#125;</div><div class="line">   &#125; <span class="keyword">else</span> &#123;</div><div class="line">     <span class="comment">// If there are no output columns, just output a single column that contains the stats.</span></div><div class="line">     statistics.map &#123; <span class="keyword">case</span> (name, _) =&gt; <span class="type">Row</span>(name) &#125;</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="comment">// All columns are string type</span></div><div class="line">   <span class="keyword">val</span> schema = <span class="type">StructType</span>(</div><div class="line">     <span class="type">StructField</span>(<span class="string">"summary"</span>, <span class="type">StringType</span>) :: outputCols.map(<span class="type">StructField</span>(_, <span class="type">StringType</span>))).toAttributes</div><div class="line">   <span class="comment">// `toArray` forces materialization to make the seq serializable</span></div><div class="line">   <span class="type">LocalRelation</span>.fromExternalRows(schema, ret.toArray.toSeq)</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<h3 id="head"><a href="#head" class="headerlink" title="head"></a>head</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns the first `n` rows.</div><div class="line">  *</div><div class="line">  * 返回前n行</div><div class="line">  *</div><div class="line">  * @note this method should only be used if the resulting array is expected to be small, as</div><div class="line">  *       all the data is loaded into the driver's memory.</div><div class="line">  *       仅适用于结果很少的时候使用，因为会将结果加载进内存中</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">head</span></span>(n: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>] = withTypedCallback(<span class="string">"head"</span>, limit(n)) &#123; df =&gt;</div><div class="line">  df.collect(needCallback = <span class="literal">false</span>)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns the first row.</div><div class="line">  *</div><div class="line">  * 返回第一行（默认1）</div><div class="line">  *</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">head</span></span>(): <span class="type">T</span> = head(<span class="number">1</span>).head</div></pre></td></tr></table></figure>
<h3 id="first"><a href="#first" class="headerlink" title="first"></a>first</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns the first row. Alias for head().</div><div class="line">  *</div><div class="line">  * 返回第一行 ，与head()一样</div><div class="line">  *</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">first</span></span>(): <span class="type">T</span> = head()</div></pre></td></tr></table></figure>
<h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Applies a function `f` to all rows.</div><div class="line">  *</div><div class="line">  * 对所有行应用函数f。</div><div class="line">  *</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreach</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = withNewExecutionId &#123;</div><div class="line">  rdd.foreach(f)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * (Java-specific)</div><div class="line">  * Runs `func` on each element of this Dataset.</div><div class="line">  *</div><div class="line">  * 在这个数据集的每个元素上运行“func”。</div><div class="line">  *</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreach</span></span>(func: <span class="type">ForeachFunction</span>[<span class="type">T</span>]): <span class="type">Unit</span> = foreach(func.call(_))</div></pre></td></tr></table></figure>
<h3 id="foreachPartition"><a href="#foreachPartition" class="headerlink" title="foreachPartition"></a>foreachPartition</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Applies a function `f` to each partition of this Dataset.</div><div class="line">  *</div><div class="line">  * 对这个数据集的每个分区应用一个函数f。</div><div class="line">  *</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreachPartition</span></span>(f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = withNewExecutionId &#123;</div><div class="line">  rdd.foreachPartition(f)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * (Java-specific)</div><div class="line">  * Runs `func` on each partition of this Dataset.</div><div class="line">  *</div><div class="line">  * 对这个数据集的每个分区应用一个函数f。</div><div class="line">  *</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreachPartition</span></span>(func: <span class="type">ForeachPartitionFunction</span>[<span class="type">T</span>]): <span class="type">Unit</span> =</div><div class="line">  foreachPartition(it =&gt; func.call(it.asJava))</div></pre></td></tr></table></figure>
<h3 id="take"><a href="#take" class="headerlink" title="take"></a>take</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns the first `n` rows in the Dataset.</div><div class="line">  *</div><div class="line">  * 返回数据集中的前“n”行。</div><div class="line">  * 同head(n)</div><div class="line">  *</div><div class="line">  * Running take requires moving data into the application's driver process, and doing so with</div><div class="line">  * a very large `n` can crash the driver process with OutOfMemoryError.</div><div class="line">  *</div><div class="line">  * take在driver端执行，n太大会造成oom</div><div class="line">  *</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(n: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>] = head(n)</div></pre></td></tr></table></figure>
<h3 id="takeAsList"><a href="#takeAsList" class="headerlink" title="takeAsList"></a>takeAsList</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns the first `n` rows in the Dataset as a list.</div><div class="line">  *</div><div class="line">  * 以List形式返回 前n行</div><div class="line">  *</div><div class="line">  * Running take requires moving data into the application's driver process, and doing so with</div><div class="line">  * a very large `n` can crash the driver process with OutOfMemoryError.</div><div class="line">  *</div><div class="line">  * take在driver端执行，n太大会造成oom</div><div class="line">  *</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAsList</span></span>(n: <span class="type">Int</span>): java.util.<span class="type">List</span>[<span class="type">T</span>] = java.util.<span class="type">Arrays</span>.asList(take(n): _*)</div></pre></td></tr></table></figure>
<h3 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns an array that contains all of [[Row]]s in this Dataset.</div><div class="line">  *</div><div class="line">  * 返回包含所有Row的 一个数组</div><div class="line">  *</div><div class="line">  * Running collect requires moving all the data into the application's driver process, and</div><div class="line">  * doing so on a very large dataset can crash the driver process with OutOfMemoryError.</div><div class="line">  *</div><div class="line">  * 会将所有数据移动到driver，所以可能会造成oom</div><div class="line">  *</div><div class="line">  * For Java API, use [[collectAsList]].</div><div class="line">  *</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">collect</span></span>(): <span class="type">Array</span>[<span class="type">T</span>] = collect(needCallback = <span class="literal">true</span>)</div></pre></td></tr></table></figure>
<h3 id="collectAsList"><a href="#collectAsList" class="headerlink" title="collectAsList"></a>collectAsList</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a Java list that contains all of [[Row]]s in this Dataset.</div><div class="line">  *</div><div class="line">  * 返回包含所有Row的一个Java List</div><div class="line">  *</div><div class="line">  * Running collect requires moving all the data into the application's driver process, and</div><div class="line">  * doing so on a very large dataset can crash the driver process with OutOfMemoryError.</div><div class="line">  *</div><div class="line">  * 会将所有数据移动到driver，所以可能会造成oom</div><div class="line">  *</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">collectAsList</span></span>(): java.util.<span class="type">List</span>[<span class="type">T</span>] = withCallback(<span class="string">"collectAsList"</span>, toDF()) &#123; _ =&gt;</div><div class="line">  withNewExecutionId &#123;</div><div class="line">    <span class="keyword">val</span> values = queryExecution.executedPlan.executeCollect().map(boundEnc.fromRow)</div><div class="line">    java.util.<span class="type">Arrays</span>.asList(values: _*)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="toLocalIterator"><a href="#toLocalIterator" class="headerlink" title="toLocalIterator"></a>toLocalIterator</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Return an iterator that contains all of [[Row]]s in this Dataset.</div><div class="line">  *</div><div class="line">  * 返回包含所有Row的一个迭代器</div><div class="line">  *</div><div class="line">  * The iterator will consume as much memory as the largest partition in this Dataset.</div><div class="line">  *</div><div class="line">  * 迭代器将消耗与此数据集中最大的分区一样多的内存。</div><div class="line">  *</div><div class="line">  * @note this results in multiple Spark jobs, and if the input Dataset is the result</div><div class="line">  *       of a wide transformation (e.g. join with different partitioners), to avoid</div><div class="line">  *       recomputing the input Dataset should be cached first.</div><div class="line">  *       这将导致多个Spark作业，如果输入数据集是宽依赖转换的结果(例如，与不同的分区连接)，</div><div class="line">  *       那么为了避免重新计算输入数据，应该首先缓存输入数据集。</div><div class="line">  * @group action</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">toLocalIterator</span></span>(): java.util.<span class="type">Iterator</span>[<span class="type">T</span>] = withCallback(<span class="string">"toLocalIterator"</span>, toDF()) &#123; _ =&gt;</div><div class="line">  withNewExecutionId &#123;</div><div class="line">    queryExecution.executedPlan.executeToIterator().map(boundEnc.fromRow).asJava</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns the number of rows in the Dataset.</div><div class="line">  *</div><div class="line">  * 返回总行数</div><div class="line">  *</div><div class="line">  * @group action</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = withCallback(<span class="string">"count"</span>, groupBy().count()) &#123; df =&gt;</div><div class="line">  df.collect(needCallback = <span class="literal">false</span>).head.getLong(<span class="number">0</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="untypedrel-无类型转换"><a href="#untypedrel-无类型转换" class="headerlink" title="untypedrel-无类型转换"></a>untypedrel-无类型转换</h2><h3 id="na"><a href="#na" class="headerlink" title="na"></a>na</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a [[DataFrameNaFunctions]] for working with missing data.</div><div class="line">  * 返回一个用于处理丢失数据的[[DataFrameNaFunctions]]。</div><div class="line">  * &#123;&#123;&#123;</div><div class="line">  *   // Dropping rows containing any null values. 删除包含任何null 值的行</div><div class="line">  *   ds.na.drop()</div><div class="line">  * &#125;&#125;&#125;</div><div class="line">  *</div><div class="line">  * @group untypedrel</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">na</span></span>: <span class="type">DataFrameNaFunctions</span> = <span class="keyword">new</span> <span class="type">DataFrameNaFunctions</span>(toDF())</div></pre></td></tr></table></figure>
<h3 id="stat"><a href="#stat" class="headerlink" title="stat"></a>stat</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a [[DataFrameStatFunctions]] for working statistic functions support.</div><div class="line">  * 返回用于支持统计功能的[[DataFrameStatFunctions]]。</div><div class="line">  * &#123;&#123;&#123;</div><div class="line">  *   // Finding frequent items in column with name 'a'. 查询列名为"a"中的频繁数据。</div><div class="line">  *   ds.stat.freqItems(Seq("a"))</div><div class="line">  * &#125;&#125;&#125;</div><div class="line">  *</div><div class="line">  * @group untypedrel</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stat</span></span>: <span class="type">DataFrameStatFunctions</span> = <span class="keyword">new</span> <span class="type">DataFrameStatFunctions</span>(toDF())</div></pre></td></tr></table></figure>
<h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">    * Join with another `DataFrame`.</div><div class="line">    * 和 另一个 `DataFrame`  jion</div><div class="line">    *</div><div class="line">    * Behaves as an INNER JOIN and requires a subsequent join predicate.</div><div class="line">    * 作为一个内部连接，并需要一个后续的连接谓词。</div><div class="line">    *</div><div class="line">    * @param right Right side of the join operation. join操作的右侧</div><div class="line">    * @group untypedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_]): <span class="type">DataFrame</span> = withPlan &#123;</div><div class="line">    <span class="type">Join</span>(logicalPlan, right.logicalPlan, joinType = <span class="type">Inner</span>, <span class="type">None</span>)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">    * Inner equi-join with another `DataFrame` using the given column.</div><div class="line">    * 给定列名的内部等值连接</div><div class="line">    *</div><div class="line">    * Different from other join functions, the join column will only appear once in the output,</div><div class="line">    * i.e. similar to SQL's `JOIN USING` syntax.</div><div class="line">    *</div><div class="line">    * &#123;&#123;&#123;</div><div class="line">    *   // Joining df1 and df2 using the column "user_id" 用"user_id"  连接 df1 和df2</div><div class="line">    *   df1.join(df2, "user_id")</div><div class="line">    * &#125;&#125;&#125;</div><div class="line">    *</div><div class="line">    * @param right       Right side of the join operation. join连接右侧</div><div class="line">    * @param usingColumn Name of the column to join on. This column must exist on both sides.</div><div class="line">    *                    列名。必须在两边都存在</div><div class="line">    * @note If you perform a self-join using this function without aliasing the input</div><div class="line">    *       `DataFrame`s, you will NOT be able to reference any columns after the join, since</div><div class="line">    *       there is no way to disambiguate which side of the join you would like to reference.</div><div class="line">    *       自连接的时候，请指定 表别名。不然干不了事</div><div class="line">    * @group untypedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_], usingColumn: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</div><div class="line">    join(right, <span class="type">Seq</span>(usingColumn))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">    * Inner equi-join with another `DataFrame` using the given columns.</div><div class="line">    * 根据指定多个列进行join</div><div class="line">    *</div><div class="line">    * Different from other join functions, the join columns will only appear once in the output,</div><div class="line">    * i.e. similar to SQL's `JOIN USING` syntax.</div><div class="line">    *</div><div class="line">    * &#123;&#123;&#123;</div><div class="line">    *   // Joining df1 and df2 using the columns "user_id" and "user_name"</div><div class="line">    *   df1.join(df2, Seq("user_id", "user_name"))</div><div class="line">    * &#125;&#125;&#125;</div><div class="line">    *</div><div class="line">    * @param right        Right side of the join operation.</div><div class="line">    * @param usingColumns Names of the columns to join on. This columns must exist on both sides.</div><div class="line">    * @note If you perform a self-join using this function without aliasing the input</div><div class="line">    *       `DataFrame`s, you will NOT be able to reference any columns after the join, since</div><div class="line">    *       there is no way to disambiguate which side of the join you would like to reference.</div><div class="line">    * @group untypedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_], usingColumns: <span class="type">Seq</span>[<span class="type">String</span>]): <span class="type">DataFrame</span> = &#123;</div><div class="line">    join(right, usingColumns, <span class="string">"inner"</span>)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">    * Equi-join with another `DataFrame` using the given columns.</div><div class="line">    *</div><div class="line">    * Different from other join functions, the join columns will only appear once in the output,</div><div class="line">    * i.e. similar to SQL's `JOIN USING` syntax.</div><div class="line">    *</div><div class="line">    * @param right        Right side of the join operation.</div><div class="line">    * @param usingColumns Names of the columns to join on. This columns must exist on both sides.</div><div class="line">    * @param joinType     One of: `inner`, `outer`, `left_outer`, `right_outer`, `leftsemi`.</div><div class="line">    *                     连接类型：内连接，外连接，左外连接，右外连接，左内连接</div><div class="line">    * @note If you perform a self-join using this function without aliasing the input</div><div class="line">    *       `DataFrame`s, you will NOT be able to reference any columns after the join, since</div><div class="line">    *       there is no way to disambiguate which side of the join you would like to reference.</div><div class="line">    * @group untypedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_], usingColumns: <span class="type">Seq</span>[<span class="type">String</span>], joinType: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</div><div class="line">    <span class="comment">// Analyze the self join. The assumption is that the analyzer will disambiguate left vs right</span></div><div class="line">    <span class="comment">// by creating a new instance for one of the branch.</span></div><div class="line">    <span class="comment">// 自连接的时候，为其中一个分支创建一个新实例来消除左vs右的歧义。</span></div><div class="line">    <span class="keyword">val</span> joined = sparkSession.sessionState.executePlan(</div><div class="line">      <span class="type">Join</span>(logicalPlan, right.logicalPlan, joinType = <span class="type">JoinType</span>(joinType), <span class="type">None</span>))</div><div class="line">      .analyzed.asInstanceOf[<span class="type">Join</span>]</div><div class="line"></div><div class="line">    withPlan &#123;</div><div class="line">      <span class="type">Join</span>(</div><div class="line">        joined.left,</div><div class="line">        joined.right,</div><div class="line">        <span class="type">UsingJoin</span>(<span class="type">JoinType</span>(joinType), usingColumns),</div><div class="line">        <span class="type">None</span>)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">    * Inner join with another `DataFrame`, using the given join expression.</div><div class="line">    * 用给定的表达式进行join</div><div class="line">    * &#123;&#123;&#123;</div><div class="line">    *   // The following two are equivalent:</div><div class="line">    *   df1.join(df2, $"df1Key" === $"df2Key")</div><div class="line">    *   df1.join(df2).where($"df1Key" === $"df2Key")</div><div class="line">    * &#125;&#125;&#125;</div><div class="line">    *</div><div class="line">    * @group untypedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_], joinExprs: <span class="type">Column</span>): <span class="type">DataFrame</span> = join(right, joinExprs, <span class="string">"inner"</span>)</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">    * Join with another `DataFrame`, using the given join expression. The following performs</div><div class="line">    * a full outer join between `df1` and `df2`.</div><div class="line">    *</div><div class="line">    * &#123;&#123;&#123;</div><div class="line">    *   // Scala:</div><div class="line">    *   import org.apache.spark.sql.functions._</div><div class="line">    *   df1.join(df2, $"df1Key" === $"df2Key", "outer")</div><div class="line">    *</div><div class="line">    *   // Java:</div><div class="line">    *   import static org.apache.spark.sql.functions.*;</div><div class="line">    *   df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer");</div><div class="line">    * &#125;&#125;&#125;</div><div class="line">    *</div><div class="line">    * @param right     Right side of the join.</div><div class="line">    * @param joinExprs Join expression.</div><div class="line">    * @param joinType  One of: `inner`, `outer`, `left_outer`, `right_outer`, `leftsemi`.</div><div class="line">    * @group untypedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_], joinExprs: <span class="type">Column</span>, joinType: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</div><div class="line">    <span class="comment">// Note that in this function, we introduce a hack in the case of self-join to automatically</span></div><div class="line">    <span class="comment">// resolve ambiguous join conditions into ones that might make sense [SPARK-6231].</span></div><div class="line">    <span class="comment">// Consider this case: df.join(df, df("key") === df("key"))</span></div><div class="line">    <span class="comment">// Since df("key") === df("key") is a trivially true condition, this actually becomes a</span></div><div class="line">    <span class="comment">// cartesian join. However, most likely users expect to perform a self join using "key".</span></div><div class="line">    <span class="comment">// With that assumption, this hack turns the trivially true condition into equality on join</span></div><div class="line">    <span class="comment">// keys that are resolved to both sides.</span></div><div class="line"></div><div class="line">    <span class="comment">// Trigger analysis so in the case of self-join, the analyzer will clone the plan.</span></div><div class="line">    <span class="comment">// After the cloning, left and right side will have distinct expression ids.</span></div><div class="line">    <span class="comment">// 针对自连接的优化：正常情况下，自连接如果使用  df.join(df, df("key") === df("key"))</span></div><div class="line">    <span class="comment">// 会造成 笛卡尔积</span></div><div class="line">    <span class="comment">// 这种情况下，分析器会 克隆计划，克隆完成后，左右两边则有不同的 id</span></div><div class="line"></div><div class="line">    <span class="keyword">val</span> plan = withPlan(</div><div class="line">      <span class="type">Join</span>(logicalPlan, right.logicalPlan, <span class="type">JoinType</span>(joinType), <span class="type">Some</span>(joinExprs.expr)))</div><div class="line">      .queryExecution.analyzed.asInstanceOf[<span class="type">Join</span>]</div><div class="line"></div><div class="line">    <span class="comment">// If auto self join alias is disabled, return the plan.</span></div><div class="line">    <span class="keyword">if</span> (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) &#123;</div><div class="line">      <span class="keyword">return</span> withPlan(plan)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// If left/right have no output set intersection, return the plan.</span></div><div class="line">    <span class="keyword">val</span> lanalyzed = withPlan(<span class="keyword">this</span>.logicalPlan).queryExecution.analyzed</div><div class="line">    <span class="keyword">val</span> ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed</div><div class="line">    <span class="keyword">if</span> (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) &#123;</div><div class="line">      <span class="keyword">return</span> withPlan(plan)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// Otherwise, find the trivially true predicates and automatically resolves them to both sides.</span></div><div class="line">    <span class="comment">// By the time we get here, since we have already run analysis, all attributes should've been</span></div><div class="line">    <span class="comment">// resolved and become AttributeReference.</span></div><div class="line">    <span class="keyword">val</span> cond = plan.condition.map &#123;</div><div class="line">      _.transform &#123;</div><div class="line">        <span class="keyword">case</span> catalyst.expressions.<span class="type">EqualTo</span>(a: <span class="type">AttributeReference</span>, b: <span class="type">AttributeReference</span>)</div><div class="line">          <span class="keyword">if</span> a.sameRef(b) =&gt;</div><div class="line">          catalyst.expressions.<span class="type">EqualTo</span>(</div><div class="line">            withPlan(plan.left).resolve(a.name),</div><div class="line">            withPlan(plan.right).resolve(b.name))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    withPlan &#123;</div><div class="line">      plan.copy(condition = cond)</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h3 id="crossJoin"><a href="#crossJoin" class="headerlink" title="crossJoin"></a>crossJoin</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Explicit cartesian join with another `DataFrame`.</div><div class="line">  * 显式笛卡尔积join</div><div class="line">  *</div><div class="line">  * @param right Right side of the join operation.</div><div class="line">  * @note Cartesian joins are very expensive without an extra filter that can be pushed down.</div><div class="line">  *       如果没有额外的过滤器，笛卡尔连接非常昂贵。</div><div class="line">  * @group untypedrel</div><div class="line">  * @since 2.1.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">crossJoin</span></span>(right: <span class="type">Dataset</span>[_]): <span class="type">DataFrame</span> = withPlan &#123;</div><div class="line">  <span class="type">Join</span>(logicalPlan, right.logicalPlan, joinType = <span class="type">Cross</span>, <span class="type">None</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="apply"><a href="#apply" class="headerlink" title="apply"></a>apply</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Selects column based on the column name and return it as a [[Column]].</div><div class="line">  *</div><div class="line">  * 选择基于列名的列，并将其作为[[Column]]返回。</div><div class="line">  *</div><div class="line">  * @note The column name can also reference to a nested column like `a.b`.</div><div class="line">  *</div><div class="line">  *       列名也可以引用像“a.b”这样的嵌套列。</div><div class="line">  * @group untypedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(colName: <span class="type">String</span>): <span class="type">Column</span> = col(colName)</div></pre></td></tr></table></figure>
<h3 id="col"><a href="#col" class="headerlink" title="col"></a>col</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Selects column based on the column name and return it as a [[Column]].</div><div class="line">  *</div><div class="line">  * 选择基于列名的列，并将其作为[[Column]]返回。</div><div class="line">  *</div><div class="line">  * @note The column name can also reference to a nested column like `a.b`.</div><div class="line">  *</div><div class="line">  *       列名也可以引用像“a.b”这样的嵌套列。</div><div class="line">  * @group untypedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">col</span></span>(colName: <span class="type">String</span>): <span class="type">Column</span> = colName <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="string">"*"</span> =&gt;</div><div class="line">    <span class="type">Column</span>(<span class="type">ResolvedStar</span>(queryExecution.analyzed.output))</div><div class="line">  <span class="keyword">case</span> _ =&gt;</div><div class="line">    <span class="keyword">val</span> expr = resolve(colName)</div><div class="line">    <span class="type">Column</span>(expr)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="select"><a href="#select" class="headerlink" title="select"></a>select</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Selects a set of column based expressions.</div><div class="line">  * &#123;&#123;&#123;</div><div class="line">  *   ds.select($"colA", $"colB" + 1)</div><div class="line">  * &#125;&#125;&#125;</div><div class="line">  *</div><div class="line">  * @group untypedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="meta">@scala</span>.annotation.varargs</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">select</span></span>(cols: <span class="type">Column</span>*): <span class="type">DataFrame</span> = withPlan &#123;</div><div class="line">  <span class="type">Project</span>(cols.map(_.named), logicalPlan)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Selects a set of columns. This is a variant of `select` that can only select</div><div class="line">  * existing columns using column names (i.e. cannot construct expressions).</div><div class="line">  *</div><div class="line">  * 只能是已经存在的列名</div><div class="line">  *</div><div class="line">  * &#123;&#123;&#123;</div><div class="line">  *   // The following two are equivalent:</div><div class="line">  *   ds.select("colA", "colB")</div><div class="line">  *   ds.select($"colA", $"colB")</div><div class="line">  * &#125;&#125;&#125;</div><div class="line">  *</div><div class="line">  * @group untypedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="meta">@scala</span>.annotation.varargs</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">select</span></span>(col: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">DataFrame</span> = select((col +: cols).map(<span class="type">Column</span>(_)): _*)</div></pre></td></tr></table></figure>
<h3 id="selectExpr"><a href="#selectExpr" class="headerlink" title="selectExpr"></a>selectExpr</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Selects a set of SQL expressions. This is a variant of `select` that accepts</div><div class="line">  * SQL expressions.</div><div class="line">  *</div><div class="line">  * 接受SQL表达式</div><div class="line">  *</div><div class="line">  * &#123;&#123;&#123;</div><div class="line">  *   // The following are equivalent:</div><div class="line">  *   以下是等价的:</div><div class="line">  *   ds.selectExpr("colA", "colB as newName", "abs(colC)")</div><div class="line">  *   ds.select(expr("colA"), expr("colB as newName"), expr("abs(colC)"))</div><div class="line">  * &#125;&#125;&#125;</div><div class="line">  *</div><div class="line">  * @group untypedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="meta">@scala</span>.annotation.varargs</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">selectExpr</span></span>(exprs: <span class="type">String</span>*): <span class="type">DataFrame</span> = &#123;</div><div class="line">  select(exprs.map &#123; expr =&gt;</div><div class="line">    <span class="type">Column</span>(sparkSession.sessionState.sqlParser.parseExpression(expr))</div><div class="line">  &#125;: _*)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Groups the Dataset using the specified columns, so we can run aggregation on them. See</div><div class="line">  * [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class="line">  *</div><div class="line">  * 使用指定的列对数据集进行分组，这样我们就可以对它们进行聚合。</div><div class="line">  * 查看[[RelationalGroupedDataset]]为所有可用的聚合函数。</div><div class="line">  *</div><div class="line">  *</div><div class="line">  * &#123;&#123;&#123;</div><div class="line">  *   // Compute the average for all numeric columns grouped by department.</div><div class="line">  *</div><div class="line">  *   计算按部门分组的所有数字列的平均值。</div><div class="line">  *</div><div class="line">  *   ds.groupBy($"department").avg()</div><div class="line">  *</div><div class="line">  *   // Compute the max age and average salary, grouped by department and gender.</div><div class="line">  *   ds.groupBy($"department", $"gender").agg(Map(</div><div class="line">  *     "salary" -&gt; "avg",</div><div class="line">  *     "age" -&gt; "max"</div><div class="line">  *   ))</div><div class="line">  * &#125;&#125;&#125;</div><div class="line">  *</div><div class="line">  * @group untypedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="meta">@scala</span>.annotation.varargs</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupBy</span></span>(cols: <span class="type">Column</span>*): <span class="type">RelationalGroupedDataset</span> = &#123;</div><div class="line">  <span class="type">RelationalGroupedDataset</span>(toDF(), cols.map(_.expr), <span class="type">RelationalGroupedDataset</span>.<span class="type">GroupByType</span>)</div><div class="line">&#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">  * Groups the Dataset using the specified columns, so that we can run aggregation on them.</div><div class="line">  * See [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class="line">  *</div><div class="line">  * This is a variant of groupBy that can only group by existing columns using column names</div><div class="line">  * (i.e. cannot construct expressions).</div><div class="line">  *</div><div class="line">  * &#123;&#123;&#123;</div><div class="line">  *   // Compute the average for all numeric columns grouped by department.</div><div class="line">  *   ds.groupBy("department").avg()</div><div class="line">  *</div><div class="line">  *   // Compute the max age and average salary, grouped by department and gender.</div><div class="line">  *   ds.groupBy($"department", $"gender").agg(Map(</div><div class="line">  *     "salary" -&gt; "avg",</div><div class="line">  *     "age" -&gt; "max"</div><div class="line">  *   ))</div><div class="line">  * &#125;&#125;&#125;</div><div class="line">  *</div><div class="line">  * @group untypedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="meta">@scala</span>.annotation.varargs</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupBy</span></span>(col1: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">RelationalGroupedDataset</span> = &#123;</div><div class="line">  <span class="keyword">val</span> colNames: <span class="type">Seq</span>[<span class="type">String</span>] = col1 +: cols</div><div class="line">  <span class="type">RelationalGroupedDataset</span>(</div><div class="line">    toDF(), colNames.map(colName =&gt; resolve(colName)), <span class="type">RelationalGroupedDataset</span>.<span class="type">GroupByType</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="rollup"><a href="#rollup" class="headerlink" title="rollup"></a>rollup</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">    * Create a multi-dimensional rollup for the current Dataset using the specified columns,</div><div class="line">    * so we can run aggregation on them.</div><div class="line">    * See [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class="line">    *</div><div class="line">    * 使用指定的列为当前数据集创建多维的汇总，因此我们可以在它们上运行聚合。</div><div class="line">    *</div><div class="line">    *</div><div class="line">    * &#123;&#123;&#123;</div><div class="line">    *   // Compute the average for all numeric columns rolluped by department and group.</div><div class="line">    *</div><div class="line">    *   汇总后 求平均值</div><div class="line">    *</div><div class="line">    *   ds.rollup($"department", $"group").avg()</div><div class="line">    *</div><div class="line">    *   // Compute the max age and average salary, rolluped by department and gender.</div><div class="line">    *   ds.rollup($"department", $"gender").agg(Map(</div><div class="line">    *     "salary" -&gt; "avg",</div><div class="line">    *     "age" -&gt; "max"</div><div class="line">    *   ))</div><div class="line">    * &#125;&#125;&#125;</div><div class="line">    *</div><div class="line">    * @group untypedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="meta">@scala</span>.annotation.varargs</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">rollup</span></span>(cols: <span class="type">Column</span>*): <span class="type">RelationalGroupedDataset</span> = &#123;</div><div class="line">    <span class="type">RelationalGroupedDataset</span>(toDF(), cols.map(_.expr), <span class="type">RelationalGroupedDataset</span>.<span class="type">RollupType</span>)</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">    <span class="comment">/**</span></div><div class="line">    * Create a multi-dimensional rollup for the current Dataset using the specified columns,</div><div class="line">    * so we can run aggregation on them.</div><div class="line">    * See [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class="line">    *</div><div class="line">    * 使用指定的列为当前数据集创建多维的rollup，因此我们可以在它们上运行聚合。</div><div class="line">    * rollup可以实现 从右到左一次递减的多级统计，显示统计某一层次结构的聚合</div><div class="line">    * 例如 rollup(a,b,c,d) =结果=&gt; (a,b,c,d),(a,b,c),(a,b),a</div><div class="line">    *</div><div class="line">    * This is a variant of rollup that can only group by existing columns using column names</div><div class="line">    * (i.e. cannot construct expressions).</div><div class="line">    *</div><div class="line">    * &#123;&#123;&#123;</div><div class="line">    *   // Compute the average for all numeric columns rolluped by department and group.</div><div class="line">    *   ds.rollup("department", "group").avg()</div><div class="line">    *</div><div class="line">    *   // Compute the max age and average salary, rolluped by department and gender.</div><div class="line">    *   ds.rollup($"department", $"gender").agg(Map(</div><div class="line">    *     "salary" -&gt; "avg",</div><div class="line">    *     "age" -&gt; "max"</div><div class="line">    *   ))</div><div class="line">    * &#125;&#125;&#125;</div><div class="line">    *</div><div class="line">    * @group untypedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="meta">@scala</span>.annotation.varargs</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">rollup</span></span>(col1: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">RelationalGroupedDataset</span> = &#123;</div><div class="line">    <span class="keyword">val</span> colNames: <span class="type">Seq</span>[<span class="type">String</span>] = col1 +: cols</div><div class="line">    <span class="type">RelationalGroupedDataset</span>(</div><div class="line">      toDF(), colNames.map(colName =&gt; resolve(colName)), <span class="type">RelationalGroupedDataset</span>.<span class="type">RollupType</span>)</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h3 id="cube"><a href="#cube" class="headerlink" title="cube"></a>cube</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">    * Create a multi-dimensional cube for the current Dataset using the specified columns,</div><div class="line">    * so we can run aggregation on them.</div><div class="line">    * See [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class="line">    *</div><div class="line">    * 使用指定的列为当前数据集创建多维数据集，因此我们可以在它们上运行聚合。</div><div class="line">    *</div><div class="line">    *</div><div class="line">    * &#123;&#123;&#123;</div><div class="line">    *   // Compute the average for all numeric columns cubed by department and group.</div><div class="line">    *   ds.cube($"department", $"group").avg()</div><div class="line">    *</div><div class="line">    *   // Compute the max age and average salary, cubed by department and gender.</div><div class="line">    *   ds.cube($"department", $"gender").agg(Map(</div><div class="line">    *     "salary" -&gt; "avg",</div><div class="line">    *     "age" -&gt; "max"</div><div class="line">    *   ))</div><div class="line">    * &#125;&#125;&#125;</div><div class="line">    *</div><div class="line">    * @group untypedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="meta">@scala</span>.annotation.varargs</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cube</span></span>(cols: <span class="type">Column</span>*): <span class="type">RelationalGroupedDataset</span> = &#123;</div><div class="line">    <span class="type">RelationalGroupedDataset</span>(toDF(), cols.map(_.expr), <span class="type">RelationalGroupedDataset</span>.<span class="type">CubeType</span>)</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">   <span class="comment">/**</span></div><div class="line">    * Create a multi-dimensional cube for the current Dataset using the specified columns,</div><div class="line">    * so we can run aggregation on them.</div><div class="line">    * See [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class="line">    *</div><div class="line">    * 魔方 例如：cube(a,b,c) =结果=&gt; (a,b),(a,c),a,(b,c),b,c 结果为所有的维度</div><div class="line">    * 使用指定的列为当前数据集创建多维多维数据集，因此我们可以在它们上运行聚合。</div><div class="line">    *</div><div class="line">    * This is a variant of cube that can only group by existing columns using column names</div><div class="line">    * (i.e. cannot construct expressions).</div><div class="line">    *</div><div class="line">    * 这是一个多维数据集的变体，它只能通过使用列名的现有列来分组</div><div class="line">    *</div><div class="line">    * &#123;&#123;&#123;</div><div class="line">    *   // Compute the average for all numeric columns cubed by department and group.</div><div class="line">    *   ds.cube("department", "group").avg()</div><div class="line">    *</div><div class="line">    *   // Compute the max age and average salary, cubed by department and gender.</div><div class="line">    *   ds.cube($"department", $"gender").agg(Map(</div><div class="line">    *     "salary" -&gt; "avg",</div><div class="line">    *     "age" -&gt; "max"</div><div class="line">    *   ))</div><div class="line">    * &#125;&#125;&#125;</div><div class="line">    *</div><div class="line">    * @group untypedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="meta">@scala</span>.annotation.varargs</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cube</span></span>(col1: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">RelationalGroupedDataset</span> = &#123;</div><div class="line">    <span class="keyword">val</span> colNames: <span class="type">Seq</span>[<span class="type">String</span>] = col1 +: cols</div><div class="line">    <span class="type">RelationalGroupedDataset</span>(</div><div class="line">      toDF(), colNames.map(colName =&gt; resolve(colName)), <span class="type">RelationalGroupedDataset</span>.<span class="type">CubeType</span>)</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h3 id="agg"><a href="#agg" class="headerlink" title="agg"></a>agg</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">   * (Scala-specific) Aggregates on the entire Dataset without groups.</div><div class="line">   * 对整个数据集进行聚合，无需分组。</div><div class="line">   * &#123;&#123;&#123;</div><div class="line">   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</div><div class="line">   *   ds.agg("age" -&gt; "max", "salary" -&gt; "avg")</div><div class="line">   *   ds.groupBy().agg("age" -&gt; "max", "salary" -&gt; "avg")</div><div class="line">   * &#125;&#125;&#125;</div><div class="line">   *</div><div class="line">   * @group untypedrel</div><div class="line">   * @since 2.0.0</div><div class="line">   */</div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">agg</span></span>(aggExpr: (<span class="type">String</span>, <span class="type">String</span>), aggExprs: (<span class="type">String</span>, <span class="type">String</span>)*): <span class="type">DataFrame</span> = &#123;</div><div class="line">   groupBy().agg(aggExpr, aggExprs: _*)</div><div class="line"> &#125;</div><div class="line"></div><div class="line"> <span class="comment">/**</span></div><div class="line">   * (Scala-specific) Aggregates on the entire Dataset without groups.</div><div class="line">   * 对整个数据集进行聚合，无需分组。</div><div class="line">   *</div><div class="line">   * &#123;&#123;&#123;</div><div class="line">   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</div><div class="line">   *   ds.agg(Map("age" -&gt; "max", "salary" -&gt; "avg"))</div><div class="line">   *   ds.groupBy().agg(Map("age" -&gt; "max", "salary" -&gt; "avg"))</div><div class="line">   * &#125;&#125;&#125;</div><div class="line">   *</div><div class="line">   * @group untypedrel</div><div class="line">   * @since 2.0.0</div><div class="line">   */</div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">agg</span></span>(exprs: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">DataFrame</span> = groupBy().agg(exprs)</div><div class="line"></div><div class="line"> <span class="comment">/**</span></div><div class="line">   * (Java-specific) Aggregates on the entire Dataset without groups.</div><div class="line">   *</div><div class="line">   * 对整个数据集进行聚合，无需分组。</div><div class="line">   *</div><div class="line">   * &#123;&#123;&#123;</div><div class="line">   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</div><div class="line">   *   ds.agg(Map("age" -&gt; "max", "salary" -&gt; "avg"))</div><div class="line">   *   ds.groupBy().agg(Map("age" -&gt; "max", "salary" -&gt; "avg"))</div><div class="line">   * &#125;&#125;&#125;</div><div class="line">   *</div><div class="line">   * @group untypedrel</div><div class="line">   * @since 2.0.0</div><div class="line">   */</div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">agg</span></span>(exprs: java.util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">DataFrame</span> = groupBy().agg(exprs)</div><div class="line"></div><div class="line"> <span class="comment">/**</span></div><div class="line">   * Aggregates on the entire Dataset without groups.</div><div class="line">   *</div><div class="line">   * 对整个数据集进行聚合，无需分组。</div><div class="line">   *</div><div class="line">   * &#123;&#123;&#123;</div><div class="line">   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</div><div class="line">   *   ds.agg(max($"age"), avg($"salary"))</div><div class="line">   *   ds.groupBy().agg(max($"age"), avg($"salary"))</div><div class="line">   * &#125;&#125;&#125;</div><div class="line">   *</div><div class="line">   * @group untypedrel</div><div class="line">   * @since 2.0.0</div><div class="line">   */</div><div class="line"> <span class="meta">@scala</span>.annotation.varargs</div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">agg</span></span>(expr: <span class="type">Column</span>, exprs: <span class="type">Column</span>*): <span class="type">DataFrame</span> = groupBy().agg(expr, exprs: _*)</div></pre></td></tr></table></figure>
<h3 id="explode"><a href="#explode" class="headerlink" title="explode"></a>explode</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more</div><div class="line">  * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of</div><div class="line">  * the input row are implicitly joined with each row that is output by the function.</div><div class="line">  *</div><div class="line">  * 根据提供的方法，该数据集的每一行都被扩展为零个或更多的行，返回一个新的数据集。</div><div class="line">  * 这类似于HiveQL的“LATERAL VIEW”。</div><div class="line">  * 输入行的列 隐式地加入了由函数输出的每一行。</div><div class="line">  *</div><div class="line">  * Given that this is deprecated, as an alternative, you can explode columns either using</div><div class="line">  * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count</div><div class="line">  * the number of books that contain a given word:</div><div class="line">  *</div><div class="line">  * 考虑到这已经被弃用，作为替代，您可以使用“functions.explode()”或“flatMap()”来引爆列。</div><div class="line">  * 下面的示例使用这些替代方法来计算包含给定单词的图书的数量:</div><div class="line">  *</div><div class="line">  * &#123;&#123;&#123;</div><div class="line">  *   case class Book(title: String, words: String)</div><div class="line">  *   val ds: Dataset[Book]</div><div class="line">  *</div><div class="line">  *   val allWords = ds.select('title, explode(split('words, " ")).as("word"))</div><div class="line">  *</div><div class="line">  *   val bookCountPerWord = allWords.groupBy("word").agg(countDistinct("title"))</div><div class="line">  * &#125;&#125;&#125;</div><div class="line">  *</div><div class="line">  * Using `flatMap()` this can similarly be exploded as:</div><div class="line">  *</div><div class="line">  * &#123;&#123;&#123;</div><div class="line">  *   ds.flatMap(_.words.split(" "))</div><div class="line">  * &#125;&#125;&#125;</div><div class="line">  *</div><div class="line">  * @group untypedrel</div><div class="line">  * @since 2.0.0 已经过时，用 flatMap() 或 functions.explode() 代替</div><div class="line">  */</div><div class="line"><span class="meta">@deprecated</span>(<span class="string">"use flatMap() or select() with functions.explode() instead"</span>, <span class="string">"2.0.0"</span>)</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">explode</span></span>[<span class="type">A</span> &lt;: <span class="type">Product</span> : <span class="type">TypeTag</span>](input: <span class="type">Column</span>*)(f: <span class="type">Row</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">A</span>]): <span class="type">DataFrame</span> = &#123;</div><div class="line">  <span class="keyword">val</span> elementSchema = <span class="type">ScalaReflection</span>.schemaFor[<span class="type">A</span>].dataType.asInstanceOf[<span class="type">StructType</span>]</div><div class="line"></div><div class="line">  <span class="keyword">val</span> convert = <span class="type">CatalystTypeConverters</span>.createToCatalystConverter(elementSchema)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> rowFunction =</div><div class="line">    f.andThen(_.map(convert(_).asInstanceOf[<span class="type">InternalRow</span>]))</div><div class="line">  <span class="keyword">val</span> generator = <span class="type">UserDefinedGenerator</span>(elementSchema, rowFunction, input.map(_.expr))</div><div class="line"></div><div class="line">  withPlan &#123;</div><div class="line">    <span class="type">Generate</span>(generator, join = <span class="literal">true</span>, outer = <span class="literal">false</span>,</div><div class="line">      qualifier = <span class="type">None</span>, generatorOutput = <span class="type">Nil</span>, logicalPlan)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero</div><div class="line">  * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All</div><div class="line">  * columns of the input row are implicitly joined with each value that is output by the function.</div><div class="line">  *</div><div class="line">  * Given that this is deprecated, as an alternative, you can explode columns either using</div><div class="line">  * `functions.explode()`:</div><div class="line">  *</div><div class="line">  * &#123;&#123;&#123;</div><div class="line">  *   ds.select(explode(split('words, " ")).as("word"))</div><div class="line">  * &#125;&#125;&#125;</div><div class="line">  *</div><div class="line">  * or `flatMap()`:</div><div class="line">  *</div><div class="line">  * &#123;&#123;&#123;</div><div class="line">  *   ds.flatMap(_.words.split(" "))</div><div class="line">  * &#125;&#125;&#125;</div><div class="line">  *</div><div class="line">  * @group untypedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="meta">@deprecated</span>(<span class="string">"use flatMap() or select() with functions.explode() instead"</span>, <span class="string">"2.0.0"</span>)</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">explode</span></span>[<span class="type">A</span>, <span class="type">B</span>: <span class="type">TypeTag</span>](inputColumn: <span class="type">String</span>, outputColumn: <span class="type">String</span>)(f: <span class="type">A</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">B</span>])</div><div class="line">: <span class="type">DataFrame</span> = &#123;</div><div class="line">  <span class="keyword">val</span> dataType = <span class="type">ScalaReflection</span>.schemaFor[<span class="type">B</span>].dataType</div><div class="line">  <span class="keyword">val</span> attributes = <span class="type">AttributeReference</span>(outputColumn, dataType)() :: <span class="type">Nil</span></div><div class="line">  <span class="comment">// TODO handle the metadata?</span></div><div class="line">  <span class="keyword">val</span> elementSchema = attributes.toStructType</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">rowFunction</span></span>(row: <span class="type">Row</span>): <span class="type">TraversableOnce</span>[<span class="type">InternalRow</span>] = &#123;</div><div class="line">    <span class="keyword">val</span> convert = <span class="type">CatalystTypeConverters</span>.createToCatalystConverter(dataType)</div><div class="line">    f(row(<span class="number">0</span>).asInstanceOf[<span class="type">A</span>]).map(o =&gt; <span class="type">InternalRow</span>(convert(o)))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">val</span> generator = <span class="type">UserDefinedGenerator</span>(elementSchema, rowFunction, apply(inputColumn).expr :: <span class="type">Nil</span>)</div><div class="line"></div><div class="line">  withPlan &#123;</div><div class="line">    <span class="type">Generate</span>(generator, join = <span class="literal">true</span>, outer = <span class="literal">false</span>,</div><div class="line">      qualifier = <span class="type">None</span>, generatorOutput = <span class="type">Nil</span>, logicalPlan)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="withColumn"><a href="#withColumn" class="headerlink" title="withColumn"></a>withColumn</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new Dataset by adding a column or replacing the existing column that has</div><div class="line">  * the same name.</div><div class="line">  * 通过添加一个列或替换具有相同名称的现有列返回新的数据集。</div><div class="line">  *</div><div class="line">  * @group untypedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">withColumn</span></span>(colName: <span class="type">String</span>, col: <span class="type">Column</span>): <span class="type">DataFrame</span> = &#123;</div><div class="line">  <span class="keyword">val</span> resolver = sparkSession.sessionState.analyzer.resolver</div><div class="line">  <span class="keyword">val</span> output = queryExecution.analyzed.output</div><div class="line">  <span class="keyword">val</span> shouldReplace = output.exists(f =&gt; resolver(f.name, colName))</div><div class="line">  <span class="keyword">if</span> (shouldReplace) &#123;</div><div class="line">    <span class="keyword">val</span> columns = output.map &#123; field =&gt;</div><div class="line">      <span class="keyword">if</span> (resolver(field.name, colName)) &#123;</div><div class="line">        col.as(colName)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="type">Column</span>(field)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    select(columns: _*)</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    select(<span class="type">Column</span>(<span class="string">"*"</span>), col.as(colName))</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new Dataset by adding a column with metadata.</div><div class="line">  * 通过添加带有元数据的列返回一个新的数据集。</div><div class="line">  */</div><div class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">withColumn</span></span>(colName: <span class="type">String</span>, col: <span class="type">Column</span>, metadata: <span class="type">Metadata</span>): <span class="type">DataFrame</span> = &#123;</div><div class="line">  <span class="keyword">val</span> resolver = sparkSession.sessionState.analyzer.resolver</div><div class="line">  <span class="keyword">val</span> output = queryExecution.analyzed.output</div><div class="line">  <span class="keyword">val</span> shouldReplace = output.exists(f =&gt; resolver(f.name, colName))</div><div class="line">  <span class="keyword">if</span> (shouldReplace) &#123;</div><div class="line">    <span class="keyword">val</span> columns = output.map &#123; field =&gt;</div><div class="line">      <span class="keyword">if</span> (resolver(field.name, colName)) &#123;</div><div class="line">        col.as(colName, metadata)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="type">Column</span>(field)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    select(columns: _*)</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    select(<span class="type">Column</span>(<span class="string">"*"</span>), col.as(colName, metadata))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="withColumnRenamed"><a href="#withColumnRenamed" class="headerlink" title="withColumnRenamed"></a>withColumnRenamed</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new Dataset with a column renamed.</div><div class="line">  * This is a no-op if schema doesn't contain existingName.</div><div class="line">  * 返回一个重命名的列的新数据集。</div><div class="line">  * 如果模式不包含存在名称，那么这是不操作的。</div><div class="line">  *</div><div class="line">  * @group untypedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">withColumnRenamed</span></span>(existingName: <span class="type">String</span>, newName: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</div><div class="line">  <span class="keyword">val</span> resolver = sparkSession.sessionState.analyzer.resolver</div><div class="line">  <span class="keyword">val</span> output = queryExecution.analyzed.output</div><div class="line">  <span class="keyword">val</span> shouldRename = output.exists(f =&gt; resolver(f.name, existingName))</div><div class="line">  <span class="keyword">if</span> (shouldRename) &#123;</div><div class="line">    <span class="keyword">val</span> columns = output.map &#123; col =&gt;</div><div class="line">      <span class="keyword">if</span> (resolver(col.name, existingName)) &#123;</div><div class="line">        <span class="type">Column</span>(col).as(newName)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="type">Column</span>(col)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    select(columns: _*)</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    toDF()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="drop"><a href="#drop" class="headerlink" title="drop"></a>drop</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain</div><div class="line">  * column name.</div><div class="line">  *</div><div class="line">  * 返回删除指定列之后的新Dataset</div><div class="line">  *</div><div class="line">  * This method can only be used to drop top level columns. the colName string is treated</div><div class="line">  * literally without further interpretation.</div><div class="line">  *</div><div class="line">  * 仅用于删除顶层的列</div><div class="line">  *</div><div class="line">  * @group untypedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop</span></span>(colName: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</div><div class="line">  drop(<span class="type">Seq</span>(colName): _*)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new Dataset with columns dropped.</div><div class="line">  * This is a no-op if schema doesn't contain column name(s).</div><div class="line">  *</div><div class="line">  * 删除指定的多个列，并返回新的dataset</div><div class="line">  *</div><div class="line">  * This method can only be used to drop top level columns. the colName string is treated literally</div><div class="line">  * without further interpretation.</div><div class="line">  *</div><div class="line">  * @group untypedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="meta">@scala</span>.annotation.varargs</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop</span></span>(colNames: <span class="type">String</span>*): <span class="type">DataFrame</span> = &#123;</div><div class="line">  <span class="keyword">val</span> resolver = sparkSession.sessionState.analyzer.resolver</div><div class="line">  <span class="keyword">val</span> allColumns = queryExecution.analyzed.output</div><div class="line">  <span class="keyword">val</span> remainingCols = allColumns.filter &#123; attribute =&gt;</div><div class="line">    colNames.forall(n =&gt; !resolver(attribute.name, n))</div><div class="line">  &#125;.map(attribute =&gt; <span class="type">Column</span>(attribute))</div><div class="line">  <span class="keyword">if</span> (remainingCols.size == allColumns.size) &#123;</div><div class="line">    toDF()</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">this</span>.select(remainingCols: _*)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new Dataset with a column dropped.</div><div class="line">  * This version of drop accepts a [[Column]] rather than a name.</div><div class="line">  * This is a no-op if the Dataset doesn't have a column</div><div class="line">  * with an equivalent expression.</div><div class="line">  *</div><div class="line">  * 删除指定的 列（根据Column）</div><div class="line">  *</div><div class="line">  * @group untypedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop</span></span>(col: <span class="type">Column</span>): <span class="type">DataFrame</span> = &#123;</div><div class="line">  <span class="keyword">val</span> expression = col <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Column</span>(u: <span class="type">UnresolvedAttribute</span>) =&gt;</div><div class="line">      queryExecution.analyzed.resolveQuoted(</div><div class="line">        u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)</div><div class="line">    <span class="keyword">case</span> <span class="type">Column</span>(expr: <span class="type">Expression</span>) =&gt; expr</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">val</span> attrs = <span class="keyword">this</span>.logicalPlan.output</div><div class="line">  <span class="keyword">val</span> colsAfterDrop = attrs.filter &#123; attr =&gt;</div><div class="line">    attr != expression</div><div class="line">  &#125;.map(attr =&gt; <span class="type">Column</span>(attr))</div><div class="line">  select(colsAfterDrop: _*)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="typedrel-有类型的转换"><a href="#typedrel-有类型的转换" class="headerlink" title="typedrel-有类型的转换"></a>typedrel-有类型的转换</h2><h3 id="joinWith"><a href="#joinWith" class="headerlink" title="joinWith"></a>joinWith</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * :: Experimental ::  实验的</div><div class="line">  * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to</div><div class="line">  * true.</div><div class="line">  * 连接这个数据集返回一个“Tuple2”对每一对的“条件”计算为true。</div><div class="line">  *</div><div class="line">  * This is similar to the relation `join` function with one important difference in the</div><div class="line">  * result schema. Since `joinWith` preserves objects present on either side of the join, the</div><div class="line">  * result schema is similarly nested into a tuple under the column names `_1` and `_2`.</div><div class="line">  * 这类似于关系“join”函数，在结果模式中有一个重要的区别。</div><div class="line">  * 由于“joinWith”保存了连接的任何一边的对象，因此结果模式类似地嵌套在列名称“_1”和“_2”下面的tuple中。</div><div class="line">  *</div><div class="line">  * This type of join can be useful both for preserving type-safety with the original object</div><div class="line">  * types as well as working with relational data where either side of the join has column</div><div class="line">  * names in common.</div><div class="line">  * 这种类型的联接既可以用于保存与原始对象类型的类型安全性，</div><div class="line">  * 也可以用于处理连接的任何一端都有列名的关系数据。</div><div class="line">  *</div><div class="line">  * @param other     Right side of the join.</div><div class="line">  * @param condition Join expression.</div><div class="line">  * @param joinType  One of: `inner`, `outer`, `left_outer`, `right_outer`, `leftsemi`.</div><div class="line">  * @group typedrel</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="meta">@Experimental</span></div><div class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">joinWith</span></span>[<span class="type">U</span>](other: <span class="type">Dataset</span>[<span class="type">U</span>], condition: <span class="type">Column</span>, joinType: <span class="type">String</span>): <span class="type">Dataset</span>[(<span class="type">T</span>, <span class="type">U</span>)] = &#123;</div><div class="line">  <span class="comment">// Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,</span></div><div class="line">  <span class="comment">// 创建一个联接节点并首先解析它，使Join条件得到解析，self - Join解析，</span></div><div class="line">  <span class="comment">// etc.</span></div><div class="line">  <span class="keyword">val</span> joined = sparkSession.sessionState.executePlan(</div><div class="line">    <span class="type">Join</span>(</div><div class="line">      <span class="keyword">this</span>.logicalPlan,</div><div class="line">      other.logicalPlan,</div><div class="line">      <span class="type">JoinType</span>(joinType),</div><div class="line">      <span class="type">Some</span>(condition.expr))).analyzed.asInstanceOf[<span class="type">Join</span>]</div><div class="line"></div><div class="line">  <span class="comment">// For both join side, combine all outputs into a single column and alias it with "_1" or "_2",</span></div><div class="line">  <span class="comment">// to match the schema for the encoder of the join result.</span></div><div class="line">  <span class="comment">// 对于这两个连接，将所有输出合并为一个列，并将其别名为“_1”或“_2”，以匹配连接结果的编码器的模式。</span></div><div class="line"></div><div class="line">  <span class="comment">// Note that we do this before joining them, to enable the join operator to return null for one</span></div><div class="line">  <span class="comment">// side, in cases like outer-join.</span></div><div class="line">  <span class="comment">// 请注意，在join它们之前，我们这样做，使join操作符在像outer - join这样的情况下返回null。</span></div><div class="line">  <span class="keyword">val</span> left = &#123;</div><div class="line">    <span class="keyword">val</span> combined = <span class="keyword">if</span> (<span class="keyword">this</span>.exprEnc.flat) &#123;</div><div class="line">      assert(joined.left.output.length == <span class="number">1</span>)</div><div class="line">      <span class="type">Alias</span>(joined.left.output.head, <span class="string">"_1"</span>)()</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="type">Alias</span>(<span class="type">CreateStruct</span>(joined.left.output), <span class="string">"_1"</span>)()</div><div class="line">    &#125;</div><div class="line">    <span class="type">Project</span>(combined :: <span class="type">Nil</span>, joined.left)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">val</span> right = &#123;</div><div class="line">    <span class="keyword">val</span> combined = <span class="keyword">if</span> (other.exprEnc.flat) &#123;</div><div class="line">      assert(joined.right.output.length == <span class="number">1</span>)</div><div class="line">      <span class="type">Alias</span>(joined.right.output.head, <span class="string">"_2"</span>)()</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="type">Alias</span>(<span class="type">CreateStruct</span>(joined.right.output), <span class="string">"_2"</span>)()</div><div class="line">    &#125;</div><div class="line">    <span class="type">Project</span>(combined :: <span class="type">Nil</span>, joined.right)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// Rewrites the join condition to make the attribute point to correct column/field, after we</span></div><div class="line">  <span class="comment">// combine the outputs of each join side.</span></div><div class="line">  <span class="comment">// 在将每个连接的输出组合在一起之后,重写联接条件，使属性指向正确的列/字段。</span></div><div class="line"></div><div class="line">  <span class="keyword">val</span> conditionExpr = joined.condition.get transformUp &#123;</div><div class="line">    <span class="keyword">case</span> a: <span class="type">Attribute</span> <span class="keyword">if</span> joined.left.outputSet.contains(a) =&gt;</div><div class="line">      <span class="keyword">if</span> (<span class="keyword">this</span>.exprEnc.flat) &#123;</div><div class="line">        left.output.head</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="keyword">val</span> index = joined.left.output.indexWhere(_.exprId == a.exprId)</div><div class="line">        <span class="type">GetStructField</span>(left.output.head, index)</div><div class="line">      &#125;</div><div class="line">    <span class="keyword">case</span> a: <span class="type">Attribute</span> <span class="keyword">if</span> joined.right.outputSet.contains(a) =&gt;</div><div class="line">      <span class="keyword">if</span> (other.exprEnc.flat) &#123;</div><div class="line">        right.output.head</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="keyword">val</span> index = joined.right.output.indexWhere(_.exprId == a.exprId)</div><div class="line">        <span class="type">GetStructField</span>(right.output.head, index)</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> tuple2Encoder: <span class="type">Encoder</span>[(<span class="type">T</span>, <span class="type">U</span>)] =</div><div class="line">    <span class="type">ExpressionEncoder</span>.tuple(<span class="keyword">this</span>.exprEnc, other.exprEnc)</div><div class="line"></div><div class="line">  withTypedPlan(<span class="type">Join</span>(left, right, joined.joinType, <span class="type">Some</span>(conditionExpr)))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * :: Experimental ::</div><div class="line">  * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair</div><div class="line">  * where `condition` evaluates to true.</div><div class="line">  *</div><div class="line">  * 使用内部的等连接加入这个数据集，为每一对返回一个“Tuple2”，其中“条件”的计算结果为true。</div><div class="line">  *</div><div class="line">  * @param other     Right side of the join.</div><div class="line">  * @param condition Join expression.</div><div class="line">  * @group typedrel</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="meta">@Experimental</span></div><div class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">joinWith</span></span>[<span class="type">U</span>](other: <span class="type">Dataset</span>[<span class="type">U</span>], condition: <span class="type">Column</span>): <span class="type">Dataset</span>[(<span class="type">T</span>, <span class="type">U</span>)] = &#123;</div><div class="line">  joinWith(other, condition, <span class="string">"inner"</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="sortWithinPartitions"><a href="#sortWithinPartitions" class="headerlink" title="sortWithinPartitions"></a>sortWithinPartitions</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new Dataset with each partition sorted by the given expressions.</div><div class="line">  *</div><div class="line">  * 返回一个新的数据集，每个分区按照给定的表达式排序。</div><div class="line">  *</div><div class="line">  * This is the same operation as "SORT BY" in SQL (Hive QL).</div><div class="line">  *</div><div class="line">  * 这与SQL(Hive QL)中“SORT BY”的操作相同。</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="meta">@scala</span>.annotation.varargs</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortWithinPartitions</span></span>(sortCol: <span class="type">String</span>, sortCols: <span class="type">String</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</div><div class="line">  sortWithinPartitions((sortCol +: sortCols).map(<span class="type">Column</span>(_)): _*)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new Dataset with each partition sorted by the given expressions.</div><div class="line">  *</div><div class="line">  * 返回一个新的数据集，每个分区按照给定的表达式排序。</div><div class="line">  *</div><div class="line">  * This is the same operation as "SORT BY" in SQL (Hive QL).</div><div class="line">  *</div><div class="line">  * 这与SQL(Hive QL)中“SORT BY”的操作相同。</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="meta">@scala</span>.annotation.varargs</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortWithinPartitions</span></span>(sortExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</div><div class="line">  sortInternal(global = <span class="literal">false</span>, sortExprs)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">    * Returns a new Dataset sorted by the specified column, all in ascending order.</div><div class="line">    * 排序 升序</div><div class="line">    * &#123;&#123;&#123;</div><div class="line">    *   // The following 3 are equivalent</div><div class="line">    *   下面3个是等价的</div><div class="line">    *   ds.sort("sortcol")</div><div class="line">    *   ds.sort($"sortcol")</div><div class="line">    *   ds.sort($"sortcol".asc)</div><div class="line">    * &#125;&#125;&#125;</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="meta">@scala</span>.annotation.varargs</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sort</span></span>(sortCol: <span class="type">String</span>, sortCols: <span class="type">String</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</div><div class="line">    sort((sortCol +: sortCols).map(apply): _*)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">    * Returns a new Dataset sorted by the given expressions. For example:</div><div class="line">    *</div><div class="line">    * 返回一个由给定表达式排序的新数据集。例如:</div><div class="line">    *</div><div class="line">    * &#123;&#123;&#123;</div><div class="line">    *   ds.sort($"col1", $"col2".desc)</div><div class="line">    * &#125;&#125;&#125;</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="meta">@scala</span>.annotation.varargs</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sort</span></span>(sortExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</div><div class="line">    sortInternal(global = <span class="literal">true</span>, sortExprs)</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h3 id="orderBy"><a href="#orderBy" class="headerlink" title="orderBy"></a>orderBy</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">   * Returns a new Dataset sorted by the given expressions.</div><div class="line">   * This is an alias of the `sort` function.</div><div class="line">   * 这是“sort”函数的别名。</div><div class="line">   *</div><div class="line">   * @group typedrel</div><div class="line">   * @since 2.0.0</div><div class="line">   */</div><div class="line"> <span class="meta">@scala</span>.annotation.varargs</div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">orderBy</span></span>(sortCol: <span class="type">String</span>, sortCols: <span class="type">String</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = sort(sortCol, sortCols: _*)</div><div class="line"></div><div class="line"> <span class="comment">/**</span></div><div class="line">   * Returns a new Dataset sorted by the given expressions.</div><div class="line">   * This is an alias of the `sort` function.</div><div class="line">   * 这是“sort”函数的别名。</div><div class="line">   *</div><div class="line">   * @group typedrel</div><div class="line">   * @since 2.0.0</div><div class="line">   */</div><div class="line"> <span class="meta">@scala</span>.annotation.varargs</div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">orderBy</span></span>(sortExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = sort(sortExprs: _*)</div></pre></td></tr></table></figure>
<h3 id="as-1"><a href="#as-1" class="headerlink" title="as"></a>as</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">    * Returns a new Dataset with an alias set.</div><div class="line">    *</div><div class="line">    * 返回一个具有别名集的新数据集。</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 1.6.0</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">as</span></span>(alias: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</div><div class="line">    <span class="type">SubqueryAlias</span>(alias, logicalPlan, <span class="type">None</span>)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">    * (Scala-specific) Returns a new Dataset with an alias set.</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">as</span></span>(alias: <span class="type">Symbol</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = as(alias.name)</div></pre></td></tr></table></figure>
<h3 id="alias"><a href="#alias" class="headerlink" title="alias"></a>alias</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">    * Returns a new Dataset with an alias set. Same as `as`.</div><div class="line">    * 返回一个具有别名集的新数据集。与“as”相同。</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">alias</span></span>(alias: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = as(alias)</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">    * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">alias</span></span>(alias: <span class="type">Symbol</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = as(alias)</div></pre></td></tr></table></figure>
<h3 id="select-1"><a href="#select-1" class="headerlink" title="select"></a>select</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">    * :: Experimental ::</div><div class="line">    * Returns a new Dataset by computing the given [[Column]] expression for each element.</div><div class="line">    *</div><div class="line">    * 通过计算每个元素的给定[[列]]表达式返回一个新的数据集。</div><div class="line">    *</div><div class="line">    * &#123;&#123;&#123;</div><div class="line">    *   val ds = Seq(1, 2, 3).toDS()</div><div class="line">    *   val newDS = ds.select(expr("value + 1").as[Int])</div><div class="line">    * &#125;&#125;&#125;</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 1.6.0</div><div class="line">    */</div><div class="line">  <span class="meta">@Experimental</span></div><div class="line">  <span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>[<span class="type">U1</span>](c1: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U1</span>]): <span class="type">Dataset</span>[<span class="type">U1</span>] = &#123;</div><div class="line">    <span class="keyword">implicit</span> <span class="keyword">val</span> encoder = c1.encoder</div><div class="line">    <span class="keyword">val</span> project = <span class="type">Project</span>(c1.withInputType(exprEnc, logicalPlan.output).named :: <span class="type">Nil</span>,</div><div class="line">      logicalPlan)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (encoder.flat) &#123;</div><div class="line">      <span class="keyword">new</span> <span class="type">Dataset</span>[<span class="type">U1</span>](sparkSession, project, encoder)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// Flattens inner fields of U1</span></div><div class="line">      <span class="comment">// 使U1的内部区域变平</span></div><div class="line">      <span class="keyword">new</span> <span class="type">Dataset</span>[<span class="type">Tuple1</span>[<span class="type">U1</span>]](sparkSession, project, <span class="type">ExpressionEncoder</span>.tuple(encoder)).map(_._1)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="comment">/**</span></div><div class="line">    * :: Experimental ::</div><div class="line">    * Returns a new Dataset by computing the given [[Column]] expressions for each element.</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 1.6.0</div><div class="line">    */</div><div class="line">  <span class="meta">@Experimental</span></div><div class="line">  <span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>[<span class="type">U1</span>, <span class="type">U2</span>](c1: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U1</span>], c2: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U2</span>]): <span class="type">Dataset</span>[(<span class="type">U1</span>, <span class="type">U2</span>)] =</div><div class="line">    selectUntyped(c1, c2).asInstanceOf[<span class="type">Dataset</span>[(<span class="type">U1</span>, <span class="type">U2</span>)]]</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">    * :: Experimental ::</div><div class="line">    * Returns a new Dataset by computing the given [[Column]] expressions for each element.</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 1.6.0</div><div class="line">    */</div><div class="line">  <span class="meta">@Experimental</span></div><div class="line">  <span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>[<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>](</div><div class="line">                          c1: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U1</span>],</div><div class="line">                          c2: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U2</span>],</div><div class="line">                          c3: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U3</span>]): <span class="type">Dataset</span>[(<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>)] =</div><div class="line">    selectUntyped(c1, c2, c3).asInstanceOf[<span class="type">Dataset</span>[(<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>)]]</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">    * :: Experimental ::</div><div class="line">    * Returns a new Dataset by computing the given [[Column]] expressions for each element.</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 1.6.0</div><div class="line">    */</div><div class="line">  <span class="meta">@Experimental</span></div><div class="line">  <span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>[<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>, <span class="type">U4</span>](</div><div class="line">                              c1: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U1</span>],</div><div class="line">                              c2: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U2</span>],</div><div class="line">                              c3: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U3</span>],</div><div class="line">                              c4: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U4</span>]): <span class="type">Dataset</span>[(<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>, <span class="type">U4</span>)] =</div><div class="line">    selectUntyped(c1, c2, c3, c4).asInstanceOf[<span class="type">Dataset</span>[(<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>, <span class="type">U4</span>)]]</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">    * :: Experimental ::</div><div class="line">    * Returns a new Dataset by computing the given [[Column]] expressions for each element.</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 1.6.0</div><div class="line">    */</div><div class="line">  <span class="meta">@Experimental</span></div><div class="line">  <span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>[<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>, <span class="type">U4</span>, <span class="type">U5</span>](</div><div class="line">                                  c1: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U1</span>],</div><div class="line">                                  c2: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U2</span>],</div><div class="line">                                  c3: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U3</span>],</div><div class="line">                                  c4: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U4</span>],</div><div class="line">                                  c5: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U5</span>]): <span class="type">Dataset</span>[(<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>, <span class="type">U4</span>, <span class="type">U5</span>)] =</div><div class="line">    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[<span class="type">Dataset</span>[(<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>, <span class="type">U4</span>, <span class="type">U5</span>)]]</div></pre></td></tr></table></figure>
<h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">    * Filters rows using the given condition.</div><div class="line">    *</div><div class="line">    * 用给定的条件过滤rows</div><div class="line">    *</div><div class="line">    * &#123;&#123;&#123;</div><div class="line">    *   // The following are equivalent:</div><div class="line">    *   以下是等价的：</div><div class="line">    *   peopleDs.filter($"age" &gt; 15)</div><div class="line">    *   peopleDs.where($"age" &gt; 15)</div><div class="line">    * &#125;&#125;&#125;</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 1.6.0</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(condition: <span class="type">Column</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</div><div class="line">    <span class="type">Filter</span>(condition.expr, logicalPlan)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">    * Filters rows using the given SQL expression.</div><div class="line">    *</div><div class="line">    * 用给定的 SQL 表达式 过滤rows</div><div class="line">    *</div><div class="line">    * &#123;&#123;&#123;</div><div class="line">    *   peopleDs.filter("age &gt; 15")</div><div class="line">    * &#125;&#125;&#125;</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 1.6.0</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(conditionExpr: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</div><div class="line">    filter(<span class="type">Column</span>(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h3 id="where"><a href="#where" class="headerlink" title="where"></a>where</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">   * Filters rows using the given condition. This is an alias for `filter`.</div><div class="line">   *</div><div class="line">   * 使用给定条件过滤行。</div><div class="line">   * 这是“filter”的别名。</div><div class="line">   *</div><div class="line">   * &#123;&#123;&#123;</div><div class="line">   *   // The following are equivalent:</div><div class="line">   *   peopleDs.filter($"age" &gt; 15)</div><div class="line">   *   peopleDs.where($"age" &gt; 15)</div><div class="line">   * &#125;&#125;&#125;</div><div class="line">   *</div><div class="line">   * @group typedrel</div><div class="line">   * @since 1.6.0</div><div class="line">   */</div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">where</span></span>(condition: <span class="type">Column</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = filter(condition)</div><div class="line"></div><div class="line"> <span class="comment">/**</span></div><div class="line">   * Filters rows using the given SQL expression.</div><div class="line">   *</div><div class="line">   * 使用给定的 SQL 表达式  过滤 rows</div><div class="line">   *</div><div class="line">   * &#123;&#123;&#123;</div><div class="line">   *   peopleDs.where("age &gt; 15")</div><div class="line">   * &#125;&#125;&#125;</div><div class="line">   *</div><div class="line">   * @group typedrel</div><div class="line">   * @since 1.6.0</div><div class="line">   */</div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">where</span></span>(conditionExpr: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</div><div class="line">   filter(<span class="type">Column</span>(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * :: Experimental ::</div><div class="line">  * (Scala-specific)</div><div class="line">  * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.</div><div class="line">  * 返回一个[[KeyValueGroupedDataset]]，数据由给定键' func '分组。</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="meta">@Experimental</span></div><div class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>[<span class="type">K</span>: <span class="type">Encoder</span>](func: <span class="type">T</span> =&gt; <span class="type">K</span>): <span class="type">KeyValueGroupedDataset</span>[<span class="type">K</span>, <span class="type">T</span>] = &#123;</div><div class="line">  <span class="keyword">val</span> inputPlan = logicalPlan</div><div class="line">  <span class="keyword">val</span> withGroupingKey = <span class="type">AppendColumns</span>(func, inputPlan)</div><div class="line">  <span class="keyword">val</span> executed = sparkSession.sessionState.executePlan(withGroupingKey)</div><div class="line"></div><div class="line">  <span class="keyword">new</span> <span class="type">KeyValueGroupedDataset</span>(</div><div class="line">    encoderFor[<span class="type">K</span>],</div><div class="line">    encoderFor[<span class="type">T</span>],</div><div class="line">    executed,</div><div class="line">    inputPlan.output,</div><div class="line">    withGroupingKey.newColumns)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * :: Experimental ::</div><div class="line">  * (Java-specific)</div><div class="line">  * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.</div><div class="line">  * 返回一个[[KeyValueGroupedDataset]]，数据由给定键' func '分组。</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="meta">@Experimental</span></div><div class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>[<span class="type">K</span>](func: <span class="type">MapFunction</span>[<span class="type">T</span>, <span class="type">K</span>], encoder: <span class="type">Encoder</span>[<span class="type">K</span>]): <span class="type">KeyValueGroupedDataset</span>[<span class="type">K</span>, <span class="type">T</span>] =</div><div class="line">  groupByKey(func.call(_))(encoder)</div></pre></td></tr></table></figure>
<h3 id="limit"><a href="#limit" class="headerlink" title="limit"></a>limit</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new Dataset by taking the first `n` rows. The difference between this function</div><div class="line">  * and `head` is that `head` is an action and returns an array (by triggering query execution)</div><div class="line">  * while `limit` returns a new Dataset.</div><div class="line">  *</div><div class="line">  * 通过使用第一个“n”行返回一个新的数据集。</div><div class="line">  * 这个函数和“head”的区别在于“head”是一个动作，</div><div class="line">  * 并返回一个数组(通过触发查询执行)，而“limit”则返回一个新的数据集。</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">limit</span></span>(n: <span class="type">Int</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</div><div class="line">  <span class="type">Limit</span>(<span class="type">Literal</span>(n), logicalPlan)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="unionAll-已过时"><a href="#unionAll-已过时" class="headerlink" title="unionAll-已过时"></a>unionAll-已过时</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new Dataset containing union of rows in this Dataset and another Dataset.</div><div class="line">  * This is equivalent to `UNION ALL` in SQL.</div><div class="line">  *</div><div class="line">  * 返回一个新的数据集，该数据集包含该数据集中的行和另一个数据集。</div><div class="line">  * 这相当于SQL中的“UNION ALL”。</div><div class="line">  *</div><div class="line">  * To do a SQL-style set union (that does deduplication of elements), use this function followed</div><div class="line">  * by a [[distinct]].</div><div class="line">  *</div><div class="line">  * 如果需要去重的话，在该方法后继续直接  [[distinct]]</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 2.0.0 已经过时</div><div class="line">  */</div><div class="line"><span class="meta">@deprecated</span>(<span class="string">"use union()"</span>, <span class="string">"2.0.0"</span>)</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">unionAll</span></span>(other: <span class="type">Dataset</span>[<span class="type">T</span>]): <span class="type">Dataset</span>[<span class="type">T</span>] = union(other)</div></pre></td></tr></table></figure>
<h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new Dataset containing union of rows in this Dataset and another Dataset.</div><div class="line">  * This is equivalent to `UNION ALL` in SQL.</div><div class="line">  *</div><div class="line">  * 返回一个新的数据集，该数据集包含该数据集中的行和另一个数据集。</div><div class="line">  * 这相当于SQL中的“UNION ALL”。</div><div class="line">  *</div><div class="line">  * To do a SQL-style set union (that does deduplication of elements), use this function followed</div><div class="line">  * by a [[distinct]].</div><div class="line">  *</div><div class="line">  * 如果需要去重的话，在该方法后继续直接  [[distinct]]</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">union</span></span>(other: <span class="type">Dataset</span>[<span class="type">T</span>]): <span class="type">Dataset</span>[<span class="type">T</span>] = withSetOperator &#123;</div><div class="line">  <span class="comment">// This breaks caching, but it's usually ok because it addresses a very specific use case:</span></div><div class="line">  <span class="comment">// using union to union many files or partitions.</span></div><div class="line">  <span class="comment">// 这打破了缓存，但通常是可以的，因为它解决了一个非常具体的用例:使用union来联合许多文件或分区。</span></div><div class="line">  <span class="type">CombineUnions</span>(<span class="type">Union</span>(logicalPlan, other.logicalPlan))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="intersect-交集"><a href="#intersect-交集" class="headerlink" title="intersect-交集"></a>intersect-交集</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new Dataset containing rows only in both this Dataset and another Dataset.</div><div class="line">  * This is equivalent to `INTERSECT` in SQL.</div><div class="line">  *</div><div class="line">  * 返回一个新的数据集，只包含该数据集和另一个数据集相同的行.</div><div class="line">  * 这相当于在SQL中“INTERSECT”。</div><div class="line">  * 会去重.</div><div class="line">  *</div><div class="line">  * @note Equality checking is performed directly on the encoded representation of the data</div><div class="line">  *       and thus is not affected by a custom `equals` function defined on `T`.</div><div class="line">  *</div><div class="line">  *       等式检查直接执行数据的编码表示，因此不受定义为“T”的自定义“equals”函数的影响。</div><div class="line">  * @group typedrel</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">intersect</span></span>(other: <span class="type">Dataset</span>[<span class="type">T</span>]): <span class="type">Dataset</span>[<span class="type">T</span>] = withSetOperator &#123;</div><div class="line">  <span class="type">Intersect</span>(logicalPlan, other.logicalPlan)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="except-只显示另个Dataset中没有的值"><a href="#except-只显示另个Dataset中没有的值" class="headerlink" title="except-只显示另个Dataset中没有的值"></a>except-只显示另个Dataset中没有的值</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new Dataset containing rows in this Dataset but not in another Dataset.</div><div class="line">  * This is equivalent to `EXCEPT` in SQL.</div><div class="line">  *</div><div class="line">  * 返回一个新的数据集，该数据集包含该数据集中的行，而不是在另一个数据集。</div><div class="line">  * 这等价于SQL中的“EXCEPT”。</div><div class="line">  * 会去重.</div><div class="line">  *</div><div class="line">  * @note Equality checking is performed directly on the encoded representation of the data</div><div class="line">  *       and thus is not affected by a custom `equals` function defined on `T`.</div><div class="line">  * @group typedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">except</span></span>(other: <span class="type">Dataset</span>[<span class="type">T</span>]): <span class="type">Dataset</span>[<span class="type">T</span>] = withSetOperator &#123;</div><div class="line">  <span class="type">Except</span>(logicalPlan, other.logicalPlan)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="sample-随机抽样"><a href="#sample-随机抽样" class="headerlink" title="sample-随机抽样"></a>sample-随机抽样</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.</div><div class="line">  *</div><div class="line">  * 通过使用用户提供的种子，通过抽样的方式返回一个新的[[Dataset]]。</div><div class="line">  *</div><div class="line">  * @param withReplacement Sample with replacement or not.</div><div class="line">  *                        样本已经取过的值是否放回</div><div class="line">  * @param fraction        Fraction of rows to generate.</div><div class="line">  *                        每一行数据被取样的概率</div><div class="line">  * @param seed            Seed for sampling.</div><div class="line">  *                        取样种子（与随机数生成有关）</div><div class="line">  * @note This is NOT guaranteed to provide exactly the fraction of the count</div><div class="line">  *       of the given [[Dataset]].</div><div class="line">  *       不能保证准确的按照给定的分数取样。（一般结果会在概率值*总数左右）</div><div class="line">  * @group typedrel</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(withReplacement: <span class="type">Boolean</span>, fraction: <span class="type">Double</span>, seed: <span class="type">Long</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</div><div class="line">  require(fraction &gt;= <span class="number">0</span>,</div><div class="line">    <span class="string">s"Fraction must be nonnegative, but got <span class="subst">$&#123;fraction&#125;</span>"</span>)</div><div class="line"></div><div class="line">  withTypedPlan &#123;</div><div class="line">    <span class="type">Sample</span>(<span class="number">0.0</span>, fraction, withReplacement, seed, logicalPlan)()</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.</div><div class="line">  *</div><div class="line">  * 通过程序随机的种子，抽样返回新的DataSet</div><div class="line">  *</div><div class="line">  * @param withReplacement Sample with replacement or not.</div><div class="line">  *                        取样结果是否放回</div><div class="line">  * @param fraction        Fraction of rows to generate.</div><div class="line">  *                        每行数据被取样的概率</div><div class="line">  * @note This is NOT guaranteed to provide exactly the fraction of the total count</div><div class="line">  *       of the given [[Dataset]].</div><div class="line">  *       不能保证准确的按照给定的分数取样。（一般结果会在概率值*总数左右）</div><div class="line">  * @group typedrel</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(withReplacement: <span class="type">Boolean</span>, fraction: <span class="type">Double</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</div><div class="line">  sample(withReplacement, fraction, <span class="type">Utils</span>.random.nextLong)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="randomSplit-按照权重分割"><a href="#randomSplit-按照权重分割" class="headerlink" title="randomSplit-按照权重分割"></a>randomSplit-按照权重分割</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">    * Randomly splits this Dataset with the provided weights.</div><div class="line">    *</div><div class="line">    * 随机将此数据集按照所提供的权重进行分割。</div><div class="line">    *</div><div class="line">    * @param weights weights for splits, will be normalized if they don't sum to 1.</div><div class="line">    *                切分的权重。如果和不为1就会被标准化。</div><div class="line">    * @param seed    Seed for sampling.</div><div class="line">    *                取样的种子（影响随机数生成器）</div><div class="line">    *</div><div class="line">    *                For Java API, use [[randomSplitAsList]].</div><div class="line">    *                Java API 使用 [[randomSplitAsList]].</div><div class="line">    * @group typedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">randomSplit</span></span>(weights: <span class="type">Array</span>[<span class="type">Double</span>], seed: <span class="type">Long</span>): <span class="type">Array</span>[<span class="type">Dataset</span>[<span class="type">T</span>]] = &#123;</div><div class="line">    require(weights.forall(_ &gt;= <span class="number">0</span>),</div><div class="line">      <span class="string">s"Weights must be nonnegative, but got <span class="subst">$&#123;weights.mkString("[", ",", "]")&#125;</span>"</span>)</div><div class="line">    require(weights.sum &gt; <span class="number">0</span>,</div><div class="line">      <span class="string">s"Sum of weights must be positive, but got <span class="subst">$&#123;weights.mkString("[", ",", "]")&#125;</span>"</span>)</div><div class="line"></div><div class="line">    <span class="comment">// It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its</span></div><div class="line">    <span class="comment">// constituent partitions each time a split is materialized which could result in</span></div><div class="line">    <span class="comment">// overlapping splits. To prevent this, we explicitly sort each input partition to make the</span></div><div class="line">    <span class="comment">// ordering deterministic.</span></div><div class="line">    <span class="comment">// MapType cannot be sorted.</span></div><div class="line">    <span class="keyword">val</span> sorted = <span class="type">Sort</span>(logicalPlan.output.filterNot(_.dataType.isInstanceOf[<span class="type">MapType</span>])</div><div class="line">      .map(<span class="type">SortOrder</span>(_, <span class="type">Ascending</span>)), global = <span class="literal">false</span>, logicalPlan)</div><div class="line">    <span class="keyword">val</span> sum = weights.sum</div><div class="line">    <span class="comment">// scanLeft 从右到右依次累计算 scanLeft(0.0d)(_+_): (0.0,(0.0+0.2),(0.0+0.2+0.8))</span></div><div class="line">    <span class="keyword">val</span> normalizedCumWeights = weights.map(_ / sum).scanLeft(<span class="number">0.0</span>d)(_ + _)</div><div class="line">    <span class="comment">// sliding(n) 每次取n个值，以步长为1向右滑动，如：(0.0,0.2,0.8).sliding(2)=(0.0,0.2),(0.2,0.8)</span></div><div class="line">    normalizedCumWeights.sliding(<span class="number">2</span>).map &#123; x =&gt;</div><div class="line">      <span class="keyword">new</span> <span class="type">Dataset</span>[<span class="type">T</span>](</div><div class="line">        sparkSession, <span class="type">Sample</span>(x(<span class="number">0</span>), x(<span class="number">1</span>), withReplacement = <span class="literal">false</span>, seed, sorted)(), encoder)</div><div class="line">    &#125;.toArray</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">    <span class="comment">/**</span></div><div class="line">    * Randomly splits this Dataset with the provided weights.</div><div class="line">    *</div><div class="line">    * 程序自动生成随机数种子，随机将此数据集按照所提供的权重进行分割。</div><div class="line">    *</div><div class="line">    * @param weights weights for splits, will be normalized if they don't sum to 1.</div><div class="line">    *                切分的权重。如果和不为1就会被标准化。</div><div class="line">    * @group typedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">randomSplit</span></span>(weights: <span class="type">Array</span>[<span class="type">Double</span>]): <span class="type">Array</span>[<span class="type">Dataset</span>[<span class="type">T</span>]] = &#123;</div><div class="line">    randomSplit(weights, <span class="type">Utils</span>.random.nextLong)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">    * Randomly splits this Dataset with the provided weights. Provided for the Python Api.</div><div class="line">    * Python 使用该方法</div><div class="line">    *</div><div class="line">    * @param weights weights for splits, will be normalized if they don't sum to 1.</div><div class="line">    * @param seed    Seed for sampling.</div><div class="line">    */</div><div class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">randomSplit</span></span>(weights: <span class="type">List</span>[<span class="type">Double</span>], seed: <span class="type">Long</span>): <span class="type">Array</span>[<span class="type">Dataset</span>[<span class="type">T</span>]] = &#123;</div><div class="line">    randomSplit(weights.toArray, seed)</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h3 id="randomSplitAsList"><a href="#randomSplitAsList" class="headerlink" title="randomSplitAsList"></a>randomSplitAsList</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a Java list that contains randomly split Dataset with the provided weights.</div><div class="line">  *</div><div class="line">  * 根据提供的权重分割DataFrames，返回Java list</div><div class="line">  *</div><div class="line">  * @param weights weights for splits, will be normalized if they don't sum to 1.</div><div class="line">  *                切分的权重。如果和不为1就会被标准化。</div><div class="line">  * @param seed    Seed for sampling.</div><div class="line">  *                取样的种子（影响随机数生成器）</div><div class="line">  * @group typedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomSplitAsList</span></span>(weights: <span class="type">Array</span>[<span class="type">Double</span>], seed: <span class="type">Long</span>): java.util.<span class="type">List</span>[<span class="type">Dataset</span>[<span class="type">T</span>]] = &#123;</div><div class="line">  <span class="keyword">val</span> values = randomSplit(weights, seed)</div><div class="line">  java.util.<span class="type">Arrays</span>.asList(values: _*)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="dropDuplicates-去重"><a href="#dropDuplicates-去重" class="headerlink" title="dropDuplicates-去重"></a>dropDuplicates-去重</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new Dataset that contains only the unique rows from this Dataset.</div><div class="line">  * This is an alias for `distinct`.</div><div class="line">  *</div><div class="line">  * 删除重复的row数据，是distinct的别名</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropDuplicates</span></span>(): <span class="type">Dataset</span>[<span class="type">T</span>] = dropDuplicates(<span class="keyword">this</span>.columns)</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only</div><div class="line">  * the subset of columns.</div><div class="line">  *</div><div class="line">  * 只删除指定列的重复数据</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropDuplicates</span></span>(colNames: <span class="type">Seq</span>[<span class="type">String</span>]): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</div><div class="line">  <span class="keyword">val</span> resolver = sparkSession.sessionState.analyzer.resolver</div><div class="line">  <span class="keyword">val</span> allColumns = queryExecution.analyzed.output</div><div class="line">  <span class="keyword">val</span> groupCols = colNames.flatMap &#123; colName =&gt;</div><div class="line">    <span class="comment">// It is possibly there are more than one columns with the same name,</span></div><div class="line">    <span class="comment">// so we call filter instead of find.</span></div><div class="line">    <span class="keyword">val</span> cols = allColumns.filter(col =&gt; resolver(col.name, colName))</div><div class="line">    <span class="keyword">if</span> (cols.isEmpty) &#123;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AnalysisException</span>(</div><div class="line">        <span class="string">s""</span><span class="string">"Cannot resolve column name "</span>$colN<span class="string">ame" among (<span class="subst">$&#123;schema.fieldNames.mkString(", ")&#125;</span>)"</span><span class="string">""</span>)</div><div class="line">    &#125;</div><div class="line">    cols</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">val</span> groupColExprIds = groupCols.map(_.exprId)</div><div class="line">  <span class="keyword">val</span> aggCols = logicalPlan.output.map &#123; attr =&gt;</div><div class="line">    <span class="keyword">if</span> (groupColExprIds.contains(attr.exprId)) &#123;</div><div class="line">      attr</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// Removing duplicate rows should not change output attributes. We should keep</span></div><div class="line">      <span class="comment">// the original exprId of the attribute. Otherwise, to select a column in original</span></div><div class="line">      <span class="comment">// dataset will cause analysis exception due to unresolved attribute.</span></div><div class="line">      <span class="comment">// 删除重复行不应该更改输出属性。</span></div><div class="line">      <span class="comment">// 我们应该保留这个属性的原始属性。</span></div><div class="line">      <span class="comment">// 否则，在原始数据集中选择一个列将导致分析异常，原因是未解析的属性。</span></div><div class="line">      <span class="type">Alias</span>(<span class="keyword">new</span> <span class="type">First</span>(attr).toAggregateExpression(), attr.name)(exprId = attr.exprId)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="type">Aggregate</span>(groupCols, aggCols, logicalPlan)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new Dataset with duplicate rows removed, considering only</div><div class="line">  * the subset of columns.</div><div class="line">  *</div><div class="line">  * 只针对特定列做去重</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropDuplicates</span></span>(colNames: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Dataset</span>[<span class="type">T</span>] = dropDuplicates(colNames.toSeq)</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new [[Dataset]] with duplicate rows removed, considering only</div><div class="line">  * the subset of columns.</div><div class="line">  *</div><div class="line">  * 只针对特定多列做去重</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 2.0.0</div><div class="line">  */</div><div class="line"><span class="meta">@scala</span>.annotation.varargs</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropDuplicates</span></span>(col1: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</div><div class="line">  <span class="keyword">val</span> colNames: <span class="type">Seq</span>[<span class="type">String</span>] = col1 +: cols</div><div class="line">  dropDuplicates(colNames)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="transform-自定义转换"><a href="#transform-自定义转换" class="headerlink" title="transform-自定义转换"></a>transform-自定义转换</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Concise syntax for chaining custom transformations.</div><div class="line">  *</div><div class="line">  * 用于链接自定义转换的简明语法。</div><div class="line">  *</div><div class="line">  * &#123;&#123;&#123;</div><div class="line">  *   def featurize(ds: Dataset[T]): Dataset[U] = ...</div><div class="line">  *</div><div class="line">  *   ds</div><div class="line">  *     .transform(featurize)</div><div class="line">  *     .transform(...)</div><div class="line">  * &#125;&#125;&#125;</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span></span>[<span class="type">U</span>](t: <span class="type">Dataset</span>[<span class="type">T</span>] =&gt; <span class="type">Dataset</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>] = t(<span class="keyword">this</span>)</div></pre></td></tr></table></figure>
<h3 id="filter-过滤"><a href="#filter-过滤" class="headerlink" title="filter-过滤"></a>filter-过滤</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">    * :: Experimental ::</div><div class="line">    * (Scala-specific)</div><div class="line">    * Returns a new Dataset that only contains elements where `func` returns `true`.</div><div class="line">    *</div><div class="line">    * 该数据集只包含“func”返回“true”的元素。</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 1.6.0</div><div class="line">    */</div><div class="line">  <span class="meta">@Experimental</span></div><div class="line">  <span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(func: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</div><div class="line">    withTypedPlan(<span class="type">TypedFilter</span>(func, logicalPlan))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">    * :: Experimental ::</div><div class="line">    * (Java-specific)</div><div class="line">    * Returns a new Dataset that only contains elements where `func` returns `true`.</div><div class="line">    *</div><div class="line">    * 返回一个新数据集，该数据集只包含“func”返回“true”的元素。</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 1.6.0</div><div class="line">    */</div><div class="line">  <span class="meta">@Experimental</span></div><div class="line">  <span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(func: <span class="type">FilterFunction</span>[<span class="type">T</span>]): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</div><div class="line">    withTypedPlan(<span class="type">TypedFilter</span>(func, logicalPlan))</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * :: Experimental ::</div><div class="line">  * (Scala-specific)</div><div class="line">  * Returns a new Dataset that contains the result of applying `func` to each element.</div><div class="line">  *</div><div class="line">  * 返回一个新的数据集，该数据集包含对每个元素应用“func”的结果。</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="meta">@Experimental</span></div><div class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">Encoder</span>](func: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">Dataset</span>[<span class="type">U</span>] = withTypedPlan &#123;</div><div class="line">  <span class="type">MapElements</span>[<span class="type">T</span>, <span class="type">U</span>](func, logicalPlan)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * :: Experimental ::</div><div class="line">  * (Java-specific)</div><div class="line">  * Returns a new Dataset that contains the result of applying `func` to each element.</div><div class="line">  *</div><div class="line">  * 返回一个新的数据集，该数据集包含对每个元素应用“func”的结果。</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="meta">@Experimental</span></div><div class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>](func: <span class="type">MapFunction</span>[<span class="type">T</span>, <span class="type">U</span>], encoder: <span class="type">Encoder</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>] = &#123;</div><div class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> uEnc = encoder</div><div class="line">  withTypedPlan(<span class="type">MapElements</span>[<span class="type">T</span>, <span class="type">U</span>](func, logicalPlan))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * :: Experimental ::</div><div class="line">  * (Scala-specific)</div><div class="line">  * Returns a new Dataset that contains the result of applying `func` to each partition.</div><div class="line">  *</div><div class="line">  * 返回一个新的数据集，该数据集包含对每个分区应用“func”的结果。</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="meta">@Experimental</span></div><div class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">Encoder</span>](func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>] = &#123;</div><div class="line">  <span class="keyword">new</span> <span class="type">Dataset</span>[<span class="type">U</span>](</div><div class="line">    sparkSession,</div><div class="line">    <span class="type">MapPartitions</span>[<span class="type">T</span>, <span class="type">U</span>](func, logicalPlan),</div><div class="line">    implicitly[<span class="type">Encoder</span>[<span class="type">U</span>]])</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * :: Experimental ::</div><div class="line">  * (Java-specific)</div><div class="line">  * Returns a new Dataset that contains the result of applying `f` to each partition.</div><div class="line">  *</div><div class="line">  * 返回一个新的数据集，该数据集包含对每个分区应用“f”的结果。</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="meta">@Experimental</span></div><div class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>](f: <span class="type">MapPartitionsFunction</span>[<span class="type">T</span>, <span class="type">U</span>], encoder: <span class="type">Encoder</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>] = &#123;</div><div class="line">  <span class="keyword">val</span> func: (<span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>] = x =&gt; f.call(x.asJava).asScala</div><div class="line">  mapPartitions(func)(encoder)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="flatMap-将map结果flat扁平化"><a href="#flatMap-将map结果flat扁平化" class="headerlink" title="flatMap-将map结果flat扁平化"></a>flatMap-将map结果flat扁平化</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * :: Experimental ::</div><div class="line">  * (Scala-specific)</div><div class="line">  * Returns a new Dataset by first applying a function to all elements of this Dataset,</div><div class="line">  * and then flattening the results.</div><div class="line">  *</div><div class="line">  * 返回一个新的数据集，首先对该数据集的所有元素应用一个函数，然后将结果扁平化。</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="meta">@Experimental</span></div><div class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">Encoder</span>](func: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>] =</div><div class="line">  mapPartitions(_.flatMap(func))</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * :: Experimental ::</div><div class="line">  * (Java-specific)</div><div class="line">  * Returns a new Dataset by first applying a function to all elements of this Dataset,</div><div class="line">  * and then flattening the results.</div><div class="line">  *</div><div class="line">  * 返回一个新的数据集，首先对该数据集的所有元素应用一个函数，然后将结果扁平化。</div><div class="line">  *</div><div class="line">  * @group typedrel</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="meta">@Experimental</span></div><div class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>](f: <span class="type">FlatMapFunction</span>[<span class="type">T</span>, <span class="type">U</span>], encoder: <span class="type">Encoder</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>] = &#123;</div><div class="line">  <span class="keyword">val</span> func: (<span class="type">T</span>) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>] = x =&gt; f.call(x).asScala</div><div class="line">  flatMap(func)(encoder)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="repartition-重分区"><a href="#repartition-重分区" class="headerlink" title="repartition-重分区"></a>repartition-重分区</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">    * Returns a new Dataset that has exactly `numPartitions` partitions.</div><div class="line">    *</div><div class="line">    * 返回一个 给定分区数量的新DataSet</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 1.6.0</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(numPartitions: <span class="type">Int</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</div><div class="line">    <span class="type">Repartition</span>(numPartitions, shuffle = <span class="literal">true</span>, logicalPlan)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">    * Returns a new Dataset partitioned by the given partitioning expressions into</div><div class="line">    * `numPartitions`. The resulting Dataset is hash partitioned.</div><div class="line">    *</div><div class="line">    * 返回一个由给定的分区表达式划分为“num分区”的新数据集。</div><div class="line">    * 生成的Dataset是哈希分区的。</div><div class="line">    *</div><div class="line">    * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).</div><div class="line">    *</div><div class="line">    * 和 SQL (Hive QL) 中的 "DISTRIBUTE BY" 作用相同</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="meta">@scala</span>.annotation.varargs</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(numPartitions: <span class="type">Int</span>, partitionExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</div><div class="line">    <span class="type">RepartitionByExpression</span>(partitionExprs.map(_.expr), logicalPlan, <span class="type">Some</span>(numPartitions))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">    * Returns a new Dataset partitioned by the given partitioning expressions, using</div><div class="line">    * `spark.sql.shuffle.partitions` as number of partitions.</div><div class="line">    * The resulting Dataset is hash partitioned.</div><div class="line">    *</div><div class="line">    * 根据指定的分区表达式进行重分区。</div><div class="line">    * 分区数量由`spark.sql.shuffle.partitions` 获得。</div><div class="line">    * 结果Dataset 是哈希分区的。</div><div class="line">    *</div><div class="line">    * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).</div><div class="line">    *</div><div class="line">    * 和 SQL (Hive QL) 中的 "DISTRIBUTE BY" 作用相同</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="meta">@scala</span>.annotation.varargs</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(partitionExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</div><div class="line">    <span class="type">RepartitionByExpression</span>(partitionExprs.map(_.expr), logicalPlan, numPartitions = <span class="type">None</span>)</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h3 id="coalesce-合并分区"><a href="#coalesce-合并分区" class="headerlink" title="coalesce-合并分区"></a>coalesce-合并分区</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">    * Returns a new Dataset that has exactly `numPartitions` partitions.</div><div class="line">    * Similar to coalesce defined on an `RDD`, this operation results in a narrow dependency, e.g.</div><div class="line">    * if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of</div><div class="line">    * the 100 new partitions will claim 10 of the current partitions.</div><div class="line">    *</div><div class="line">    * 合并。</div><div class="line">    * 返回确定分区数量的Dataset。</div><div class="line">    * 和RDD中的合并方法类似，这个操作导致了一个窄依赖。</div><div class="line">    * 例如：将1000个分区合并为100个分区，这个过程没有shuffle，而是100个新分区中的每个分区将声明当前的10个分区。</div><div class="line">    *</div><div class="line">    * @group typedrel</div><div class="line">    * @since 1.6.0</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">coalesce</span></span>(numPartitions: <span class="type">Int</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</div><div class="line">    <span class="type">Repartition</span>(numPartitions, shuffle = <span class="literal">false</span>, logicalPlan)</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h3 id="distinct-去重"><a href="#distinct-去重" class="headerlink" title="distinct-去重"></a>distinct-去重</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">    * Returns a new Dataset that contains only the unique rows from this Dataset.</div><div class="line">    * This is an alias for `dropDuplicates`.</div><div class="line">    *</div><div class="line">    * 去重。</div><div class="line">    * 返回去重后的Dataset。</div><div class="line">    * 和 `dropDuplicates` 方法一致。</div><div class="line">    *</div><div class="line">    * @note Equality checking is performed directly on the encoded representation of the data</div><div class="line">    *       and thus is not affected by a custom `equals` function defined on `T`.</div><div class="line">    * @group typedrel</div><div class="line">    * @since 2.0.0</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>(): <span class="type">Dataset</span>[<span class="type">T</span>] = dropDuplicates()</div></pre></td></tr></table></figure>
<!--对不起，到时间了，请停止装逼-->
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[JavaRDDLike.scala]]></title>
      <url>https://stanxia.github.io/2017/11/21/JavaRDDLike-scala/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><div class="note info"><p>使用Java开发Spark程序，JavaRDD的功能算子中英文注释<br>JavaRDDLike的实现应该扩展这个虚拟抽象类，而不是直接继承这个特性。</p></div>
<!--请开始装逼-->
<h2 id="JavaRDD"><a href="#JavaRDD" class="headerlink" title="JavaRDD"></a>JavaRDD</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> org.apache.spark.api.java</div><div class="line"></div><div class="line"><span class="keyword">private</span>[spark] <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AbstractJavaRDDLike</span>[<span class="type">T</span>, <span class="type">This</span> &lt;: <span class="type">JavaRDDLike</span>[<span class="type">T</span>, <span class="type">This</span>]]</span></div><div class="line">  <span class="keyword">extends</span> <span class="type">JavaRDDLike</span>[<span class="type">T</span>, <span class="type">This</span>]</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Defines operations common to several Java RDD implementations.</div><div class="line">  *</div><div class="line">  * 定义几个Java RDD实现的常见操作。</div><div class="line">  *</div><div class="line">  * @note This trait is not intended to be implemented by user code.</div><div class="line">  *</div><div class="line">  *       该特性不打算由用户代码实现。</div><div class="line">  */</div><div class="line"><span class="class"><span class="keyword">trait</span> <span class="title">JavaRDDLike</span>[<span class="type">T</span>, <span class="type">This</span> &lt;: <span class="type">JavaRDDLike</span>[<span class="type">T</span>, <span class="type">This</span>]] <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">wrapRDD</span></span>(rdd: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">This</span></div><div class="line"></div><div class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> classTag: <span class="type">ClassTag</span>[<span class="type">T</span>]</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">rdd</span></span>: <span class="type">RDD</span>[<span class="type">T</span>]</div></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="partitions"><a href="#partitions" class="headerlink" title="partitions"></a>partitions</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/** Set of partitions in this RDD.</span></div><div class="line">  * 在这个RDD中设置的分区。</div><div class="line">  * */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitions</span></span>: <span class="type">JList</span>[<span class="type">Partition</span>] = rdd.partitions.toSeq.asJava</div></pre></td></tr></table></figure>
<h3 id="getNumPartitions"><a href="#getNumPartitions" class="headerlink" title="getNumPartitions"></a>getNumPartitions</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/** Return the number of partitions in this RDD.</span></div><div class="line">  * 返回该RDD中的分区数。</div><div class="line">  * */</div><div class="line"><span class="meta">@Since</span>(<span class="string">"1.6.0"</span>)</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getNumPartitions</span></span>: <span class="type">Int</span> = rdd.getNumPartitions</div></pre></td></tr></table></figure>
<h3 id="partitioner"><a href="#partitioner" class="headerlink" title="partitioner"></a>partitioner</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/** The partitioner of this RDD.</span></div><div class="line">  * 这个RDD的分区。</div><div class="line">  * */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitioner</span></span>: <span class="type">Optional</span>[<span class="type">Partitioner</span>] = <span class="type">JavaUtils</span>.optionToOptional(rdd.partitioner)</div></pre></td></tr></table></figure>
<h3 id="context"><a href="#context" class="headerlink" title="context"></a>context</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/** The [[org.apache.spark.SparkContext]] that this RDD was created on.</span></div><div class="line">  *</div><div class="line">  * 这个RDD是在[[org.apache.spark.SparkContext]]上面创建的。</div><div class="line">  * */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">context</span></span>: <span class="type">SparkContext</span> = rdd.context</div></pre></td></tr></table></figure>
<h3 id="id"><a href="#id" class="headerlink" title="id"></a>id</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/** A unique ID for this RDD (within its SparkContext).</span></div><div class="line">  * 这个RDD的惟一ID(在它的SparkContext内)。</div><div class="line">  * */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">id</span></span>: <span class="type">Int</span> = rdd.id</div></pre></td></tr></table></figure>
<h3 id="name"><a href="#name" class="headerlink" title="name"></a>name</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">name</span></span>(): <span class="type">String</span> = rdd.name</div></pre></td></tr></table></figure>
<h3 id="getStorageLevel"><a href="#getStorageLevel" class="headerlink" title="getStorageLevel"></a>getStorageLevel</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/** Get the RDD's current storage level, or StorageLevel.NONE if none is set.</span></div><div class="line">  * 获取RDD的当前存储级别，或StorageLevel。如果没有设置就没有。</div><div class="line">  * */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getStorageLevel</span></span>: <span class="type">StorageLevel</span> = rdd.getStorageLevel</div></pre></td></tr></table></figure>
<h3 id="iterator"><a href="#iterator" class="headerlink" title="iterator"></a>iterator</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Internal method to this RDD; will read from cache if applicable, or otherwise compute it.</div><div class="line">  * This should ''not'' be called by users directly, but is available for implementors of custom</div><div class="line">  * subclasses of RDD.</div><div class="line">  * 内部方法的RDD;将从缓存读取，如果适用的话，或者计算它。</div><div class="line">  * 这应该“不是”直接由用户调用，而是用于RDD的自定义子类的实现者</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>(split: <span class="type">Partition</span>, taskContext: <span class="type">TaskContext</span>): <span class="type">JIterator</span>[<span class="type">T</span>] =</div><div class="line">  rdd.iterator(split, taskContext).asJavs</div></pre></td></tr></table></figure>
<h2 id="Transformations-return-a-new-RDD"><a href="#Transformations-return-a-new-RDD" class="headerlink" title="Transformations (return a new RDD)"></a>Transformations (return a new RDD)</h2><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Return a new RDD by applying a function to all elements of this RDD.</div><div class="line">  * 将一个函数应用于这个RDD的所有元素，返回一个新的RDD。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">R</span>](f: <span class="type">JFunction</span>[<span class="type">T</span>, <span class="type">R</span>]): <span class="type">JavaRDD</span>[<span class="type">R</span>] =</div><div class="line">  <span class="keyword">new</span> <span class="type">JavaRDD</span>(rdd.map(f)(fakeClassTag))(fakeClassTag)</div></pre></td></tr></table></figure>
<h3 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Return a new RDD by applying a function to each partition of this RDD, while tracking the index</div><div class="line">  * of the original partition.</div><div class="line">  * 通过在RDD的每个分区上应用一个函数来返回一个新的RDD，同时跟踪原始分区的索引。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span></span>[<span class="type">R</span>](</div><div class="line">                               f: <span class="type">JFunction2</span>[jl.<span class="type">Integer</span>, <span class="type">JIterator</span>[<span class="type">T</span>], <span class="type">JIterator</span>[<span class="type">R</span>]],</div><div class="line">                               preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">JavaRDD</span>[<span class="type">R</span>] =</div><div class="line">  <span class="keyword">new</span> <span class="type">JavaRDD</span>(rdd.mapPartitionsWithIndex((a, b) =&gt; f.call(a, b.asJava).asScala,</div><div class="line">    preservesPartitioning)(fakeClassTag))(fakeClassTag)</div></pre></td></tr></table></figure>
<h3 id="mapToDouble"><a href="#mapToDouble" class="headerlink" title="mapToDouble"></a>mapToDouble</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Return a new RDD by applying a function to all elements of this RDD.</div><div class="line">  * 将一个函数应用于这个RDD的所有元素，返回一个新的RDD。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapToDouble</span></span>[<span class="type">R</span>](f: <span class="type">DoubleFunction</span>[<span class="type">T</span>]): <span class="type">JavaDoubleRDD</span> = &#123;</div><div class="line">  <span class="keyword">new</span> <span class="type">JavaDoubleRDD</span>(rdd.map(f.call(_).doubleValue()))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="mapToPair"><a href="#mapToPair" class="headerlink" title="mapToPair"></a>mapToPair</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Return a new RDD by applying a function to all elements of this RDD.</div><div class="line">  * 将一个函数应用于这个RDD的所有元素，返回一个新的RDD。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapToPair</span></span>[<span class="type">K2</span>, <span class="type">V2</span>](f: <span class="type">PairFunction</span>[<span class="type">T</span>, <span class="type">K2</span>, <span class="type">V2</span>]): <span class="type">JavaPairRDD</span>[<span class="type">K2</span>, <span class="type">V2</span>] = &#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cm</span></span>: <span class="type">ClassTag</span>[(<span class="type">K2</span>, <span class="type">V2</span>)] = implicitly[<span class="type">ClassTag</span>[(<span class="type">K2</span>, <span class="type">V2</span>)]]</div><div class="line">  <span class="keyword">new</span> <span class="type">JavaPairRDD</span>(rdd.map[(<span class="type">K2</span>, <span class="type">V2</span>)](f)(cm))(fakeClassTag[<span class="type">K2</span>], fakeClassTag[<span class="type">V2</span>])</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  *  Return a new RDD by first applying a function to all elements of this</div><div class="line">  *  RDD, and then flattening the results.</div><div class="line">  *  返回一个新的RDD，首先将一个函数应用于这个RDD的所有元素，然后将结果扁平化。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>](f: <span class="type">FlatMapFunction</span>[<span class="type">T</span>, <span class="type">U</span>]): <span class="type">JavaRDD</span>[<span class="type">U</span>] = &#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fn</span></span>: (<span class="type">T</span>) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>] = (x: <span class="type">T</span>) =&gt; f.call(x).asScala</div><div class="line">  <span class="type">JavaRDD</span>.fromRDD(rdd.flatMap(fn)(fakeClassTag[<span class="type">U</span>]))(fakeClassTag[<span class="type">U</span>])</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="flatMapToDouble"><a href="#flatMapToDouble" class="headerlink" title="flatMapToDouble"></a>flatMapToDouble</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  *  Return a new RDD by first applying a function to all elements of this</div><div class="line">  *  RDD, and then flattening the results.</div><div class="line">  *  返回一个新的RDD，首先将一个函数应用于这个RDD的所有元素，然后将结果扁平化。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMapToDouble</span></span>(f: <span class="type">DoubleFlatMapFunction</span>[<span class="type">T</span>]): <span class="type">JavaDoubleRDD</span> = &#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fn</span></span>: (<span class="type">T</span>) =&gt; <span class="type">Iterator</span>[jl.<span class="type">Double</span>] = (x: <span class="type">T</span>) =&gt; f.call(x).asScala</div><div class="line">  <span class="keyword">new</span> <span class="type">JavaDoubleRDD</span>(rdd.flatMap(fn).map(_.doubleValue()))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="flatMapToPair"><a href="#flatMapToPair" class="headerlink" title="flatMapToPair"></a>flatMapToPair</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  *  Return a new RDD by first applying a function to all elements of this</div><div class="line">  *  RDD, and then flattening the results.</div><div class="line">  *  返回一个新的RDD，首先将一个函数应用于这个RDD的所有元素，然后将结果扁平化。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMapToPair</span></span>[<span class="type">K2</span>, <span class="type">V2</span>](f: <span class="type">PairFlatMapFunction</span>[<span class="type">T</span>, <span class="type">K2</span>, <span class="type">V2</span>]): <span class="type">JavaPairRDD</span>[<span class="type">K2</span>, <span class="type">V2</span>] = &#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fn</span></span>: (<span class="type">T</span>) =&gt; <span class="type">Iterator</span>[(<span class="type">K2</span>, <span class="type">V2</span>)] = (x: <span class="type">T</span>) =&gt; f.call(x).asScala</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cm</span></span>: <span class="type">ClassTag</span>[(<span class="type">K2</span>, <span class="type">V2</span>)] = implicitly[<span class="type">ClassTag</span>[(<span class="type">K2</span>, <span class="type">V2</span>)]]</div><div class="line">  <span class="type">JavaPairRDD</span>.fromRDD(rdd.flatMap(fn)(cm))(fakeClassTag[<span class="type">K2</span>], fakeClassTag[<span class="type">V2</span>])</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Return a new RDD by applying a function to each partition of this RDD.</div><div class="line">  * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>](f: <span class="type">FlatMapFunction</span>[<span class="type">JIterator</span>[<span class="type">T</span>], <span class="type">U</span>]): <span class="type">JavaRDD</span>[<span class="type">U</span>] = &#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fn</span></span>: (<span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>] = &#123;</div><div class="line">    (x: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; f.call(x.asJava).asScala</div><div class="line">  &#125;</div><div class="line">  <span class="type">JavaRDD</span>.fromRDD(rdd.mapPartitions(fn)(fakeClassTag[<span class="type">U</span>]))(fakeClassTag[<span class="type">U</span>])</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Return a new RDD by applying a function to each partition of this RDD.</div><div class="line">  * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>](f: <span class="type">FlatMapFunction</span>[<span class="type">JIterator</span>[<span class="type">T</span>], <span class="type">U</span>],</div><div class="line">                     preservesPartitioning: <span class="type">Boolean</span>): <span class="type">JavaRDD</span>[<span class="type">U</span>] = &#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fn</span></span>: (<span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>] = &#123;</div><div class="line">    (x: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; f.call(x.asJava).asScala</div><div class="line">  &#125;</div><div class="line">  <span class="type">JavaRDD</span>.fromRDD(</div><div class="line">    rdd.mapPartitions(fn, preservesPartitioning)(fakeClassTag[<span class="type">U</span>]))(fakeClassTag[<span class="type">U</span>])</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="mapPartitionsToDouble"><a href="#mapPartitionsToDouble" class="headerlink" title="mapPartitionsToDouble"></a>mapPartitionsToDouble</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Return a new RDD by applying a function to each partition of this RDD.</div><div class="line">  * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsToDouble</span></span>(f: <span class="type">DoubleFlatMapFunction</span>[<span class="type">JIterator</span>[<span class="type">T</span>]]): <span class="type">JavaDoubleRDD</span> = &#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fn</span></span>: (<span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[jl.<span class="type">Double</span>] = &#123;</div><div class="line">    (x: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; f.call(x.asJava).asScala</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">new</span> <span class="type">JavaDoubleRDD</span>(rdd.mapPartitions(fn).map(_.doubleValue()))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Return a new RDD by applying a function to each partition of this RDD.</div><div class="line">  * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsToDouble</span></span>(f: <span class="type">DoubleFlatMapFunction</span>[<span class="type">JIterator</span>[<span class="type">T</span>]],</div><div class="line">                          preservesPartitioning: <span class="type">Boolean</span>): <span class="type">JavaDoubleRDD</span> = &#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fn</span></span>: (<span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[jl.<span class="type">Double</span>] = &#123;</div><div class="line">    (x: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; f.call(x.asJava).asScala</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">new</span> <span class="type">JavaDoubleRDD</span>(rdd.mapPartitions(fn, preservesPartitioning)</div><div class="line">    .map(_.doubleValue()))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="mapPartitionsToPair"><a href="#mapPartitionsToPair" class="headerlink" title="mapPartitionsToPair"></a>mapPartitionsToPair</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Return a new RDD by applying a function to each partition of this RDD.</div><div class="line">  * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsToPair</span></span>[<span class="type">K2</span>, <span class="type">V2</span>](f: <span class="type">PairFlatMapFunction</span>[<span class="type">JIterator</span>[<span class="type">T</span>], <span class="type">K2</span>, <span class="type">V2</span>]):</div><div class="line"><span class="type">JavaPairRDD</span>[<span class="type">K2</span>, <span class="type">V2</span>] = &#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fn</span></span>: (<span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[(<span class="type">K2</span>, <span class="type">V2</span>)] = &#123;</div><div class="line">    (x: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; f.call(x.asJava).asScala</div><div class="line">  &#125;</div><div class="line">  <span class="type">JavaPairRDD</span>.fromRDD(rdd.mapPartitions(fn))(fakeClassTag[<span class="type">K2</span>], fakeClassTag[<span class="type">V2</span>])</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Return a new RDD by applying a function to each partition of this RDD.</div><div class="line">  * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsToPair</span></span>[<span class="type">K2</span>, <span class="type">V2</span>](f: <span class="type">PairFlatMapFunction</span>[<span class="type">JIterator</span>[<span class="type">T</span>], <span class="type">K2</span>, <span class="type">V2</span>],</div><div class="line">                                preservesPartitioning: <span class="type">Boolean</span>): <span class="type">JavaPairRDD</span>[<span class="type">K2</span>, <span class="type">V2</span>] = &#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fn</span></span>: (<span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[(<span class="type">K2</span>, <span class="type">V2</span>)] = &#123;</div><div class="line">    (x: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; f.call(x.asJava).asScala</div><div class="line">  &#125;</div><div class="line">  <span class="type">JavaPairRDD</span>.fromRDD(</div><div class="line">    rdd.mapPartitions(fn, preservesPartitioning))(fakeClassTag[<span class="type">K2</span>], fakeClassTag[<span class="type">V2</span>])</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="foreachPartition"><a href="#foreachPartition" class="headerlink" title="foreachPartition"></a>foreachPartition</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Applies a function f to each partition of this RDD.</div><div class="line">  * 将函数f应用于该RDD的每个分区。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreachPartition</span></span>(f: <span class="type">VoidFunction</span>[<span class="type">JIterator</span>[<span class="type">T</span>]]): <span class="type">Unit</span> = &#123;</div><div class="line">  rdd.foreachPartition(x =&gt; f.call(x.asJava))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Return an RDD created by coalescing all elements within each partition into an array.</div><div class="line">  * 返回一个RDD，它将每个分区中的所有元素合并到一个数组中。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">glom</span></span>(): <span class="type">JavaRDD</span>[<span class="type">JList</span>[<span class="type">T</span>]] =</div><div class="line">  <span class="keyword">new</span> <span class="type">JavaRDD</span>(rdd.glom().map(_.toSeq.asJava))</div></pre></td></tr></table></figure>
<h3 id="cartesian"><a href="#cartesian" class="headerlink" title="cartesian"></a>cartesian</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of</div><div class="line">  * elements (a, b) where a is in `this` and b is in `other`.</div><div class="line">  * 返回这个RDD和另一个的笛卡尔乘积，即所有元素对的RDD(a,b) ：a在该RDD中，b在另一个RDD中</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cartesian</span></span>[<span class="type">U</span>](other: <span class="type">JavaRDDLike</span>[<span class="type">U</span>, _]): <span class="type">JavaPairRDD</span>[<span class="type">T</span>, <span class="type">U</span>] =</div><div class="line">  <span class="type">JavaPairRDD</span>.fromRDD(rdd.cartesian(other.rdd)(other.classTag))(classTag, other.classTag)</div></pre></td></tr></table></figure>
<h3 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements</div><div class="line">  * mapping to that key.</div><div class="line">  * 返回分组元素的RDD。</div><div class="line">  * 每个组由一个键和一个映射到该键的元素序列组成。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupBy</span></span>[<span class="type">U</span>](f: <span class="type">JFunction</span>[<span class="type">T</span>, <span class="type">U</span>]): <span class="type">JavaPairRDD</span>[<span class="type">U</span>, <span class="type">JIterable</span>[<span class="type">T</span>]] = &#123;</div><div class="line">  <span class="comment">// The type parameter is U instead of K in order to work around a compiler bug; see SPARK-4459</span></div><div class="line">  <span class="comment">// 类型参数是U而不是K，是为了绕过编译器错误</span></div><div class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> ctagK: <span class="type">ClassTag</span>[<span class="type">U</span>] = fakeClassTag</div><div class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> ctagV: <span class="type">ClassTag</span>[<span class="type">JList</span>[<span class="type">T</span>]] = fakeClassTag</div><div class="line">  <span class="type">JavaPairRDD</span>.fromRDD(groupByResultToJava(rdd.groupBy(f)(fakeClassTag)))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements</div><div class="line">  * mapping to that key.</div><div class="line">  * 返回分组元素的RDD。</div><div class="line">  * 每个组由一个键和一个映射到该键的元素序列组成。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupBy</span></span>[<span class="type">U</span>](f: <span class="type">JFunction</span>[<span class="type">T</span>, <span class="type">U</span>], numPartitions: <span class="type">Int</span>): <span class="type">JavaPairRDD</span>[<span class="type">U</span>, <span class="type">JIterable</span>[<span class="type">T</span>]] = &#123;</div><div class="line">  <span class="comment">// The type parameter is U instead of K in order to work around a compiler bug; see SPARK-4459</span></div><div class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> ctagK: <span class="type">ClassTag</span>[<span class="type">U</span>] = fakeClassTag</div><div class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> ctagV: <span class="type">ClassTag</span>[<span class="type">JList</span>[<span class="type">T</span>]] = fakeClassTag</div><div class="line">  <span class="type">JavaPairRDD</span>.fromRDD(groupByResultToJava(rdd.groupBy(f, numPartitions)(fakeClassTag[<span class="type">U</span>])))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="pipe"><a href="#pipe" class="headerlink" title="pipe"></a>pipe</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Return an RDD created by piping elements to a forked external process.</div><div class="line">  * 返回由管道元素调用外部程序返回新的RDD</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">pipe</span></span>(command: <span class="type">String</span>): <span class="type">JavaRDD</span>[<span class="type">String</span>] = &#123;</div><div class="line">  rdd.pipe(command)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Return an RDD created by piping elements to a forked external process.</div><div class="line">  * 返回由管道元素调用外部程序返回新的RDD</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">pipe</span></span>(command: <span class="type">JList</span>[<span class="type">String</span>]): <span class="type">JavaRDD</span>[<span class="type">String</span>] = &#123;</div><div class="line">  rdd.pipe(command.asScala)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Return an RDD created by piping elements to a forked external process.</div><div class="line">  * 返回由管道元素调用外部程序返回新的RDD</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">pipe</span></span>(command: <span class="type">JList</span>[<span class="type">String</span>], env: <span class="type">JMap</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">JavaRDD</span>[<span class="type">String</span>] = &#123;</div><div class="line">  rdd.pipe(command.asScala, env.asScala)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Return an RDD created by piping elements to a forked external process.</div><div class="line">  * 返回由管道元素调用外部程序返回新的RDD</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">pipe</span></span>(command: <span class="type">JList</span>[<span class="type">String</span>],</div><div class="line">         env: <span class="type">JMap</span>[<span class="type">String</span>, <span class="type">String</span>],</div><div class="line">         separateWorkingDir: <span class="type">Boolean</span>,</div><div class="line">         bufferSize: <span class="type">Int</span>): <span class="type">JavaRDD</span>[<span class="type">String</span>] = &#123;</div><div class="line">  rdd.pipe(command.asScala, env.asScala, <span class="literal">null</span>, <span class="literal">null</span>, separateWorkingDir, bufferSize)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Return an RDD created by piping elements to a forked external process.</div><div class="line">  * 返回由管道元素调用外部程序返回新的RDD</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">pipe</span></span>(command: <span class="type">JList</span>[<span class="type">String</span>],</div><div class="line">         env: <span class="type">JMap</span>[<span class="type">String</span>, <span class="type">String</span>],</div><div class="line">         separateWorkingDir: <span class="type">Boolean</span>,</div><div class="line">         bufferSize: <span class="type">Int</span>,</div><div class="line">         encoding: <span class="type">String</span>): <span class="type">JavaRDD</span>[<span class="type">String</span>] = &#123;</div><div class="line">  rdd.pipe(command.asScala, env.asScala, <span class="literal">null</span>, <span class="literal">null</span>, separateWorkingDir, bufferSize, encoding)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Zips this RDD with another one, returning key-value pairs with the first element in each RDD,</div><div class="line">  * second element in each RDD, etc. Assumes that the two RDDs have the *same number of</div><div class="line">  * partitions* and the *same number of elements in each partition* (e.g. one was made through</div><div class="line">  * a map on the other).</div><div class="line">  * 将此RDD与另一个RDD进行Zips，返回键值对，每个RDD中的第一个元素，每个RDD中的第二个元素，等等。</div><div class="line">  * 假设两个RDDs拥有相同数量的分区和每个分区中相同数量的元素</div><div class="line">  * (例如，一个是通过另一个map的)。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">zip</span></span>[<span class="type">U</span>](other: <span class="type">JavaRDDLike</span>[<span class="type">U</span>, _]): <span class="type">JavaPairRDD</span>[<span class="type">T</span>, <span class="type">U</span>] = &#123;</div><div class="line">  <span class="type">JavaPairRDD</span>.fromRDD(rdd.zip(other.rdd)(other.classTag))(classTag, other.classTag)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="zipPartitions"><a href="#zipPartitions" class="headerlink" title="zipPartitions"></a>zipPartitions</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by</div><div class="line">  * applying a function to the zipped partitions. Assumes that all the RDDs have the</div><div class="line">  * *same number of partitions*, but does *not* require them to have the same number</div><div class="line">  * of elements in each partition.</div><div class="line">  * 用一个(或多个)RDD(或多个)来压缩这个RDD的分区，并返回一个新的RDD将函数应用于压缩分区。</div><div class="line">  * 假设所有RDDs拥有相同数量的分区，但不要求它们在每个分区中拥有相同数量的元素。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">zipPartitions</span></span>[<span class="type">U</span>, <span class="type">V</span>](</div><div class="line">                         other: <span class="type">JavaRDDLike</span>[<span class="type">U</span>, _],</div><div class="line">                         f: <span class="type">FlatMapFunction2</span>[<span class="type">JIterator</span>[<span class="type">T</span>], <span class="type">JIterator</span>[<span class="type">U</span>], <span class="type">V</span>]): <span class="type">JavaRDD</span>[<span class="type">V</span>] = &#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fn</span></span>: (<span class="type">Iterator</span>[<span class="type">T</span>], <span class="type">Iterator</span>[<span class="type">U</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">V</span>] = &#123;</div><div class="line">    (x: <span class="type">Iterator</span>[<span class="type">T</span>], y: <span class="type">Iterator</span>[<span class="type">U</span>]) =&gt; f.call(x.asJava, y.asJava).asScala</div><div class="line">  &#125;</div><div class="line">  <span class="type">JavaRDD</span>.fromRDD(</div><div class="line">    rdd.zipPartitions(other.rdd)(fn)(other.classTag, fakeClassTag[<span class="type">V</span>]))(fakeClassTag[<span class="type">V</span>])</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="zipWithUniqueId"><a href="#zipWithUniqueId" class="headerlink" title="zipWithUniqueId"></a>zipWithUniqueId</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k,</div><div class="line">  * 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method</div><div class="line">  * won't trigger a spark job, which is different from [[org.apache.spark.rdd.RDD#zipWithIndex]].</div><div class="line">  * 用生成的唯一长的id来压缩这个RDD。</div><div class="line">  * 第k个分区的项将得到id k,n + k,2 *n+ k，…，其中n是分区数。</div><div class="line">  * 因此，可能存在差距，但这种方法不会触发spark作业，它与[org .apache.spark. spark.rdd. rdd. rdd # zipWithIndex]不同。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">zipWithUniqueId</span></span>(): <span class="type">JavaPairRDD</span>[<span class="type">T</span>, jl.<span class="type">Long</span>] = &#123;</div><div class="line">  <span class="type">JavaPairRDD</span>.fromRDD(rdd.zipWithUniqueId()).asInstanceOf[<span class="type">JavaPairRDD</span>[<span class="type">T</span>, jl.<span class="type">Long</span>]]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="zipWithIndex"><a href="#zipWithIndex" class="headerlink" title="zipWithIndex"></a>zipWithIndex</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Zips this RDD with its element indices. The ordering is first based on the partition index</div><div class="line"> * and then the ordering of items within each partition. So the first item in the first</div><div class="line"> * partition gets index 0, and the last item in the last partition receives the largest index.</div><div class="line"> * This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type.</div><div class="line"> * This method needs to trigger a spark job when this RDD contains more than one partitions.</div><div class="line">  *</div><div class="line">  * 用它的元素索引来压缩这个RDD。</div><div class="line">  * 排序首先基于分区索引，然后是每个分区中的条目的排序。</div><div class="line">  * 因此，第一个分区中的第一个项的索引值为0，最后一个分区中的最后一个项得到最大的索引。</div><div class="line">  * 这类似于Scala的zipWithIndex，但它使用的是Long而不是Int作为索引类型。</div><div class="line">  * 当这个RDD包含多个分区时，这个方法需要触发一个spark作业。</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">zipWithIndex</span></span>(): <span class="type">JavaPairRDD</span>[<span class="type">T</span>, jl.<span class="type">Long</span>] = &#123;</div><div class="line">  <span class="type">JavaPairRDD</span>.fromRDD(rdd.zipWithIndex()).asInstanceOf[<span class="type">JavaPairRDD</span>[<span class="type">T</span>, jl.<span class="type">Long</span>]]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Actions-launch-a-job-to-return-a-value-to-the-user-program"><a href="#Actions-launch-a-job-to-return-a-value-to-the-user-program" class="headerlink" title="Actions (launch a job to return a value to the user program)"></a>Actions (launch a job to return a value to the user program)</h2><h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Applies a function f to all elements of this RDD.</div><div class="line">  * 将函数f应用于该RDD的所有元素。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreach</span></span>(f: <span class="type">VoidFunction</span>[<span class="type">T</span>]) &#123;</div><div class="line">  rdd.foreach(x =&gt; f.call(x))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Return an array that contains all of the elements in this RDD.</div><div class="line">  * 返回包含该RDD中所有元素的数组。</div><div class="line">  *</div><div class="line">  * @note this method should only be used if the resulting array is expected to be small, as</div><div class="line">  * all the data is loaded into the driver's memory.</div><div class="line">  * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">collect</span></span>(): <span class="type">JList</span>[<span class="type">T</span>] =</div><div class="line">  rdd.collect().toSeq.asJava</div></pre></td></tr></table></figure>
<h3 id="toLocalIterator"><a href="#toLocalIterator" class="headerlink" title="toLocalIterator"></a>toLocalIterator</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Return an iterator that contains all of the elements in this RDD.</div><div class="line">  * 返回包含该RDD中所有元素的迭代器。</div><div class="line">  *</div><div class="line">  * The iterator will consume as much memory as the largest partition in this RDD.</div><div class="line">  * 迭代器将消耗与此RDD中最大的分区一样多的内存。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">toLocalIterator</span></span>(): <span class="type">JIterator</span>[<span class="type">T</span>] =</div><div class="line">  asJavaIteratorConverter(rdd.toLocalIterator).asJava</div></pre></td></tr></table></figure>
<h3 id="collectPartitions"><a href="#collectPartitions" class="headerlink" title="collectPartitions"></a>collectPartitions</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">/**</span></div><div class="line">  * Return an array that contains all of the elements in a specific partition of this RDD.</div><div class="line">  * 返回包含该RDD的特定分区中的所有元素的数组。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">collectPartitions</span></span>(partitionIds: <span class="type">Array</span>[<span class="type">Int</span>]): <span class="type">Array</span>[<span class="type">JList</span>[<span class="type">T</span>]] = &#123;</div><div class="line">  <span class="comment">// This is useful for implementing `take` from other language frontends</span></div><div class="line">  <span class="comment">// like Python where the data is serialized.</span></div><div class="line">  <span class="comment">// 这有助于从其他语言的前沿实现“take”，如Python，数据被序列化。</span></div><div class="line">  <span class="keyword">val</span> res = context.runJob(rdd, (it: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; it.toArray, partitionIds)</div><div class="line">  res.map(_.toSeq.asJava)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Reduces the elements of this RDD using the specified commutative and associative binary</div><div class="line">  * operator.</div><div class="line">  * 使用指定的交换和关联二元运算符来减少该RDD的元素。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(f: <span class="type">JFunction2</span>[<span class="type">T</span>, <span class="type">T</span>, <span class="type">T</span>]): <span class="type">T</span> = rdd.reduce(f)</div></pre></td></tr></table></figure>
<h3 id="treeReduce"><a href="#treeReduce" class="headerlink" title="treeReduce"></a>treeReduce</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Reduces the elements of this RDD in a multi-level tree pattern.</div><div class="line">  * 将此RDD的元素简化为多层树模式。</div><div class="line">  *</div><div class="line">  * @param depth suggested depth of the tree 建议树的深度</div><div class="line">  * @see [[org.apache.spark.api.java.JavaRDDLike#reduce]]</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">treeReduce</span></span>(f: <span class="type">JFunction2</span>[<span class="type">T</span>, <span class="type">T</span>, <span class="type">T</span>], depth: <span class="type">Int</span>): <span class="type">T</span> = rdd.treeReduce(f, depth)</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * [[org.apache.spark.api.java.JavaRDDLike#treeReduce]] 建议深度 2 .</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">treeReduce</span></span>(f: <span class="type">JFunction2</span>[<span class="type">T</span>, <span class="type">T</span>, <span class="type">T</span>]): <span class="type">T</span> = treeReduce(f, <span class="number">2</span>)</div></pre></td></tr></table></figure>
<h3 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">/**</span></div><div class="line">  * Aggregate the elements of each partition, and then the results for all the partitions, using a</div><div class="line">  * given associative function and a neutral "zero value". The function</div><div class="line">  * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object</div><div class="line">  * allocation; however, it should not modify t2.</div><div class="line">  * 对每个分区的元素进行聚合，然后使用给定的关联函数和中立的“零值”，对所有分区进行结果。</div><div class="line">  * 函数op(t1,t2)被允许修改t1，并将其作为其结果值返回，以避免对象分配;但是，它不应该修改t2。</div><div class="line">  *</div><div class="line">  * This behaves somewhat differently from fold operations implemented for non-distributed</div><div class="line">  * collections in functional languages like Scala. This fold operation may be applied to</div><div class="line">  * partitions individually, and then fold those results into the final result, rather than</div><div class="line">  * apply the fold to each element sequentially in some defined ordering. For functions</div><div class="line">  * that are not commutative, the result may differ from that of a fold applied to a</div><div class="line">  * non-distributed collection.</div><div class="line">  * 这与在Scala等函数式语言中实现非分布式集合的折叠操作有一定的不同。</div><div class="line">  * 这个折叠操作可以单独应用于分区，然后将这些结果折叠到最终结果中，而不是在某些定义的排序中顺序地对每个元素进行折叠。</div><div class="line">  * 对于非交换的函数，结果可能与应用于非分布式集合的函数不同。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fold</span></span>(zeroValue: <span class="type">T</span>)(f: <span class="type">JFunction2</span>[<span class="type">T</span>, <span class="type">T</span>, <span class="type">T</span>]): <span class="type">T</span> =</div><div class="line">  rdd.fold(zeroValue)(f)</div></pre></td></tr></table></figure>
<h3 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">/**</span></div><div class="line">  * Aggregate the elements of each partition, and then the results for all the partitions, using</div><div class="line">  * given combine functions and a neutral "zero value". This function can return a different result</div><div class="line">  * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U</div><div class="line">  * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are</div><div class="line">  * allowed to modify and return their first argument instead of creating a new U to avoid memory</div><div class="line">  * allocation.</div><div class="line">  * 对每个分区的元素进行聚合，然后使用给定的组合函数和一个中立的“零值”，对所有分区进行结果。</div><div class="line">  * 这个函数可以返回一个不同的结果类型U，而不是这个RDD的类型。</div><div class="line">  * 因此，我们需要一个操作来将一个T合并到一个U和一个合并两个U的操作，就像在scala . traversableonce中那样。</div><div class="line">  * 这两个函数都可以修改和返回第一个参数，而不是创建一个新的U，以避免内存分配。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregate</span></span>[<span class="type">U</span>](zeroValue: <span class="type">U</span>)(seqOp: <span class="type">JFunction2</span>[<span class="type">U</span>, <span class="type">T</span>, <span class="type">U</span>],</div><div class="line">                               combOp: <span class="type">JFunction2</span>[<span class="type">U</span>, <span class="type">U</span>, <span class="type">U</span>]): <span class="type">U</span> =</div><div class="line">  rdd.aggregate(zeroValue)(seqOp, combOp)(fakeClassTag[<span class="type">U</span>])</div></pre></td></tr></table></figure>
<h3 id="treeAggregate"><a href="#treeAggregate" class="headerlink" title="treeAggregate"></a>treeAggregate</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">  <span class="comment">/**</span></div><div class="line">  * Aggregates the elements of this RDD in a multi-level tree pattern.</div><div class="line">  * 将此RDD的元素聚集在多层树模式中。</div><div class="line">  *</div><div class="line">  * @param depth suggested depth of the tree  建议的树的深度</div><div class="line">  * @see [[org.apache.spark.api.java.JavaRDDLike#aggregate]]</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">treeAggregate</span></span>[<span class="type">U</span>](</div><div class="line">                      zeroValue: <span class="type">U</span>,</div><div class="line">                      seqOp: <span class="type">JFunction2</span>[<span class="type">U</span>, <span class="type">T</span>, <span class="type">U</span>],</div><div class="line">                      combOp: <span class="type">JFunction2</span>[<span class="type">U</span>, <span class="type">U</span>, <span class="type">U</span>],</div><div class="line">                      depth: <span class="type">Int</span>): <span class="type">U</span> = &#123;</div><div class="line">  rdd.treeAggregate(zeroValue)(seqOp, combOp, depth)(fakeClassTag[<span class="type">U</span>])</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * [[org.apache.spark.api.java.JavaRDDLike#treeAggregate]] with suggested depth 2.</div><div class="line">  * 建议的树的深度为 2</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">treeAggregate</span></span>[<span class="type">U</span>](</div><div class="line">                      zeroValue: <span class="type">U</span>,</div><div class="line">                      seqOp: <span class="type">JFunction2</span>[<span class="type">U</span>, <span class="type">T</span>, <span class="type">U</span>],</div><div class="line">                      combOp: <span class="type">JFunction2</span>[<span class="type">U</span>, <span class="type">U</span>, <span class="type">U</span>]): <span class="type">U</span> = &#123;</div><div class="line">  treeAggregate(zeroValue, seqOp, combOp, <span class="number">2</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Return the number of elements in the RDD.</div><div class="line">  * 返回RDD中元素的数量。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = rdd.count()</div></pre></td></tr></table></figure>
<h3 id="countApprox"><a href="#countApprox" class="headerlink" title="countApprox"></a>countApprox</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">  <span class="comment">/**</span></div><div class="line">  * Approximate version of count() that returns a potentially incomplete result</div><div class="line">  * within a timeout, even if not all tasks have finished.</div><div class="line">  * 近似版本的count()，即使不是所有的任务都完成了，也会在一个超时中返回一个潜在的不完整的结果。</div><div class="line">  *</div><div class="line">  *</div><div class="line">  * The confidence is the probability that the error bounds of the result will</div><div class="line">  * contain the true value. That is, if countApprox were called repeatedly</div><div class="line">  * with confidence 0.9, we would expect 90% of the results to contain the</div><div class="line">  * true count. The confidence must be in the range [0,1] or an exception will</div><div class="line">  * be thrown.</div><div class="line">  * 置信值是结果的误差边界包含真实值的概率。</div><div class="line">  * 也就是说，如果countApprox被反复调用，confidence 0.9，我们将期望90%的结果包含真实的计数。</div><div class="line">  * confidence必须在范围[0,1]中，否则将抛出异常。</div><div class="line">  *</div><div class="line">  *</div><div class="line">  * @param timeout maximum time to wait for the job, in milliseconds</div><div class="line">  *                等待工作的最大时间，以毫秒为单位</div><div class="line">  * @param confidence the desired statistical confidence in the result</div><div class="line">  *                   对结果的期望的统计信心</div><div class="line">  * @return a potentially incomplete result, with error bounds</div><div class="line">  *         一个可能不完整的结果，有错误界限</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">countApprox</span></span>(timeout: <span class="type">Long</span>, confidence: <span class="type">Double</span>): <span class="type">PartialResult</span>[<span class="type">BoundedDouble</span>] =</div><div class="line">  rdd.countApprox(timeout, confidence)</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Approximate version of count() that returns a potentially incomplete result</div><div class="line">  * within a timeout, even if not all tasks have finished.</div><div class="line">  * 近似版本的count()，即使不是所有的任务都完成了，也会在一个超时中返回一个潜在的不完整的结果。</div><div class="line">  *</div><div class="line">  *</div><div class="line">  * @param timeout maximum time to wait for the job, in milliseconds</div><div class="line">  *                等待工作的最大时间，以毫秒为单位</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">countApprox</span></span>(timeout: <span class="type">Long</span>): <span class="type">PartialResult</span>[<span class="type">BoundedDouble</span>] =</div><div class="line">  rdd.countApprox(timeout)</div></pre></td></tr></table></figure>
<h3 id="countByValue"><a href="#countByValue" class="headerlink" title="countByValue"></a>countByValue</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Return the count of each unique value in this RDD as a map of (value, count) pairs. The final</div><div class="line">  * combine step happens locally on the master, equivalent to running a single reduce task.</div><div class="line">  * 将此RDD中的每个惟一值的计数作为(值、计数)对的映射。</div><div class="line">  * 最后的联合步骤在master的本地发生，相当于运行一个reduce任务。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">countByValue</span></span>(): <span class="type">JMap</span>[<span class="type">T</span>, jl.<span class="type">Long</span>] =</div><div class="line">  mapAsSerializableJavaMap(rdd.countByValue()).asInstanceOf[<span class="type">JMap</span>[<span class="type">T</span>, jl.<span class="type">Long</span>]]</div></pre></td></tr></table></figure>
<h3 id="countByValueApprox"><a href="#countByValueApprox" class="headerlink" title="countByValueApprox"></a>countByValueApprox</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">  <span class="comment">/**</span></div><div class="line">  * Approximate version of countByValue().</div><div class="line">  * countByValue()近似的版本。</div><div class="line">  *</div><div class="line">  * The confidence is the probability that the error bounds of the result will</div><div class="line">  * contain the true value. That is, if countApprox were called repeatedly</div><div class="line">  * with confidence 0.9, we would expect 90% of the results to contain the</div><div class="line">  * true count. The confidence must be in the range [0,1] or an exception will</div><div class="line">  * be thrown.</div><div class="line">  * 置信值是结果的误差边界包含真实值的概率。</div><div class="line">  * 也就是说，如果countApprox被反复调用，confidence 0.9，我们将期望90%的结果包含真实的计数。</div><div class="line">  * confidence必须在范围[0,1]中，否则将抛出异常。</div><div class="line">  *</div><div class="line">  *</div><div class="line">  * @param timeout maximum time to wait for the job, in milliseconds</div><div class="line">  *                等待工作的最大时间，毫秒为单位。</div><div class="line">  * @param confidence the desired statistical confidence in the result</div><div class="line">  *                   对结果的期望的统计信心</div><div class="line">  * @return a potentially incomplete result, with error bounds</div><div class="line">  *         一个可能不完整的结果，有错误界限</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">countByValueApprox</span></span>(</div><div class="line">                        timeout: <span class="type">Long</span>,</div><div class="line">                        confidence: <span class="type">Double</span></div><div class="line">                      ): <span class="type">PartialResult</span>[<span class="type">JMap</span>[<span class="type">T</span>, <span class="type">BoundedDouble</span>]] =</div><div class="line">  rdd.countByValueApprox(timeout, confidence).map(mapAsSerializableJavaMap)</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Approximate version of countByValue().</div><div class="line">  * countByValue().的近似版本.</div><div class="line">  *</div><div class="line">  * @param timeout maximum time to wait for the job, in milliseconds</div><div class="line">  *                等待工作的最大时间，毫秒为单位。</div><div class="line">  * @return a potentially incomplete result, with error bounds</div><div class="line">  *         一个可能不完整的结果，有错误界限</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">countByValueApprox</span></span>(timeout: <span class="type">Long</span>): <span class="type">PartialResult</span>[<span class="type">JMap</span>[<span class="type">T</span>, <span class="type">BoundedDouble</span>]] =</div><div class="line">  rdd.countByValueApprox(timeout).map(mapAsSerializableJavaMap)</div></pre></td></tr></table></figure>
<h3 id="take"><a href="#take" class="headerlink" title="take"></a>take</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">  <span class="comment">/**</span></div><div class="line">  * Take the first num elements of the RDD. This currently scans the partitions *one by one*, so</div><div class="line">  * it will be slow if a lot of partitions are required. In that case, use collect() to get the</div><div class="line">  * whole RDD instead.</div><div class="line">  * 获取RDD的第一个num元素。</div><div class="line">  * 这将会一次一个地扫描分区，所以如果需要很多分区，它将会很慢。</div><div class="line">  * 在这种情况下，使用collect()来获得整个RDD。</div><div class="line">  *</div><div class="line">  *</div><div class="line">  * @note this method should only be used if the resulting array is expected to be small, as</div><div class="line">  * all the data is loaded into the driver's memory.</div><div class="line">  * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(num: <span class="type">Int</span>): <span class="type">JList</span>[<span class="type">T</span>] =</div><div class="line">  rdd.take(num).toSeq.asJava</div></pre></td></tr></table></figure>
<h3 id="takeSample"><a href="#takeSample" class="headerlink" title="takeSample"></a>takeSample</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeSample</span></span>(withReplacement: <span class="type">Boolean</span>, num: <span class="type">Int</span>): <span class="type">JList</span>[<span class="type">T</span>] =</div><div class="line">  takeSample(withReplacement, num, <span class="type">Utils</span>.random.nextLong)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeSample</span></span>(withReplacement: <span class="type">Boolean</span>, num: <span class="type">Int</span>, seed: <span class="type">Long</span>): <span class="type">JList</span>[<span class="type">T</span>] =</div><div class="line">  rdd.takeSample(withReplacement, num, seed).toSeq.asJava</div></pre></td></tr></table></figure>
<h3 id="first"><a href="#first" class="headerlink" title="first"></a>first</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">  <span class="comment">/**</span></div><div class="line">  * Return the first element in this RDD.</div><div class="line">  * 返回这个RDD中的第一个元素。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">first</span></span>(): <span class="type">T</span> = rdd.first()</div></pre></td></tr></table></figure>
<h3 id="isEmpty"><a href="#isEmpty" class="headerlink" title="isEmpty"></a>isEmpty</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * @return true if and only if the RDD contains no elements at all. Note that an RDD</div><div class="line">  *         may be empty even when it has at least 1 partition.</div><div class="line">  *         当且仅当RDD不包含任何元素，则为真。</div><div class="line">  *         请注意，即使在至少有一个分区的情况下，RDD也可能是空的。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">isEmpty</span></span>(): <span class="type">Boolean</span> = rdd.isEmpty()</div></pre></td></tr></table></figure>
<h3 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile"></a>saveAsTextFile</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Save this RDD as a text file, using string representations of elements.</div><div class="line">  * 将此RDD保存为文本文件，使用元素的字符串表示形式。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsTextFile</span></span>(path: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  rdd.saveAsTextFile(path)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Save this RDD as a compressed text file, using string representations of elements.</div><div class="line">  * 将此RDD保存为一个压缩文本文件，使用元素的字符串表示形式。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsTextFile</span></span>(path: <span class="type">String</span>, codec: <span class="type">Class</span>[_ &lt;: <span class="type">CompressionCodec</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">  rdd.saveAsTextFile(path, codec)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="saveAsObjectFile"><a href="#saveAsObjectFile" class="headerlink" title="saveAsObjectFile"></a>saveAsObjectFile</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Save this RDD as a SequenceFile of serialized objects.</div><div class="line">  * 将此RDD保存为序列化对象的序列文件。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsObjectFile</span></span>(path: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  rdd.saveAsObjectFile(path)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="keyBy"><a href="#keyBy" class="headerlink" title="keyBy"></a>keyBy</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Creates tuples of the elements in this RDD by applying `f`.</div><div class="line">  * 通过应用“f”创建这个RDD中元素的元组。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">keyBy</span></span>[<span class="type">U</span>](f: <span class="type">JFunction</span>[<span class="type">T</span>, <span class="type">U</span>]): <span class="type">JavaPairRDD</span>[<span class="type">U</span>, <span class="type">T</span>] = &#123;</div><div class="line">  <span class="comment">// The type parameter is U instead of K in order to work around a compiler bug; see SPARK-4459</span></div><div class="line">  <span class="comment">// 类型参数用U替代K，为了绕过编译器错误;</span></div><div class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> ctag: <span class="type">ClassTag</span>[<span class="type">U</span>] = fakeClassTag</div><div class="line">  <span class="type">JavaPairRDD</span>.fromRDD(rdd.keyBy(f))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="checkpoint"><a href="#checkpoint" class="headerlink" title="checkpoint"></a>checkpoint</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint</div><div class="line">  * directory set with SparkContext.setCheckpointDir() and all references to its parent</div><div class="line">  * RDDs will be removed. This function must be called before any job has been</div><div class="line">  * executed on this RDD. It is strongly recommended that this RDD is persisted in</div><div class="line">  * memory, otherwise saving it on a file will require recomputation.</div><div class="line">  * 将此RDD标记为检查点。</div><div class="line">  * 它将被保存到由SparkContext.setCheckpointDir()设置的检查点目录下的文件中。</div><div class="line">  * 所有对其父RDDs的引用将被删除。</div><div class="line">  * 在此RDD上执行任何作业之前，必须调用此函数。</div><div class="line">  * 强烈建议将此RDD保存在内存中，否则将其保存在文件中需要重新计算。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkpoint</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  rdd.checkpoint()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="isCheckpointed"><a href="#isCheckpointed" class="headerlink" title="isCheckpointed"></a>isCheckpointed</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Return whether this RDD has been checkpointed or not</div><div class="line">  * 返回  RDD是否已被检查过</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">isCheckpointed</span></span>: <span class="type">Boolean</span> = rdd.isCheckpointed</div></pre></td></tr></table></figure>
<h3 id="getCheckpointFile"><a href="#getCheckpointFile" class="headerlink" title="getCheckpointFile"></a>getCheckpointFile</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Gets the name of the file to which this RDD was checkpointed</div><div class="line">  * 获取该RDD所指向的checkpointed文件的名称</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getCheckpointFile</span></span>(): <span class="type">Optional</span>[<span class="type">String</span>] = &#123;</div><div class="line">  <span class="type">JavaUtils</span>.optionToOptional(rdd.getCheckpointFile)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="toDebugString"><a href="#toDebugString" class="headerlink" title="toDebugString"></a>toDebugString</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/** A description of this RDD and its recursive dependencies for debugging.</span></div><div class="line">  * 对该RDD及其对调试的递归依赖的描述。</div><div class="line">  * */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">toDebugString</span></span>(): <span class="type">String</span> = &#123;</div><div class="line">  rdd.toDebugString</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="top"><a href="#top" class="headerlink" title="top"></a>top</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns the top k (largest) elements from this RDD as defined by</div><div class="line">  * the specified Comparator[T] and maintains the order.</div><div class="line">  * 根据指定的比较器[T]，从这个RDD中返回最大的k(最大)元素，并维护顺序。</div><div class="line">  *</div><div class="line">  *</div><div class="line">  * @note this method should only be used if the resulting array is expected to be small, as</div><div class="line">  * all the data is loaded into the driver's memory.</div><div class="line">  * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。</div><div class="line">  *</div><div class="line">  * @param num k, the number of top elements to return  返回的元素数量</div><div class="line">  * @param comp the comparator that defines the order  定义排序的比较器</div><div class="line">  * @return an array of top elements  返回最大元素的数组</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">top</span></span>(num: <span class="type">Int</span>, comp: <span class="type">Comparator</span>[<span class="type">T</span>]): <span class="type">JList</span>[<span class="type">T</span>] = &#123;</div><div class="line">  rdd.top(num)(<span class="type">Ordering</span>.comparatorToOrdering(comp)).toSeq.asJava</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns the top k (largest) elements from this RDD using the</div><div class="line">  * natural ordering for T and maintains the order.</div><div class="line">  * 使用T的自然顺序，从这个RDD中返回最大的k(最大)元素，并维护顺序。</div><div class="line">  *</div><div class="line">  * @note this method should only be used if the resulting array is expected to be small, as</div><div class="line">  * all the data is loaded into the driver's memory.</div><div class="line">  * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。</div><div class="line">  *</div><div class="line">  * @param num k, the number of top elements to return  返回的元素数量</div><div class="line">  * @return an array of top elements 最大元素的数组</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">top</span></span>(num: <span class="type">Int</span>): <span class="type">JList</span>[<span class="type">T</span>] = &#123;</div><div class="line">  <span class="keyword">val</span> comp = com.google.common.collect.<span class="type">Ordering</span>.natural().asInstanceOf[<span class="type">Comparator</span>[<span class="type">T</span>]]</div><div class="line">  top(num, comp)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns the first k (smallest) elements from this RDD as defined by</div><div class="line">  * the specified Comparator[T] and maintains the order.</div><div class="line">  * 从这个RDD中返回第一个k(最小)元素，由指定的Comparator[T]定义，并维护该顺序。</div><div class="line">  *</div><div class="line">  *</div><div class="line">  * @note this method should only be used if the resulting array is expected to be small, as</div><div class="line">  * all the data is loaded into the driver's memory.</div><div class="line">  * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。</div><div class="line">  *</div><div class="line">  * @param num k, the number of elements to return  返回的元素数量</div><div class="line">  * @param comp the comparator that defines the order  排序比较器</div><div class="line">  * @return an array of top elements  元素数组</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>, comp: <span class="type">Comparator</span>[<span class="type">T</span>]): <span class="type">JList</span>[<span class="type">T</span>] = &#123;</div><div class="line">  rdd.takeOrdered(num)(<span class="type">Ordering</span>.comparatorToOrdering(comp)).toSeq.asJava</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns the first k (smallest) elements from this RDD using the</div><div class="line">  * natural ordering for T while maintain the order.</div><div class="line">  * 使用原生的 T排序比较器，返回 k个 最小值，并维护这个顺序</div><div class="line">  *</div><div class="line">  * @note this method should only be used if the resulting array is expected to be small, as</div><div class="line">  * all the data is loaded into the driver's memory.</div><div class="line">  * 尽量应用于小的数组，因为会加载到driver内存中。</div><div class="line">  * @param num k, the number of top elements to return</div><div class="line">  * @return an array of top elements</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>): <span class="type">JList</span>[<span class="type">T</span>] = &#123;</div><div class="line">  <span class="keyword">val</span> comp = com.google.common.collect.<span class="type">Ordering</span>.natural().asInstanceOf[<span class="type">Comparator</span>[<span class="type">T</span>]]</div><div class="line">  takeOrdered(num, comp)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="max"><a href="#max" class="headerlink" title="max"></a>max</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns the maximum element from this RDD as defined by the specified</div><div class="line">  * Comparator[T].</div><div class="line">  * 按照指定比较器[T]定义的RDD，</div><div class="line">  * 返回最大元素。</div><div class="line">  *</div><div class="line">  * @param comp the comparator that defines ordering  指定的比较器</div><div class="line">  * @return the maximum of the RDD  最大值</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">max</span></span>(comp: <span class="type">Comparator</span>[<span class="type">T</span>]): <span class="type">T</span> = &#123;</div><div class="line">  rdd.max()(<span class="type">Ordering</span>.comparatorToOrdering(comp))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="min"><a href="#min" class="headerlink" title="min"></a>min</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns the minimum element from this RDD as defined by the specified</div><div class="line">  * Comparator[T].</div><div class="line">  * 按照指定比较器[T]定义的RDD，</div><div class="line">  * 返回最小元素。</div><div class="line">  *</div><div class="line">  * @param comp the comparator that defines ordering   指定的比较器</div><div class="line">  * @return the minimum of the RDD  最小值</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">min</span></span>(comp: <span class="type">Comparator</span>[<span class="type">T</span>]): <span class="type">T</span> = &#123;</div><div class="line">  rdd.min()(<span class="type">Ordering</span>.comparatorToOrdering(comp))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="countApproxDistinct"><a href="#countApproxDistinct" class="headerlink" title="countApproxDistinct"></a>countApproxDistinct</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">* Return approximate number of distinct elements in the RDD.</div><div class="line">* 返回RDD中不重复元素的数量近似数。</div><div class="line">*</div><div class="line">* The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice:</div><div class="line">* Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available</div><div class="line">  * &lt;a href="http://dx.doi.org/10.1145/2452376.2452456"&gt;here&lt;/a&gt;.</div><div class="line">  * 所使用的算法是基于streamlib在实践中的“HyperLogLog”的实现:</div><div class="line">  * “一种艺术基数估计算法状态的算法工程”，</div><div class="line">  *</div><div class="line">  *</div><div class="line">  * @param relativeSD Relative accuracy. Smaller values create counters that require more space.</div><div class="line">  *                   It must be greater than 0.000017.</div><div class="line">  *                   相对精度。</div><div class="line">  *                   较小的值创建需要更多空间的计数器。</div><div class="line">  *                   它必须大于0.000017。</div><div class="line">  */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">countApproxDistinct</span></span>(relativeSD: <span class="type">Double</span>): <span class="type">Long</span> = rdd.countApproxDistinct(relativeSD)</div></pre></td></tr></table></figure>
<h3 id="countAsync"><a href="#countAsync" class="headerlink" title="countAsync"></a>countAsync</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * The asynchronous version of `count`, which returns a</div><div class="line">  * future for counting the number of elements in this RDD.</div><div class="line">  * “count”的异步版本，它为计算这个RDD中元素的数量返回一个未来。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">countAsync</span></span>(): <span class="type">JavaFutureAction</span>[jl.<span class="type">Long</span>] = &#123;</div><div class="line"><span class="keyword">new</span> <span class="type">JavaFutureActionWrapper</span>[<span class="type">Long</span>, jl.<span class="type">Long</span>](rdd.countAsync(), jl.<span class="type">Long</span>.valueOf)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="collectAsync"><a href="#collectAsync" class="headerlink" title="collectAsync"></a>collectAsync</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * The asynchronous version of `collect`, which returns a future for</div><div class="line">  * retrieving an array containing all of the elements in this RDD.</div><div class="line">  * “collect”的异步版本，</div><div class="line">  * 它返回一个用于检索包含该RDD中所有元素的数组的未来。</div><div class="line">  *</div><div class="line">  * @note this method should only be used if the resulting array is expected to be small, as</div><div class="line">  * all the data is loaded into the driver's memory.</div><div class="line">  * 尽量应用于小数量数组。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">collectAsync</span></span>(): <span class="type">JavaFutureAction</span>[<span class="type">JList</span>[<span class="type">T</span>]] = &#123;</div><div class="line"><span class="keyword">new</span> <span class="type">JavaFutureActionWrapper</span>(rdd.collectAsync(), (x: <span class="type">Seq</span>[<span class="type">T</span>]) =&gt; x.asJava)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="takeAsync"><a href="#takeAsync" class="headerlink" title="takeAsync"></a>takeAsync</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * The asynchronous version of the `take` action, which returns a</div><div class="line">  * future for retrieving the first `num` elements of this RDD.</div><div class="line">  * “take”操作的异步版本，</div><div class="line">  * 它将返回用于检索此RDD的第一个“num”元素的未来。</div><div class="line">  *</div><div class="line">  * @note this method should only be used if the resulting array is expected to be small, as</div><div class="line">  * all the data is loaded into the driver's memory.</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAsync</span></span>(num: <span class="type">Int</span>): <span class="type">JavaFutureAction</span>[<span class="type">JList</span>[<span class="type">T</span>]] = &#123;</div><div class="line"><span class="keyword">new</span> <span class="type">JavaFutureActionWrapper</span>(rdd.takeAsync(num), (x: <span class="type">Seq</span>[<span class="type">T</span>]) =&gt; x.asJava)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="foreachAsync"><a href="#foreachAsync" class="headerlink" title="foreachAsync"></a>foreachAsync</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * The asynchronous version of the `foreach` action, which</div><div class="line">  * applies a function f to all the elements of this RDD.</div><div class="line">  * “foreach”操作的异步版本，</div><div class="line">  * 它将函数f应用于这个RDD的所有元素。</div><div class="line">  *</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreachAsync</span></span>(f: <span class="type">VoidFunction</span>[<span class="type">T</span>]): <span class="type">JavaFutureAction</span>[<span class="type">Void</span>] = &#123;</div><div class="line"><span class="keyword">new</span> <span class="type">JavaFutureActionWrapper</span>[<span class="type">Unit</span>, <span class="type">Void</span>](rdd.foreachAsync(x =&gt; f.call(x)),</div><div class="line">&#123; x =&gt; <span class="literal">null</span>.asInstanceOf[<span class="type">Void</span>] &#125;)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="foreachPartitionAsync"><a href="#foreachPartitionAsync" class="headerlink" title="foreachPartitionAsync"></a>foreachPartitionAsync</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * The asynchronous version of the `foreachPartition` action, which</div><div class="line">  * applies a function f to each partition of this RDD.</div><div class="line">  * “foreachPartition”操作的异步版本，</div><div class="line">  * 它将函数f应用于该RDD的每个分区。</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreachPartitionAsync</span></span>(f: <span class="type">VoidFunction</span>[<span class="type">JIterator</span>[<span class="type">T</span>]]): <span class="type">JavaFutureAction</span>[<span class="type">Void</span>] = &#123;</div><div class="line"><span class="keyword">new</span> <span class="type">JavaFutureActionWrapper</span>[<span class="type">Unit</span>, <span class="type">Void</span>](rdd.foreachPartitionAsync(x =&gt; f.call(x.asJava)),</div><div class="line">&#123; x =&gt; <span class="literal">null</span>.asInstanceOf[<span class="type">Void</span>] &#125;)</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<!--对不起，到时间了，请停止装逼-->
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[spark取样函数分析]]></title>
      <url>https://stanxia.github.io/2017/11/08/spark%E5%8F%96%E6%A0%B7%E5%87%BD%E6%95%B0%E5%88%86%E6%9E%90/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><div class="note info"><p>Spark取样操作<br>无法获取随机样本的解决方案</p></div>
<!--请开始装逼-->
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Dataset中sample函数源码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.</div><div class="line">  *</div><div class="line">  * 通过使用用户提供的种子，通过抽样的方式返回一个新的[[Dataset]]。</div><div class="line">  *</div><div class="line">  * @param withReplacement Sample with replacement or not.</div><div class="line">  *                        如果withReplacement=true的话表示有放回的抽样，采用泊松抽样算法实现.</div><div class="line">  *                        如果withReplacement=false的话表示无放回的抽样，采用伯努利抽样算法实现.</div><div class="line">  * @param fraction        Fraction of rows to generate.</div><div class="line">  *                        每一行数据被取样的概率.服从二项分布.当withReplacement=true的时候fraction&gt;=0,当withReplacement=false的时候 0 &lt; fraction &lt; 1.</div><div class="line">  * @param seed            Seed for sampling.</div><div class="line">  *                        取样种子（与随机数生成有关）</div><div class="line">  * @note This is NOT guaranteed to provide exactly the fraction of the count</div><div class="line">  *       of the given [[Dataset]].</div><div class="line">  *       不能保证准确的按照给定的分数取样。（一般结果会在概率值*总数左右）</div><div class="line">  * @group typedrel</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(withReplacement: <span class="type">Boolean</span>, fraction: <span class="type">Double</span>, seed: <span class="type">Long</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</div><div class="line">  require(fraction &gt;= <span class="number">0</span>,</div><div class="line">    <span class="string">s"Fraction must be nonnegative, but got <span class="subst">$&#123;fraction&#125;</span>"</span>)</div><div class="line"></div><div class="line">  withTypedPlan &#123;</div><div class="line">    <span class="type">Sample</span>(<span class="number">0.0</span>, fraction, withReplacement, seed, logicalPlan)()</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.</div><div class="line">  *</div><div class="line">  * 通过程序随机的种子，抽样返回新的DataSet</div><div class="line">  *</div><div class="line">  * @param withReplacement Sample with replacement or not.</div><div class="line">  *                        取样结果是否放回</div><div class="line">  * @param fraction        Fraction of rows to generate.</div><div class="line">  *                        每行数据被取样的概率</div><div class="line">  * @note This is NOT guaranteed to provide exactly the fraction of the total count</div><div class="line">  *       of the given [[Dataset]].</div><div class="line">  *       不能保证准确的按照给定的分数取样。（一般结果会在概率值*总数左右）</div><div class="line">  * @group typedrel</div><div class="line">  * @since 1.6.0</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(withReplacement: <span class="type">Boolean</span>, fraction: <span class="type">Double</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</div><div class="line">  sample(withReplacement, fraction, <span class="type">Utils</span>.random.nextLong)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>结果数据的行数一般在（fraction*总数）左右。没有一个固定的值，如果需要得到固定行数的随机数据的话不建议采用该方法。</p>
<h2 id="办法"><a href="#办法" class="headerlink" title="办法"></a>办法</h2><p>获取随机取样的替代方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">df.createOrReplaceTempView(<span class="string">"test_sample"</span>); <span class="comment">// 生成临时表</span></div><div class="line">df.sqlContext() <span class="comment">// 添加随机数列，并根据其进行排序</span></div><div class="line">  .sql(<span class="string">"select * ,rand() as random from test_sample order by random"</span>)</div><div class="line">  .limit(<span class="number">2</span>) <span class="comment">// 根据参数的fraction计算需要获取的取样结果</span></div><div class="line">  .drop(<span class="string">"random"</span>) <span class="comment">// 删除掉添加的随机列</span></div><div class="line">  .show();</div></pre></td></tr></table></figure>
<!--对不起，到时间了，请停止装逼-->
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[spark源码注释翻译]]></title>
      <url>https://stanxia.github.io/2017/11/06/spark%E6%BA%90%E7%A0%81%E6%B3%A8%E9%87%8A%E7%BF%BB%E8%AF%91/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><div class="note info"><p>版本：spark2.1.1<br>目的：方便中文用户阅读源码，把时间花在理解而不是翻译上</p></div>
<!--请开始装逼-->
<h2 id="初衷"><a href="#初衷" class="headerlink" title="初衷"></a>初衷</h2><p>开始立项进行翻译，一方面方便日后阅读源码，另一方面先粗粒度的熟悉下spark框架和组件。优化完之后希望能帮助更多的中文用户，节省翻译时间。</p>
<a id="more"></a>
<h2 id="进度"><a href="#进度" class="headerlink" title="进度"></a>进度</h2><p>已完成：</p>
<p>正在作：spark core模块</p>
<table>
<thead>
<tr>
<th style="text-align:center">模块名</th>
<th style="text-align:center">模块介绍</th>
<th style="text-align:center">完成度</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">api</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">broadcast</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">deploy</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">executor</td>
<td style="text-align:center">执行器：用于启动线程池，是真正负责执行task的部件</td>
<td style="text-align:center">已完成</td>
</tr>
<tr>
<td style="text-align:center">input</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">internal</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">io</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">launcher</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">mapred</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">memory</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">metrics</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">network</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">partial</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">rdd</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">rpc</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">scheduler</td>
<td style="text-align:center">调度器：spark应用程序的任务调度器</td>
<td style="text-align:center">正在作</td>
</tr>
<tr>
<td style="text-align:center">security</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">serializer</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">shuffle</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">status.api.v1</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">storage</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">util</td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<!--对不起，到时间了，请停止装逼-->
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[spark关于parquet的优化]]></title>
      <url>https://stanxia.github.io/2017/11/01/spark%E5%85%B3%E4%BA%8Eparquet%E7%9A%84%E4%BC%98%E5%8C%96/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><div class="note info"><p>parquet是一种列式存储。可以提供面向列的存储和查询。</p></div>
<h2 id="Parquet的优势"><a href="#Parquet的优势" class="headerlink" title="Parquet的优势"></a>Parquet的优势</h2><p>在sparkSQL程序中使用parquet格式存储文件，在存储空间和查询性能方面都有很高的效率。</p>
<h3 id="存储方面"><a href="#存储方面" class="headerlink" title="存储方面"></a>存储方面</h3><p>因为是面向列的存储，同一列的类型相同，因而在存储的过程中可以使用更高效的压缩方案，可以节省大量的存储空间。</p>
<h3 id="查询方面"><a href="#查询方面" class="headerlink" title="查询方面"></a>查询方面</h3><p>在执行查询任务时，只会扫描需要的列，而不是全部，高度灵活性使查询变得非常高效。<br><a id="more"></a></p>
<h3 id="实例测试"><a href="#实例测试" class="headerlink" title="实例测试"></a>实例测试</h3><table>
<thead>
<tr>
<th>测试数据大小</th>
<th>存储类型</th>
<th>存储所占空间</th>
<th>查询性能</th>
</tr>
</thead>
<tbody>
<tr>
<td>1T</td>
<td>TEXTFILE</td>
<td>897.9G</td>
<td>698s</td>
</tr>
<tr>
<td>1T</td>
<td>Parquet</td>
<td>231.4G</td>
<td>21s</td>
</tr>
</tbody>
</table>
<h2 id="Parquet的使用"><a href="#Parquet的使用" class="headerlink" title="Parquet的使用"></a>Parquet的使用</h2><p>使用parquet的简单demo：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Encoders for most common types are automatically provided by importing spark.implicits._</span></div><div class="line"><span class="keyword">import</span> spark.implicits._</div><div class="line"></div><div class="line"><span class="keyword">val</span> peopleDF = spark.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</div><div class="line"></div><div class="line"><span class="comment">// DataFrames can be saved as Parquet files, maintaining the schema information</span></div><div class="line">peopleDF.write.parquet(<span class="string">"people.parquet"</span>)</div><div class="line"></div><div class="line"><span class="comment">// Read in the parquet file created above</span></div><div class="line"><span class="comment">// Parquet files are self-describing so the schema is preserved</span></div><div class="line"><span class="comment">// The result of loading a Parquet file is also a DataFrame</span></div><div class="line"><span class="keyword">val</span> parquetFileDF = spark.read.parquet(<span class="string">"people.parquet"</span>)</div><div class="line"></div><div class="line"><span class="comment">// Parquet files can also be used to create a temporary view and then used in SQL statements</span></div><div class="line">parquetFileDF.createOrReplaceTempView(<span class="string">"parquetFile"</span>)</div><div class="line"><span class="keyword">val</span> namesDF = spark.sql(<span class="string">"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19"</span>)</div><div class="line">namesDF.map(attributes =&gt; <span class="string">"Name: "</span> + attributes(<span class="number">0</span>)).show()</div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |       value|</span></div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |Name: Justin|</span></div><div class="line"><span class="comment">// +------------+</span></div></pre></td></tr></table></figure>
<h2 id="Parquet-的问题"><a href="#Parquet-的问题" class="headerlink" title="Parquet 的问题"></a>Parquet 的问题</h2><p> spark 写入数据到 hive 中，使用 Parquet 存储格式，查询该表时报错如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="type">Error</span>: java.io.<span class="type">IOException</span>: org.apache.parquet.io.<span class="type">ParquetDecodingException</span>: <span class="type">Can</span> not read value at <span class="number">0</span> in block <span class="number">-1</span> in file</div></pre></td></tr></table></figure>
<p>当时设置的字段属性为：<br><img src="http://oliji9s3j.bkt.clouddn.com/15118606648730.jpg" alt=""></p>
<p>经过比对，发现是 decimal 类型出了问题，查询 decimal 的字段时候就会报错，而查询其他的并不会报错。（这应该是 spark 引起的，因为在 hive 客户端执行 decimal 类型的操作时并不会出错。）</p>
<p>查阅网上，也有些朋友遇到了类似的事情，应该是官方的 bug ，暂时的解决办法是:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="number">1.</span> 将 <span class="type">Parquet</span> 的存储格式转换为 <span class="type">ORC</span> </div><div class="line"><span class="number">2.</span> 或将 decimal 换为 double 类型存储字段</div></pre></td></tr></table></figure>
<!--对不起，到时间了，请停止装逼-->
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[三步走战略]]></title>
      <url>https://stanxia.github.io/2017/11/01/%E4%B8%89%E6%AD%A5%E8%B5%B0%E6%88%98%E7%95%A5/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><div class="note info"><p>设定中长期规划<br>稳扎稳打，逐个击破，实现技术上的重大突破</p></div>
<a id="more"></a>
<!--请开始装逼-->
<h2 id="第一步深刻了解spark运行机制"><a href="#第一步深刻了解spark运行机制" class="headerlink" title="第一步深刻了解spark运行机制"></a>第一步深刻了解spark运行机制</h2><h2 id="第二步深度剖析sparkSQL和sparkStreaming"><a href="#第二步深度剖析sparkSQL和sparkStreaming" class="headerlink" title="第二步深度剖析sparkSQL和sparkStreaming"></a>第二步深度剖析sparkSQL和sparkStreaming</h2><h2 id="第三步实现对spark机器学习的深度掌握"><a href="#第三步实现对spark机器学习的深度掌握" class="headerlink" title="第三步实现对spark机器学习的深度掌握"></a>第三步实现对spark机器学习的深度掌握</h2><!--对不起，到时间了，请停止装逼-->
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[手把手搭建vps和shadowsocks]]></title>
      <url>https://stanxia.github.io/2017/10/31/%E6%89%8B%E6%8A%8A%E6%89%8B%E6%90%AD%E5%BB%BAvps%E5%92%8Cshadowsocks/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><div class="note info"><p>记性不好，做个记录，日后有需要时难得费神。</p></div>
<h2 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h2><p>了解一些原理，熟悉一些名词，也方便理解接下来安装过程中的操作。</p>
<h3 id="vps"><a href="#vps" class="headerlink" title="vps"></a>vps</h3><p>VPS(Virtual private server) 译作虚拟专用伺服器。你可以把它简单地理解为一台在远端的强劲电脑。当你租用了它以后，可以给它安装操作系统、软件，并通过一些工具连接和远程操控它。</p>
<h3 id="vultr"><a href="#vultr" class="headerlink" title="vultr"></a>vultr</h3><p><a href="https://www.vultr.com/" target="_blank" rel="external">Vultr</a> 是一家 VPS 服务器提供商，有美国、亚洲、欧洲等多地的 VPS。它家的服务器以性价比高闻名，按时间计费，最低的资费为每月 $2.5。</p>
<h3 id="linux"><a href="#linux" class="headerlink" title="linux"></a>linux</h3><p>Linux 是免费开源的操作系统，大概被世界上过半服务器所采用。有大量优秀的开源软件可以安装，上述 Shadowsocks 就是其一。你可以通过命令行来直接给 Linux 操作系统「下命令」，比如 $ cd ~/Desktop 就是进入你根目录下的 Desktop 文件夹。</p>
<h3 id="ssh"><a href="#ssh" class="headerlink" title="ssh"></a>ssh</h3><p> SSH 是一种网络协议，作为每一台 Linux 电脑的标准配置，用于计算机之间的加密登录。当你为租用的 VPS 安装 Linux 系统后，只要借助一些工具，就可以用 SSH 在你自己的 Mac/PC 电脑上远程登录该 VPS 了。</p>
<h3 id="shadowsocks"><a href="#shadowsocks" class="headerlink" title="shadowsocks"></a>shadowsocks</h3><p>Shadowsocks(ss) 是由 <a href="https://github.com/Clowwindy" target="_blank" rel="external">Clowwindy</a> 开发的一款软件，其作用本来是加密传输资料。当然，也正因为它加密传输资料的特性，使得 GFW 没法将由它传输的资料和其他普通资料区分开来，也就不能干扰我们访问那些「不存在」的网站了。<br><a id="more"></a><br><img src="/images/pic/1.png" alt="121"></p>
<h2 id="搭建vps"><a href="#搭建vps" class="headerlink" title="搭建vps"></a>搭建vps</h2><p>目的就是搭建梯子。无建站的需求。推荐vultr，最便宜的有2.5美元一个月。500g流量完全够用了。且现在支持支付宝付款，颇为方便。现阶段的优惠活动是新注册的用户完成指定的任务会获得3美元的奖励。（详细情况可依参见官网。）</p>
<h3 id="注册"><a href="#注册" class="headerlink" title="注册"></a>注册</h3><p>首先点击右侧注册链接：<a href="https://www.vultr.com/?ref=7008162" target="_blank" rel="external">https://www.vultr.com/2017Promo</a>，然后会来到下图所示的注册页面。</p>
<p><img src="https://www.vultr.net.cn/resources/images/goumai-01.png" alt="11"></p>
<p>第一个框中填写注册邮箱，第二个框中填写注册密码（至少包含1个小写字母、1个大写字母和1个数字），最后点击Create Account创建账户。</p>
<p>创建账户后注册邮箱会收到一封验证邮件，我们需要点击Verify Your E-mail来验证邮箱。</p>
<p>如果注册邮箱收不到验证邮件请更换注册邮箱后重复第一步。</p>
<p><img src="https://www.vultr.net.cn/resources/images/goumai-02.png" alt="12"></p>
<p>验证邮箱后我们会来到下图所示的登录界面，按下图中指示填写信息，然后点击Login登录。</p>
<p><img src="https://www.vultr.net.cn/resources/images/goumai-03.png" alt="13"></p>
<p>登陆后我们会来到充值界面。Vultr要求新账户充值后才可以正常创建服务器。Vultr已经支持支付宝了，在这里推荐大家使用支付宝充值，最低金额为10美元。</p>
<p><img src="https://www.vultr.net.cn/resources/images/goumai-04.png" alt="14"></p>
<h3 id="购买"><a href="#购买" class="headerlink" title="购买"></a>购买</h3><p>充值完毕后点击右上角的蓝色加号按钮进入创建服务器界面。</p>
<p>首先需要选择Server Location即机房位置，从左到右、从上到下依次为东京、新加坡、伦敦、法兰克福、巴黎、阿姆斯特丹、迈阿密、亚特兰大、芝加哥、硅谷、达拉斯、洛杉矶、纽约、西雅图、悉尼。</p>
<p><img src="https://www.vultr.net.cn/resources/images/goumai-06.png" alt="16"></p>
<p>然后需要选择Server Type即服务类型，这里大家需要选择安装Debian 7 x64系统，因为这个系统折腾起来比较容易，搭建东西也简单便捷。</p>
<p><img src="https://www.vultr.net.cn/resources/images/goumai-07.png" alt="17"></p>
<p>然后需要选择Server Size即方案类型，这里大家可以按照需要自行选择，如果只是普通使用那么选择第二个5美元方案即可。</p>
<p><img src="https://www.vultr.net.cn/resources/images/goumai-08.png" alt="111"></p>
<p>然后Additional Features、Startup Script、SSH Keys以及Server Hostname &amp; Label等四部分大家保持默认即可，最后点击右下方的蓝色Deploy Now按钮确认创建服务器。</p>
<p><img src="https://www.vultr.net.cn/resources/images/goumai-09.png" alt="222"></p>
<p>创建服务器后我们会看到下图所示界面。</p>
<p><img src="https://www.vultr.net.cn/resources/images/goumai-10.png" alt="22"></p>
<p>上图中我们需要耐心等待3~4分钟，等红色Installing字变为绿色Running字后，点击Cloud Instance即可进入服务器详细信息界面，如下图所示。</p>
<p><img src="https://www.vultr.net.cn/resources/images/goumai-11.png" alt="33"></p>
<p>左侧红框内四行信息依次为机房位置、IP地址、登录用户名、登录密码。IP地址后面的按钮为复制IP地址，登录密码后面的按钮为复制密码及显示/隐藏密码。右上角红框内后面四个按钮分别是关闭服务器、重启服务器、重装系统、删除服务器。</p>
<h2 id="远程登录"><a href="#远程登录" class="headerlink" title="远程登录"></a>远程登录</h2><p>安装远程登录软件。这里以windos端的xshell为例。使用mac的同学可以下载iTerm。</p>
<p>下载安装后打开软件。根据下图中的指示，我们点击会话框中的新建按钮。</p>
<p><img src="https://www.vultr.net.cn/resources/images/ssh-001.png" alt="111"></p>
<p>点击新建按钮后会弹出下图所示界面。根据图中指示，我们首先填写IP地址，然后点击确定按钮。</p>
<p><img src="https://www.vultr.net.cn/resources/images/ssh-002.png" alt="333"></p>
<p>点击确定按钮后我们会回到下图所示界面。根据图中指示，我们双击打开新建会话或者点击下方连接按钮打开新建会话。</p>
<p><img src="https://www.vultr.net.cn/resources/images/ssh-003.png" alt="444"></p>
<p>开新建会话后会弹出下图所示界面。根据图中指示，我们点击接受并保存按钮。</p>
<p><img src="https://www.vultr.net.cn/resources/images/ssh-004.png" alt="555"></p>
<p>点击接受并保存按钮会弹出下图所示界面。根据图中指示，我们首先填写SSH连接密码，然后打钩记住密码，最后点击确定按钮。</p>
<p>如果提示需要输入用户名（登录名），那么请输入root！</p>
<p><img src="https://www.vultr.net.cn/resources/images/ssh-005.png" alt="56"></p>
<p>点击确定按钮后服务器会自动连接，连接完毕后我们会来到下图所示界面</p>
<p><img src="https://www.vultr.net.cn/resources/images/ssh-006.png" alt="7"></p>
<h2 id="部署shadowsocks"><a href="#部署shadowsocks" class="headerlink" title="部署shadowsocks"></a>部署shadowsocks</h2><p>这里采用网上整理的一键部署的方案。简单方便操作。 </p>
<p>首先复制以下内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget -N --no-check-certificate https://0123.cool/download/55r.sh &amp;&amp; chmod +x 55r.sh &amp;&amp; ./55r.sh</div></pre></td></tr></table></figure>
<p>然后回到Xshell软件，右击选择粘贴，粘贴完毕后回车继续。</p>
<p><img src="https://www.vultr.net.cn/resources/images/ssr-001.png" alt="i"></p>
<p>回车后系统会自行下载脚本文件并运行。根据下图图中指示，我们依次输入SSR的各项连接信息，最后回车继续。</p>
<p><img src="https://www.vultr.net.cn/resources/images/ssr-002.png" alt="2"></p>
<p>安装完成后会出现下图所示界面。根据图中指示，我们将红框圈中的信息保存到记事本内。</p>
<p><img src="https://www.vultr.net.cn/resources/images/ssr-003.png" alt="3"></p>
<h2 id="配置锐意加速"><a href="#配置锐意加速" class="headerlink" title="配置锐意加速"></a>配置锐意加速</h2><p>根据下图图中指示，我们继续复制下列信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget -N --no-check-certificate https://0123.cool/download/rs.sh &amp;&amp; bash rs.sh install</div></pre></td></tr></table></figure>
<p>然后回到Xshell软件，右击选择粘贴，粘贴完毕后回车继续。</p>
<p><img src="https://www.vultr.net.cn/resources/images/rs-001-2.png" alt="4"></p>
<p>回车后系统会自行下载脚本文件并运行。根据下图图中指示，我们依次输入锐速的各项配置信息，最后回车继续。</p>
<p><img src="https://www.vultr.net.cn/resources/images/rs-002.png" alt="5"></p>
<p>回车后，系统自动执行命令完成破解版锐速安装，如下图所示。</p>
<p><img src="https://www.vultr.net.cn/resources/images/rs-003.png" alt="6"></p>
<p>我们首先输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">reboot</div></pre></td></tr></table></figure>
<p>然后回车，Xshell会断开连接，系统会在1分钟后重启完毕，此时可以关闭Xshell软件了。</p>
<p>搭建教程到此结束，亲测成功。如果不能连接的，请检查自己的每一步操作。</p>
<!--视频end-->
<!--对不起，到时间了，请停止装逼-->
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[spark报错集]]></title>
      <url>https://stanxia.github.io/2017/10/30/spark%E6%8A%A5%E9%94%99%E9%9B%86/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><h2 id="有话要说"><a href="#有话要说" class="headerlink" title="有话要说"></a>有话要说</h2><p>针对一个老毛病：有些错误屡犯屡改，屡改屡犯，没有引起根本上的注意，或者没有从源头理解错误发生的底层原理，导致做很多无用功。</p>
<p>总结历史，并从中吸取教训，减少无用功造成的时间浪费。特此将从目前遇到的spark问题全部记录在这里，搞清楚问题，自信向前。<br><a id="more"></a></p>
<h2 id="问题汇总"><a href="#问题汇总" class="headerlink" title="问题汇总"></a>问题汇总</h2><h3 id="关键词：spark-hive"><a href="#关键词：spark-hive" class="headerlink" title="关键词：spark-hive"></a>关键词：spark-hive</h3><h4 id="概述："><a href="#概述：" class="headerlink" title="概述："></a>概述：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Unable to instantiate SparkSession with Hive support because Hive classes are not found.</div></pre></td></tr></table></figure>
<h4 id="场景："><a href="#场景：" class="headerlink" title="场景："></a>场景：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">在本地调试spark程序，连接虚拟机上的集群，尝试执行sparkSQL时，启动任务就报错。</div></pre></td></tr></table></figure>
<h4 id="原理："><a href="#原理：" class="headerlink" title="原理："></a>原理：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">缺少sparkSQL连接hive的必要和依赖jar包</div></pre></td></tr></table></figure>
<h4 id="办法："><a href="#办法：" class="headerlink" title="办法："></a>办法：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">在项目／模块的pom.xml中添加相关的spark-hive依赖jar包。</div><div class="line">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive_2.11 --&gt;</div><div class="line">&lt;dependency&gt;</div><div class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;2.1.1&lt;/version&gt;</div><div class="line">    &lt;scope&gt;provided&lt;/scope&gt;</div><div class="line">&lt;/dependency&gt;</div><div class="line">重新编译项目／模块即可。</div></pre></td></tr></table></figure>
<!--对不起，到时间了，请停止装逼-->
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[life]]></title>
      <url>https://stanxia.github.io/2017/10/29/life/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><!--请开始装逼-->
<!--视频start-->
<div id="dplayer0" class="dplayer" style="margin-bottom: 20px;"></div><script>var dplayer0 = new DPlayer({"element":document.getElementById("dplayer0"),"autoplay":false,"theme":"#FADFA3","loop":true,"video":{"url":"http://oliji9s3j.bkt.clouddn.com/Unbroken%20-%20Motivational%20Video.mp4","pic":"/images/pic/life.jpeg"}});</script>
<div class="note info"><p>Life is simple &amp;&amp; funny.</p></div>
<a id="more"></a>
<!--视频end-->
<!--对不起，到时间了，请停止装逼-->
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[杂乱无章]]></title>
      <url>https://stanxia.github.io/2017/10/29/%E6%9D%82%E4%B9%B1%E6%97%A0%E7%AB%A0/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><div class="note info"><p>时光的机器，加足马力冲回过去</p>
<p>历史的长河，丝丝涟漪涌向未来</p></div>
<a id="more"></a>
<!--请开始装逼-->
<p>道不清楚，说不明白，夜深人静的时候，说一些想到的废话。窗外隆隆作响，不知疲倦的机器不知疲倦的执行着不知疲倦的动作。窗内屏幕暗淡，双眼干涩，思索着宇宙外的回想。</p>
<p>小时候，望向星空，那时的天空群星闪烁，哪像现在，嘿，享受了大城市的霓虹，哪里再给你无垠的星空，贪。</p>
<p>躺在草地，微风轻拂脸颊，初秋的夜晚，有点微凉。</p>
<p>仰望星河，也想着外面的世界，多精彩。</p>
<p>揣摩着无垠的宇宙，翻过地球，越过银河，驶向无限拓展的星际，身上的烦恼，微风一吹，全散了。</p>
<p>风轻拂，静静望着天空，思考着外面的朋友或许也在渴望着远方的我，伸手触摸这天空，抓一把星辰贪婪的放入梦。</p>
<p>深邃的夜空，望不尽的远方，是光明中的无尽黑暗，也似黑暗道路的一束亮光，洒向我，思绪跟着遨游，呵，世界与我万千美好，我与世界却念念叨叨，琐琐碎碎，麻麻烦烦。心里是想放飞的。</p>
<p>夜深，车水呼啸，诉说着城市的不眠，可我困，关窗，闷。开窗，嘿，不知疲倦的机器又开始不知疲倦的执行不知疲倦的动作。这样的夜晚，眠难。</p>
<p>深夜思考，写作。夜使我宁静，内心的宁静，这白天的大城市给予不了。感谢夜的馈赠，接收这无上的加冕，驰骋在思绪的星空，痛快，精彩，精彩。</p>
<p>杂乱无章，呵，可以。</p>
<!--对不起，到时间了，请停止装逼-->
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[闲谈]]></title>
      <url>https://stanxia.github.io/2017/10/28/%E9%97%B2%E8%B0%88/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><div class="note info"><p>九九登高忆重阳</p></div>
<a id="more"></a>
<!--请开始装逼-->
<!--对不起，到时间了，请停止装逼-->
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[这个杀手不太冷]]></title>
      <url>https://stanxia.github.io/2017/10/28/%E8%BF%99%E4%B8%AA%E6%9D%80%E6%89%8B%E4%B8%8D%E5%A4%AA%E5%86%B7/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><div class="note info"><p>Is life always this hard,or is it just when you’re a kid?<br>Always like this.</p></div>
<p><img src="http://oliji9s3j.bkt.clouddn.com/15114113550802.jpg" alt=""></p>
<a id="more"></a>
<!--请开始装逼-->
<!--对不起，到时间了，请停止装逼-->
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[mac使用小技巧]]></title>
      <url>https://stanxia.github.io/2017/10/28/mac%E4%BD%BF%E7%94%A8%E5%B0%8F%E6%8A%80%E5%B7%A7/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><div class="note info"><p>记录mac使用的小技巧<br>持续更新ing </p></div>
<!--请开始装逼-->
<h2 id="开启充电提示音（类似于iphone充电提示音，默认关闭）"><a href="#开启充电提示音（类似于iphone充电提示音，默认关闭）" class="headerlink" title="开启充电提示音（类似于iphone充电提示音，默认关闭）"></a>开启充电提示音（类似于iphone充电提示音，默认关闭）</h2><p>终端输入（开启）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">defaults write com.apple.PowerChime ChimeOnAllHardware -bool true; open /System/Library/CoreServices/PowerChime.app &amp;</div></pre></td></tr></table></figure>
<p>关闭：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">defaults write com.apple.PowerChime ChimeOnAllHardware -bool false;killall PowerChime</div></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="隐藏文件夹"><a href="#隐藏文件夹" class="headerlink" title="隐藏文件夹"></a>隐藏文件夹</h2><p>更好的保护学习资料，有时候需要设置隐藏文件夹：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mv foldername .foldername</div></pre></td></tr></table></figure>
<h2 id="查看隐藏文件夹"><a href="#查看隐藏文件夹" class="headerlink" title="查看隐藏文件夹"></a>查看隐藏文件夹</h2><p>mac最新版本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">⌘⇧.(Command + Shift + .)  #隐藏 和显示</div></pre></td></tr></table></figure>
<h2 id="Macbook-Pro-用外接显示器时，如何关闭笔记本屏幕，同时开盖使用"><a href="#Macbook-Pro-用外接显示器时，如何关闭笔记本屏幕，同时开盖使用" class="headerlink" title="Macbook Pro 用外接显示器时，如何关闭笔记本屏幕，同时开盖使用"></a>Macbook Pro 用外接显示器时，如何关闭笔记本屏幕，同时开盖使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo nvram boot-args=&quot;iog=0x0&quot; #(10.10以前版本)</div><div class="line">sudo nvram boot-args=&quot;niog=1&quot; #(10.10及以后版本)这个命令的意思就是外接显示器时关闭自身屏幕，重启生效</div></pre></td></tr></table></figure>
<p>开机流程：连上电源和外接显示器，按开机键，立即合盖，等外接显示器有信号时开盖即可如果报错 (已知 10.11/10.12 会报错)<br>nvram: Error setting variable - ‘boot-args’: (iokit/common) general error<br><div class="note info"><ol>
<li>重启，按住command + r 进入恢复界面</li>
<li>左上角菜单里面找到终端，输入nvram boot-args=”niog=1”，回车问题解决。重启生效</li>
</ol></div></p>
<!--对不起，到时间了，请停止装逼-->
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[photo]]></title>
      <url>https://stanxia.github.io/1999/10/27/photo/</url>
      <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><!--请开始装逼-->
<div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="http://oliji9s3j.bkt.clouddn.com/%E4%B8%80%E8%B7%AF%E9%80%86%E9%A3%8E.png" alt=""></div><div class="group-picture-column" style="width: 50%;"><img src="http://oliji9s3j.bkt.clouddn.com/%E5%86%8D%E8%A7%81mtv.png" alt=""></div></div><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="http://oliji9s3j.bkt.clouddn.com/%E6%96%B0%E7%9A%84%E5%BF%83%E8%B7%B3.png" alt=""></div></div></div></div>
<!--对不起，到时间了，请停止装逼-->
]]></content>
    </entry>
    
  
  
</search>
