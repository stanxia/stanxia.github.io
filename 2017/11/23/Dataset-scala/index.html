<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/1281.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/323.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/161.png?v=5.1.3">


  <link rel="mask-icon" href="/images/atom.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="spark,源码,">










<meta name="description" content="前言Dataset 是一种强类型的领域特定对象集合，可以在使用功能或关系操作的同时进行转换。每个 Dataset 也有一个名为 “DataFrame” 的无类型视图，它是 [[Row]] 的 Dataset。Dataset 上可用的操作分为转换和动作:  转换：产生新的 Dataset ；包括 map, filter, select, and aggregate (groupBy).动作：触发计算">
<meta name="keywords" content="spark,源码">
<meta property="og:type" content="article">
<meta property="og:title" content="Dataset.scala">
<meta property="og:url" content="https://stanxia.github.io/2017/11/23/Dataset-scala/index.html">
<meta property="og:site_name" content="东篱下">
<meta property="og:description" content="前言Dataset 是一种强类型的领域特定对象集合，可以在使用功能或关系操作的同时进行转换。每个 Dataset 也有一个名为 “DataFrame” 的无类型视图，它是 [[Row]] 的 Dataset。Dataset 上可用的操作分为转换和动作:  转换：产生新的 Dataset ；包括 map, filter, select, and aggregate (groupBy).动作：触发计算">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2020-02-05T17:10:30.353Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Dataset.scala">
<meta name="twitter:description" content="前言Dataset 是一种强类型的领域特定对象集合，可以在使用功能或关系操作的同时进行转换。每个 Dataset 也有一个名为 “DataFrame” 的无类型视图，它是 [[Row]] 的 Dataset。Dataset 上可用的操作分为转换和动作:  转换：产生新的 Dataset ；包括 map, filter, select, and aggregate (groupBy).动作：触发计算">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.3',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://stanxia.github.io/2017/11/23/Dataset-scala/">





  <title>Dataset.scala | 东篱下</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">东篱下</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">闲庭舞键</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-paper">
          <a href="/tags/paper/" rel="section">
            
            PAPER
          </a>
        </li>
      
        
        <li class="menu-item menu-item-economist">
          <a href="/tags/economist/" rel="section">
            
            ECONOMIST
          </a>
        </li>
      
        
        <li class="menu-item menu-item-spark">
          <a href="/tags/spark/" rel="section">
            
            SPARK
          </a>
        </li>
      
        
        <li class="menu-item menu-item-hive">
          <a href="/tags/hive/" rel="section">
            
            HIVE
          </a>
        </li>
      
        
        <li class="menu-item menu-item-flink">
          <a href="/tags/flink/" rel="section">
            
            FLINK
          </a>
        </li>
      
        
        <li class="menu-item menu-item-algo">
          <a href="/tags/algo/" rel="section">
            
            ALGO
          </a>
        </li>
      
        
        <li class="menu-item menu-item-gem">
          <a href="/gem/" rel="section">
            
            GEM
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>
    
    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://stanxia.github.io/2017/11/23/Dataset-scala/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="舒服一夏">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>
    
    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="东篱下">
    </span>
    
    
      <header class="post-header">
    
        
        
          <h1 class="post-title" itemprop="name headline">Dataset.scala</h1>
        
    
        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-23T09:34:44+00:00">
                2017-11-23
              </time>
            
    
            
    
            
          </span>
    
          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
    
                
                
              
            </span>
          
    
          
            
          
    
          
          
    
          
    
          
    
          
    
        </div>
      </header>
    
    
    
    
    
    <div class="post-body" itemprop="articleBody">
    
      
      
    
      
        <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Dataset 是一种强类型的领域特定对象集合，可以在使用功能或关系操作的同时进行转换。每个 Dataset 也有一个名为 “DataFrame” 的无类型视图，它是 [[Row]] 的 Dataset。<br>Dataset 上可用的操作分为转换和动作:</p>
<blockquote>
<p>转换：产生新的 Dataset ；包括 map, filter, select, and aggregate (<code>groupBy</code>).<br>动作：触发计算并返回结果 ；包括 count, show, or 写数据到文件系统。</p>
</blockquote>
<p>Dataset是懒加载的，例如：只有提交动作的时候才会触发计算。在内部，Datasets表示一个逻辑计划，它描述生成数据所需的计算。当提交动作时，Spark的查询优化器会优化逻辑计划，并以并行和分布式的方式生成有效执行的物理计划。请使用<code>explain</code> 功能，探索逻辑计划和优化的物理计划。</p>
<p>为了有效地支持特定于领域的对象，需要[[Encoder]]。编码器将特定类型的“T”映射到Spark的内部类型系统。例如：给一个 <code>Person</code> 类，并带有两个属性：<code>name</code> (string) and <code>age</code> (int),编码器告诉Spark在运行时生成代码，序列化 <code>Person</code> 对象为二进制结构。</p>
<p>通常有两种创建Dataset的方法:</p>
<blockquote>
<p>使用 <code>SparkSession</code> 上可用的 <code>read</code> 方法读取 Spark 指向的存储系统上的文件。<br>用现存的 Datasets 转换而来。</p>
</blockquote>
<p>Dataset操作也可以是无类型的，通过多种领域专用语言（DSL）方法定义：这些操作非常类似于 R或Python语言中的 数据框架抽象中可用的操作。<br><a id="more"></a></p>
<!--请开始装逼-->
<h2 id="basic-基础方法"><a href="#basic-基础方法" class="headerlink" title="basic-基础方法"></a>basic-基础方法</h2><h3 id="toDF"><a href="#toDF" class="headerlink" title="toDF"></a>toDF</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Converts this strongly typed collection of data to generic Dataframe. In contrast to the</span></span><br><span class="line"><span class="comment">  * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]]</span></span><br><span class="line"><span class="comment">  * objects that allow fields to be accessed by ordinal or name.</span></span><br><span class="line"><span class="comment">  * 将这种强类型的数据集合转换为一般的Dataframe。</span></span><br><span class="line"><span class="comment">  * 与Dataset操作所使用的强类型对象相反，</span></span><br><span class="line"><span class="comment">  * Dataframe返回泛型[[Row]]对象，这些对象允许通过序号或名称访问字段</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="comment">// This is declared with parentheses to prevent the Scala compiler from treating</span></span><br><span class="line"><span class="comment">// `ds.toDF("1")` as invoking this toDF and then apply on the returned DataFrame.</span></span><br><span class="line"><span class="comment">// 这是用括号声明的，以防止Scala编译器处理ds.toDF(“1”)调用这个toDF，然后在返回的DataFrame上应用。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toDF</span></span>(): <span class="type">DataFrame</span> = <span class="keyword">new</span> <span class="type">Dataset</span>[<span class="type">Row</span>](sparkSession, queryExecution, <span class="type">RowEncoder</span>(schema))</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.</span></span><br><span class="line"><span class="comment">  * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with</span></span><br><span class="line"><span class="comment">  * meaningful names. For example:</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 将这种强类型的数据集合转换为通用的“DataFrame”，并将列重命名。</span></span><br><span class="line"><span class="comment">  * 在将tuple的RDD转换为富有含义的名称的“DataFrame”时，这是非常方便的，如：</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">  *   val rdd: RDD[(Int, String)] = ...</span></span><br><span class="line"><span class="comment">  *   rdd.toDF()  // 隐式转换创建了 DataFrame ，列名为： `_1` and `_2`</span></span><br><span class="line"><span class="comment">  *   rdd.toDF("id", "name")  // 创建了 DataFrame ，列名为： "id" and "name"</span></span><br><span class="line"><span class="comment">  * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@scala</span>.annotation.varargs</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toDF</span></span>(colNames: <span class="type">String</span>*): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  require(schema.size == colNames.size,</span><br><span class="line">    <span class="string">"The number of columns doesn't match.\n"</span> +</span><br><span class="line">      <span class="string">s"Old column names (<span class="subst">$&#123;schema.size&#125;</span>): "</span> + schema.fields.map(_.name).mkString(<span class="string">", "</span>) + <span class="string">"\n"</span> +</span><br><span class="line">      <span class="string">s"New column names (<span class="subst">$&#123;colNames.size&#125;</span>): "</span> + colNames.mkString(<span class="string">", "</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> newCols = logicalPlan.output.zip(colNames).map &#123; <span class="keyword">case</span> (oldAttribute, newName) =&gt;</span><br><span class="line">    <span class="type">Column</span>(oldAttribute).as(newName)</span><br><span class="line">  &#125;</span><br><span class="line">  select(newCols: _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="as"><a href="#as" class="headerlink" title="as"></a>as</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * :: Experimental ::</span></span><br><span class="line"><span class="comment">    * Returns a new Dataset where each record has been mapped on to the specified type. The</span></span><br><span class="line"><span class="comment">    * method used to map columns depend on the type of `U`:</span></span><br><span class="line"><span class="comment">    * </span></span><br><span class="line"><span class="comment">    * 返回一个新的Dataset，其中每个记录都被映射到指定的类型。用于映射列的方法取决于“U”的类型:</span></span><br><span class="line"><span class="comment">    * </span></span><br><span class="line"><span class="comment">    *  - When `U` is a class, fields for the class will be mapped to columns of the same name</span></span><br><span class="line"><span class="comment">    * (case sensitivity is determined by `spark.sql.caseSensitive`).</span></span><br><span class="line"><span class="comment">    * </span></span><br><span class="line"><span class="comment">    * 当“U”是类时：类的属性将映射到相同名称的列</span></span><br><span class="line"><span class="comment">    * </span></span><br><span class="line"><span class="comment">    *  - When `U` is a tuple, the columns will be be mapped by ordinal (i.e. the first column will</span></span><br><span class="line"><span class="comment">    * be assigned to `_1`).</span></span><br><span class="line"><span class="comment">    * </span></span><br><span class="line"><span class="comment">    * 当“U”是元组时：列将由序数映射 （例如，第一列将为 "_1"）</span></span><br><span class="line"><span class="comment">    * </span></span><br><span class="line"><span class="comment">    *  - When `U` is a primitive type (i.e. String, Int, etc), then the first column of the</span></span><br><span class="line"><span class="comment">    * `DataFrame` will be used.</span></span><br><span class="line"><span class="comment">    * </span></span><br><span class="line"><span class="comment">    * 当“U”是 基本类型（如 String，Int等）：然后将使用“DataFrame”的第一列。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * If the schema of the Dataset does not match the desired `U` type, you can use `select`</span></span><br><span class="line"><span class="comment">    * along with `alias` or `as` to rearrange or rename as required.</span></span><br><span class="line"><span class="comment">    * </span></span><br><span class="line"><span class="comment">    * 如果数据集的模式与所需的“U”类型不匹配，您可以使用“select”和“alias”或“as”来重新排列或重命名。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group basic</span></span><br><span class="line"><span class="comment">    * @since 1.6.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="meta">@Experimental</span></span><br><span class="line">  <span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">as</span></span>[<span class="type">U</span>: <span class="type">Encoder</span>]: <span class="type">Dataset</span>[<span class="type">U</span>] = <span class="type">Dataset</span>[<span class="type">U</span>](sparkSession, logicalPlan)</span><br></pre></td></tr></table></figure>
<h3 id="schema"><a href="#schema" class="headerlink" title="schema"></a>schema</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns the schema of this Dataset.</span></span><br><span class="line"><span class="comment">  * 返回该Dataset的模版</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">schema</span></span>: <span class="type">StructType</span> = queryExecution.analyzed.schema</span><br></pre></td></tr></table></figure>
<h3 id="printSchema"><a href="#printSchema" class="headerlink" title="printSchema"></a>printSchema</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Prints the schema to the console in a nice tree format.</span></span><br><span class="line"><span class="comment">  * </span></span><br><span class="line"><span class="comment">  * 以一种漂亮的树格式将模式打印到控制台。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="comment">// scalastyle:off println</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printSchema</span></span>(): <span class="type">Unit</span> = println(schema.treeString)</span><br></pre></td></tr></table></figure>
<h3 id="explain"><a href="#explain" class="headerlink" title="explain"></a>explain</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Prints the plans (logical and physical) to the console for debugging purposes.</span></span><br><span class="line"><span class="comment">  * </span></span><br><span class="line"><span class="comment">  * 将计划(逻辑和物理)打印到控制台以进行调试。</span></span><br><span class="line"><span class="comment">  * 参数：extended = false 为物理计划</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">explain</span></span>(extended: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> explain = <span class="type">ExplainCommand</span>(queryExecution.logical, extended = extended)</span><br><span class="line">  sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach &#123;</span><br><span class="line">    <span class="comment">// scalastyle:off println</span></span><br><span class="line">    r =&gt; println(r.getString(<span class="number">0</span>))</span><br><span class="line">    <span class="comment">// scalastyle:on println</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Prints the physical plan to the console for debugging purposes.</span></span><br><span class="line"><span class="comment">  * 将物理计划打印到控制台以进行调试。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">explain</span></span>(): <span class="type">Unit</span> = explain(extended = <span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
<h3 id="dtypes"><a href="#dtypes" class="headerlink" title="dtypes"></a>dtypes</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns all column names and their data types as an array.</span></span><br><span class="line"><span class="comment">  * 以数组的形式返回所有列名称和它们的数据类型</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dtypes</span></span>: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">String</span>)] = schema.fields.map &#123; field =&gt;</span><br><span class="line">  (field.name, field.dataType.toString)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="columns"><a href="#columns" class="headerlink" title="columns"></a>columns</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns all column names as an array.</span></span><br><span class="line"><span class="comment">  * 以数组的形式返回 所有列名</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">columns</span></span>: <span class="type">Array</span>[<span class="type">String</span>] = schema.fields.map(_.name)</span><br></pre></td></tr></table></figure>
<h3 id="isLocal"><a href="#isLocal" class="headerlink" title="isLocal"></a>isLocal</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns true if the `collect` and `take` methods can be run locally</span></span><br><span class="line"><span class="comment">  * (without any Spark executors).</span></span><br><span class="line"><span class="comment">  * 如果`collect` and `take` 方法能在本地运行，则返回true</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isLocal</span></span>: <span class="type">Boolean</span> = logicalPlan.isInstanceOf[<span class="type">LocalRelation</span>]</span><br></pre></td></tr></table></figure>
<h3 id="checkpoint"><a href="#checkpoint" class="headerlink" title="checkpoint"></a>checkpoint</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate</span></span><br><span class="line"><span class="comment">  * the logical plan of this Dataset, which is especially useful in iterative algorithms where the</span></span><br><span class="line"><span class="comment">  * plan may grow exponentially. It will be saved to files inside the checkpoint</span></span><br><span class="line"><span class="comment">  * directory set with `SparkContext#setCheckpointDir`.</span></span><br><span class="line"><span class="comment">  * </span></span><br><span class="line"><span class="comment">  * 急切地检查一个数据集并返回新的数据集。</span></span><br><span class="line"><span class="comment">  * 检查点能用来清除Dataset的逻辑计划，尤其是在可能生成指数级别的迭代算法中尤其有用。</span></span><br><span class="line"><span class="comment">  * 将会在检查点目录中保存检查文件。可以在`SparkContext#setCheckpointDir`中设置。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 2.1.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkpoint</span></span>(): <span class="type">Dataset</span>[<span class="type">T</span>] = checkpoint(eager = <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the</span></span><br><span class="line"><span class="comment">  * logical plan of this Dataset, which is especially useful in iterative algorithms where the</span></span><br><span class="line"><span class="comment">  * plan may grow exponentially. It will be saved to files inside the checkpoint</span></span><br><span class="line"><span class="comment">  * directory set with `SparkContext#setCheckpointDir`.</span></span><br><span class="line"><span class="comment">  * 返回Dataset 之前检查过的版本。</span></span><br><span class="line"><span class="comment">  * 检查点能用来清除Dataset的逻辑计划，尤其是在可能生成指数级别的迭代算法中尤其有用。</span></span><br><span class="line"><span class="comment">  * 将会在检查点目录中保存检查文件。可以在`SparkContext#setCheckpointDir`中设置。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 2.1.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkpoint</span></span>(eager: <span class="type">Boolean</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> internalRdd = queryExecution.toRdd.map(_.copy())</span><br><span class="line">  internalRdd.checkpoint()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (eager) &#123;</span><br><span class="line">    internalRdd.count()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> physicalPlan = queryExecution.executedPlan</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the</span></span><br><span class="line">  <span class="comment">// size of `PartitioningCollection` may grow exponentially for queries involving deep inner</span></span><br><span class="line">  <span class="comment">// joins.</span></span><br><span class="line">  <span class="comment">// 每当我们看到“PartitioningCollection”时，就采用第一个叶子分区</span></span><br><span class="line">  <span class="comment">// 否则，用于涉及深度内连接的查询，“PartitioningCollection”的大小可能会以指数形式增长。</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">firstLeafPartitioning</span></span>(partitioning: <span class="type">Partitioning</span>): <span class="type">Partitioning</span> = &#123;</span><br><span class="line">    partitioning <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> p: <span class="type">PartitioningCollection</span> =&gt; firstLeafPartitioning(p.partitionings.head)</span><br><span class="line">      <span class="keyword">case</span> p =&gt; p</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)</span><br><span class="line"></span><br><span class="line">  <span class="type">Dataset</span>.ofRows(</span><br><span class="line">    sparkSession,</span><br><span class="line">    <span class="type">LogicalRDD</span>(</span><br><span class="line">      logicalPlan.output,</span><br><span class="line">      internalRdd,</span><br><span class="line">      outputPartitioning,</span><br><span class="line">      physicalPlan.outputOrdering</span><br><span class="line">    )(sparkSession)).as[<span class="type">T</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="persist"><a href="#persist" class="headerlink" title="persist"></a>persist</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 持久化。</span></span><br><span class="line"><span class="comment">  * 根据默认的 存储级别 (`MEMORY_AND_DISK`)  持久化Dataset。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= &#123;</span><br><span class="line">  sparkSession.sharedState.cacheManager.cacheQuery(<span class="keyword">this</span>)</span><br><span class="line">  <span class="keyword">this</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Persist this Dataset with the given storage level.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 根据指定的 存储级别 持久化 Dataset。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param newLevel One of:</span></span><br><span class="line"><span class="comment">  *                 `MEMORY_ONLY`,</span></span><br><span class="line"><span class="comment">  *                 `MEMORY_AND_DISK`,</span></span><br><span class="line"><span class="comment">  *                 `MEMORY_ONLY_SER`,</span></span><br><span class="line"><span class="comment">  *                 `MEMORY_AND_DISK_SER`,</span></span><br><span class="line"><span class="comment">  *                 `DISK_ONLY`,</span></span><br><span class="line"><span class="comment">  *                 `MEMORY_ONLY_2`, 与MEMORY_ONLY的区别是会备份数据到其他节点上</span></span><br><span class="line"><span class="comment">  *                 `MEMORY_AND_DISK_2`, 与MEMORY_AND_DISK的区别是会备份数据到其他节点上</span></span><br><span class="line"><span class="comment">  *                 etc.</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= &#123;</span><br><span class="line">  sparkSession.sharedState.cacheManager.cacheQuery(<span class="keyword">this</span>, <span class="type">None</span>, newLevel)</span><br><span class="line">  <span class="keyword">this</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 持久化。</span></span><br><span class="line"><span class="comment">  * 根据默认的 存储级别 (`MEMORY_AND_DISK`)  持久化Dataset。</span></span><br><span class="line"><span class="comment">  * 和 persist 一致。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= persist()</span><br></pre></td></tr></table></figure>
<h3 id="storageLevel"><a href="#storageLevel" class="headerlink" title="storageLevel"></a>storageLevel</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 获取当前Dataset的当前存储级别。如果没有缓存则 StorageLevel.NONE。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 2.1.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storageLevel</span></span>: <span class="type">StorageLevel</span> = &#123;</span><br><span class="line">  sparkSession.sharedState.cacheManager.lookupCachedData(<span class="keyword">this</span>).map &#123; cachedData =&gt;</span><br><span class="line">    cachedData.cachedRepresentation.storageLevel</span><br><span class="line">  &#125;.getOrElse(<span class="type">StorageLevel</span>.<span class="type">NONE</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="unpersist"><a href="#unpersist" class="headerlink" title="unpersist"></a>unpersist</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 解除持久化。</span></span><br><span class="line"><span class="comment">  * 将Dataset标记为非持久化，并从内存和磁盘中移除所有的块。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param blocking Whether to block until all blocks are deleted.</span></span><br><span class="line"><span class="comment">  *                 是否阻塞，直到删除所有的块。</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unpersist</span></span>(blocking: <span class="type">Boolean</span>): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= &#123;</span><br><span class="line">  sparkSession.sharedState.cacheManager.uncacheQuery(<span class="keyword">this</span>, blocking)</span><br><span class="line">  <span class="keyword">this</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 解除持久化。</span></span><br><span class="line"><span class="comment">  * 将Dataset标记为非持久化，并从内存和磁盘中移除所有的块。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unpersist</span></span>(): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= unpersist(blocking = <span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
<h3 id="rdd"><a href="#rdd" class="headerlink" title="rdd"></a>rdd</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Represents the content of the Dataset as an `RDD` of [[T]].</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 转换为[[T]]的“RDD”，表示Dataset的内容</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> objectType = exprEnc.deserializer.dataType</span><br><span class="line">  <span class="keyword">val</span> deserialized = <span class="type">CatalystSerde</span>.deserialize[<span class="type">T</span>](logicalPlan)</span><br><span class="line">  sparkSession.sessionState.executePlan(deserialized).toRdd.mapPartitions &#123; rows =&gt;</span><br><span class="line">    rows.map(_.get(<span class="number">0</span>, objectType).asInstanceOf[<span class="type">T</span>])</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="toJavaRDD"><a href="#toJavaRDD" class="headerlink" title="toJavaRDD"></a>toJavaRDD</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns the content of the Dataset as a `JavaRDD` of [[T]]s.</span></span><br><span class="line"><span class="comment">  * </span></span><br><span class="line"><span class="comment">  * 转换为JavaRDD</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toJavaRDD</span></span>: <span class="type">JavaRDD</span>[<span class="type">T</span>] = rdd.toJavaRDD()</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns the content of the Dataset as a `JavaRDD` of [[T]]s.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 转换为JavaRDD</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">javaRDD</span></span>: <span class="type">JavaRDD</span>[<span class="type">T</span>] = toJavaRDD</span><br></pre></td></tr></table></figure>
<h3 id="registerTempTable"><a href="#registerTempTable" class="headerlink" title="registerTempTable"></a>registerTempTable</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Registers this Dataset as a temporary table using the given name. The lifetime of this</span></span><br><span class="line"><span class="comment">  * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 根据指定的表名，注册临时表。</span></span><br><span class="line"><span class="comment">  * 生命周期为[[SparkSession]]的生命周期。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@deprecated</span>(<span class="string">"Use createOrReplaceTempView(viewName) instead."</span>, <span class="string">"2.0.0"</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">registerTempTable</span></span>(tableName: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  createOrReplaceTempView(tableName)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="createTempView"><a href="#createTempView" class="headerlink" title="createTempView"></a>createTempView</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Creates a local temporary view using the given name. The lifetime of this</span></span><br><span class="line"><span class="comment">  * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 用指定的名字创建本地临时表。</span></span><br><span class="line"><span class="comment">  * 与[[SparkSession]] 同生命周期。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that</span></span><br><span class="line"><span class="comment">  * created it, i.e. it will be automatically dropped when the session terminates. It's not</span></span><br><span class="line"><span class="comment">  * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 本地临时表是 session范围内的。当创建它的session停止的时候，该表也随之停止。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @throws AnalysisException if the view name already exists</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@throws</span>[<span class="type">AnalysisException</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTempView</span></span>(viewName: <span class="type">String</span>): <span class="type">Unit</span> = withPlan &#123;</span><br><span class="line">  createTempViewCommand(viewName, replace = <span class="literal">false</span>, global = <span class="literal">false</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="createOrReplaceTempView"><a href="#createOrReplaceTempView" class="headerlink" title="createOrReplaceTempView"></a>createOrReplaceTempView</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Creates a local temporary view using the given name. The lifetime of this</span></span><br><span class="line"><span class="comment">  * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 用指定的名字创建本地临时表。如果已经有了则替换。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createOrReplaceTempView</span></span>(viewName: <span class="type">String</span>): <span class="type">Unit</span> = withPlan &#123;</span><br><span class="line">  createTempViewCommand(viewName, replace = <span class="literal">true</span>, global = <span class="literal">false</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="createGlobalTempView"><a href="#createGlobalTempView" class="headerlink" title="createGlobalTempView"></a>createGlobalTempView</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Creates a global temporary view using the given name. The lifetime of this</span></span><br><span class="line"><span class="comment">  * temporary view is tied to this Spark application.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 创建全局临时表。</span></span><br><span class="line"><span class="comment">  * 生命周期为整个Spark application.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,</span></span><br><span class="line"><span class="comment">  * i.e. it will be automatically dropped when the application terminates. It's tied to a system</span></span><br><span class="line"><span class="comment">  * preserved database `_global_temp`, and we must use the qualified name to refer a global temp</span></span><br><span class="line"><span class="comment">  * view, e.g. `SELECT * FROM _global_temp.view1`.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 全局临时表是跨session的。属于 _global_temp 数据库。e.g. `SELECT * FROM _global_temp.view1`.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @throws AnalysisException if the view name already exists</span></span><br><span class="line"><span class="comment">  *                           如果表已经存在，则报错。</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 2.1.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@throws</span>[<span class="type">AnalysisException</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createGlobalTempView</span></span>(viewName: <span class="type">String</span>): <span class="type">Unit</span> = withPlan &#123;</span><br><span class="line">  createTempViewCommand(viewName, replace = <span class="literal">false</span>, global = <span class="literal">true</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="write"><a href="#write" class="headerlink" title="write"></a>write</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Interface for saving the content of the non-streaming Dataset out into external storage.</span></span><br><span class="line"><span class="comment">  * </span></span><br><span class="line"><span class="comment">  * 将非流Dataset的内容保存到外部存储中的接口。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write</span></span>: <span class="type">DataFrameWriter</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (isStreaming) &#123;</span><br><span class="line">    logicalPlan.failAnalysis(</span><br><span class="line">      <span class="string">"'write' can not be called on streaming Dataset/DataFrame"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">DataFrameWriter</span>[<span class="type">T</span>](<span class="keyword">this</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="writeStream"><a href="#writeStream" class="headerlink" title="writeStream"></a>writeStream</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * :: Experimental ::</span></span><br><span class="line"><span class="comment">  * Interface for saving the content of the streaming Dataset out into external storage.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 将流Dataset保存在外部存储。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">writeStream</span></span>: <span class="type">DataStreamWriter</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (!isStreaming) &#123;</span><br><span class="line">    logicalPlan.failAnalysis(</span><br><span class="line">      <span class="string">"'writeStream' can be called only on streaming Dataset/DataFrame"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">DataStreamWriter</span>[<span class="type">T</span>](<span class="keyword">this</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="toJSON"><a href="#toJSON" class="headerlink" title="toJSON"></a>toJSON</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns the content of the Dataset as a Dataset of JSON strings.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 将Dataset转换为JSON。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toJSON</span></span>: <span class="type">Dataset</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> rowSchema = <span class="keyword">this</span>.schema</span><br><span class="line">  <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = queryExecution.toRdd.mapPartitions &#123; iter =&gt;</span><br><span class="line">    <span class="keyword">val</span> writer = <span class="keyword">new</span> <span class="type">CharArrayWriter</span>()</span><br><span class="line">    <span class="comment">// create the Generator without separator inserted between 2 records</span></span><br><span class="line">    <span class="keyword">val</span> gen = <span class="keyword">new</span> <span class="type">JacksonGenerator</span>(rowSchema, writer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Iterator</span>[<span class="type">String</span>] &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = iter.hasNext</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">        gen.write(iter.next())</span><br><span class="line">        gen.flush()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> json = writer.toString</span><br><span class="line">        <span class="keyword">if</span> (hasNext) &#123;</span><br><span class="line">          writer.reset()</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          gen.close()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        json</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">import</span> sparkSession.implicits.newStringEncoder</span><br><span class="line">  sparkSession.createDataset(rdd)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="inputFiles"><a href="#inputFiles" class="headerlink" title="inputFiles"></a>inputFiles</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a best-effort snapshot of the files that compose this Dataset. This method simply</span></span><br><span class="line"><span class="comment">  * asks each constituent BaseRelation for its respective files and takes the union of all results.</span></span><br><span class="line"><span class="comment">  * Depending on the source relations, this may not find all input files. Duplicates are removed.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回组成这个Dataset的所有文件的最佳快照。</span></span><br><span class="line"><span class="comment">  * 该方法简单地要求每个组件BaseRelation对其各自的文件进行处理，并联合所有结果。</span></span><br><span class="line"><span class="comment">  * 基于源关系，应该可以找到所有的输入文件。</span></span><br><span class="line"><span class="comment">  * 重复的也会被移除。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group basic</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputFiles</span></span>: <span class="type">Array</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> files: <span class="type">Seq</span>[<span class="type">String</span>] = queryExecution.optimizedPlan.collect &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">LogicalRelation</span>(fsBasedRelation: <span class="type">FileRelation</span>, _, _) =&gt;</span><br><span class="line">      fsBasedRelation.inputFiles</span><br><span class="line">    <span class="keyword">case</span> fr: <span class="type">FileRelation</span> =&gt;</span><br><span class="line">      fr.inputFiles</span><br><span class="line">  &#125;.flatten</span><br><span class="line">  files.toSet.toArray</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="streaming"><a href="#streaming" class="headerlink" title="streaming"></a>streaming</h2><h3 id="isStreaming"><a href="#isStreaming" class="headerlink" title="isStreaming"></a>isStreaming</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns true if this Dataset contains one or more sources that continuously</span></span><br><span class="line"><span class="comment">  * return data as it arrives. A Dataset that reads data from a streaming source</span></span><br><span class="line"><span class="comment">  * must be executed as a `StreamingQuery` using the `start()` method in</span></span><br><span class="line"><span class="comment">  * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or</span></span><br><span class="line"><span class="comment">  * `collect()`, will throw an [[AnalysisException]] when there is a streaming</span></span><br><span class="line"><span class="comment">  * source present.</span></span><br><span class="line"><span class="comment">  * </span></span><br><span class="line"><span class="comment">  * 如果Dataset包含一个或多个持续返回数据的源，则返回true；</span></span><br><span class="line"><span class="comment">  * 如果Dataset从streaming源读取数据，则必须像 `StreamingQuery` 一样执行：使用 `DataStreamWriter` 中的 `start()`方法。</span></span><br><span class="line"><span class="comment">  * 返回单个值的方法，例如： `count()` or `collect()`，当存在streaming源时，将会抛出[[AnalysisException]]。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group streaming</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isStreaming</span></span>: <span class="type">Boolean</span> = logicalPlan.isStreaming</span><br></pre></td></tr></table></figure>
<h3 id="withWatermark"><a href="#withWatermark" class="headerlink" title="withWatermark"></a>withWatermark</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * :: Experimental :: 实验性的</span></span><br><span class="line"><span class="comment">  * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time</span></span><br><span class="line"><span class="comment">  * before which we assume no more late data is going to arrive.</span></span><br><span class="line"><span class="comment">  * </span></span><br><span class="line"><span class="comment">  * 为这个[[Dataset]]定义事件时间水印。</span></span><br><span class="line"><span class="comment">  * 我们假设没有更多的晚期数据将到达之前，一个水印跟踪一个时间点。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * Spark will use this watermark for several purposes:</span></span><br><span class="line"><span class="comment">  * Spark用水印有几个目的：</span></span><br><span class="line"><span class="comment">  *  - To know when a given time window aggregation can be finalized and thus can be emitted when</span></span><br><span class="line"><span class="comment">  * using output modes that do not allow updates.</span></span><br><span class="line"><span class="comment">  * </span></span><br><span class="line"><span class="comment">  * 可以知道何时完成给定的时间窗口聚合能够完成，因此当使用不允许更新的输出模式时能够被放出。</span></span><br><span class="line"><span class="comment">  *  - To minimize the amount of state that we need to keep for on-going aggregations.</span></span><br><span class="line"><span class="comment">  * 为了最小化我们需要持续不断的聚合的状态数量。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * The current watermark is computed by looking at the `MAX(eventTime)` seen across</span></span><br><span class="line"><span class="comment">  * all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost</span></span><br><span class="line"><span class="comment">  * of coordinating this value across partitions, the actual watermark used is only guaranteed</span></span><br><span class="line"><span class="comment">  * to be at least `delayThreshold` behind the actual event time.  In some cases we may still</span></span><br><span class="line"><span class="comment">  * process records that arrive more than `delayThreshold` late.</span></span><br><span class="line"><span class="comment">  * </span></span><br><span class="line"><span class="comment">  * 当前的水印 = 查看查询中所有分区上看到的`MAX(eventTime)` - 用户指定的`delayThreshold`</span></span><br><span class="line"><span class="comment">  * 由于在分区之间协调这个值的花销，实际使用的水印只保证在实际事件时间后至少是“delayThreshold”。</span></span><br><span class="line"><span class="comment">  * 在某些情况下，我们可能还会处理比“delayThreshold”晚些时候到达的记录。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param eventTime      the name of the column that contains the event time of the row.</span></span><br><span class="line"><span class="comment">  *                       包含行的事件时间的列名</span></span><br><span class="line"><span class="comment">  * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest</span></span><br><span class="line"><span class="comment">  *                       record that has been processed in the form of an interval</span></span><br><span class="line"><span class="comment">  *                       (e.g. "1 minute" or "5 hours").</span></span><br><span class="line"><span class="comment">  *                       等待晚到数据的最少延迟，相对于以间隔形式处理的最新记录</span></span><br><span class="line"><span class="comment">  * @group streaming</span></span><br><span class="line"><span class="comment">  * @since 2.1.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="comment">// We only accept an existing column name, not a derived column here as a watermark that is</span></span><br><span class="line"><span class="comment">// defined on a derived column cannot referenced elsewhere in the plan.</span></span><br><span class="line"><span class="comment">// 我们只接受一个现有的列名，而不是作为一个在派生列上定义的水印的派生列，而不能在该计划的其他地方引用。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">withWatermark</span></span>(eventTime: <span class="type">String</span>, delayThreshold: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</span><br><span class="line">  <span class="keyword">val</span> parsedDelay =</span><br><span class="line">    <span class="type">Option</span>(<span class="type">CalendarInterval</span>.fromString(<span class="string">"interval "</span> + delayThreshold))</span><br><span class="line">      .getOrElse(<span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AnalysisException</span>(<span class="string">s"Unable to parse time delay '<span class="subst">$delayThreshold</span>'"</span>))</span><br><span class="line">  <span class="type">EventTimeWatermark</span>(<span class="type">UnresolvedAttribute</span>(eventTime), parsedDelay, logicalPlan)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="action"><a href="#action" class="headerlink" title="action"></a>action</h2><h3 id="show"><a href="#show" class="headerlink" title="show"></a>show</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,</span></span><br><span class="line"><span class="comment">  * and all cells will be aligned right. For example:</span></span><br><span class="line"><span class="comment">  * </span></span><br><span class="line"><span class="comment">  * 以表格形式显示数据集。</span></span><br><span class="line"><span class="comment">  * 字符串超过20个字符将被截断，</span></span><br><span class="line"><span class="comment">  * 所有单元格将被对齐。</span></span><br><span class="line"><span class="comment">  * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">  *   year  month AVG('Adj Close) MAX('Adj Close)</span></span><br><span class="line"><span class="comment">  *   1980  12    0.503218        0.595103</span></span><br><span class="line"><span class="comment">  *   1981  01    0.523289        0.570307</span></span><br><span class="line"><span class="comment">  *   1982  02    0.436504        0.475256</span></span><br><span class="line"><span class="comment">  *   1983  03    0.410516        0.442194</span></span><br><span class="line"><span class="comment">  *   1984  04    0.450090        0.483521</span></span><br><span class="line"><span class="comment">  * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param numRows Number of rows to show 要显示的行数</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span></span>(numRows: <span class="type">Int</span>): <span class="type">Unit</span> = show(numRows, truncate = <span class="literal">true</span>)</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters</span></span><br><span class="line"><span class="comment">  * will be truncated, and all cells will be aligned right.</span></span><br><span class="line"><span class="comment">  * 显示头20行</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span></span>(): <span class="type">Unit</span> = show(<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Displays the top 20 rows of Dataset in a tabular form.</span></span><br><span class="line"><span class="comment">  * 显示头20行</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param truncate Whether truncate long strings. If true, strings more than 20 characters will</span></span><br><span class="line"><span class="comment">  *                 be truncated and all cells will be aligned right</span></span><br><span class="line"><span class="comment">  *                 是否截断长字符串。如果 true：超过20个字符就会被截断</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span></span>(truncate: <span class="type">Boolean</span>): <span class="type">Unit</span> = show(<span class="number">20</span>, truncate)</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Displays the Dataset in a tabular form. For example:</span></span><br><span class="line"><span class="comment">  * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">  *   year  month AVG('Adj Close) MAX('Adj Close)</span></span><br><span class="line"><span class="comment">  *   1980  12    0.503218        0.595103</span></span><br><span class="line"><span class="comment">  *   1981  01    0.523289        0.570307</span></span><br><span class="line"><span class="comment">  *   1982  02    0.436504        0.475256</span></span><br><span class="line"><span class="comment">  *   1983  03    0.410516        0.442194</span></span><br><span class="line"><span class="comment">  *   1984  04    0.450090        0.483521</span></span><br><span class="line"><span class="comment">  * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param numRows  Number of rows to show 显示的行数</span></span><br><span class="line"><span class="comment">  * @param truncate Whether truncate long strings. If true, strings more than 20 characters will</span></span><br><span class="line"><span class="comment">  *                 be truncated and all cells will be aligned right</span></span><br><span class="line"><span class="comment">  *                 是否截断长字符串</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="comment">// scalastyle:off println</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span></span>(numRows: <span class="type">Int</span>, truncate: <span class="type">Boolean</span>): <span class="type">Unit</span> = <span class="keyword">if</span> (truncate) &#123;</span><br><span class="line">  println(showString(numRows, truncate = <span class="number">20</span>))</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  println(showString(numRows, truncate = <span class="number">0</span>))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// scalastyle:on println</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Displays the Dataset in a tabular form. For example:</span></span><br><span class="line"><span class="comment">  * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">  *   year  month AVG('Adj Close) MAX('Adj Close)</span></span><br><span class="line"><span class="comment">  *   1980  12    0.503218        0.595103</span></span><br><span class="line"><span class="comment">  *   1981  01    0.523289        0.570307</span></span><br><span class="line"><span class="comment">  *   1982  02    0.436504        0.475256</span></span><br><span class="line"><span class="comment">  *   1983  03    0.410516        0.442194</span></span><br><span class="line"><span class="comment">  *   1984  04    0.450090        0.483521</span></span><br><span class="line"><span class="comment">  * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param numRows  Number of rows to show</span></span><br><span class="line"><span class="comment">  * @param truncate If set to more than 0, truncates strings to `truncate` characters and</span></span><br><span class="line"><span class="comment">  *                 all cells will be aligned right.</span></span><br><span class="line"><span class="comment">  *                 设置 触发截断字符串的阈值</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="comment">// scalastyle:off println</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span></span>(numRows: <span class="type">Int</span>, truncate: <span class="type">Int</span>): <span class="type">Unit</span> = println(showString(numRows, truncate))</span><br></pre></td></tr></table></figure>
<h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * :: Experimental ::</span></span><br><span class="line"><span class="comment">  * (Scala-specific)</span></span><br><span class="line"><span class="comment">  * Reduces the elements of this Dataset using the specified binary function. The given `func`</span></span><br><span class="line"><span class="comment">  * must be commutative and associative or the result may be non-deterministic.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 使用指定的二进制函数减少这个数据集的元素。给定的“func”必须是可交换的和关联的，否则结果可能是不确定性的。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(func: (<span class="type">T</span>, <span class="type">T</span>) =&gt; <span class="type">T</span>): <span class="type">T</span> = rdd.reduce(func)</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * :: Experimental ::</span></span><br><span class="line"><span class="comment">  * (Java-specific)</span></span><br><span class="line"><span class="comment">  * Reduces the elements of this Dataset using the specified binary function. The given `func`</span></span><br><span class="line"><span class="comment">  * must be commutative and associative or the result may be non-deterministic.</span></span><br><span class="line"><span class="comment">  * </span></span><br><span class="line"><span class="comment">  * 使用指定的二进制函数减少这个数据集的元素。给定的“func”必须是可交换的和关联的，否则结果可能是不确定性的。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(func: <span class="type">ReduceFunction</span>[<span class="type">T</span>]): <span class="type">T</span> = reduce(func.call(_, _))</span><br></pre></td></tr></table></figure>
<h3 id="describe"><a href="#describe" class="headerlink" title="describe"></a>describe</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Computes statistics for numeric and string columns, including count, mean, stddev, min, and</span></span><br><span class="line"><span class="comment">   * max. If no columns are given, this function computes statistics for all numerical or string</span></span><br><span class="line"><span class="comment">   * columns.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * 计算数字和字符串列的统计数据，包括count、mean、stddev、min和max。</span></span><br><span class="line"><span class="comment">   * 如果没有给出任何列，该函数计算所有数值或字符串列的统计信息。</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * This function is meant for exploratory data analysis, as we make no guarantee about the</span></span><br><span class="line"><span class="comment">   * backward compatibility of the schema of the resulting Dataset. If you want to</span></span><br><span class="line"><span class="comment">   * programmatically compute summary statistics, use the `agg` function instead.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * 这个函数用于探索性的数据分析，因为我们不能保证生成数据集的模式的向后兼容性。</span></span><br><span class="line"><span class="comment">   * 如果您想通过编程计算汇总统计信息，可以使用“agg”函数。</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">   *   ds.describe("age", "height").show()</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   *   // output:</span></span><br><span class="line"><span class="comment">   *   // summary age   height</span></span><br><span class="line"><span class="comment">   *   // count   10.0  10.0</span></span><br><span class="line"><span class="comment">   *   // mean    53.3  178.05</span></span><br><span class="line"><span class="comment">   *   // stddev  11.6  15.7</span></span><br><span class="line"><span class="comment">   *   // min     18.0  163.0</span></span><br><span class="line"><span class="comment">   *   // max     92.0  192.0</span></span><br><span class="line"><span class="comment">   * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @group action</span></span><br><span class="line"><span class="comment">   * @since 1.6.0</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"> <span class="meta">@scala</span>.annotation.varargs</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">describe</span></span>(cols: <span class="type">String</span>*): <span class="type">DataFrame</span> = withPlan &#123;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// The list of summary statistics to compute, in the form of expressions.</span></span><br><span class="line">   <span class="keyword">val</span> statistics = <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Expression</span> =&gt; <span class="type">Expression</span>)](</span><br><span class="line">     <span class="string">"count"</span> -&gt; ((child: <span class="type">Expression</span>) =&gt; <span class="type">Count</span>(child).toAggregateExpression()),</span><br><span class="line">     <span class="string">"mean"</span> -&gt; ((child: <span class="type">Expression</span>) =&gt; <span class="type">Average</span>(child).toAggregateExpression()),</span><br><span class="line">     <span class="string">"stddev"</span> -&gt; ((child: <span class="type">Expression</span>) =&gt; <span class="type">StddevSamp</span>(child).toAggregateExpression()),</span><br><span class="line">     <span class="string">"min"</span> -&gt; ((child: <span class="type">Expression</span>) =&gt; <span class="type">Min</span>(child).toAggregateExpression()),</span><br><span class="line">     <span class="string">"max"</span> -&gt; ((child: <span class="type">Expression</span>) =&gt; <span class="type">Max</span>(child).toAggregateExpression()))</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> outputCols =</span><br><span class="line">     (<span class="keyword">if</span> (cols.isEmpty) aggregatableColumns.map(usePrettyExpression(_).sql) <span class="keyword">else</span> cols).toList</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> ret: <span class="type">Seq</span>[<span class="type">Row</span>] = <span class="keyword">if</span> (outputCols.nonEmpty) &#123;</span><br><span class="line">     <span class="keyword">val</span> aggExprs = statistics.flatMap &#123; <span class="keyword">case</span> (_, colToAgg) =&gt;</span><br><span class="line">       outputCols.map(c =&gt; <span class="type">Column</span>(<span class="type">Cast</span>(colToAgg(<span class="type">Column</span>(c).expr), <span class="type">StringType</span>)).as(c))</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     <span class="keyword">val</span> row = groupBy().agg(aggExprs.head, aggExprs.tail: _*).head().toSeq</span><br><span class="line"></span><br><span class="line">     <span class="comment">// Pivot the data so each summary is one row</span></span><br><span class="line">     row.grouped(outputCols.size).toSeq.zip(statistics).map &#123; <span class="keyword">case</span> (aggregation, (statistic, _)) =&gt;</span><br><span class="line">       <span class="type">Row</span>(statistic :: aggregation.toList: _*)</span><br><span class="line">     &#125;</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="comment">// If there are no output columns, just output a single column that contains the stats.</span></span><br><span class="line">     statistics.map &#123; <span class="keyword">case</span> (name, _) =&gt; <span class="type">Row</span>(name) &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// All columns are string type</span></span><br><span class="line">   <span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">     <span class="type">StructField</span>(<span class="string">"summary"</span>, <span class="type">StringType</span>) :: outputCols.map(<span class="type">StructField</span>(_, <span class="type">StringType</span>))).toAttributes</span><br><span class="line">   <span class="comment">// `toArray` forces materialization to make the seq serializable</span></span><br><span class="line">   <span class="type">LocalRelation</span>.fromExternalRows(schema, ret.toArray.toSeq)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h3 id="head"><a href="#head" class="headerlink" title="head"></a>head</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns the first `n` rows.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回前n行</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @note this method should only be used if the resulting array is expected to be small, as</span></span><br><span class="line"><span class="comment">  *       all the data is loaded into the driver's memory.</span></span><br><span class="line"><span class="comment">  *       仅适用于结果很少的时候使用，因为会将结果加载进内存中</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">head</span></span>(n: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>] = withTypedCallback(<span class="string">"head"</span>, limit(n)) &#123; df =&gt;</span><br><span class="line">  df.collect(needCallback = <span class="literal">false</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns the first row.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回第一行（默认1）</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">head</span></span>(): <span class="type">T</span> = head(<span class="number">1</span>).head</span><br></pre></td></tr></table></figure>
<h3 id="first"><a href="#first" class="headerlink" title="first"></a>first</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns the first row. Alias for head().</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回第一行 ，与head()一样</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">first</span></span>(): <span class="type">T</span> = head()</span><br></pre></td></tr></table></figure>
<h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Applies a function `f` to all rows.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 对所有行应用函数f。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreach</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = withNewExecutionId &#123;</span><br><span class="line">  rdd.foreach(f)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * (Java-specific)</span></span><br><span class="line"><span class="comment">  * Runs `func` on each element of this Dataset.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 在这个数据集的每个元素上运行“func”。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreach</span></span>(func: <span class="type">ForeachFunction</span>[<span class="type">T</span>]): <span class="type">Unit</span> = foreach(func.call(_))</span><br></pre></td></tr></table></figure>
<h3 id="foreachPartition"><a href="#foreachPartition" class="headerlink" title="foreachPartition"></a>foreachPartition</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Applies a function `f` to each partition of this Dataset.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 对这个数据集的每个分区应用一个函数f。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreachPartition</span></span>(f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = withNewExecutionId &#123;</span><br><span class="line">  rdd.foreachPartition(f)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * (Java-specific)</span></span><br><span class="line"><span class="comment">  * Runs `func` on each partition of this Dataset.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 对这个数据集的每个分区应用一个函数f。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreachPartition</span></span>(func: <span class="type">ForeachPartitionFunction</span>[<span class="type">T</span>]): <span class="type">Unit</span> =</span><br><span class="line">  foreachPartition(it =&gt; func.call(it.asJava))</span><br></pre></td></tr></table></figure>
<h3 id="take"><a href="#take" class="headerlink" title="take"></a>take</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns the first `n` rows in the Dataset.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回数据集中的前“n”行。</span></span><br><span class="line"><span class="comment">  * 同head(n)</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * Running take requires moving data into the application's driver process, and doing so with</span></span><br><span class="line"><span class="comment">  * a very large `n` can crash the driver process with OutOfMemoryError.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * take在driver端执行，n太大会造成oom</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(n: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>] = head(n)</span><br></pre></td></tr></table></figure>
<h3 id="takeAsList"><a href="#takeAsList" class="headerlink" title="takeAsList"></a>takeAsList</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns the first `n` rows in the Dataset as a list.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 以List形式返回 前n行</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * Running take requires moving data into the application's driver process, and doing so with</span></span><br><span class="line"><span class="comment">  * a very large `n` can crash the driver process with OutOfMemoryError.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * take在driver端执行，n太大会造成oom</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAsList</span></span>(n: <span class="type">Int</span>): java.util.<span class="type">List</span>[<span class="type">T</span>] = java.util.<span class="type">Arrays</span>.asList(take(n): _*)</span><br></pre></td></tr></table></figure>
<h3 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns an array that contains all of [[Row]]s in this Dataset.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回包含所有Row的 一个数组</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * Running collect requires moving all the data into the application's driver process, and</span></span><br><span class="line"><span class="comment">  * doing so on a very large dataset can crash the driver process with OutOfMemoryError.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 会将所有数据移动到driver，所以可能会造成oom</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * For Java API, use [[collectAsList]].</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collect</span></span>(): <span class="type">Array</span>[<span class="type">T</span>] = collect(needCallback = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>
<h3 id="collectAsList"><a href="#collectAsList" class="headerlink" title="collectAsList"></a>collectAsList</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a Java list that contains all of [[Row]]s in this Dataset.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回包含所有Row的一个Java List</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * Running collect requires moving all the data into the application's driver process, and</span></span><br><span class="line"><span class="comment">  * doing so on a very large dataset can crash the driver process with OutOfMemoryError.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 会将所有数据移动到driver，所以可能会造成oom</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collectAsList</span></span>(): java.util.<span class="type">List</span>[<span class="type">T</span>] = withCallback(<span class="string">"collectAsList"</span>, toDF()) &#123; _ =&gt;</span><br><span class="line">  withNewExecutionId &#123;</span><br><span class="line">    <span class="keyword">val</span> values = queryExecution.executedPlan.executeCollect().map(boundEnc.fromRow)</span><br><span class="line">    java.util.<span class="type">Arrays</span>.asList(values: _*)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="toLocalIterator"><a href="#toLocalIterator" class="headerlink" title="toLocalIterator"></a>toLocalIterator</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Return an iterator that contains all of [[Row]]s in this Dataset.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回包含所有Row的一个迭代器</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * The iterator will consume as much memory as the largest partition in this Dataset.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 迭代器将消耗与此数据集中最大的分区一样多的内存。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @note this results in multiple Spark jobs, and if the input Dataset is the result</span></span><br><span class="line"><span class="comment">  *       of a wide transformation (e.g. join with different partitioners), to avoid</span></span><br><span class="line"><span class="comment">  *       recomputing the input Dataset should be cached first.</span></span><br><span class="line"><span class="comment">  *       这将导致多个Spark作业，如果输入数据集是宽依赖转换的结果(例如，与不同的分区连接)，</span></span><br><span class="line"><span class="comment">  *       那么为了避免重新计算输入数据，应该首先缓存输入数据集。</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toLocalIterator</span></span>(): java.util.<span class="type">Iterator</span>[<span class="type">T</span>] = withCallback(<span class="string">"toLocalIterator"</span>, toDF()) &#123; _ =&gt;</span><br><span class="line">  withNewExecutionId &#123;</span><br><span class="line">    queryExecution.executedPlan.executeToIterator().map(boundEnc.fromRow).asJava</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns the number of rows in the Dataset.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回总行数</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group action</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = withCallback(<span class="string">"count"</span>, groupBy().count()) &#123; df =&gt;</span><br><span class="line">  df.collect(needCallback = <span class="literal">false</span>).head.getLong(<span class="number">0</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="untypedrel-无类型转换"><a href="#untypedrel-无类型转换" class="headerlink" title="untypedrel-无类型转换"></a>untypedrel-无类型转换</h2><h3 id="na"><a href="#na" class="headerlink" title="na"></a>na</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a [[DataFrameNaFunctions]] for working with missing data.</span></span><br><span class="line"><span class="comment">  * 返回一个用于处理丢失数据的[[DataFrameNaFunctions]]。</span></span><br><span class="line"><span class="comment">  * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">  *   // Dropping rows containing any null values. 删除包含任何null 值的行</span></span><br><span class="line"><span class="comment">  *   ds.na.drop()</span></span><br><span class="line"><span class="comment">  * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group untypedrel</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">na</span></span>: <span class="type">DataFrameNaFunctions</span> = <span class="keyword">new</span> <span class="type">DataFrameNaFunctions</span>(toDF())</span><br></pre></td></tr></table></figure>
<h3 id="stat"><a href="#stat" class="headerlink" title="stat"></a>stat</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a [[DataFrameStatFunctions]] for working statistic functions support.</span></span><br><span class="line"><span class="comment">  * 返回用于支持统计功能的[[DataFrameStatFunctions]]。</span></span><br><span class="line"><span class="comment">  * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">  *   // Finding frequent items in column with name 'a'. 查询列名为"a"中的频繁数据。</span></span><br><span class="line"><span class="comment">  *   ds.stat.freqItems(Seq("a"))</span></span><br><span class="line"><span class="comment">  * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group untypedrel</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stat</span></span>: <span class="type">DataFrameStatFunctions</span> = <span class="keyword">new</span> <span class="type">DataFrameStatFunctions</span>(toDF())</span><br></pre></td></tr></table></figure>
<h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Join with another `DataFrame`.</span></span><br><span class="line"><span class="comment">    * 和 另一个 `DataFrame`  jion</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * Behaves as an INNER JOIN and requires a subsequent join predicate.</span></span><br><span class="line"><span class="comment">    * 作为一个内部连接，并需要一个后续的连接谓词。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param right Right side of the join operation. join操作的右侧</span></span><br><span class="line"><span class="comment">    * @group untypedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_]): <span class="type">DataFrame</span> = withPlan &#123;</span><br><span class="line">    <span class="type">Join</span>(logicalPlan, right.logicalPlan, joinType = <span class="type">Inner</span>, <span class="type">None</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Inner equi-join with another `DataFrame` using the given column.</span></span><br><span class="line"><span class="comment">    * 给定列名的内部等值连接</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * Different from other join functions, the join column will only appear once in the output,</span></span><br><span class="line"><span class="comment">    * i.e. similar to SQL's `JOIN USING` syntax.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">    *   // Joining df1 and df2 using the column "user_id" 用"user_id"  连接 df1 和df2</span></span><br><span class="line"><span class="comment">    *   df1.join(df2, "user_id")</span></span><br><span class="line"><span class="comment">    * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param right       Right side of the join operation. join连接右侧</span></span><br><span class="line"><span class="comment">    * @param usingColumn Name of the column to join on. This column must exist on both sides.</span></span><br><span class="line"><span class="comment">    *                    列名。必须在两边都存在</span></span><br><span class="line"><span class="comment">    * @note If you perform a self-join using this function without aliasing the input</span></span><br><span class="line"><span class="comment">    *       `DataFrame`s, you will NOT be able to reference any columns after the join, since</span></span><br><span class="line"><span class="comment">    *       there is no way to disambiguate which side of the join you would like to reference.</span></span><br><span class="line"><span class="comment">    *       自连接的时候，请指定 表别名。不然干不了事</span></span><br><span class="line"><span class="comment">    * @group untypedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_], usingColumn: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    join(right, <span class="type">Seq</span>(usingColumn))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Inner equi-join with another `DataFrame` using the given columns.</span></span><br><span class="line"><span class="comment">    * 根据指定多个列进行join</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * Different from other join functions, the join columns will only appear once in the output,</span></span><br><span class="line"><span class="comment">    * i.e. similar to SQL's `JOIN USING` syntax.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">    *   // Joining df1 and df2 using the columns "user_id" and "user_name"</span></span><br><span class="line"><span class="comment">    *   df1.join(df2, Seq("user_id", "user_name"))</span></span><br><span class="line"><span class="comment">    * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param right        Right side of the join operation.</span></span><br><span class="line"><span class="comment">    * @param usingColumns Names of the columns to join on. This columns must exist on both sides.</span></span><br><span class="line"><span class="comment">    * @note If you perform a self-join using this function without aliasing the input</span></span><br><span class="line"><span class="comment">    *       `DataFrame`s, you will NOT be able to reference any columns after the join, since</span></span><br><span class="line"><span class="comment">    *       there is no way to disambiguate which side of the join you would like to reference.</span></span><br><span class="line"><span class="comment">    * @group untypedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_], usingColumns: <span class="type">Seq</span>[<span class="type">String</span>]): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    join(right, usingColumns, <span class="string">"inner"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Equi-join with another `DataFrame` using the given columns.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * Different from other join functions, the join columns will only appear once in the output,</span></span><br><span class="line"><span class="comment">    * i.e. similar to SQL's `JOIN USING` syntax.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param right        Right side of the join operation.</span></span><br><span class="line"><span class="comment">    * @param usingColumns Names of the columns to join on. This columns must exist on both sides.</span></span><br><span class="line"><span class="comment">    * @param joinType     One of: `inner`, `outer`, `left_outer`, `right_outer`, `leftsemi`.</span></span><br><span class="line"><span class="comment">    *                     连接类型：内连接，外连接，左外连接，右外连接，左内连接</span></span><br><span class="line"><span class="comment">    * @note If you perform a self-join using this function without aliasing the input</span></span><br><span class="line"><span class="comment">    *       `DataFrame`s, you will NOT be able to reference any columns after the join, since</span></span><br><span class="line"><span class="comment">    *       there is no way to disambiguate which side of the join you would like to reference.</span></span><br><span class="line"><span class="comment">    * @group untypedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_], usingColumns: <span class="type">Seq</span>[<span class="type">String</span>], joinType: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    <span class="comment">// Analyze the self join. The assumption is that the analyzer will disambiguate left vs right</span></span><br><span class="line">    <span class="comment">// by creating a new instance for one of the branch.</span></span><br><span class="line">    <span class="comment">// 自连接的时候，为其中一个分支创建一个新实例来消除左vs右的歧义。</span></span><br><span class="line">    <span class="keyword">val</span> joined = sparkSession.sessionState.executePlan(</span><br><span class="line">      <span class="type">Join</span>(logicalPlan, right.logicalPlan, joinType = <span class="type">JoinType</span>(joinType), <span class="type">None</span>))</span><br><span class="line">      .analyzed.asInstanceOf[<span class="type">Join</span>]</span><br><span class="line"></span><br><span class="line">    withPlan &#123;</span><br><span class="line">      <span class="type">Join</span>(</span><br><span class="line">        joined.left,</span><br><span class="line">        joined.right,</span><br><span class="line">        <span class="type">UsingJoin</span>(<span class="type">JoinType</span>(joinType), usingColumns),</span><br><span class="line">        <span class="type">None</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Inner join with another `DataFrame`, using the given join expression.</span></span><br><span class="line"><span class="comment">    * 用给定的表达式进行join</span></span><br><span class="line"><span class="comment">    * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">    *   // The following two are equivalent:</span></span><br><span class="line"><span class="comment">    *   df1.join(df2, $"df1Key" === $"df2Key")</span></span><br><span class="line"><span class="comment">    *   df1.join(df2).where($"df1Key" === $"df2Key")</span></span><br><span class="line"><span class="comment">    * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group untypedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_], joinExprs: <span class="type">Column</span>): <span class="type">DataFrame</span> = join(right, joinExprs, <span class="string">"inner"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Join with another `DataFrame`, using the given join expression. The following performs</span></span><br><span class="line"><span class="comment">    * a full outer join between `df1` and `df2`.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">    *   // Scala:</span></span><br><span class="line"><span class="comment">    *   import org.apache.spark.sql.functions._</span></span><br><span class="line"><span class="comment">    *   df1.join(df2, $"df1Key" === $"df2Key", "outer")</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    *   // Java:</span></span><br><span class="line"><span class="comment">    *   import static org.apache.spark.sql.functions.*;</span></span><br><span class="line"><span class="comment">    *   df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer");</span></span><br><span class="line"><span class="comment">    * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param right     Right side of the join.</span></span><br><span class="line"><span class="comment">    * @param joinExprs Join expression.</span></span><br><span class="line"><span class="comment">    * @param joinType  One of: `inner`, `outer`, `left_outer`, `right_outer`, `leftsemi`.</span></span><br><span class="line"><span class="comment">    * @group untypedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_], joinExprs: <span class="type">Column</span>, joinType: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    <span class="comment">// Note that in this function, we introduce a hack in the case of self-join to automatically</span></span><br><span class="line">    <span class="comment">// resolve ambiguous join conditions into ones that might make sense [SPARK-6231].</span></span><br><span class="line">    <span class="comment">// Consider this case: df.join(df, df("key") === df("key"))</span></span><br><span class="line">    <span class="comment">// Since df("key") === df("key") is a trivially true condition, this actually becomes a</span></span><br><span class="line">    <span class="comment">// cartesian join. However, most likely users expect to perform a self join using "key".</span></span><br><span class="line">    <span class="comment">// With that assumption, this hack turns the trivially true condition into equality on join</span></span><br><span class="line">    <span class="comment">// keys that are resolved to both sides.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Trigger analysis so in the case of self-join, the analyzer will clone the plan.</span></span><br><span class="line">    <span class="comment">// After the cloning, left and right side will have distinct expression ids.</span></span><br><span class="line">    <span class="comment">// 针对自连接的优化：正常情况下，自连接如果使用  df.join(df, df("key") === df("key"))</span></span><br><span class="line">    <span class="comment">// 会造成 笛卡尔积</span></span><br><span class="line">    <span class="comment">// 这种情况下，分析器会 克隆计划，克隆完成后，左右两边则有不同的 id</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> plan = withPlan(</span><br><span class="line">      <span class="type">Join</span>(logicalPlan, right.logicalPlan, <span class="type">JoinType</span>(joinType), <span class="type">Some</span>(joinExprs.expr)))</span><br><span class="line">      .queryExecution.analyzed.asInstanceOf[<span class="type">Join</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If auto self join alias is disabled, return the plan.</span></span><br><span class="line">    <span class="keyword">if</span> (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) &#123;</span><br><span class="line">      <span class="keyword">return</span> withPlan(plan)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If left/right have no output set intersection, return the plan.</span></span><br><span class="line">    <span class="keyword">val</span> lanalyzed = withPlan(<span class="keyword">this</span>.logicalPlan).queryExecution.analyzed</span><br><span class="line">    <span class="keyword">val</span> ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed</span><br><span class="line">    <span class="keyword">if</span> (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) &#123;</span><br><span class="line">      <span class="keyword">return</span> withPlan(plan)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Otherwise, find the trivially true predicates and automatically resolves them to both sides.</span></span><br><span class="line">    <span class="comment">// By the time we get here, since we have already run analysis, all attributes should've been</span></span><br><span class="line">    <span class="comment">// resolved and become AttributeReference.</span></span><br><span class="line">    <span class="keyword">val</span> cond = plan.condition.map &#123;</span><br><span class="line">      _.transform &#123;</span><br><span class="line">        <span class="keyword">case</span> catalyst.expressions.<span class="type">EqualTo</span>(a: <span class="type">AttributeReference</span>, b: <span class="type">AttributeReference</span>)</span><br><span class="line">          <span class="keyword">if</span> a.sameRef(b) =&gt;</span><br><span class="line">          catalyst.expressions.<span class="type">EqualTo</span>(</span><br><span class="line">            withPlan(plan.left).resolve(a.name),</span><br><span class="line">            withPlan(plan.right).resolve(b.name))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    withPlan &#123;</span><br><span class="line">      plan.copy(condition = cond)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h3 id="crossJoin"><a href="#crossJoin" class="headerlink" title="crossJoin"></a>crossJoin</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Explicit cartesian join with another `DataFrame`.</span></span><br><span class="line"><span class="comment">  * 显式笛卡尔积join</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param right Right side of the join operation.</span></span><br><span class="line"><span class="comment">  * @note Cartesian joins are very expensive without an extra filter that can be pushed down.</span></span><br><span class="line"><span class="comment">  *       如果没有额外的过滤器，笛卡尔连接非常昂贵。</span></span><br><span class="line"><span class="comment">  * @group untypedrel</span></span><br><span class="line"><span class="comment">  * @since 2.1.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crossJoin</span></span>(right: <span class="type">Dataset</span>[_]): <span class="type">DataFrame</span> = withPlan &#123;</span><br><span class="line">  <span class="type">Join</span>(logicalPlan, right.logicalPlan, joinType = <span class="type">Cross</span>, <span class="type">None</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="apply"><a href="#apply" class="headerlink" title="apply"></a>apply</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Selects column based on the column name and return it as a [[Column]].</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 选择基于列名的列，并将其作为[[Column]]返回。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @note The column name can also reference to a nested column like `a.b`.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  *       列名也可以引用像“a.b”这样的嵌套列。</span></span><br><span class="line"><span class="comment">  * @group untypedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(colName: <span class="type">String</span>): <span class="type">Column</span> = col(colName)</span><br></pre></td></tr></table></figure>
<h3 id="col"><a href="#col" class="headerlink" title="col"></a>col</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Selects column based on the column name and return it as a [[Column]].</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 选择基于列名的列，并将其作为[[Column]]返回。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @note The column name can also reference to a nested column like `a.b`.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  *       列名也可以引用像“a.b”这样的嵌套列。</span></span><br><span class="line"><span class="comment">  * @group untypedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">col</span></span>(colName: <span class="type">String</span>): <span class="type">Column</span> = colName <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="string">"*"</span> =&gt;</span><br><span class="line">    <span class="type">Column</span>(<span class="type">ResolvedStar</span>(queryExecution.analyzed.output))</span><br><span class="line">  <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    <span class="keyword">val</span> expr = resolve(colName)</span><br><span class="line">    <span class="type">Column</span>(expr)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="select"><a href="#select" class="headerlink" title="select"></a>select</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Selects a set of column based expressions.</span></span><br><span class="line"><span class="comment">  * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">  *   ds.select($"colA", $"colB" + 1)</span></span><br><span class="line"><span class="comment">  * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group untypedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@scala</span>.annotation.varargs</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select</span></span>(cols: <span class="type">Column</span>*): <span class="type">DataFrame</span> = withPlan &#123;</span><br><span class="line">  <span class="type">Project</span>(cols.map(_.named), logicalPlan)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Selects a set of columns. This is a variant of `select` that can only select</span></span><br><span class="line"><span class="comment">  * existing columns using column names (i.e. cannot construct expressions).</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 只能是已经存在的列名</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">  *   // The following two are equivalent:</span></span><br><span class="line"><span class="comment">  *   ds.select("colA", "colB")</span></span><br><span class="line"><span class="comment">  *   ds.select($"colA", $"colB")</span></span><br><span class="line"><span class="comment">  * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group untypedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@scala</span>.annotation.varargs</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select</span></span>(col: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">DataFrame</span> = select((col +: cols).map(<span class="type">Column</span>(_)): _*)</span><br></pre></td></tr></table></figure>
<h3 id="selectExpr"><a href="#selectExpr" class="headerlink" title="selectExpr"></a>selectExpr</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Selects a set of SQL expressions. This is a variant of `select` that accepts</span></span><br><span class="line"><span class="comment">  * SQL expressions.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 接受SQL表达式</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">  *   // The following are equivalent:</span></span><br><span class="line"><span class="comment">  *   以下是等价的:</span></span><br><span class="line"><span class="comment">  *   ds.selectExpr("colA", "colB as newName", "abs(colC)")</span></span><br><span class="line"><span class="comment">  *   ds.select(expr("colA"), expr("colB as newName"), expr("abs(colC)"))</span></span><br><span class="line"><span class="comment">  * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group untypedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@scala</span>.annotation.varargs</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selectExpr</span></span>(exprs: <span class="type">String</span>*): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  select(exprs.map &#123; expr =&gt;</span><br><span class="line">    <span class="type">Column</span>(sparkSession.sessionState.sqlParser.parseExpression(expr))</span><br><span class="line">  &#125;: _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Groups the Dataset using the specified columns, so we can run aggregation on them. See</span></span><br><span class="line"><span class="comment">  * [[RelationalGroupedDataset]] for all the available aggregate functions.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 使用指定的列对数据集进行分组，这样我们就可以对它们进行聚合。</span></span><br><span class="line"><span class="comment">  * 查看[[RelationalGroupedDataset]]为所有可用的聚合函数。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">  *   // Compute the average for all numeric columns grouped by department.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  *   计算按部门分组的所有数字列的平均值。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  *   ds.groupBy($"department").avg()</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  *   // Compute the max age and average salary, grouped by department and gender.</span></span><br><span class="line"><span class="comment">  *   ds.groupBy($"department", $"gender").agg(Map(</span></span><br><span class="line"><span class="comment">  *     "salary" -&gt; "avg",</span></span><br><span class="line"><span class="comment">  *     "age" -&gt; "max"</span></span><br><span class="line"><span class="comment">  *   ))</span></span><br><span class="line"><span class="comment">  * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group untypedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@scala</span>.annotation.varargs</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupBy</span></span>(cols: <span class="type">Column</span>*): <span class="type">RelationalGroupedDataset</span> = &#123;</span><br><span class="line">  <span class="type">RelationalGroupedDataset</span>(toDF(), cols.map(_.expr), <span class="type">RelationalGroupedDataset</span>.<span class="type">GroupByType</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Groups the Dataset using the specified columns, so that we can run aggregation on them.</span></span><br><span class="line"><span class="comment">  * See [[RelationalGroupedDataset]] for all the available aggregate functions.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * This is a variant of groupBy that can only group by existing columns using column names</span></span><br><span class="line"><span class="comment">  * (i.e. cannot construct expressions).</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">  *   // Compute the average for all numeric columns grouped by department.</span></span><br><span class="line"><span class="comment">  *   ds.groupBy("department").avg()</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  *   // Compute the max age and average salary, grouped by department and gender.</span></span><br><span class="line"><span class="comment">  *   ds.groupBy($"department", $"gender").agg(Map(</span></span><br><span class="line"><span class="comment">  *     "salary" -&gt; "avg",</span></span><br><span class="line"><span class="comment">  *     "age" -&gt; "max"</span></span><br><span class="line"><span class="comment">  *   ))</span></span><br><span class="line"><span class="comment">  * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group untypedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@scala</span>.annotation.varargs</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupBy</span></span>(col1: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">RelationalGroupedDataset</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> colNames: <span class="type">Seq</span>[<span class="type">String</span>] = col1 +: cols</span><br><span class="line">  <span class="type">RelationalGroupedDataset</span>(</span><br><span class="line">    toDF(), colNames.map(colName =&gt; resolve(colName)), <span class="type">RelationalGroupedDataset</span>.<span class="type">GroupByType</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="rollup"><a href="#rollup" class="headerlink" title="rollup"></a>rollup</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Create a multi-dimensional rollup for the current Dataset using the specified columns,</span></span><br><span class="line"><span class="comment">    * so we can run aggregation on them.</span></span><br><span class="line"><span class="comment">    * See [[RelationalGroupedDataset]] for all the available aggregate functions.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 使用指定的列为当前数据集创建多维的汇总，因此我们可以在它们上运行聚合。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">    *   // Compute the average for all numeric columns rolluped by department and group.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    *   汇总后 求平均值</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    *   ds.rollup($"department", $"group").avg()</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    *   // Compute the max age and average salary, rolluped by department and gender.</span></span><br><span class="line"><span class="comment">    *   ds.rollup($"department", $"gender").agg(Map(</span></span><br><span class="line"><span class="comment">    *     "salary" -&gt; "avg",</span></span><br><span class="line"><span class="comment">    *     "age" -&gt; "max"</span></span><br><span class="line"><span class="comment">    *   ))</span></span><br><span class="line"><span class="comment">    * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group untypedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="meta">@scala</span>.annotation.varargs</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">rollup</span></span>(cols: <span class="type">Column</span>*): <span class="type">RelationalGroupedDataset</span> = &#123;</span><br><span class="line">    <span class="type">RelationalGroupedDataset</span>(toDF(), cols.map(_.expr), <span class="type">RelationalGroupedDataset</span>.<span class="type">RollupType</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Create a multi-dimensional rollup for the current Dataset using the specified columns,</span></span><br><span class="line"><span class="comment">    * so we can run aggregation on them.</span></span><br><span class="line"><span class="comment">    * See [[RelationalGroupedDataset]] for all the available aggregate functions.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 使用指定的列为当前数据集创建多维的rollup，因此我们可以在它们上运行聚合。</span></span><br><span class="line"><span class="comment">    * rollup可以实现 从右到左一次递减的多级统计，显示统计某一层次结构的聚合</span></span><br><span class="line"><span class="comment">    * 例如 rollup(a,b,c,d) =结果=&gt; (a,b,c,d),(a,b,c),(a,b),a</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * This is a variant of rollup that can only group by existing columns using column names</span></span><br><span class="line"><span class="comment">    * (i.e. cannot construct expressions).</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">    *   // Compute the average for all numeric columns rolluped by department and group.</span></span><br><span class="line"><span class="comment">    *   ds.rollup("department", "group").avg()</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    *   // Compute the max age and average salary, rolluped by department and gender.</span></span><br><span class="line"><span class="comment">    *   ds.rollup($"department", $"gender").agg(Map(</span></span><br><span class="line"><span class="comment">    *     "salary" -&gt; "avg",</span></span><br><span class="line"><span class="comment">    *     "age" -&gt; "max"</span></span><br><span class="line"><span class="comment">    *   ))</span></span><br><span class="line"><span class="comment">    * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group untypedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="meta">@scala</span>.annotation.varargs</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">rollup</span></span>(col1: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">RelationalGroupedDataset</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> colNames: <span class="type">Seq</span>[<span class="type">String</span>] = col1 +: cols</span><br><span class="line">    <span class="type">RelationalGroupedDataset</span>(</span><br><span class="line">      toDF(), colNames.map(colName =&gt; resolve(colName)), <span class="type">RelationalGroupedDataset</span>.<span class="type">RollupType</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h3 id="cube"><a href="#cube" class="headerlink" title="cube"></a>cube</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Create a multi-dimensional cube for the current Dataset using the specified columns,</span></span><br><span class="line"><span class="comment">    * so we can run aggregation on them.</span></span><br><span class="line"><span class="comment">    * See [[RelationalGroupedDataset]] for all the available aggregate functions.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 使用指定的列为当前数据集创建多维数据集，因此我们可以在它们上运行聚合。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">    *   // Compute the average for all numeric columns cubed by department and group.</span></span><br><span class="line"><span class="comment">    *   ds.cube($"department", $"group").avg()</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    *   // Compute the max age and average salary, cubed by department and gender.</span></span><br><span class="line"><span class="comment">    *   ds.cube($"department", $"gender").agg(Map(</span></span><br><span class="line"><span class="comment">    *     "salary" -&gt; "avg",</span></span><br><span class="line"><span class="comment">    *     "age" -&gt; "max"</span></span><br><span class="line"><span class="comment">    *   ))</span></span><br><span class="line"><span class="comment">    * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group untypedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="meta">@scala</span>.annotation.varargs</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cube</span></span>(cols: <span class="type">Column</span>*): <span class="type">RelationalGroupedDataset</span> = &#123;</span><br><span class="line">    <span class="type">RelationalGroupedDataset</span>(toDF(), cols.map(_.expr), <span class="type">RelationalGroupedDataset</span>.<span class="type">CubeType</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Create a multi-dimensional cube for the current Dataset using the specified columns,</span></span><br><span class="line"><span class="comment">    * so we can run aggregation on them.</span></span><br><span class="line"><span class="comment">    * See [[RelationalGroupedDataset]] for all the available aggregate functions.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 魔方 例如：cube(a,b,c) =结果=&gt; (a,b),(a,c),a,(b,c),b,c 结果为所有的维度</span></span><br><span class="line"><span class="comment">    * 使用指定的列为当前数据集创建多维多维数据集，因此我们可以在它们上运行聚合。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * This is a variant of cube that can only group by existing columns using column names</span></span><br><span class="line"><span class="comment">    * (i.e. cannot construct expressions).</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 这是一个多维数据集的变体，它只能通过使用列名的现有列来分组</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">    *   // Compute the average for all numeric columns cubed by department and group.</span></span><br><span class="line"><span class="comment">    *   ds.cube("department", "group").avg()</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    *   // Compute the max age and average salary, cubed by department and gender.</span></span><br><span class="line"><span class="comment">    *   ds.cube($"department", $"gender").agg(Map(</span></span><br><span class="line"><span class="comment">    *     "salary" -&gt; "avg",</span></span><br><span class="line"><span class="comment">    *     "age" -&gt; "max"</span></span><br><span class="line"><span class="comment">    *   ))</span></span><br><span class="line"><span class="comment">    * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group untypedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="meta">@scala</span>.annotation.varargs</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cube</span></span>(col1: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">RelationalGroupedDataset</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> colNames: <span class="type">Seq</span>[<span class="type">String</span>] = col1 +: cols</span><br><span class="line">    <span class="type">RelationalGroupedDataset</span>(</span><br><span class="line">      toDF(), colNames.map(colName =&gt; resolve(colName)), <span class="type">RelationalGroupedDataset</span>.<span class="type">CubeType</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h3 id="agg"><a href="#agg" class="headerlink" title="agg"></a>agg</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * (Scala-specific) Aggregates on the entire Dataset without groups.</span></span><br><span class="line"><span class="comment">   * 对整个数据集进行聚合，无需分组。</span></span><br><span class="line"><span class="comment">   * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</span></span><br><span class="line"><span class="comment">   *   ds.agg("age" -&gt; "max", "salary" -&gt; "avg")</span></span><br><span class="line"><span class="comment">   *   ds.groupBy().agg("age" -&gt; "max", "salary" -&gt; "avg")</span></span><br><span class="line"><span class="comment">   * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @group untypedrel</span></span><br><span class="line"><span class="comment">   * @since 2.0.0</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">agg</span></span>(aggExpr: (<span class="type">String</span>, <span class="type">String</span>), aggExprs: (<span class="type">String</span>, <span class="type">String</span>)*): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">   groupBy().agg(aggExpr, aggExprs: _*)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * (Scala-specific) Aggregates on the entire Dataset without groups.</span></span><br><span class="line"><span class="comment">   * 对整个数据集进行聚合，无需分组。</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</span></span><br><span class="line"><span class="comment">   *   ds.agg(Map("age" -&gt; "max", "salary" -&gt; "avg"))</span></span><br><span class="line"><span class="comment">   *   ds.groupBy().agg(Map("age" -&gt; "max", "salary" -&gt; "avg"))</span></span><br><span class="line"><span class="comment">   * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @group untypedrel</span></span><br><span class="line"><span class="comment">   * @since 2.0.0</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">agg</span></span>(exprs: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">DataFrame</span> = groupBy().agg(exprs)</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * (Java-specific) Aggregates on the entire Dataset without groups.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * 对整个数据集进行聚合，无需分组。</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</span></span><br><span class="line"><span class="comment">   *   ds.agg(Map("age" -&gt; "max", "salary" -&gt; "avg"))</span></span><br><span class="line"><span class="comment">   *   ds.groupBy().agg(Map("age" -&gt; "max", "salary" -&gt; "avg"))</span></span><br><span class="line"><span class="comment">   * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @group untypedrel</span></span><br><span class="line"><span class="comment">   * @since 2.0.0</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">agg</span></span>(exprs: java.util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">DataFrame</span> = groupBy().agg(exprs)</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Aggregates on the entire Dataset without groups.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * 对整个数据集进行聚合，无需分组。</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</span></span><br><span class="line"><span class="comment">   *   ds.agg(max($"age"), avg($"salary"))</span></span><br><span class="line"><span class="comment">   *   ds.groupBy().agg(max($"age"), avg($"salary"))</span></span><br><span class="line"><span class="comment">   * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @group untypedrel</span></span><br><span class="line"><span class="comment">   * @since 2.0.0</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"> <span class="meta">@scala</span>.annotation.varargs</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">agg</span></span>(expr: <span class="type">Column</span>, exprs: <span class="type">Column</span>*): <span class="type">DataFrame</span> = groupBy().agg(expr, exprs: _*)</span><br></pre></td></tr></table></figure>
<h3 id="explode"><a href="#explode" class="headerlink" title="explode"></a>explode</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more</span></span><br><span class="line"><span class="comment">  * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of</span></span><br><span class="line"><span class="comment">  * the input row are implicitly joined with each row that is output by the function.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 根据提供的方法，该数据集的每一行都被扩展为零个或更多的行，返回一个新的数据集。</span></span><br><span class="line"><span class="comment">  * 这类似于HiveQL的“LATERAL VIEW”。</span></span><br><span class="line"><span class="comment">  * 输入行的列 隐式地加入了由函数输出的每一行。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * Given that this is deprecated, as an alternative, you can explode columns either using</span></span><br><span class="line"><span class="comment">  * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count</span></span><br><span class="line"><span class="comment">  * the number of books that contain a given word:</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 考虑到这已经被弃用，作为替代，您可以使用“functions.explode()”或“flatMap()”来引爆列。</span></span><br><span class="line"><span class="comment">  * 下面的示例使用这些替代方法来计算包含给定单词的图书的数量:</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">  *   case class Book(title: String, words: String)</span></span><br><span class="line"><span class="comment">  *   val ds: Dataset[Book]</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  *   val allWords = ds.select('title, explode(split('words, " ")).as("word"))</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  *   val bookCountPerWord = allWords.groupBy("word").agg(countDistinct("title"))</span></span><br><span class="line"><span class="comment">  * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * Using `flatMap()` this can similarly be exploded as:</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">  *   ds.flatMap(_.words.split(" "))</span></span><br><span class="line"><span class="comment">  * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group untypedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0 已经过时，用 flatMap() 或 functions.explode() 代替</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@deprecated</span>(<span class="string">"use flatMap() or select() with functions.explode() instead"</span>, <span class="string">"2.0.0"</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">explode</span></span>[<span class="type">A</span> &lt;: <span class="type">Product</span> : <span class="type">TypeTag</span>](input: <span class="type">Column</span>*)(f: <span class="type">Row</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">A</span>]): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> elementSchema = <span class="type">ScalaReflection</span>.schemaFor[<span class="type">A</span>].dataType.asInstanceOf[<span class="type">StructType</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> convert = <span class="type">CatalystTypeConverters</span>.createToCatalystConverter(elementSchema)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> rowFunction =</span><br><span class="line">    f.andThen(_.map(convert(_).asInstanceOf[<span class="type">InternalRow</span>]))</span><br><span class="line">  <span class="keyword">val</span> generator = <span class="type">UserDefinedGenerator</span>(elementSchema, rowFunction, input.map(_.expr))</span><br><span class="line"></span><br><span class="line">  withPlan &#123;</span><br><span class="line">    <span class="type">Generate</span>(generator, join = <span class="literal">true</span>, outer = <span class="literal">false</span>,</span><br><span class="line">      qualifier = <span class="type">None</span>, generatorOutput = <span class="type">Nil</span>, logicalPlan)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero</span></span><br><span class="line"><span class="comment">  * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All</span></span><br><span class="line"><span class="comment">  * columns of the input row are implicitly joined with each value that is output by the function.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * Given that this is deprecated, as an alternative, you can explode columns either using</span></span><br><span class="line"><span class="comment">  * `functions.explode()`:</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">  *   ds.select(explode(split('words, " ")).as("word"))</span></span><br><span class="line"><span class="comment">  * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * or `flatMap()`:</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">  *   ds.flatMap(_.words.split(" "))</span></span><br><span class="line"><span class="comment">  * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group untypedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@deprecated</span>(<span class="string">"use flatMap() or select() with functions.explode() instead"</span>, <span class="string">"2.0.0"</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">explode</span></span>[<span class="type">A</span>, <span class="type">B</span>: <span class="type">TypeTag</span>](inputColumn: <span class="type">String</span>, outputColumn: <span class="type">String</span>)(f: <span class="type">A</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">B</span>])</span><br><span class="line">: <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> dataType = <span class="type">ScalaReflection</span>.schemaFor[<span class="type">B</span>].dataType</span><br><span class="line">  <span class="keyword">val</span> attributes = <span class="type">AttributeReference</span>(outputColumn, dataType)() :: <span class="type">Nil</span></span><br><span class="line">  <span class="comment">// TODO handle the metadata?</span></span><br><span class="line">  <span class="keyword">val</span> elementSchema = attributes.toStructType</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">rowFunction</span></span>(row: <span class="type">Row</span>): <span class="type">TraversableOnce</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> convert = <span class="type">CatalystTypeConverters</span>.createToCatalystConverter(dataType)</span><br><span class="line">    f(row(<span class="number">0</span>).asInstanceOf[<span class="type">A</span>]).map(o =&gt; <span class="type">InternalRow</span>(convert(o)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> generator = <span class="type">UserDefinedGenerator</span>(elementSchema, rowFunction, apply(inputColumn).expr :: <span class="type">Nil</span>)</span><br><span class="line"></span><br><span class="line">  withPlan &#123;</span><br><span class="line">    <span class="type">Generate</span>(generator, join = <span class="literal">true</span>, outer = <span class="literal">false</span>,</span><br><span class="line">      qualifier = <span class="type">None</span>, generatorOutput = <span class="type">Nil</span>, logicalPlan)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="withColumn"><a href="#withColumn" class="headerlink" title="withColumn"></a>withColumn</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset by adding a column or replacing the existing column that has</span></span><br><span class="line"><span class="comment">  * the same name.</span></span><br><span class="line"><span class="comment">  * 通过添加一个列或替换具有相同名称的现有列返回新的数据集。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group untypedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">withColumn</span></span>(colName: <span class="type">String</span>, col: <span class="type">Column</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> resolver = sparkSession.sessionState.analyzer.resolver</span><br><span class="line">  <span class="keyword">val</span> output = queryExecution.analyzed.output</span><br><span class="line">  <span class="keyword">val</span> shouldReplace = output.exists(f =&gt; resolver(f.name, colName))</span><br><span class="line">  <span class="keyword">if</span> (shouldReplace) &#123;</span><br><span class="line">    <span class="keyword">val</span> columns = output.map &#123; field =&gt;</span><br><span class="line">      <span class="keyword">if</span> (resolver(field.name, colName)) &#123;</span><br><span class="line">        col.as(colName)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">Column</span>(field)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    select(columns: _*)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    select(<span class="type">Column</span>(<span class="string">"*"</span>), col.as(colName))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset by adding a column with metadata.</span></span><br><span class="line"><span class="comment">  * 通过添加带有元数据的列返回一个新的数据集。</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">withColumn</span></span>(colName: <span class="type">String</span>, col: <span class="type">Column</span>, metadata: <span class="type">Metadata</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> resolver = sparkSession.sessionState.analyzer.resolver</span><br><span class="line">  <span class="keyword">val</span> output = queryExecution.analyzed.output</span><br><span class="line">  <span class="keyword">val</span> shouldReplace = output.exists(f =&gt; resolver(f.name, colName))</span><br><span class="line">  <span class="keyword">if</span> (shouldReplace) &#123;</span><br><span class="line">    <span class="keyword">val</span> columns = output.map &#123; field =&gt;</span><br><span class="line">      <span class="keyword">if</span> (resolver(field.name, colName)) &#123;</span><br><span class="line">        col.as(colName, metadata)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">Column</span>(field)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    select(columns: _*)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    select(<span class="type">Column</span>(<span class="string">"*"</span>), col.as(colName, metadata))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="withColumnRenamed"><a href="#withColumnRenamed" class="headerlink" title="withColumnRenamed"></a>withColumnRenamed</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset with a column renamed.</span></span><br><span class="line"><span class="comment">  * This is a no-op if schema doesn't contain existingName.</span></span><br><span class="line"><span class="comment">  * 返回一个重命名的列的新数据集。</span></span><br><span class="line"><span class="comment">  * 如果模式不包含存在名称，那么这是不操作的。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group untypedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">withColumnRenamed</span></span>(existingName: <span class="type">String</span>, newName: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> resolver = sparkSession.sessionState.analyzer.resolver</span><br><span class="line">  <span class="keyword">val</span> output = queryExecution.analyzed.output</span><br><span class="line">  <span class="keyword">val</span> shouldRename = output.exists(f =&gt; resolver(f.name, existingName))</span><br><span class="line">  <span class="keyword">if</span> (shouldRename) &#123;</span><br><span class="line">    <span class="keyword">val</span> columns = output.map &#123; col =&gt;</span><br><span class="line">      <span class="keyword">if</span> (resolver(col.name, existingName)) &#123;</span><br><span class="line">        <span class="type">Column</span>(col).as(newName)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">Column</span>(col)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    select(columns: _*)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    toDF()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="drop"><a href="#drop" class="headerlink" title="drop"></a>drop</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain</span></span><br><span class="line"><span class="comment">  * column name.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回删除指定列之后的新Dataset</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * This method can only be used to drop top level columns. the colName string is treated</span></span><br><span class="line"><span class="comment">  * literally without further interpretation.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 仅用于删除顶层的列</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group untypedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop</span></span>(colName: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  drop(<span class="type">Seq</span>(colName): _*)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset with columns dropped.</span></span><br><span class="line"><span class="comment">  * This is a no-op if schema doesn't contain column name(s).</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 删除指定的多个列，并返回新的dataset</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * This method can only be used to drop top level columns. the colName string is treated literally</span></span><br><span class="line"><span class="comment">  * without further interpretation.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group untypedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@scala</span>.annotation.varargs</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop</span></span>(colNames: <span class="type">String</span>*): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> resolver = sparkSession.sessionState.analyzer.resolver</span><br><span class="line">  <span class="keyword">val</span> allColumns = queryExecution.analyzed.output</span><br><span class="line">  <span class="keyword">val</span> remainingCols = allColumns.filter &#123; attribute =&gt;</span><br><span class="line">    colNames.forall(n =&gt; !resolver(attribute.name, n))</span><br><span class="line">  &#125;.map(attribute =&gt; <span class="type">Column</span>(attribute))</span><br><span class="line">  <span class="keyword">if</span> (remainingCols.size == allColumns.size) &#123;</span><br><span class="line">    toDF()</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">this</span>.select(remainingCols: _*)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset with a column dropped.</span></span><br><span class="line"><span class="comment">  * This version of drop accepts a [[Column]] rather than a name.</span></span><br><span class="line"><span class="comment">  * This is a no-op if the Dataset doesn't have a column</span></span><br><span class="line"><span class="comment">  * with an equivalent expression.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 删除指定的 列（根据Column）</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group untypedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop</span></span>(col: <span class="type">Column</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> expression = col <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Column</span>(u: <span class="type">UnresolvedAttribute</span>) =&gt;</span><br><span class="line">      queryExecution.analyzed.resolveQuoted(</span><br><span class="line">        u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Column</span>(expr: <span class="type">Expression</span>) =&gt; expr</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> attrs = <span class="keyword">this</span>.logicalPlan.output</span><br><span class="line">  <span class="keyword">val</span> colsAfterDrop = attrs.filter &#123; attr =&gt;</span><br><span class="line">    attr != expression</span><br><span class="line">  &#125;.map(attr =&gt; <span class="type">Column</span>(attr))</span><br><span class="line">  select(colsAfterDrop: _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="typedrel-有类型的转换"><a href="#typedrel-有类型的转换" class="headerlink" title="typedrel-有类型的转换"></a>typedrel-有类型的转换</h2><h3 id="joinWith"><a href="#joinWith" class="headerlink" title="joinWith"></a>joinWith</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * :: Experimental ::  实验的</span></span><br><span class="line"><span class="comment">  * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to</span></span><br><span class="line"><span class="comment">  * true.</span></span><br><span class="line"><span class="comment">  * 连接这个数据集返回一个“Tuple2”对每一对的“条件”计算为true。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * This is similar to the relation `join` function with one important difference in the</span></span><br><span class="line"><span class="comment">  * result schema. Since `joinWith` preserves objects present on either side of the join, the</span></span><br><span class="line"><span class="comment">  * result schema is similarly nested into a tuple under the column names `_1` and `_2`.</span></span><br><span class="line"><span class="comment">  * 这类似于关系“join”函数，在结果模式中有一个重要的区别。</span></span><br><span class="line"><span class="comment">  * 由于“joinWith”保存了连接的任何一边的对象，因此结果模式类似地嵌套在列名称“_1”和“_2”下面的tuple中。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * This type of join can be useful both for preserving type-safety with the original object</span></span><br><span class="line"><span class="comment">  * types as well as working with relational data where either side of the join has column</span></span><br><span class="line"><span class="comment">  * names in common.</span></span><br><span class="line"><span class="comment">  * 这种类型的联接既可以用于保存与原始对象类型的类型安全性，</span></span><br><span class="line"><span class="comment">  * 也可以用于处理连接的任何一端都有列名的关系数据。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param other     Right side of the join.</span></span><br><span class="line"><span class="comment">  * @param condition Join expression.</span></span><br><span class="line"><span class="comment">  * @param joinType  One of: `inner`, `outer`, `left_outer`, `right_outer`, `leftsemi`.</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">joinWith</span></span>[<span class="type">U</span>](other: <span class="type">Dataset</span>[<span class="type">U</span>], condition: <span class="type">Column</span>, joinType: <span class="type">String</span>): <span class="type">Dataset</span>[(<span class="type">T</span>, <span class="type">U</span>)] = &#123;</span><br><span class="line">  <span class="comment">// Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,</span></span><br><span class="line">  <span class="comment">// 创建一个联接节点并首先解析它，使Join条件得到解析，self - Join解析，</span></span><br><span class="line">  <span class="comment">// etc.</span></span><br><span class="line">  <span class="keyword">val</span> joined = sparkSession.sessionState.executePlan(</span><br><span class="line">    <span class="type">Join</span>(</span><br><span class="line">      <span class="keyword">this</span>.logicalPlan,</span><br><span class="line">      other.logicalPlan,</span><br><span class="line">      <span class="type">JoinType</span>(joinType),</span><br><span class="line">      <span class="type">Some</span>(condition.expr))).analyzed.asInstanceOf[<span class="type">Join</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment">// For both join side, combine all outputs into a single column and alias it with "_1" or "_2",</span></span><br><span class="line">  <span class="comment">// to match the schema for the encoder of the join result.</span></span><br><span class="line">  <span class="comment">// 对于这两个连接，将所有输出合并为一个列，并将其别名为“_1”或“_2”，以匹配连接结果的编码器的模式。</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Note that we do this before joining them, to enable the join operator to return null for one</span></span><br><span class="line">  <span class="comment">// side, in cases like outer-join.</span></span><br><span class="line">  <span class="comment">// 请注意，在join它们之前，我们这样做，使join操作符在像outer - join这样的情况下返回null。</span></span><br><span class="line">  <span class="keyword">val</span> left = &#123;</span><br><span class="line">    <span class="keyword">val</span> combined = <span class="keyword">if</span> (<span class="keyword">this</span>.exprEnc.flat) &#123;</span><br><span class="line">      assert(joined.left.output.length == <span class="number">1</span>)</span><br><span class="line">      <span class="type">Alias</span>(joined.left.output.head, <span class="string">"_1"</span>)()</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">Alias</span>(<span class="type">CreateStruct</span>(joined.left.output), <span class="string">"_1"</span>)()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">Project</span>(combined :: <span class="type">Nil</span>, joined.left)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> right = &#123;</span><br><span class="line">    <span class="keyword">val</span> combined = <span class="keyword">if</span> (other.exprEnc.flat) &#123;</span><br><span class="line">      assert(joined.right.output.length == <span class="number">1</span>)</span><br><span class="line">      <span class="type">Alias</span>(joined.right.output.head, <span class="string">"_2"</span>)()</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">Alias</span>(<span class="type">CreateStruct</span>(joined.right.output), <span class="string">"_2"</span>)()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">Project</span>(combined :: <span class="type">Nil</span>, joined.right)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Rewrites the join condition to make the attribute point to correct column/field, after we</span></span><br><span class="line">  <span class="comment">// combine the outputs of each join side.</span></span><br><span class="line">  <span class="comment">// 在将每个连接的输出组合在一起之后,重写联接条件，使属性指向正确的列/字段。</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> conditionExpr = joined.condition.get transformUp &#123;</span><br><span class="line">    <span class="keyword">case</span> a: <span class="type">Attribute</span> <span class="keyword">if</span> joined.left.outputSet.contains(a) =&gt;</span><br><span class="line">      <span class="keyword">if</span> (<span class="keyword">this</span>.exprEnc.flat) &#123;</span><br><span class="line">        left.output.head</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> index = joined.left.output.indexWhere(_.exprId == a.exprId)</span><br><span class="line">        <span class="type">GetStructField</span>(left.output.head, index)</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">case</span> a: <span class="type">Attribute</span> <span class="keyword">if</span> joined.right.outputSet.contains(a) =&gt;</span><br><span class="line">      <span class="keyword">if</span> (other.exprEnc.flat) &#123;</span><br><span class="line">        right.output.head</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> index = joined.right.output.indexWhere(_.exprId == a.exprId)</span><br><span class="line">        <span class="type">GetStructField</span>(right.output.head, index)</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> tuple2Encoder: <span class="type">Encoder</span>[(<span class="type">T</span>, <span class="type">U</span>)] =</span><br><span class="line">    <span class="type">ExpressionEncoder</span>.tuple(<span class="keyword">this</span>.exprEnc, other.exprEnc)</span><br><span class="line"></span><br><span class="line">  withTypedPlan(<span class="type">Join</span>(left, right, joined.joinType, <span class="type">Some</span>(conditionExpr)))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * :: Experimental ::</span></span><br><span class="line"><span class="comment">  * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair</span></span><br><span class="line"><span class="comment">  * where `condition` evaluates to true.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 使用内部的等连接加入这个数据集，为每一对返回一个“Tuple2”，其中“条件”的计算结果为true。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param other     Right side of the join.</span></span><br><span class="line"><span class="comment">  * @param condition Join expression.</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">joinWith</span></span>[<span class="type">U</span>](other: <span class="type">Dataset</span>[<span class="type">U</span>], condition: <span class="type">Column</span>): <span class="type">Dataset</span>[(<span class="type">T</span>, <span class="type">U</span>)] = &#123;</span><br><span class="line">  joinWith(other, condition, <span class="string">"inner"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="sortWithinPartitions"><a href="#sortWithinPartitions" class="headerlink" title="sortWithinPartitions"></a>sortWithinPartitions</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset with each partition sorted by the given expressions.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回一个新的数据集，每个分区按照给定的表达式排序。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * This is the same operation as "SORT BY" in SQL (Hive QL).</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 这与SQL(Hive QL)中“SORT BY”的操作相同。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@scala</span>.annotation.varargs</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortWithinPartitions</span></span>(sortCol: <span class="type">String</span>, sortCols: <span class="type">String</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  sortWithinPartitions((sortCol +: sortCols).map(<span class="type">Column</span>(_)): _*)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset with each partition sorted by the given expressions.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回一个新的数据集，每个分区按照给定的表达式排序。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * This is the same operation as "SORT BY" in SQL (Hive QL).</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 这与SQL(Hive QL)中“SORT BY”的操作相同。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@scala</span>.annotation.varargs</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortWithinPartitions</span></span>(sortExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  sortInternal(global = <span class="literal">false</span>, sortExprs)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Returns a new Dataset sorted by the specified column, all in ascending order.</span></span><br><span class="line"><span class="comment">    * 排序 升序</span></span><br><span class="line"><span class="comment">    * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">    *   // The following 3 are equivalent</span></span><br><span class="line"><span class="comment">    *   下面3个是等价的</span></span><br><span class="line"><span class="comment">    *   ds.sort("sortcol")</span></span><br><span class="line"><span class="comment">    *   ds.sort($"sortcol")</span></span><br><span class="line"><span class="comment">    *   ds.sort($"sortcol".asc)</span></span><br><span class="line"><span class="comment">    * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="meta">@scala</span>.annotation.varargs</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sort</span></span>(sortCol: <span class="type">String</span>, sortCols: <span class="type">String</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    sort((sortCol +: sortCols).map(apply): _*)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Returns a new Dataset sorted by the given expressions. For example:</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 返回一个由给定表达式排序的新数据集。例如:</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">    *   ds.sort($"col1", $"col2".desc)</span></span><br><span class="line"><span class="comment">    * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="meta">@scala</span>.annotation.varargs</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sort</span></span>(sortExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    sortInternal(global = <span class="literal">true</span>, sortExprs)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h3 id="orderBy"><a href="#orderBy" class="headerlink" title="orderBy"></a>orderBy</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Returns a new Dataset sorted by the given expressions.</span></span><br><span class="line"><span class="comment">   * This is an alias of the `sort` function.</span></span><br><span class="line"><span class="comment">   * 这是“sort”函数的别名。</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @group typedrel</span></span><br><span class="line"><span class="comment">   * @since 2.0.0</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"> <span class="meta">@scala</span>.annotation.varargs</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">orderBy</span></span>(sortCol: <span class="type">String</span>, sortCols: <span class="type">String</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = sort(sortCol, sortCols: _*)</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Returns a new Dataset sorted by the given expressions.</span></span><br><span class="line"><span class="comment">   * This is an alias of the `sort` function.</span></span><br><span class="line"><span class="comment">   * 这是“sort”函数的别名。</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @group typedrel</span></span><br><span class="line"><span class="comment">   * @since 2.0.0</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"> <span class="meta">@scala</span>.annotation.varargs</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">orderBy</span></span>(sortExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = sort(sortExprs: _*)</span><br></pre></td></tr></table></figure>
<h3 id="as-1"><a href="#as-1" class="headerlink" title="as"></a>as</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Returns a new Dataset with an alias set.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 返回一个具有别名集的新数据集。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 1.6.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">as</span></span>(alias: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</span><br><span class="line">    <span class="type">SubqueryAlias</span>(alias, logicalPlan, <span class="type">None</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * (Scala-specific) Returns a new Dataset with an alias set.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">as</span></span>(alias: <span class="type">Symbol</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = as(alias.name)</span><br></pre></td></tr></table></figure>
<h3 id="alias"><a href="#alias" class="headerlink" title="alias"></a>alias</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Returns a new Dataset with an alias set. Same as `as`.</span></span><br><span class="line"><span class="comment">    * 返回一个具有别名集的新数据集。与“as”相同。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">alias</span></span>(alias: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = as(alias)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">alias</span></span>(alias: <span class="type">Symbol</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = as(alias)</span><br></pre></td></tr></table></figure>
<h3 id="select-1"><a href="#select-1" class="headerlink" title="select"></a>select</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * :: Experimental ::</span></span><br><span class="line"><span class="comment">    * Returns a new Dataset by computing the given [[Column]] expression for each element.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 通过计算每个元素的给定[[列]]表达式返回一个新的数据集。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">    *   val ds = Seq(1, 2, 3).toDS()</span></span><br><span class="line"><span class="comment">    *   val newDS = ds.select(expr("value + 1").as[Int])</span></span><br><span class="line"><span class="comment">    * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 1.6.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="meta">@Experimental</span></span><br><span class="line">  <span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>[<span class="type">U1</span>](c1: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U1</span>]): <span class="type">Dataset</span>[<span class="type">U1</span>] = &#123;</span><br><span class="line">    <span class="keyword">implicit</span> <span class="keyword">val</span> encoder = c1.encoder</span><br><span class="line">    <span class="keyword">val</span> project = <span class="type">Project</span>(c1.withInputType(exprEnc, logicalPlan.output).named :: <span class="type">Nil</span>,</span><br><span class="line">      logicalPlan)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (encoder.flat) &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Dataset</span>[<span class="type">U1</span>](sparkSession, project, encoder)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Flattens inner fields of U1</span></span><br><span class="line">      <span class="comment">// 使U1的内部区域变平</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">Dataset</span>[<span class="type">Tuple1</span>[<span class="type">U1</span>]](sparkSession, project, <span class="type">ExpressionEncoder</span>.tuple(encoder)).map(_._1)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * :: Experimental ::</span></span><br><span class="line"><span class="comment">    * Returns a new Dataset by computing the given [[Column]] expressions for each element.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 1.6.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="meta">@Experimental</span></span><br><span class="line">  <span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>[<span class="type">U1</span>, <span class="type">U2</span>](c1: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U1</span>], c2: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U2</span>]): <span class="type">Dataset</span>[(<span class="type">U1</span>, <span class="type">U2</span>)] =</span><br><span class="line">    selectUntyped(c1, c2).asInstanceOf[<span class="type">Dataset</span>[(<span class="type">U1</span>, <span class="type">U2</span>)]]</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * :: Experimental ::</span></span><br><span class="line"><span class="comment">    * Returns a new Dataset by computing the given [[Column]] expressions for each element.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 1.6.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="meta">@Experimental</span></span><br><span class="line">  <span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>[<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>](</span><br><span class="line">                          c1: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U1</span>],</span><br><span class="line">                          c2: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U2</span>],</span><br><span class="line">                          c3: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U3</span>]): <span class="type">Dataset</span>[(<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>)] =</span><br><span class="line">    selectUntyped(c1, c2, c3).asInstanceOf[<span class="type">Dataset</span>[(<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>)]]</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * :: Experimental ::</span></span><br><span class="line"><span class="comment">    * Returns a new Dataset by computing the given [[Column]] expressions for each element.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 1.6.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="meta">@Experimental</span></span><br><span class="line">  <span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>[<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>, <span class="type">U4</span>](</span><br><span class="line">                              c1: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U1</span>],</span><br><span class="line">                              c2: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U2</span>],</span><br><span class="line">                              c3: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U3</span>],</span><br><span class="line">                              c4: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U4</span>]): <span class="type">Dataset</span>[(<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>, <span class="type">U4</span>)] =</span><br><span class="line">    selectUntyped(c1, c2, c3, c4).asInstanceOf[<span class="type">Dataset</span>[(<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>, <span class="type">U4</span>)]]</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * :: Experimental ::</span></span><br><span class="line"><span class="comment">    * Returns a new Dataset by computing the given [[Column]] expressions for each element.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 1.6.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="meta">@Experimental</span></span><br><span class="line">  <span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>[<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>, <span class="type">U4</span>, <span class="type">U5</span>](</span><br><span class="line">                                  c1: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U1</span>],</span><br><span class="line">                                  c2: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U2</span>],</span><br><span class="line">                                  c3: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U3</span>],</span><br><span class="line">                                  c4: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U4</span>],</span><br><span class="line">                                  c5: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U5</span>]): <span class="type">Dataset</span>[(<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>, <span class="type">U4</span>, <span class="type">U5</span>)] =</span><br><span class="line">    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[<span class="type">Dataset</span>[(<span class="type">U1</span>, <span class="type">U2</span>, <span class="type">U3</span>, <span class="type">U4</span>, <span class="type">U5</span>)]]</span><br></pre></td></tr></table></figure>
<h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Filters rows using the given condition.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 用给定的条件过滤rows</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">    *   // The following are equivalent:</span></span><br><span class="line"><span class="comment">    *   以下是等价的：</span></span><br><span class="line"><span class="comment">    *   peopleDs.filter($"age" &gt; 15)</span></span><br><span class="line"><span class="comment">    *   peopleDs.where($"age" &gt; 15)</span></span><br><span class="line"><span class="comment">    * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 1.6.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(condition: <span class="type">Column</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</span><br><span class="line">    <span class="type">Filter</span>(condition.expr, logicalPlan)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Filters rows using the given SQL expression.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 用给定的 SQL 表达式 过滤rows</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">    *   peopleDs.filter("age &gt; 15")</span></span><br><span class="line"><span class="comment">    * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 1.6.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(conditionExpr: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    filter(<span class="type">Column</span>(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h3 id="where"><a href="#where" class="headerlink" title="where"></a>where</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Filters rows using the given condition. This is an alias for `filter`.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * 使用给定条件过滤行。</span></span><br><span class="line"><span class="comment">   * 这是“filter”的别名。</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">   *   // The following are equivalent:</span></span><br><span class="line"><span class="comment">   *   peopleDs.filter($"age" &gt; 15)</span></span><br><span class="line"><span class="comment">   *   peopleDs.where($"age" &gt; 15)</span></span><br><span class="line"><span class="comment">   * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @group typedrel</span></span><br><span class="line"><span class="comment">   * @since 1.6.0</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">where</span></span>(condition: <span class="type">Column</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = filter(condition)</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Filters rows using the given SQL expression.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * 使用给定的 SQL 表达式  过滤 rows</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">   *   peopleDs.where("age &gt; 15")</span></span><br><span class="line"><span class="comment">   * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @group typedrel</span></span><br><span class="line"><span class="comment">   * @since 1.6.0</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">where</span></span>(conditionExpr: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">   filter(<span class="type">Column</span>(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * :: Experimental ::</span></span><br><span class="line"><span class="comment">  * (Scala-specific)</span></span><br><span class="line"><span class="comment">  * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.</span></span><br><span class="line"><span class="comment">  * 返回一个[[KeyValueGroupedDataset]]，数据由给定键' func '分组。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>[<span class="type">K</span>: <span class="type">Encoder</span>](func: <span class="type">T</span> =&gt; <span class="type">K</span>): <span class="type">KeyValueGroupedDataset</span>[<span class="type">K</span>, <span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> inputPlan = logicalPlan</span><br><span class="line">  <span class="keyword">val</span> withGroupingKey = <span class="type">AppendColumns</span>(func, inputPlan)</span><br><span class="line">  <span class="keyword">val</span> executed = sparkSession.sessionState.executePlan(withGroupingKey)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> <span class="type">KeyValueGroupedDataset</span>(</span><br><span class="line">    encoderFor[<span class="type">K</span>],</span><br><span class="line">    encoderFor[<span class="type">T</span>],</span><br><span class="line">    executed,</span><br><span class="line">    inputPlan.output,</span><br><span class="line">    withGroupingKey.newColumns)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * :: Experimental ::</span></span><br><span class="line"><span class="comment">  * (Java-specific)</span></span><br><span class="line"><span class="comment">  * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.</span></span><br><span class="line"><span class="comment">  * 返回一个[[KeyValueGroupedDataset]]，数据由给定键' func '分组。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>[<span class="type">K</span>](func: <span class="type">MapFunction</span>[<span class="type">T</span>, <span class="type">K</span>], encoder: <span class="type">Encoder</span>[<span class="type">K</span>]): <span class="type">KeyValueGroupedDataset</span>[<span class="type">K</span>, <span class="type">T</span>] =</span><br><span class="line">  groupByKey(func.call(_))(encoder)</span><br></pre></td></tr></table></figure>
<h3 id="limit"><a href="#limit" class="headerlink" title="limit"></a>limit</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset by taking the first `n` rows. The difference between this function</span></span><br><span class="line"><span class="comment">  * and `head` is that `head` is an action and returns an array (by triggering query execution)</span></span><br><span class="line"><span class="comment">  * while `limit` returns a new Dataset.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 通过使用第一个“n”行返回一个新的数据集。</span></span><br><span class="line"><span class="comment">  * 这个函数和“head”的区别在于“head”是一个动作，</span></span><br><span class="line"><span class="comment">  * 并返回一个数组(通过触发查询执行)，而“limit”则返回一个新的数据集。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">limit</span></span>(n: <span class="type">Int</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</span><br><span class="line">  <span class="type">Limit</span>(<span class="type">Literal</span>(n), logicalPlan)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="unionAll-已过时"><a href="#unionAll-已过时" class="headerlink" title="unionAll-已过时"></a>unionAll-已过时</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset containing union of rows in this Dataset and another Dataset.</span></span><br><span class="line"><span class="comment">  * This is equivalent to `UNION ALL` in SQL.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回一个新的数据集，该数据集包含该数据集中的行和另一个数据集。</span></span><br><span class="line"><span class="comment">  * 这相当于SQL中的“UNION ALL”。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * To do a SQL-style set union (that does deduplication of elements), use this function followed</span></span><br><span class="line"><span class="comment">  * by a [[distinct]].</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 如果需要去重的话，在该方法后继续直接  [[distinct]]</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0 已经过时</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@deprecated</span>(<span class="string">"use union()"</span>, <span class="string">"2.0.0"</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unionAll</span></span>(other: <span class="type">Dataset</span>[<span class="type">T</span>]): <span class="type">Dataset</span>[<span class="type">T</span>] = union(other)</span><br></pre></td></tr></table></figure>
<h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset containing union of rows in this Dataset and another Dataset.</span></span><br><span class="line"><span class="comment">  * This is equivalent to `UNION ALL` in SQL.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回一个新的数据集，该数据集包含该数据集中的行和另一个数据集。</span></span><br><span class="line"><span class="comment">  * 这相当于SQL中的“UNION ALL”。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * To do a SQL-style set union (that does deduplication of elements), use this function followed</span></span><br><span class="line"><span class="comment">  * by a [[distinct]].</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 如果需要去重的话，在该方法后继续直接  [[distinct]]</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">union</span></span>(other: <span class="type">Dataset</span>[<span class="type">T</span>]): <span class="type">Dataset</span>[<span class="type">T</span>] = withSetOperator &#123;</span><br><span class="line">  <span class="comment">// This breaks caching, but it's usually ok because it addresses a very specific use case:</span></span><br><span class="line">  <span class="comment">// using union to union many files or partitions.</span></span><br><span class="line">  <span class="comment">// 这打破了缓存，但通常是可以的，因为它解决了一个非常具体的用例:使用union来联合许多文件或分区。</span></span><br><span class="line">  <span class="type">CombineUnions</span>(<span class="type">Union</span>(logicalPlan, other.logicalPlan))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="intersect-交集"><a href="#intersect-交集" class="headerlink" title="intersect-交集"></a>intersect-交集</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset containing rows only in both this Dataset and another Dataset.</span></span><br><span class="line"><span class="comment">  * This is equivalent to `INTERSECT` in SQL.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回一个新的数据集，只包含该数据集和另一个数据集相同的行.</span></span><br><span class="line"><span class="comment">  * 这相当于在SQL中“INTERSECT”。</span></span><br><span class="line"><span class="comment">  * 会去重.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @note Equality checking is performed directly on the encoded representation of the data</span></span><br><span class="line"><span class="comment">  *       and thus is not affected by a custom `equals` function defined on `T`.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  *       等式检查直接执行数据的编码表示，因此不受定义为“T”的自定义“equals”函数的影响。</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">intersect</span></span>(other: <span class="type">Dataset</span>[<span class="type">T</span>]): <span class="type">Dataset</span>[<span class="type">T</span>] = withSetOperator &#123;</span><br><span class="line">  <span class="type">Intersect</span>(logicalPlan, other.logicalPlan)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="except-只显示另个Dataset中没有的值"><a href="#except-只显示另个Dataset中没有的值" class="headerlink" title="except-只显示另个Dataset中没有的值"></a>except-只显示另个Dataset中没有的值</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset containing rows in this Dataset but not in another Dataset.</span></span><br><span class="line"><span class="comment">  * This is equivalent to `EXCEPT` in SQL.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回一个新的数据集，该数据集包含该数据集中的行，而不是在另一个数据集。</span></span><br><span class="line"><span class="comment">  * 这等价于SQL中的“EXCEPT”。</span></span><br><span class="line"><span class="comment">  * 会去重.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @note Equality checking is performed directly on the encoded representation of the data</span></span><br><span class="line"><span class="comment">  *       and thus is not affected by a custom `equals` function defined on `T`.</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">except</span></span>(other: <span class="type">Dataset</span>[<span class="type">T</span>]): <span class="type">Dataset</span>[<span class="type">T</span>] = withSetOperator &#123;</span><br><span class="line">  <span class="type">Except</span>(logicalPlan, other.logicalPlan)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="sample-随机抽样"><a href="#sample-随机抽样" class="headerlink" title="sample-随机抽样"></a>sample-随机抽样</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 通过使用用户提供的种子，通过抽样的方式返回一个新的[[Dataset]]。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param withReplacement Sample with replacement or not.</span></span><br><span class="line"><span class="comment">  *                        样本已经取过的值是否放回</span></span><br><span class="line"><span class="comment">  * @param fraction        Fraction of rows to generate.</span></span><br><span class="line"><span class="comment">  *                        每一行数据被取样的概率</span></span><br><span class="line"><span class="comment">  * @param seed            Seed for sampling.</span></span><br><span class="line"><span class="comment">  *                        取样种子（与随机数生成有关）</span></span><br><span class="line"><span class="comment">  * @note This is NOT guaranteed to provide exactly the fraction of the count</span></span><br><span class="line"><span class="comment">  *       of the given [[Dataset]].</span></span><br><span class="line"><span class="comment">  *       不能保证准确的按照给定的分数取样。（一般结果会在概率值*总数左右）</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(withReplacement: <span class="type">Boolean</span>, fraction: <span class="type">Double</span>, seed: <span class="type">Long</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  require(fraction &gt;= <span class="number">0</span>,</span><br><span class="line">    <span class="string">s"Fraction must be nonnegative, but got <span class="subst">$&#123;fraction&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">  withTypedPlan &#123;</span><br><span class="line">    <span class="type">Sample</span>(<span class="number">0.0</span>, fraction, withReplacement, seed, logicalPlan)()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 通过程序随机的种子，抽样返回新的DataSet</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param withReplacement Sample with replacement or not.</span></span><br><span class="line"><span class="comment">  *                        取样结果是否放回</span></span><br><span class="line"><span class="comment">  * @param fraction        Fraction of rows to generate.</span></span><br><span class="line"><span class="comment">  *                        每行数据被取样的概率</span></span><br><span class="line"><span class="comment">  * @note This is NOT guaranteed to provide exactly the fraction of the total count</span></span><br><span class="line"><span class="comment">  *       of the given [[Dataset]].</span></span><br><span class="line"><span class="comment">  *       不能保证准确的按照给定的分数取样。（一般结果会在概率值*总数左右）</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(withReplacement: <span class="type">Boolean</span>, fraction: <span class="type">Double</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  sample(withReplacement, fraction, <span class="type">Utils</span>.random.nextLong)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="randomSplit-按照权重分割"><a href="#randomSplit-按照权重分割" class="headerlink" title="randomSplit-按照权重分割"></a>randomSplit-按照权重分割</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Randomly splits this Dataset with the provided weights.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 随机将此数据集按照所提供的权重进行分割。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param weights weights for splits, will be normalized if they don't sum to 1.</span></span><br><span class="line"><span class="comment">    *                切分的权重。如果和不为1就会被标准化。</span></span><br><span class="line"><span class="comment">    * @param seed    Seed for sampling.</span></span><br><span class="line"><span class="comment">    *                取样的种子（影响随机数生成器）</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    *                For Java API, use [[randomSplitAsList]].</span></span><br><span class="line"><span class="comment">    *                Java API 使用 [[randomSplitAsList]].</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">randomSplit</span></span>(weights: <span class="type">Array</span>[<span class="type">Double</span>], seed: <span class="type">Long</span>): <span class="type">Array</span>[<span class="type">Dataset</span>[<span class="type">T</span>]] = &#123;</span><br><span class="line">    require(weights.forall(_ &gt;= <span class="number">0</span>),</span><br><span class="line">      <span class="string">s"Weights must be nonnegative, but got <span class="subst">$&#123;weights.mkString("[", ",", "]")&#125;</span>"</span>)</span><br><span class="line">    require(weights.sum &gt; <span class="number">0</span>,</span><br><span class="line">      <span class="string">s"Sum of weights must be positive, but got <span class="subst">$&#123;weights.mkString("[", ",", "]")&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its</span></span><br><span class="line">    <span class="comment">// constituent partitions each time a split is materialized which could result in</span></span><br><span class="line">    <span class="comment">// overlapping splits. To prevent this, we explicitly sort each input partition to make the</span></span><br><span class="line">    <span class="comment">// ordering deterministic.</span></span><br><span class="line">    <span class="comment">// MapType cannot be sorted.</span></span><br><span class="line">    <span class="keyword">val</span> sorted = <span class="type">Sort</span>(logicalPlan.output.filterNot(_.dataType.isInstanceOf[<span class="type">MapType</span>])</span><br><span class="line">      .map(<span class="type">SortOrder</span>(_, <span class="type">Ascending</span>)), global = <span class="literal">false</span>, logicalPlan)</span><br><span class="line">    <span class="keyword">val</span> sum = weights.sum</span><br><span class="line">    <span class="comment">// scanLeft 从右到右依次累计算 scanLeft(0.0d)(_+_): (0.0,(0.0+0.2),(0.0+0.2+0.8))</span></span><br><span class="line">    <span class="keyword">val</span> normalizedCumWeights = weights.map(_ / sum).scanLeft(<span class="number">0.0</span>d)(_ + _)</span><br><span class="line">    <span class="comment">// sliding(n) 每次取n个值，以步长为1向右滑动，如：(0.0,0.2,0.8).sliding(2)=(0.0,0.2),(0.2,0.8)</span></span><br><span class="line">    normalizedCumWeights.sliding(<span class="number">2</span>).map &#123; x =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Dataset</span>[<span class="type">T</span>](</span><br><span class="line">        sparkSession, <span class="type">Sample</span>(x(<span class="number">0</span>), x(<span class="number">1</span>), withReplacement = <span class="literal">false</span>, seed, sorted)(), encoder)</span><br><span class="line">    &#125;.toArray</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Randomly splits this Dataset with the provided weights.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 程序自动生成随机数种子，随机将此数据集按照所提供的权重进行分割。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param weights weights for splits, will be normalized if they don't sum to 1.</span></span><br><span class="line"><span class="comment">    *                切分的权重。如果和不为1就会被标准化。</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">randomSplit</span></span>(weights: <span class="type">Array</span>[<span class="type">Double</span>]): <span class="type">Array</span>[<span class="type">Dataset</span>[<span class="type">T</span>]] = &#123;</span><br><span class="line">    randomSplit(weights, <span class="type">Utils</span>.random.nextLong)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Randomly splits this Dataset with the provided weights. Provided for the Python Api.</span></span><br><span class="line"><span class="comment">    * Python 使用该方法</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param weights weights for splits, will be normalized if they don't sum to 1.</span></span><br><span class="line"><span class="comment">    * @param seed    Seed for sampling.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">randomSplit</span></span>(weights: <span class="type">List</span>[<span class="type">Double</span>], seed: <span class="type">Long</span>): <span class="type">Array</span>[<span class="type">Dataset</span>[<span class="type">T</span>]] = &#123;</span><br><span class="line">    randomSplit(weights.toArray, seed)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h3 id="randomSplitAsList"><a href="#randomSplitAsList" class="headerlink" title="randomSplitAsList"></a>randomSplitAsList</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a Java list that contains randomly split Dataset with the provided weights.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 根据提供的权重分割DataFrames，返回Java list</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param weights weights for splits, will be normalized if they don't sum to 1.</span></span><br><span class="line"><span class="comment">  *                切分的权重。如果和不为1就会被标准化。</span></span><br><span class="line"><span class="comment">  * @param seed    Seed for sampling.</span></span><br><span class="line"><span class="comment">  *                取样的种子（影响随机数生成器）</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomSplitAsList</span></span>(weights: <span class="type">Array</span>[<span class="type">Double</span>], seed: <span class="type">Long</span>): java.util.<span class="type">List</span>[<span class="type">Dataset</span>[<span class="type">T</span>]] = &#123;</span><br><span class="line">  <span class="keyword">val</span> values = randomSplit(weights, seed)</span><br><span class="line">  java.util.<span class="type">Arrays</span>.asList(values: _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="dropDuplicates-去重"><a href="#dropDuplicates-去重" class="headerlink" title="dropDuplicates-去重"></a>dropDuplicates-去重</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset that contains only the unique rows from this Dataset.</span></span><br><span class="line"><span class="comment">  * This is an alias for `distinct`.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 删除重复的row数据，是distinct的别名</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropDuplicates</span></span>(): <span class="type">Dataset</span>[<span class="type">T</span>] = dropDuplicates(<span class="keyword">this</span>.columns)</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only</span></span><br><span class="line"><span class="comment">  * the subset of columns.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 只删除指定列的重复数据</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropDuplicates</span></span>(colNames: <span class="type">Seq</span>[<span class="type">String</span>]): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</span><br><span class="line">  <span class="keyword">val</span> resolver = sparkSession.sessionState.analyzer.resolver</span><br><span class="line">  <span class="keyword">val</span> allColumns = queryExecution.analyzed.output</span><br><span class="line">  <span class="keyword">val</span> groupCols = colNames.flatMap &#123; colName =&gt;</span><br><span class="line">    <span class="comment">// It is possibly there are more than one columns with the same name,</span></span><br><span class="line">    <span class="comment">// so we call filter instead of find.</span></span><br><span class="line">    <span class="keyword">val</span> cols = allColumns.filter(col =&gt; resolver(col.name, colName))</span><br><span class="line">    <span class="keyword">if</span> (cols.isEmpty) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AnalysisException</span>(</span><br><span class="line">        <span class="string">s""</span><span class="string">"Cannot resolve column name "</span>$colN<span class="string">ame" among (<span class="subst">$&#123;schema.fieldNames.mkString(", ")&#125;</span>)"</span><span class="string">""</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    cols</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> groupColExprIds = groupCols.map(_.exprId)</span><br><span class="line">  <span class="keyword">val</span> aggCols = logicalPlan.output.map &#123; attr =&gt;</span><br><span class="line">    <span class="keyword">if</span> (groupColExprIds.contains(attr.exprId)) &#123;</span><br><span class="line">      attr</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Removing duplicate rows should not change output attributes. We should keep</span></span><br><span class="line">      <span class="comment">// the original exprId of the attribute. Otherwise, to select a column in original</span></span><br><span class="line">      <span class="comment">// dataset will cause analysis exception due to unresolved attribute.</span></span><br><span class="line">      <span class="comment">// 删除重复行不应该更改输出属性。</span></span><br><span class="line">      <span class="comment">// 我们应该保留这个属性的原始属性。</span></span><br><span class="line">      <span class="comment">// 否则，在原始数据集中选择一个列将导致分析异常，原因是未解析的属性。</span></span><br><span class="line">      <span class="type">Alias</span>(<span class="keyword">new</span> <span class="type">First</span>(attr).toAggregateExpression(), attr.name)(exprId = attr.exprId)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">Aggregate</span>(groupCols, aggCols, logicalPlan)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset with duplicate rows removed, considering only</span></span><br><span class="line"><span class="comment">  * the subset of columns.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 只针对特定列做去重</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropDuplicates</span></span>(colNames: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Dataset</span>[<span class="type">T</span>] = dropDuplicates(colNames.toSeq)</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Returns a new [[Dataset]] with duplicate rows removed, considering only</span></span><br><span class="line"><span class="comment">  * the subset of columns.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 只针对特定多列做去重</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 2.0.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@scala</span>.annotation.varargs</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropDuplicates</span></span>(col1: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> colNames: <span class="type">Seq</span>[<span class="type">String</span>] = col1 +: cols</span><br><span class="line">  dropDuplicates(colNames)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="transform-自定义转换"><a href="#transform-自定义转换" class="headerlink" title="transform-自定义转换"></a>transform-自定义转换</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Concise syntax for chaining custom transformations.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 用于链接自定义转换的简明语法。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">  *   def featurize(ds: Dataset[T]): Dataset[U] = ...</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  *   ds</span></span><br><span class="line"><span class="comment">  *     .transform(featurize)</span></span><br><span class="line"><span class="comment">  *     .transform(...)</span></span><br><span class="line"><span class="comment">  * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span></span>[<span class="type">U</span>](t: <span class="type">Dataset</span>[<span class="type">T</span>] =&gt; <span class="type">Dataset</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>] = t(<span class="keyword">this</span>)</span><br></pre></td></tr></table></figure>
<h3 id="filter-过滤"><a href="#filter-过滤" class="headerlink" title="filter-过滤"></a>filter-过滤</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * :: Experimental ::</span></span><br><span class="line"><span class="comment">    * (Scala-specific)</span></span><br><span class="line"><span class="comment">    * Returns a new Dataset that only contains elements where `func` returns `true`.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 该数据集只包含“func”返回“true”的元素。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 1.6.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="meta">@Experimental</span></span><br><span class="line">  <span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(func: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    withTypedPlan(<span class="type">TypedFilter</span>(func, logicalPlan))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * :: Experimental ::</span></span><br><span class="line"><span class="comment">    * (Java-specific)</span></span><br><span class="line"><span class="comment">    * Returns a new Dataset that only contains elements where `func` returns `true`.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 返回一个新数据集，该数据集只包含“func”返回“true”的元素。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 1.6.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="meta">@Experimental</span></span><br><span class="line">  <span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(func: <span class="type">FilterFunction</span>[<span class="type">T</span>]): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    withTypedPlan(<span class="type">TypedFilter</span>(func, logicalPlan))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * :: Experimental ::</span></span><br><span class="line"><span class="comment">  * (Scala-specific)</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset that contains the result of applying `func` to each element.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回一个新的数据集，该数据集包含对每个元素应用“func”的结果。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">Encoder</span>](func: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">Dataset</span>[<span class="type">U</span>] = withTypedPlan &#123;</span><br><span class="line">  <span class="type">MapElements</span>[<span class="type">T</span>, <span class="type">U</span>](func, logicalPlan)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * :: Experimental ::</span></span><br><span class="line"><span class="comment">  * (Java-specific)</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset that contains the result of applying `func` to each element.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回一个新的数据集，该数据集包含对每个元素应用“func”的结果。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>](func: <span class="type">MapFunction</span>[<span class="type">T</span>, <span class="type">U</span>], encoder: <span class="type">Encoder</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> uEnc = encoder</span><br><span class="line">  withTypedPlan(<span class="type">MapElements</span>[<span class="type">T</span>, <span class="type">U</span>](func, logicalPlan))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * :: Experimental ::</span></span><br><span class="line"><span class="comment">  * (Scala-specific)</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset that contains the result of applying `func` to each partition.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回一个新的数据集，该数据集包含对每个分区应用“func”的结果。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">Encoder</span>](func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Dataset</span>[<span class="type">U</span>](</span><br><span class="line">    sparkSession,</span><br><span class="line">    <span class="type">MapPartitions</span>[<span class="type">T</span>, <span class="type">U</span>](func, logicalPlan),</span><br><span class="line">    implicitly[<span class="type">Encoder</span>[<span class="type">U</span>]])</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * :: Experimental ::</span></span><br><span class="line"><span class="comment">  * (Java-specific)</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset that contains the result of applying `f` to each partition.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回一个新的数据集，该数据集包含对每个分区应用“f”的结果。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>](f: <span class="type">MapPartitionsFunction</span>[<span class="type">T</span>, <span class="type">U</span>], encoder: <span class="type">Encoder</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> func: (<span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>] = x =&gt; f.call(x.asJava).asScala</span><br><span class="line">  mapPartitions(func)(encoder)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="flatMap-将map结果flat扁平化"><a href="#flatMap-将map结果flat扁平化" class="headerlink" title="flatMap-将map结果flat扁平化"></a>flatMap-将map结果flat扁平化</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * :: Experimental ::</span></span><br><span class="line"><span class="comment">  * (Scala-specific)</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset by first applying a function to all elements of this Dataset,</span></span><br><span class="line"><span class="comment">  * and then flattening the results.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回一个新的数据集，首先对该数据集的所有元素应用一个函数，然后将结果扁平化。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">Encoder</span>](func: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>] =</span><br><span class="line">  mapPartitions(_.flatMap(func))</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * :: Experimental ::</span></span><br><span class="line"><span class="comment">  * (Java-specific)</span></span><br><span class="line"><span class="comment">  * Returns a new Dataset by first applying a function to all elements of this Dataset,</span></span><br><span class="line"><span class="comment">  * and then flattening the results.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 返回一个新的数据集，首先对该数据集的所有元素应用一个函数，然后将结果扁平化。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @group typedrel</span></span><br><span class="line"><span class="comment">  * @since 1.6.0</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>](f: <span class="type">FlatMapFunction</span>[<span class="type">T</span>, <span class="type">U</span>], encoder: <span class="type">Encoder</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> func: (<span class="type">T</span>) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>] = x =&gt; f.call(x).asScala</span><br><span class="line">  flatMap(func)(encoder)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="repartition-重分区"><a href="#repartition-重分区" class="headerlink" title="repartition-重分区"></a>repartition-重分区</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Returns a new Dataset that has exactly `numPartitions` partitions.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 返回一个 给定分区数量的新DataSet</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 1.6.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(numPartitions: <span class="type">Int</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</span><br><span class="line">    <span class="type">Repartition</span>(numPartitions, shuffle = <span class="literal">true</span>, logicalPlan)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Returns a new Dataset partitioned by the given partitioning expressions into</span></span><br><span class="line"><span class="comment">    * `numPartitions`. The resulting Dataset is hash partitioned.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 返回一个由给定的分区表达式划分为“num分区”的新数据集。</span></span><br><span class="line"><span class="comment">    * 生成的Dataset是哈希分区的。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 和 SQL (Hive QL) 中的 "DISTRIBUTE BY" 作用相同</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="meta">@scala</span>.annotation.varargs</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(numPartitions: <span class="type">Int</span>, partitionExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</span><br><span class="line">    <span class="type">RepartitionByExpression</span>(partitionExprs.map(_.expr), logicalPlan, <span class="type">Some</span>(numPartitions))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Returns a new Dataset partitioned by the given partitioning expressions, using</span></span><br><span class="line"><span class="comment">    * `spark.sql.shuffle.partitions` as number of partitions.</span></span><br><span class="line"><span class="comment">    * The resulting Dataset is hash partitioned.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 根据指定的分区表达式进行重分区。</span></span><br><span class="line"><span class="comment">    * 分区数量由`spark.sql.shuffle.partitions` 获得。</span></span><br><span class="line"><span class="comment">    * 结果Dataset 是哈希分区的。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 和 SQL (Hive QL) 中的 "DISTRIBUTE BY" 作用相同</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="meta">@scala</span>.annotation.varargs</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(partitionExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</span><br><span class="line">    <span class="type">RepartitionByExpression</span>(partitionExprs.map(_.expr), logicalPlan, numPartitions = <span class="type">None</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h3 id="coalesce-合并分区"><a href="#coalesce-合并分区" class="headerlink" title="coalesce-合并分区"></a>coalesce-合并分区</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Returns a new Dataset that has exactly `numPartitions` partitions.</span></span><br><span class="line"><span class="comment">    * Similar to coalesce defined on an `RDD`, this operation results in a narrow dependency, e.g.</span></span><br><span class="line"><span class="comment">    * if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of</span></span><br><span class="line"><span class="comment">    * the 100 new partitions will claim 10 of the current partitions.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 合并。</span></span><br><span class="line"><span class="comment">    * 返回确定分区数量的Dataset。</span></span><br><span class="line"><span class="comment">    * 和RDD中的合并方法类似，这个操作导致了一个窄依赖。</span></span><br><span class="line"><span class="comment">    * 例如：将1000个分区合并为100个分区，这个过程没有shuffle，而是100个新分区中的每个分区将声明当前的10个分区。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 1.6.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">coalesce</span></span>(numPartitions: <span class="type">Int</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</span><br><span class="line">    <span class="type">Repartition</span>(numPartitions, shuffle = <span class="literal">false</span>, logicalPlan)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h3 id="distinct-去重"><a href="#distinct-去重" class="headerlink" title="distinct-去重"></a>distinct-去重</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Returns a new Dataset that contains only the unique rows from this Dataset.</span></span><br><span class="line"><span class="comment">    * This is an alias for `dropDuplicates`.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 去重。</span></span><br><span class="line"><span class="comment">    * 返回去重后的Dataset。</span></span><br><span class="line"><span class="comment">    * 和 `dropDuplicates` 方法一致。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @note Equality checking is performed directly on the encoded representation of the data</span></span><br><span class="line"><span class="comment">    *       and thus is not affected by a custom `equals` function defined on `T`.</span></span><br><span class="line"><span class="comment">    * @group typedrel</span></span><br><span class="line"><span class="comment">    * @since 2.0.0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>(): <span class="type">Dataset</span>[<span class="type">T</span>] = dropDuplicates()</span><br></pre></td></tr></table></figure>
<!--对不起，到时间了，请停止装逼-->

      
    </div>
    
    
    
    
    
    
    <div>
      
        <div>
<div>



<div style="text-align:center;color: #ccc;font-size:14px;">

-------------THE<i class="fa fa-paw"></i>END-------------</div>



</div>
</div>
      
    </div>
    
    
    
    
    
    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/spark/" rel="tag"><i class="fa fa-tag"></i> spark</a>
          
            <a href="/tags/源码/" rel="tag"><i class="fa fa-tag"></i> 源码</a>
          
        </div>
      
    
      
      
      
    
      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/11/21/JavaRDDLike-scala/" rel="next" title="JavaRDDLike.scala">
                <i class="fa fa-chevron-left"></i> JavaRDDLike.scala
              </a>
            
          </div>
    
          <span class="post-nav-divider"></span>
    
          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/11/27/spark分层取样/" rel="prev" title="spark分层取样">
                spark分层取样 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      
    
      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="舒服一夏">
            
              <p class="site-author-name" itemprop="name">舒服一夏</p>
              <p class="site-description motion-element" itemprop="description">技术改变世界，阅读塑造人生</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">50</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">25</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/stanxia" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i></a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#basic-基础方法"><span class="nav-number">2.</span> <span class="nav-text">basic-基础方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#toDF"><span class="nav-number">2.1.</span> <span class="nav-text">toDF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#as"><span class="nav-number">2.2.</span> <span class="nav-text">as</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#schema"><span class="nav-number">2.3.</span> <span class="nav-text">schema</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#printSchema"><span class="nav-number">2.4.</span> <span class="nav-text">printSchema</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#explain"><span class="nav-number">2.5.</span> <span class="nav-text">explain</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dtypes"><span class="nav-number">2.6.</span> <span class="nav-text">dtypes</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#columns"><span class="nav-number">2.7.</span> <span class="nav-text">columns</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#isLocal"><span class="nav-number">2.8.</span> <span class="nav-text">isLocal</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#checkpoint"><span class="nav-number">2.9.</span> <span class="nav-text">checkpoint</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#persist"><span class="nav-number">2.10.</span> <span class="nav-text">persist</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cache"><span class="nav-number">2.11.</span> <span class="nav-text">cache</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#storageLevel"><span class="nav-number">2.12.</span> <span class="nav-text">storageLevel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#unpersist"><span class="nav-number">2.13.</span> <span class="nav-text">unpersist</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rdd"><span class="nav-number">2.14.</span> <span class="nav-text">rdd</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#toJavaRDD"><span class="nav-number">2.15.</span> <span class="nav-text">toJavaRDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#registerTempTable"><span class="nav-number">2.16.</span> <span class="nav-text">registerTempTable</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#createTempView"><span class="nav-number">2.17.</span> <span class="nav-text">createTempView</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#createOrReplaceTempView"><span class="nav-number">2.18.</span> <span class="nav-text">createOrReplaceTempView</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#createGlobalTempView"><span class="nav-number">2.19.</span> <span class="nav-text">createGlobalTempView</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#write"><span class="nav-number">2.20.</span> <span class="nav-text">write</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#writeStream"><span class="nav-number">2.21.</span> <span class="nav-text">writeStream</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#toJSON"><span class="nav-number">2.22.</span> <span class="nav-text">toJSON</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#inputFiles"><span class="nav-number">2.23.</span> <span class="nav-text">inputFiles</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#streaming"><span class="nav-number">3.</span> <span class="nav-text">streaming</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#isStreaming"><span class="nav-number">3.1.</span> <span class="nav-text">isStreaming</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#withWatermark"><span class="nav-number">3.2.</span> <span class="nav-text">withWatermark</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#action"><span class="nav-number">4.</span> <span class="nav-text">action</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#show"><span class="nav-number">4.1.</span> <span class="nav-text">show</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#reduce"><span class="nav-number">4.2.</span> <span class="nav-text">reduce</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#describe"><span class="nav-number">4.3.</span> <span class="nav-text">describe</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#head"><span class="nav-number">4.4.</span> <span class="nav-text">head</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#first"><span class="nav-number">4.5.</span> <span class="nav-text">first</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#foreach"><span class="nav-number">4.6.</span> <span class="nav-text">foreach</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#foreachPartition"><span class="nav-number">4.7.</span> <span class="nav-text">foreachPartition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#take"><span class="nav-number">4.8.</span> <span class="nav-text">take</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#takeAsList"><span class="nav-number">4.9.</span> <span class="nav-text">takeAsList</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#collect"><span class="nav-number">4.10.</span> <span class="nav-text">collect</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#collectAsList"><span class="nav-number">4.11.</span> <span class="nav-text">collectAsList</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#toLocalIterator"><span class="nav-number">4.12.</span> <span class="nav-text">toLocalIterator</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#count"><span class="nav-number">4.13.</span> <span class="nav-text">count</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#untypedrel-无类型转换"><span class="nav-number">5.</span> <span class="nav-text">untypedrel-无类型转换</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#na"><span class="nav-number">5.1.</span> <span class="nav-text">na</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#stat"><span class="nav-number">5.2.</span> <span class="nav-text">stat</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#join"><span class="nav-number">5.3.</span> <span class="nav-text">join</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#crossJoin"><span class="nav-number">5.4.</span> <span class="nav-text">crossJoin</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#apply"><span class="nav-number">5.5.</span> <span class="nav-text">apply</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#col"><span class="nav-number">5.6.</span> <span class="nav-text">col</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#select"><span class="nav-number">5.7.</span> <span class="nav-text">select</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#selectExpr"><span class="nav-number">5.8.</span> <span class="nav-text">selectExpr</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#groupBy"><span class="nav-number">5.9.</span> <span class="nav-text">groupBy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rollup"><span class="nav-number">5.10.</span> <span class="nav-text">rollup</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cube"><span class="nav-number">5.11.</span> <span class="nav-text">cube</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#agg"><span class="nav-number">5.12.</span> <span class="nav-text">agg</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#explode"><span class="nav-number">5.13.</span> <span class="nav-text">explode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#withColumn"><span class="nav-number">5.14.</span> <span class="nav-text">withColumn</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#withColumnRenamed"><span class="nav-number">5.15.</span> <span class="nav-text">withColumnRenamed</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#drop"><span class="nav-number">5.16.</span> <span class="nav-text">drop</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#typedrel-有类型的转换"><span class="nav-number">6.</span> <span class="nav-text">typedrel-有类型的转换</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#joinWith"><span class="nav-number">6.1.</span> <span class="nav-text">joinWith</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sortWithinPartitions"><span class="nav-number">6.2.</span> <span class="nav-text">sortWithinPartitions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sort"><span class="nav-number">6.3.</span> <span class="nav-text">sort</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#orderBy"><span class="nav-number">6.4.</span> <span class="nav-text">orderBy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#as-1"><span class="nav-number">6.5.</span> <span class="nav-text">as</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#alias"><span class="nav-number">6.6.</span> <span class="nav-text">alias</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#select-1"><span class="nav-number">6.7.</span> <span class="nav-text">select</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#filter"><span class="nav-number">6.8.</span> <span class="nav-text">filter</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#where"><span class="nav-number">6.9.</span> <span class="nav-text">where</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#groupByKey"><span class="nav-number">6.10.</span> <span class="nav-text">groupByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#limit"><span class="nav-number">6.11.</span> <span class="nav-text">limit</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#unionAll-已过时"><span class="nav-number">6.12.</span> <span class="nav-text">unionAll-已过时</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#union"><span class="nav-number">6.13.</span> <span class="nav-text">union</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#intersect-交集"><span class="nav-number">6.14.</span> <span class="nav-text">intersect-交集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#except-只显示另个Dataset中没有的值"><span class="nav-number">6.15.</span> <span class="nav-text">except-只显示另个Dataset中没有的值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sample-随机抽样"><span class="nav-number">6.16.</span> <span class="nav-text">sample-随机抽样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#randomSplit-按照权重分割"><span class="nav-number">6.17.</span> <span class="nav-text">randomSplit-按照权重分割</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#randomSplitAsList"><span class="nav-number">6.18.</span> <span class="nav-text">randomSplitAsList</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dropDuplicates-去重"><span class="nav-number">6.19.</span> <span class="nav-text">dropDuplicates-去重</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transform-自定义转换"><span class="nav-number">6.20.</span> <span class="nav-text">transform-自定义转换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#filter-过滤"><span class="nav-number">6.21.</span> <span class="nav-text">filter-过滤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#map"><span class="nav-number">6.22.</span> <span class="nav-text">map</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mapPartitions"><span class="nav-number">6.23.</span> <span class="nav-text">mapPartitions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#flatMap-将map结果flat扁平化"><span class="nav-number">6.24.</span> <span class="nav-text">flatMap-将map结果flat扁平化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#repartition-重分区"><span class="nav-number">6.25.</span> <span class="nav-text">repartition-重分区</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#coalesce-合并分区"><span class="nav-number">6.26.</span> <span class="nav-text">coalesce-合并分区</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#distinct-去重"><span class="nav-number">6.27.</span> <span class="nav-text">distinct-去重</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      



    </div>
  </aside>


        
      </div>
    </main>
    
    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-paw"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">舒服一夏</span>

  
</div>









        







        
      </div>
    </footer>
    
    
    
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  

  
  <script type="text/javascript" src="/js/src/js.cookie.js?v=5.1.3"></script>
  <script type="text/javascript" src="/js/src/scroll-cookie.js?v=5.1.3"></script>


  


<!-- 页面点击小红心 -->

<script type="text/javascript" src="/js/src/love.js"></script>

</body></html>