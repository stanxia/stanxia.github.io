{"meta":{"title":"小世界，大梦想","subtitle":"夏的onepiece","description":"随便说说","author":"夏","url":"https://stanxia.github.io"},"posts":[{"title":"Airbnb的大数据平台架构","slug":"Airbnb的大数据平台架构","date":"2017-04-26T14:59:32.000Z","updated":"2017-04-26T15:04:13.000Z","comments":true,"path":"2017/04/26/Airbnb的大数据平台架构/","link":"","permalink":"https://stanxia.github.io/2017/04/26/Airbnb的大数据平台架构/","excerpt":"","text":"转载自：http://www.infoq.com/cn/articles/Airbnb-Hadoop-Hive Airbnb成立于2008年8月，拥有世界一流的客户服务和日益增长的用户社区。随着Airbnb的业务日益复杂，其大数据平台数据量也迎来了爆炸式增长。 本文为Airbnb公司工程师James Mayfield分析的Airbnb大数据平台构架，提供了详尽的思想和实施。 Part 1：大数据架构背后的哲理Airbnb公司提倡数据信息化，凡事以数据说话。收集指标，通过实验验证假设、构建机器学习模型和挖掘商业机会使得Airbnb公司高速、灵活的成长。 经过多版本迭代之后，大数据架构栈基本稳定、可靠和可扩展的。本文分享了Airbnb公司大数据架构经验给社区。后续会给出一系列的文章来讲述分布式架构和使用的相应的组件。James Mayfield说，“我们每天使用着开源社区提供的优秀的项目，这些项目让大家更好的工作。我们在使用这些有用的项目得到好处之后也得反馈社区。” 下面基于在Airbnb公司大数据平台架构构建过程的经验，给出一些有效的观点。 多关注开源社区：在开源社区有很多大数据架构方面优秀的资源，需要去采用这些系统。同样，当我们自己开发了有用的项目也最好回馈给社区，这样会良性循环。 多采用标准组件和方法：有时候自己造轮子并不如使用已有的更好资源。当凭直觉去开发出一种“与众不同”的方法时，你得考虑维护和修复这些程序的隐性成本。 确保大数据平台的可扩展性：当前业务数据已不仅仅是随着业务线性增长了，而是爆发性增长。我们得确保产品能满足这种业务的增长。 多倾听同事的反馈来解决问题：倾听公司数据的使用者反馈意见是架构路线图中非常重要的一步。 预留多余资源：集群资源的超负荷使用让我们培养了一种探索无限可能的文化。对于架构团队来说，经常沉浸在早期资源充足的兴奋中，但Airbnb大数据团队总是假设数据仓库的新业务规模比现有机器资源大。 Part 2：大数据架构预览这里是大数据平台架构一览图。 Airbnb数据源主要来自两方面：数据埋点发送事件日志到Kafka；MySQL数据库dumps存储在AWS的RDS，通过数据传输组件Sqoop传输到Hive“金”集群（其实就是Hive集群，只是Airbnb内部有两个Hive集群，分别为“金”集群和“银”集群，具体分开两个集群的原因会在文章末尾给出。）。 包含用户行为以及纬度快照的数据发送到Hive“金”集群存储，并进行数据清洗。这步会做些业务逻辑计算，聚合数据表，并进行数据校验。 在以上架构图中，Hive集群单独区分“金”集群和“银”集群大面上的原因是为了把数据存储和计算进行分离。这样可以保证灾难性恢复。这个架构中，“金”集群运行着更重要的作业和服务，对资源占用和即席查询可以达到无感知。“银”集群只是作为一个产品环境。 “金”集群存储的是原始数据，然后复制“金”集群上的所有数据到“银”集群。但是在“银”集群上生成的数据不会再复制到“金”集群。你可以认为 “银”集群是所有数据的一个超集。由于Airbnb大部分数据分析和报表都出自“银”集群，所以得保证“银”集群能够无延迟的复制数据。更严格的讲，对于“金”集群上已存在的数据进行更新也得迅速的同步到“银”集群。集群间的数据同步优化在开源社区并没有很好的解决方案，Airbnb自己实现了一个工具，后续文章会详细的讲。 在HDFS存储和Hive表的管理方面做了不少优化。数据仓库的质量依赖于数据的不变性（Hive表的分区）。更进一步，Airbnb不提倡建立不同的数据系统，也不想单独为数据源和终端用户报表维护单独的架构。以以往的经验看，中间数据系统会造成数据的不一致性，增加ETL的负担，让回溯数据源到数据指标的演化链变得异常艰难。Airbnb采用Presto来查询Hive表，代替Oracle、 Teradata、 Vertica、 Redshift等。在未来，希望可以直接用Presto连接Tableau。 另外一个值得注意的几个事情，在架构图中的Airpal，一个基于Presto，web查询系统，已经开源。Airpal是Airbnb公司用户基于数据仓库的即席SQL查询借口，有超过1/3的Airbnb同事在使用此工具查询。任务调度系统Airflow ，可以跨平台运行Hive，Presto，Spark，MySQL等Job，并提供调度和监控功能。Spark集群时工程师和数据分析师偏爱的工具，可以提供机器学习和流处理。S3作为一个独立的存储，大数据团队从HDFS上收回部分数据，这样可以减少存储的成本。并更新Hive的表指向S3文件，容易访问数据和元数据管理。 Part 3：Hadoop集群演化Airbnb公司在今年迁移集群到“金和银”集群。为了后续的可扩展，两年前迁移Amazon EMR到 EC2实例上运行HDFS，存储有300 TB数据。现在，Airbnb公司有两个独立的HDFS集群，存储的数据量达11PB。S3上也存储了几PB数据。 下面是遇到的主要问题和解决方案：A) 基于Mesos运行Hadoop早期Airbnb工程师发现Mesos计算框架可以跨服务发布。在AWS c3.8xlarge机器上搭建集群，在EBS上存储3TB的数据。在Mesos上运行所有Hadoop、 Hive、Presto、 Chronos和Marathon。 基于Mesos的Hadoop集群遇到的问题： Job运行和产生的日志不可见 Hadoop集群健康状态不可见 Mesos只支持MR1 task tracker连接导致性能问题 系统的高负载，并很难定位 不兼容Hadoop安全认证Kerberos 解决方法：不自己造轮子，直接采用其它大公司的解决方案。 B) 远程读数据和写数据所有的HDFS数据都存储在持久性数据块级存储卷（EBS），当查询时都是通过网络访问Amazon EC2。Hadoop设计在节点本地读写速度会更快，而现在的部署跟这相悖。 Hadoop集群数据分成三部分存储在AWS一个分区三个节点上，每个节点都在不同的机架上。所以三个不同的副本就存储在不同的机架上，导致一直在远程的读数据和写入数据。这个问题导致在数据移动或者远程复制的过程出现丢失或者崩溃。 解决方法：使用本地存储的实例，并运行在单个节点上。 C) 在同构机器上混布任务纵观所有的任务，发现整体的架构中有两种完全不同的需求配置。Hive/Hadoop/HDFS是存储密集型，基本不耗内存和CPU。而Presto和Spark是耗内存和CPU型，并不怎么需要存储。在AWS c3.8xlarge机器上持久性数据块级存储卷（EBS）里存储3 TB是非常昂贵的。 解决方法：迁移到Mesos计算框架后，可以选择不同类型的机器运行不同的集群。比如，选择AWS c3.8xlarge实例运行Spark。AWS后来发布了“D系列”实例。从AWS c3.8xlarge实例每节点远程的3 TB存储迁移数据到AWS d2.8xlarge 4 TB本地存储，这给Airbnb公司未来三年节约了上亿美元。 D) HDFS Federation早期Airbnb公司使用Pinky和Brain两个集群联合，数据存储共享，但mappers和reducers是在每个集群上逻辑独立的。这导致用户访问数据需要在Pinky和Brain两个集群都查询一遍。并且这种集群联合不能广泛被支持，运行也不稳定。 解决方法：迁移数据到各HDFS节点，达到机器水平的隔离性，这样更容易容灾。 E) 繁重的系统监控个性化系统架构的严重问题之一是需要自己开发独立的监控和报警系统。Hadoop、Hive和HDFS都是复杂的系统，经常出现各种bug。试图跟踪所有失败的状态，并能设置合适的阈值是一项非常具有挑战性的工作。 解决方法：通过和大数据公司Cloudera签订协议获得专家在架构和运维这些大系统的支持。减少公司维护的负担。Cloudera提供的Manager工具减少了监控和报警的工作。 最后陈述在评估老系统的问题和低效率后进行了系统的修复。无感知的迁移PB级数据和成百上千的Jobs是一个长期的过程。作者提出后面会单独写相关的文章，并开源对于的工具给开源社区。 大数据平台的演化给公司减少大量成本，并且优化集群的性能，下面是一些统计数据： 磁盘读写数据的速度从70 – 150 MB / sec到400 + MB / sec。 Hive任务提高了两倍的CPU时间 读吞吐量提高了三倍 写吞吐量提高了两倍 成本减少百分之七十 **查看英文原文：Data Infrastructure at Airbnb","raw":null,"content":null,"categories":[{"name":"大数据案例","slug":"大数据案例","permalink":"https://stanxia.github.io/categories/大数据案例/"}],"tags":[{"name":"大数据案例","slug":"大数据案例","permalink":"https://stanxia.github.io/tags/大数据案例/"}]},{"title":"hive变量传递设置","slug":"hive变量传递设置","date":"2017-04-26T13:51:57.000Z","updated":"2017-04-26T14:32:06.000Z","comments":true,"path":"2017/04/26/hive变量传递设置/","link":"","permalink":"https://stanxia.github.io/2017/04/26/hive变量传递设置/","excerpt":"","text":"在oozie的workflow中执行一个hive查询，但是直接就报异常：Variable substitution depth too large:40，从网上查询可知，可以确认是由于语句中使用了过多的变量导致，在hive以前的版本中，这个限制是写死的40个，查询Hive的最新的原代码，虽然判断的位置的提示信息已经变化，但是原理一样： ### org.apache.hadoop.hive.ql.parse.VariableSubstitution： 123456789101112 public String substitute(HiveConf conf, String expr) &#123;if (expr == null) &#123; return expr; &#125; if (HiveConf.getBoolVar(conf, ConfVars.HIVEVARIABLESUBSTITUTE)) &#123; l4j.debug(\"Substitution is on: \" + expr); &#125; else &#123; return expr; &#125; int depth = HiveConf.getIntVar(conf, ConfVars.HIVEVARIABLESUBSTITUTEDEPTH); return substitute(conf, expr, depth); &#125;123456789101112 如果开启hive.variable.substitute（默认开启），则使用SystemVariables的substitute方法和hive.variable.substitute.depth(默认为40)进行进一步的判断： 12345678910111213141516171819202122 protected final String substitute(Configuration conf, String expr, int depth) &#123; Matcher match = varPat.matcher(\"\"); String eval = expr; StringBuilder builder = new StringBuilder(); int s = 0; for (; s &lt;= depth; s++) &#123; match.reset(eval); builder.setLength(0); int prev = 0; boolean found = false; while (match.find(prev)) &#123; String group = match.group(); String var = group.substring(2, group.length() - 1); // remove $&#123; .. &#125; String substitute = getSubstitute(conf, var); if (substitute == null) &#123; substitute = group; // append as-is &#125; else &#123; found = true; &#125; builder.append(eval.substring(prev, match.start())).append(substitute); prev = match.end(); &#125; if (!found) &#123; return eval; &#125; builder.append(eval.substring(prev)); eval = builder.toString(); &#125; if (s &gt; depth) &#123; throw new IllegalStateException( \"Variable substitution depth is deeper than \" + depth + \" for expression \" + expr); &#125; return eval; &#125; 12345678910111213141516171819202122232425262728293031323334 如果使用的${}参数超过hive.variable.substitute.depth的数量，则直接抛出异常，所以我们在语句的前面直接加上set hive.variable.substitute.depth=100; 问题解决! set命令的执行是在CommandProcessor实现类SetProcessor里具体执行，但是substitute语句同时也会在CompileProcessor中调用，也就是在hive语句编译时就调用了，所以oozie在使用时调用beeline执行语句时，compile阶段就报出异常。 但是为什么Hue直接执行这个语句时没有问题? 因为hue在执行hive时使用的是python开发的beeswax，而beeswax是自己直接处理了这些变量，使用变量实际的值替换变量后再提交给hive执行： 123456789def substitute_variables(input_data, substitutions): \"\"\" Replaces variables with values from substitutions. \"\"\" def f(value): if not isinstance(value, basestring): return value new_value = Template(value).safe_substitute(substitutions) if new_value != value: LOG.debug(\"Substituted %s -&gt; %s\" % (repr(value), repr(new_value))) return new_value return recursive_walk(f, input_data)","raw":null,"content":null,"categories":[{"name":"hive","slug":"hive","permalink":"https://stanxia.github.io/categories/hive/"}],"tags":[{"name":"hive","slug":"hive","permalink":"https://stanxia.github.io/tags/hive/"}]},{"title":"Music Queen","slug":"Music Queen","date":"2017-04-26T13:00:34.000Z","updated":"2017-04-26T14:27:27.000Z","comments":true,"path":"2017/04/26/Music Queen/","link":"","permalink":"https://stanxia.github.io/2017/04/26/Music Queen/","excerpt":"","text":"不脆弱 不沉默 不协妥 不退缩 不慌张 不绝望 不狂妄 不投降 一路逆风 我们不投降 一路逆风 顽强的飞翔 一路逆风 var dplayer0 = new DPlayer({\"element\":document.getElementById(\"dplayer0\"),\"autoplay\":false,\"theme\":\"#FADFA3\",\"loop\":true,\"video\":{\"url\":\"http://oliji9s3j.bkt.clouddn.com/G.E.M.%E3%80%90%E4%B8%80%E8%B7%AF%E9%80%86%E9%A2%A8%20AGAINST%20THE%20WIND%E3%80%91Official%20MV%20%5BHD%5D%20%E9%84%A7%E7%B4%AB%E6%A3%8B.mp4\",\"pic\":\"http://oliji9s3j.bkt.clouddn.com/%E4%B8%80%E8%B7%AF%E9%80%86%E9%A3%8E.png\"}}); 来自天堂的魔鬼 var dplayer1 = new DPlayer({\"element\":document.getElementById(\"dplayer1\"),\"autoplay\":false,\"theme\":\"#FADFA3\",\"loop\":true,\"video\":{\"url\":\"http://oliji9s3j.bkt.clouddn.com/G.E.M.%E3%80%90%E4%BE%86%E8%87%AA%E5%A4%A9%E5%A0%82%E7%9A%84%E9%AD%94%E9%AC%BC%20AWAY%E3%80%91Official%20MV%20%5BHD%5D%20%E9%84%A7%E7%B4%AB%E6%A3%8B.mp4\",\"pic\":\"http://oliji9s3j.bkt.clouddn.com/%E6%9D%A5%E8%87%AA%E5%A4%A9%E5%A0%82%E7%9A%84%E9%AD%94%E9%AC%BC.png\"}}); 再见MTV var dplayer2 = new DPlayer({\"element\":document.getElementById(\"dplayer2\"),\"autoplay\":false,\"theme\":\"#FADFA3\",\"loop\":true,\"video\":{\"url\":\"http://oliji9s3j.bkt.clouddn.com/G.E.M.%E3%80%90%E5%86%8D%E8%A6%8B%20GOODBYE%E3%80%91Official%20MV%20%5BHD%5D%20%E9%84%A7%E7%B4%AB%E6%A3%8B.mp4\",\"pic\":\"http://oliji9s3j.bkt.clouddn.com/%E5%86%8D%E8%A7%81mtv.png\"}}); 再见LIVE var dplayer3 = new DPlayer({\"element\":document.getElementById(\"dplayer3\"),\"autoplay\":false,\"theme\":\"#FADFA3\",\"loop\":true,\"video\":{\"url\":\"http://oliji9s3j.bkt.clouddn.com/G.E.M.%E3%80%90%E5%86%8D%E8%A6%8B%20GOODBYE%E3%80%91LIVE%20PIANO%20SESSION%20II%20%28Part%203_3%29%20%5BHD%5D%20%E9%84%A7%E7%B4%AB%E6%A3%8B.mp4\",\"pic\":\"http://oliji9s3j.bkt.clouddn.com/%E5%86%8D%E8%A7%81.png\"}}); 多远都要在一起 var dplayer4 = new DPlayer({\"element\":document.getElementById(\"dplayer4\"),\"autoplay\":false,\"theme\":\"#FADFA3\",\"loop\":true,\"video\":{\"url\":\"http://oliji9s3j.bkt.clouddn.com/G.E.M.%E3%80%90%E5%A4%9A%E9%81%A0%E9%83%BD%E8%A6%81%E5%9C%A8%E4%B8%80%E8%B5%B7%20LONG%20DISTANCE%E3%80%91Official%20MV%20%5BHD%5D%20%E9%84%A7%E7%B4%AB%E6%A3%8B.mp4\",\"pic\":\"http://oliji9s3j.bkt.clouddn.com/%E5%A4%9A%E8%BF%9C%E9%83%BD%E8%A6%81%E5%9C%A8%E4%B8%80%E8%B5%B7.png\"}}); 光年之外 var dplayer5 = new DPlayer({\"element\":document.getElementById(\"dplayer5\"),\"autoplay\":false,\"theme\":\"#FADFA3\",\"loop\":true,\"video\":{\"url\":\"http://oliji9s3j.bkt.clouddn.com/G.E.M.%E3%80%90%E5%85%89%E5%B9%B4%E4%B9%8B%E5%A4%96%E3%80%91MV%20%28%E9%9B%BB%E5%BD%B1%E3%80%8A%E5%A4%AA%E7%A9%BA%E6%BD%9B%E8%A1%8C%E8%80%85%20Passengers%E3%80%8B%E4%B8%AD%E6%96%87%E4%B8%BB%E9%A1%8C%E6%9B%B2%29%20%5BHD%5D%20%E9%84%A7%E7%B4%AB%E6%A3%8B.mp4\",\"pic\":\"http://oliji9s3j.bkt.clouddn.com/%E5%85%89%E5%B9%B4%E4%B9%8B%E5%A4%96.png\"}}); 新的心跳 var dplayer6 = new DPlayer({\"element\":document.getElementById(\"dplayer6\"),\"autoplay\":false,\"theme\":\"#FADFA3\",\"loop\":true,\"video\":{\"url\":\"http://oliji9s3j.bkt.clouddn.com/G.E.M.%E3%80%90%E6%96%B0%E7%9A%84%E5%BF%83%E8%B7%B3%20HEARTBEAT%E3%80%91Official%20MV%20%5BHD%5D%20%E9%84%A7%E7%B4%AB%E6%A3%8B.mp4\",\"pic\":\"http://oliji9s3j.bkt.clouddn.com/%E6%96%B0%E7%9A%84%E5%BF%83%E8%B7%B3.png\"}});","raw":null,"content":null,"categories":[{"name":"GEM","slug":"GEM","permalink":"https://stanxia.github.io/categories/GEM/"}],"tags":[{"name":"GEM","slug":"GEM","permalink":"https://stanxia.github.io/tags/GEM/"}]},{"title":"企业大数据平台下数仓建设思路","slug":"企业大数据平台下数仓建设思路","date":"2017-04-25T15:06:53.000Z","updated":"2017-04-25T15:18:04.000Z","comments":true,"path":"2017/04/25/企业大数据平台下数仓建设思路/","link":"","permalink":"https://stanxia.github.io/2017/04/25/企业大数据平台下数仓建设思路/","excerpt":"","text":"大牛解读 转载自：https://yq.aliyun.com/articles/67020 数仓上遇到的挑战：数据质量保障、稳定和重复性在数据魔方、淘宝指数和阿里大数据数仓解决方案设计中，介然遇到了不少有挑战性的技术问题，主要集中在以下三点： 1.数据质量保障：随着业务的复杂度增加，数据源头的类型和数据量也会越来越多，经常会碰到某些数据源因为一些偶发的原因同步过来的数据质量出现问题。比如日志出现乱码、数据库因为切库造成数据同步量变少等等。这就要求在整个数仓体系的搭建过程中不只要完成数据业务逻辑的处理，还需要增加数据质量的监控。“我们在核心的数据处理流程中，增加数据质量监控代码，如果碰到数据量的突变或者核心指标的突变，会将数据处理流程暂停并预警，让数据运维人员处理数据质量问题后再进行后续数据流程的运行，保障有质量问题的数据不流到下游应用中。” 2.数据产出稳定性保障：随着数据量的增加、计算资源的逐渐饱和，业务数据最终产出的时间开始延迟，并有可能不能按照业务要求的时间点产出。“这个时候我们会分析数据产出的关键路径，找出关键路径下消耗时间最多的运行JOB，通过数据模型优化、计算任务拆解或者计算任务代码优化的手段减少任务产出的时间，同时保障整体产出时间满足预期。” 3.重复的数据处理代码：由于业务的特殊性，会对某种类型的数据加工操作需求非常多。比如计算交易中,TOP N的商家、TOP N 的品牌、TOP N的商品，商家中TOP N的商品、品牌中TOP N的商家等等。 这类代码都是非常类似的，如果每个计算都独立任务，会造成计算资源的大量浪费。“我们通过特殊的代码框架，让一份基础数据中多种TOPN的数据可以在一次计算过程中产出，大大减少资源消耗，保障数据产出稳定。” 优秀数仓的三要素：清晰、保障和扩展性好介然认为，优秀的数据仓库应该包含以下要素： 1.结构、分层清晰：不一定需要多少个分层和主题，但是一定要清晰。用数据的人能够很快找到需要数据的位置。 2.数据质量和产出时间有保障； 3.扩展性好：不会因为业务的些许变化造成模型的大面积重构。 而从系统架构、数据架构两个纬度来看，要想设计好大数据应用下的数据仓库，还应做到以下两点。 1.系统架构上：足够的容错性，减少不必要的系统间的强耦合。因为你会碰到各种问题，不要因为一个不必要的依赖造成数据无法产出。 2.数据架构上：简单、清晰、强质量控制。数据架构上扁平化的数据处理流程会对数据质量的控制和数据产出的稳定性提供非常好的基础。 互联网人转型做大数据数仓需要注意哪几个点？对于之前做互联网数据仓库，现在想转型做大数据仓库的人，介然也提了一些建议，主要是四点： 1.不必再苛刻的精打细算：基于传统平台构建数仓时，为了照顾平台的处理能力，我们经常会构建多层数据结构，预先对不同粒度的数据做预先汇总，以方便使用者在使用数据时能够已最小的计算代价获得计算结果。这也造成了整个数据处理流程较长，步骤很多，问题追溯困难。 新的大数据仓库基于分布式计算平台，平台的计算能力通常都比传统的平台强大很多。 所以有时候需要时再计算数据，或者基于明细进行各粒度的数据汇总已经能够满足需求，并能够大大减少整体数据处理流程步骤，用计算的代价减少人工的成本，更划算，数据体系也更健壮。 2.不是模型层次越多越好：在传统的数仓架构中，大家都喜欢多数据模型进行分层设计，不同的模型层次拥有不同的数据域和作用域。这样设计固然看起来更清晰，但实际情况时多层之间可能存在重复数据，或者数据使用者在上层找不到完全切合的数据时，更愿意从底层的明细数据上自己去加工。一方面造成了数据使用上的混乱，一方面也会让数据整个处理流程长度增加，对于数据的运维带来较大的成本消耗。合理的层次设计，及在计算成本和人力成本间的平衡，是一个好的数仓架构的表现。 3.质量是生命线：不再是你拿到的数据都是正确的，新的环境下的数据什么情况都会发生，而好的数仓架构需要有足够的容错性和质量保障。不要因为一条日志的乱码造成整个数据流程无法走通，也不要说一份日志50%的乱码你的程序还发现不了。在数据质量上投入再多的资源都不是浪费。 4.数据变成生产资料：传统的数据应用绝大部分都是以报表和BI分析的形式支持业务。也许你的报表晚出来会被老板骂一通，但是对业务的影响并不大。 但是在新的数据应用场景下，数据已经变成生产资料，数据会服务化直接应用到业务系统中，也许一份数据的质量出现问题或者产出延迟，都可能对你的业务系统产生致命的影响。所以数仓开始承担新的使命。","raw":null,"content":null,"categories":[{"name":"大数据案例","slug":"大数据案例","permalink":"https://stanxia.github.io/categories/大数据案例/"}],"tags":[{"name":"大数据案例","slug":"大数据案例","permalink":"https://stanxia.github.io/tags/大数据案例/"}]},{"title":"使用 Hexo 插件插入音乐/视频","slug":"dplayer-test","date":"2017-04-25T12:55:12.000Z","updated":"2017-04-25T13:55:53.000Z","comments":true,"path":"2017/04/25/dplayer-test/","link":"","permalink":"https://stanxia.github.io/2017/04/25/dplayer-test/","excerpt":"","text":"使用 Hexo 插件插入音乐/视频用于播放视频和音乐的的hexo插件： hexo-tag-aplayer：https://github.com/grzhan/hexo-tag-aplayer hexo-tag-dplayer： https://github.com/NextMoe/hexo-tag-dplayer 播放音乐的aplayer在cmd页面内，使用npm安装：npm install hexo-tag-aplayer 在markdown内添加以下代码： 1&#123;% aplayer \"她的睫毛\" \"周杰伦\" \"http://home.ustc.edu.cn/~mmmwhy/%d6%dc%bd%dc%c2%d7%20-%20%cb%fd%b5%c4%bd%de%c3%ab.mp3\" \"http://home.ustc.edu.cn/~mmmwhy/jay.jpg\" \"autoplay=false\" %&#125; 播放视频的dplayer在cmd页面内，使用npm安装：npm install hexo-tag-dplayer 在markdown内添加以下代码： 1&#123;% dplayer &quot;url=http://home.ustc.edu.cn/~mmmwhy/GEM.mp4&quot; &quot;pic=http://home.ustc.edu.cn/~mmmwhy/GEM.jpg&quot; &quot;loop=yes&quot; &quot;theme=#FADFA3&quot; &quot;autoplay=false&quot; &quot;token=tokendemo&quot; %&#125; var dplayer7 = new DPlayer({\"element\":document.getElementById(\"dplayer7\"),\"autoplay\":false,\"theme\":\"#FADFA3\",\"loop\":true,\"video\":{\"url\":\"http://home.ustc.edu.cn/~mmmwhy/GEM.mp4\",\"pic\":\"http://home.ustc.edu.cn/~mmmwhy/GEM.jpg\"}});","raw":null,"content":null,"categories":[{"name":"hexo","slug":"hexo","permalink":"https://stanxia.github.io/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://stanxia.github.io/tags/hexo/"}]},{"title":"Apache Kylin在唯品会大数据的应用","slug":"test-youtube","date":"2017-04-24T12:54:56.000Z","updated":"2017-04-24T14:58:56.000Z","comments":true,"path":"2017/04/24/test-youtube/","link":"","permalink":"https://stanxia.github.io/2017/04/24/test-youtube/","excerpt":"","text":"转载自：http://www.infoq.com/cn/articles/application-of-apache-kylin-in-vip-big-data 背景介绍引子：随着传统基于RDBMS的EDW往大数据的演进的过程中，Batch可处理的数据量越来越大，时间越来越快，但是Ad-hoc的响应速度却始终是大数据的瓶颈。 在2015年 唯品会的数据分析碰到了以下两个瓶颈：第一是数据准备的流程长，第二是缺少合适数据提取和分析工具。 首先，从数据准备流程来看，常见的流程是业务人员提出需求，BI同事定角度、找数据， 如果数据不完善，还得继续找数据开发。这就导致同一个需求，需要和不同的人反复沟通，在沟通过程中参与的人越多，信息衰减也就越厉害。再加上排期的等待，最终的结果一方面可能与初衷有所偏差，另一方面时间一长也失去了对热点关注度，分析变得非常滞后，不能及时的反应线上业务并加以改进。 其次，对于有分析能力的业务侧同学，没有趁手的工具就导致即使有能力准备撩袖子大干一场了也发现巧妇难为无米之炊，大家只能感慨大数据的门槛太高了，又回到了第一点的长时间等待的恶性循环里去了。 我们总结下来，在唯品会这样规模的公司里，数据分析有两个痛点： 需要一个可以自由组合的维度和指标的平台，业务人员可以根据自己的视角自给自足的完成数据提取和分析； 这个平台，不仅数据要够丰富，即使大数据量响应速度也要快。 针对这两个痛点，本着“让大数据成为唯品会的增长引擎”这个目标，我们大数据部门的提供了一套完整的解决方案：自助多维分析平台。我们通过有较高可扩展性的维度建模准备数据，在此之上搭建一套数据查询引擎，并配上操作简单的数据可视化前端，为业务人员搭了数据分析的台子。随着大家数据分析技能的提升，人人都是数据分析师的这个理念就逐渐在公司内部扩展开来了。 唯品会如何使用Kylin数据和前端是皮和肉，需要通过好的数据引擎才能支撑起来。在数据引擎角度，我们通过一段时间的积累和演进，从基于Presto的ROLAP模型进化到了基于Kylin和Presto的双计算引擎。往超大数据集也要快速ad-hoc响应的方向走近了一步。 第一阶段，我们的目标是在Ad-hoc响应时间&lt;= 10秒的前提条件下，支持： 平均每次查询10亿+明细数据做汇总； 平均每个查询0-15个维度； 平均每个查询1-5个指标。 根据这个目标，我们选择使用Presto作为计算引擎，Presto MPP的架构 + 通过Hive Connector直接访问HDFS上的数据，为我们提供良好的Ad-hoc响应速度和相对较低的维护成本。为了满足高Ad-hoc响应速度的需求，常见的做法是把HDFS上处理完的数据同步到Ad-hoc响应友好的数据库中，比如GreenPlum或Hbase等，但这样的缺点是虽然速度上去了，但数据模型在Hive和Ad-hoc库中需要维护两份并保持一致，维护的成本非常高。Presto的Connector机制很好的解决了这个问题，同时他的计算能力也满足了我们第一阶段的需求。 然后我们通过SQL Parser，将前端拖拽或事件描述的对象转化为SQL，同时完成SQL的变形和性能优化，把计算引擎和用户操作连接在一起，完成了第一阶段的目标。 自助多维分析平台一阶段逻辑架构 随着业务的不断增长，在自助多维分析平台上逐渐出现了很多维度和指标组合类似、频率较高的查询，这些查询有着明显的模式，且通过分析我们了解到这些维度和指标的组合是业务部门常用的核心数据。这些查询反复的在Presto上执行，显然不是最佳选择，也达不到业务部门提出的新目标，核心数据查询响应时间&lt;=3秒。此时，Kylin就成了我们的首选。我们的数据引擎的架构，也从单纯的操纵SQL扩展到计算引擎的路由。通过读取Metadata并根据规则，在Kylin和Presto两个计算引擎之间路由，我们可以在不显著提高数据模型维护成本的前提条件下，通过Kylin对关键数据做预计算，提高核心数据的响应速度。 为什么选择Kylin首先，Kylin利用空间换时间，从原理上已经确保了Ad-hoc响应速度达标，和Oracle CUBE/物化视图的原理相同易于理解。 第二，Kylin支持SQL，这对于数据分析而言至关重要，同时满足我们一个SQL在不同计算引擎之间路由的需求。 另外，Kylin的SQL on Hbase的实现也很好的解决了Hbase不易查询的问题。第三是支持Dimension-Fact的join，这极大的解耦了数据模型和计算引擎之间的关系，不像ES或Pinot只支持单表，还有为他们专门处理数据的额外工作。第四是对数据开发来说，创建和管理CUBE比较简单，且透明化了MR和HBASE同步。第五是可以很方便的在调度系统中调用Kylin API定时刷新CUBE。综上所述，Kylin对于一个数据分析系统来说是一个好的解决方案。 经过一段时间的测试和线上运行，我们在之前把Kylin覆盖到核心指标的查询基础上还扩展到了在Presto上查询需要30秒以上的指标和维度组合上。因为这类查询往往需要扫描大量的基础数据，在Kylin上预计算可以有效的较低资源使用。另一方面，基于自助多维分析平台的业务场景，我们也在以下两个场景中不启用Kylin。第一是维度的基数大于1亿的场景，主要是由于大基数的维度加载的Kylin Server的内存中容易引起OOM。第二是数据模型经常变化的主题，在Kylin中维护CUBE的成本就很高了，每次变化都需要重建CUBE，重刷数据，这显然与我们提高复用降低重复开发的初衷不符。对于这两个场景，由Presto完成计算也可以很好的满足需求。 基于以上的原则，目前我们累计有20+个CUBE，10+T存储，最大CUBE记录数上千亿，覆盖了23%的查询。同时，Ad-hoc的响应速度也令人满意。Kylin的平均响应速度是Presto的10.5倍，中位数响应速度是Presto的4.5倍。 唯品会对Kylin做的改进针对唯品会的痛点，我们也在开源框架的基础上进行了修改。基础升级方面，我们针对自助多维分析平台的需求进行了升级。比如，在查找CUBE的时候，仅当CUBE内数据包含SQL查询的时间范围才命中CUBE，避免给用户不完整的数据集。同时我们采集了Kylin运行中metadata，并给予这些数据提供SQL分析API以解析Kylin能运行的SQL子查询。另外一些BUG修复也提交到了社区。 除此之外，我们基于Presto+Kylin双引擎的架构，开发了Presto on Kylin这个功能。通过在Presto侧增加Kylin Connector，我们支持了Kylin与Hive数据源的跨源Join，支持Raw data汇总后的数据和Kylin Cube 数据Join。为了支持以上两个功能，我们在Kylin增加了Explain功能简化了Cube命中探查的复杂度。同时，为了进一步降低数据开发寻找查询组合的复杂度，我们开发了Cube Advisor，通过统计分析Presto SQL获得所有维度和指标的组合频次，根据最常使用和响应时间长两个条件，推荐合适的Cube定义建议，数据开发可以直接根据推荐的建议创建Cube。 下一步，我们会改造Kylin维表的Cache机制，解决大基数维表不能创建CUBE的问题，同时进一步扩展CUBE Advisor支持一键生成CUBE的功能并能够支持自动刷新历史数据，降低人工维护成本。同时，将Kylin的应用推广到报表类数据产品。 在提高大数据分析Ad-hoc响应速度的路上，可谓八仙过海各显神通，我们通过Presto和Kylin的结合满足了当前的需求，后面我们也会继续探索更多解决方案，寻找下一代的多维分析引擎，在此过程中，欢迎大家与我们一起讨论。 讲师介绍谢麟炯，唯品会大数据平台高级技术架构经理，主要负责大数据自助多维分析平台，离线数据开发平台及分析引擎团队的开发和管理工作 ，加入唯品会以来还曾负责流量基础数据的 采集和数据仓库建设以及移动流量分析等数据产品的工作。","raw":null,"content":null,"categories":[{"name":"大数据案例","slug":"大数据案例","permalink":"https://stanxia.github.io/categories/大数据案例/"}],"tags":[{"name":"大数据案例","slug":"大数据案例","permalink":"https://stanxia.github.io/tags/大数据案例/"}]},{"title":"EasyHadoop集群部署入门","slug":"EasyHadoop集群部署入门","date":"2017-03-21T02:13:32.000Z","updated":"2017-03-21T03:32:35.000Z","comments":true,"path":"2017/03/21/EasyHadoop集群部署入门/","link":"","permalink":"https://stanxia.github.io/2017/03/21/EasyHadoop集群部署入门/","excerpt":"","text":"EasyHadoop 让你的Hadoop应用飞起来!EasyHadoop集群部署入门目录 EasyHadoop集群部署入门文档……………………………………………………………………………………………………………………………………………………………………….. 2 目录……………………………………………………………………………………………………………………………………………………………………………………………………………………….. 2 1. 文档概述……………………………………………………………………………………………………………………………………………………………………………………………….. 3 2. 背景………………………………………………………………………………………………………………………………………………………………………………………………………… 3 3. 名词解释……………………………………………………………………………………………………………………………………………………………………………………………….. 4 4. 服务器结构…………………………………………………………………………………………………………………………………………………………………………………………… 4 #Hadoop试验集群的部署结构……………………………………………………………………………………………………………………………………………………… 4 #系统和组建的依赖关系……………………………………………………………………………………………………………………………………………………………….. 5 #生产环境的部署结构……………………………………………………………………………………………………………………………………………………………………. 6 5. Red hat Linux基础环境搭建…………………………………………………………………………………………………………………………………………………………………. 6 #linux 安装 (vm虚拟机)………………………………………………………………………………………………………………………………………………………………… 6 #配置机器时间同步………………………………………………………………………………………………………………………………………………………………………… 6 #配置机器网络环境………………………………………………………………………………………………………………………………………………………………………… 7 #配置集群hosts列表…………………………………………………………………………………………………………………………………………………………………….. 10 #下载并安装 JAVA JDK系统软件……………………………………………………………………………………………………………………………………………….. 10 #生成登陆密钥………………………………………………………………………………………………………………………………………………………………………………. 11 #创建用户账号和Hadoop部署目录和数据目录…………………………………………………………………………………………………………………… 11 #检查基础环境………………………………………………………………………………………………………………………………………………………………………………. 12 6. Hadoop 单机系统 安装配置…………………………………………………………………………………………………………………………………………………………… 13 #Hadoop 文件下载和解压…………………………………………………………………………………………………………………………………………………………… 13 #配置 hadoop-env.sh 环境变量………………………………………………………………………………………………………………………………………………….. 13 #Hadoop Common组件 配置core-site.xml………………………………………………………………………………………………………………………………… 13 #HDFS NameNode,DataNode组建配置hdfs-site.xml…………………………………………………………………………………………………………………… 14 #配置MapReduce - JobTracker TaskTracker 启动配置………………………………………………………………………………………………………………… 15 #Hadoop单机系统,启动执行和异常检查………………………………………………………………………………………………………………………………… 17 #通过界面查看集群部署部署成功…………………………………………………………………………………………………………………………………………… 18 #通过执行 Hadoop pi 运行样例检查集群是否成功…………………………………………………………………………………………………………….. 19 #安装部署 常见错误……………………………………………………………………………………………………………………………………………………………………. 20 7. Hadoop 集群系统 配置安装配置………………………………………………………………………………………………………………………………………………….. 20 #检查node节点linux 基础环境是否正常,参考 [ linux 基础环境搭建]一节。…………………………………………………………….. 20 #配置从master 机器到 node 节点无密钥登陆…………………………………………………………………………………………………………………….. 20 #检查master到每个node节点在hadoop用户下使用密钥登陆是否正常………………………………………………………………………. 21 #配置master 集群服务器地址 stop-all.sh start-all.sh 的时候调用……………………………………………………………………………………….. 21 #通过界面查看集群部署部署成功…………………………………………………………………………………………………………………………………………… 22 #通过执行 Hadoop pi 运行样例检查集群是否成功…………………………………………………………………………………………………………….. 24 8. 自动化安装脚本………………………………………………………………………………………………………………………………………………………………………………… 25 #master 服务器自动安装脚本…………………………………………………………………………………………………………………………………………………….. 25 Hive仓库集群部署入门文档…………………………………………………………………………………………………………………………………………………………………………. 27 1. 名词解释……………………………………………………………………………………………………………………………………………………………………………………………… 27 2. Hive的作用和原理说明…………………………………………………………………………………………………………………………………………………………………….. 27 #数据仓库结构图………………………………………………………………………………………………………………………………………………………………………….. 27 #Hive仓库流程图…………………………………………………………………………………………………………………………………………………………………………… 27 #hive内部结构图……………………………………………………………………………………………………………………………………………………………………………. 27 3. Hive 部署和安装…………………………………………………………………………………………………………………………………………………………………………………. 27 #安装Hadoop集群,看EasyHadoop安装文档。……………………………………………………………………………………………………………………….. 27 #安装Mysql,启动Mysql,检查gc++包。………………………………………………………………………………………………………………………………………. 27 #解压Hive包并配置JDBC连接地址。……………………………………………………………………………………………………………………………………… 27 #启动Hive thrift Server。………………………………………………………………………………………………………………………………………………………………… 27 #启动内置的Hive UI。………………………………………………………………………………………………………………………………………………………………….. 27 4. Hive Cli 的基本用法……………………………………………………………………………………………………………………………………………………………………………. 28 #登陆查询……………………………………………………………………………………………………………………………………………………………………………………….. 28 #查询文件方式………………………………………………………………………………………………………………………………………………………………………………. 28 #命令行模式…………………………………………………………………………………………………………………………………………………………………………………… 28 5. HQL基本语法 (创建表,加载表,分析查询,删除表)……………………………………………………………………………………………………………………… 28 #创建表……………………………………………………………………………………………………………………………………………………………………………………………. 28 6. 使用Mysql构建简单数据集市……………………………………………………………………………………………………………………………………………………….. 29 #Mysql的两种引擎介绍……………………………………………………………………………………………………………………………………………………………….. 29 #创建一个数据表使用Hive cli 进行数据分析………………………………………………………………………………………………………………………… 29 #使用shell 编写Hsql 并使用HiveCli导出数据,使用Mysql命令加载到数据库中。……………………………………………………… 29 #使用crontab 新增每日运行任务定时器……………………………………………………………………………………………………………………………….. 29 7. 使用FineReport 数据展现数据………………………………………………………………………………………………………………………………………………………. 29 #安装FineReport,使用注册码!……………………………………………………………………………………………………………………………………………………… 29 #使用FineReport,快速展现数据报表。…………………………………………………………………………………………………………………………………….. 29 #FineReport 的问题和局限…………………………………………………………………………………………………………………………………………………………… 29 1. 文档概述本文档是Hadoop部署文档,提供了Hadoop单机安装和Hadoop集群安装的方法和步骤,本文档希望让Hadoop安装部署更简单(Easy)。 本安装文档适用于 centos 5 /red hat 5.2 32位,64位版本,ubuntu 等操作系统需要做部分修改。 2. 背景Hadoop为分布式文件系统和计算的基础框架系统，其中包含hadoop程序，hdfs系统等。 3. 名词解释1.Hadoop, Apache开源的分布式框架。 2.HDFS, hadoop的分布式文件系统 3.NameNode, hadoop HDFS元数据主节点服务器，负责保存DataNode 文件存储元数据信息。 4.JobTracker, hadoop的Map/Reduce调度器，负责与TackTracker通信分配计算任务并跟踪任务进度。 5.DataNode, hadoop数据节点，负责存储数据。 6.TaskTracker, hadoop调度程序，负责Map,Reduce 任务的具体启动和执行。 7.Fuse, 多文件系统内核程序，可将不同的文件系统mount成linux可读写模式 4. 服务器结构#Hadoop试验集群的部署结构部署路径：/opt/modules/hadoop/hadoop-1.0.3/ #系统和组建的依赖关系 #生产环境的部署结构 5. Red hat Linux基础环境搭建#linux 安装 (vm虚拟机)请参考其他 vmware 虚拟机安装文档。设置网络为 net 模式。 root hadoop #配置机器时间同步#配置时间同步 crontab -e 10 1 * * * /usr/sbin/ntpdate cn.pool.ntp.org #手动同步时间 /usr/sbin/ntpdatecn.pool.ntp.org #配置机器网络环境#修第一台 hostname 为 master hostname master #检测 hostname #配置主机名 (hostname) vi/etc/sysconfig/network 123NETWORKING=yes #启动网络NETWORKING_IPV6=noHOSTNAME=master #主机名 #使用 setup 命令配置系统环境 setup ![img](file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image016.png) cat /etc/sysconfig/network-scripts/ifcfg-eth0 #检查ip配置 12345678# Advanced Micro Devices [AMD] 79c970 [PCnet32 LANCE]DEVICE=eth0BOOTPROTO=noneHWADDR=00:0c:29:fc:3a:09ONBOOT=yesNETMASK=255.255.255.0IPADDR=192.168.1.100TYPE=Ethernet /sbin/service network restart #重新启动网络服务 /sbin/ifconfig #检查网络ip配置 #关闭防火墙 如果不关闭 报错如下。 123456************************************************************/2012-07-18 02:47:26,331 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties2012-07-18 02:47:26,529 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.2012-07-18 02:47:26,533 ERROR org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Error getting localhost name. Using &apos;localhost&apos;...java.net.UnknownHostException: node1: node1 at java.net.InetAddress.getLocalHost(InetAddress.java:1354) #关闭防火墙 #配置集群hosts列表vi /etc/hosts #添加一下内容到 vi 中 #下载并安装 JAVA JDK系统软件#下载jdk wgethttp://60.28.110.228/source/package/jdk-6u21-linux-i586-rpm.bin #安装jdk chmod +x jdk-6u21-linux-i586-rpm.bin ./jdk-6u21-linux-i586-rpm.bin #配置环境变量 vi /etc/profile.d/java.sh #复制粘贴一下内容 到 vi 中。 #手动立即生效 source /etc/profile #测试 jps #生成登陆密钥#切换Hadoop 用户下 su hadoop cd /home/hadoop/ #生成公钥和私钥 ssh-keygen -q -t rsa -N “” -f/home/hadoop/.ssh/id_rsa cd .ssh cat id_rsa.pub &gt; authorized_keys chmod go-wx authorized_keys #公钥:复制文件内容 id_rsa.pub到authorized_keys #集群环境 id_ras_pub 复制到 node1:/home/hadoop/.ssh/authorized_keys #检查 ll -a /home/hadoop/.ssh/ #创建用户账号和Hadoop部署目录和数据目录#创建 hadoop 用户 /usr/sbin/groupadd hadoop #分配 hadoop 到 hadoop 组中 /usr/sbin/useradd hadoop -g hadoop #创建 hadoop 代码目录结构 mkdir -p /opt/modules/hadoop/ #创建 hadoop 数据目录结构 mkdir -p /opt/data/hadoop/ #修改 目录结构权限为为hadoop chown -R hadoop:hadoop /opt/modules/hadoop/ chown -R hadoop:hadoop /opt/data/hadoop/ #检查基础环境/sbin/ifconfig #测试命令 /sbin/ifconfig ping master ssh master jps echo $JAVA_HOME echo $HADOOP_HOME hadoop 6. Hadoop 单机系统 安装配置#Hadoop 文件下载和解压#切到 hadoop 安装路径下 cd /opt/modules/hadoop/ #从 hadoop.apache.org 下载Hadoop 安装文件 wget http://labs.renren.com/apache-mirror/hadoop/common/hadoop-1.0.3/hadoop-1.0.3.tar.gz #如果已经下载请复制文件到安装hadoop 文件夹 cp hadoop-1.0.3.tar.gz /opt/modules/hadoop/ #加压 复制或者下载的Hadoop 文件 cd /opt/modules/hadoop/ tar -xzvf hadoop-1.0.3.tar.gz hadoop #配置 hadoop-env.sh 环境变量#配置Hadoop 最大HADOOP_HEAPSIZE大小, 默认为 1000,因为虚拟机最大内存配置512m,这里配置较小。 #配置 压缩类库地址 vi/opt/modules/hadoop/hadoop-1.0.3/conf/hadoop-env.sh #Hadoop Common组件 配置 core-site.xml#编辑 core-site.xml 文件 vi /opt/modules/hadoop/hadoop-1.0.3/conf/core-site.xml fs.default.name hdfs://master:9000 fs.checkpoint.dir /opt/data/hadoop/hdfs/namesecondary fs.checkpoint.period 1800 fs.checkpoint.size 33554432 io.compression.codecs org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec fs.trash.interval 1440 Hadoop文件回收站,自动回收时间,单位分钟,这里设置是1天。 #HDFS NameNode,DataNode组建配置hdfs-site.xmlvi /opt/modules/hadoop/hadoop-1.0.3/conf/hdfs-site.xml #配置MapReduce - JobTrackerTaskTracker 启动配置vi /opt/modules/hadoop/hadoop-1.0.3/conf/mapred-site.xml #Hadoop单机系统,启动执行和异常检查描述系统重启，启动，停止，升级，以及其他故障的处理方式 #创建Hadoop mapred 和 hdfs namenode 和 datanode 目录 在 root 下 mkdir -p/data/hadoop/ chown -Rhadoop:hadoop /data/* #切换到 hadoop 用户下 su hadoop #创建mapreduce mkdir -p /opt/data/hadoop/mapred/mrlocalmkdir -p /opt/data/hadoop/mapred/mrsystem mkdir -p /opt/data/hadoop/hdfs/name mkdir -p/opt/data/hadoop/hdfs/data mkdir -p /opt/data/hadoop/hdfs/namesecondary #启动 切换到hadoop用户 su hadoop #格式化文件 /opt/modules/hadoop/hadoop-1.0.3/bin/hadoopnamenode -format #启动 Master node ： /opt/modules/hadoop/hadoop-1.0.3/bin/hadoop-daemon.shstart namenode #启动 JobTracker： /opt/modules/hadoop/hadoop-1.0.3/bin/hadoop-daemon.shstart jobtracker #启动 secondarynamenode： /opt/modules/hadoop/hadoop-1.0.3/bin/hadoop-daemon.shstart secondarynamenode #启动 DataNode &amp;&amp; TaskTracker： /opt/modules/hadoop/hadoop-1.0.3/bin/hadoop-daemon.shstart datanode /opt/modules/hadoop/hadoop-1.0.3/bin/hadoop-daemon.shstart tasktracker 停止，命令相同，将start换为stop #出现错误可查看日志 tail -f/opt/modules/hadoop/hadoop-1.0.3/logs/* #通过界面查看集群部署部署成功#检查 namenode 和 datanode 是否正常 http://master:50070/ #检查 jobtracker 和 tasktracker 是否正常 http://master:50030/ #通过执行 Hadoop pi 运行样例检查集群是否成功cd /opt/modules/hadoop/hadoop-1.0.3 bin/hadoop jar hadoop-examples-1.0.3.jar pi10 100 #集群正常效果如下 #安装调试方法#启动程序 namenodedatanode jobtracker tasktracker hadoop-daemon.shstart xxxx #检查进程是否存活jps #检查日志是否正常 tail-100f /opt/modules/hadoop/hadoop-1.0.3/libexec/../logs/hadoop-hadoop-xxxx-master.log #检查端口是否正常 http://master:50070 http://master:50030 #检查 hdfs 是否正常 hadoopfs -ls / hadoopfs -mkdir /data/ hadoopfs -put xxx.log /data/ #检查 mapreduce 是否正常 hadoopjar hadoop-examples-1.0.3.jar pi 100 100 #安装部署 常见错误主机文件/etc/hosts中主机列表IP错误。mapred-site.xml中任务分配过多或过少，导致效率降低或内存溢出。物理硬盘的权限均应为hadoop:hadoop，执行启动也应su为hadoop用户。比较常见是出现权限错误导致无法启动故障。 如果遇到服务无法启动。请检查 $HADOOP_HOME/logs/ 目录具体日志情况。 tail -n 100 $HADOOP_HOME/logs/namenode #检查namenode 服务日志 tail -n 100$HADOOP_HOME/logs/datanode #检查datanode服务日志 Tail -n 100$HADOOP_HOME/logs/jobtracker #检查jobtracker服务日志 7. Hadoop 集群系统 配置安装配置#检查node节点linux 基础环境是否正常,参考 [ linux 基础环境搭建]一节。#配置从master 机器到 node 节点无密钥登陆#切换到Hadoop 用户下 su hadoop cd /home/hadoop/ #生成公钥和私钥 ssh-keygen -q -t rsa -N “” -f/home/hadoop/.ssh/id_rsa #查看密钥内容 cd /home/hadoop/.ssh cat id_rsa.pub #看到如下内容举例 #复制id_rsa.pub公钥到authorized_keys 目录 cat id_rsa.pub &gt; authorized_keys #修改master 密钥权限,非常容易错误的地方。 chmod go-rwx /home/hadoop/.ssh/authorized_keys #把 master 机器上的 authorized_keys文件 copy 到 node1 节点上。 scp /home/hadoop/.ssh/authorized_keys /home/hadoop/.ssh/ #输入 hadoop 密码 #修改 node1 密钥权限 chmod go-rwx /home/hadoop/.ssh/authorized_keys #验证本机无密钥登陆,如果无需密码算成功。 ssh 192.168.1.100 exit #退出 ssh master exit #退出 #验证登陆 192.168.1.101 ,如果无需密码算成功。 ssh 192.168.1.101 exit #退出 ssh node1 exit #退出 #检查master到每个node节点在hadoop用户下使用密钥登陆是否正常su hadoop #检查master 登陆master正常 ssh master exit #退出 #检查 master 登陆node1 正常 ssh node1 exit #退出 #配置master 集群服务器地址stop-all.sh start-all.sh 的时候调用#设置 hadoop secondary node hostname批量启动的地址 #配置secondary的地址 vi /opt/modules/hadoop/hadoop-1.0.3/conf/masters #配置 datanode 和 tasktracker 的地址 vi /opt/modules/hadoop/hadoop-1.0.3/conf/slaves #复制 master hadoop到 node1 node2节点服务器上 #切换到 hadoop 用户下 su hadoop scp -r /opt/modules/hadoop/hadoop-1.0.3/ node1:/opt/modules/hadoop/ #登陆到 node1 节点上 ssh node1 #创建mapreduce mkdir -p /data/hadoop/mapred/mrlocalmkdir -p /data/hadoop/mapred/mrsystem mkdir -p /data/hadoop/hdfs/name mkdir -p /data/hadoop/hdfs/data chmod go-w /data/hadoop/hdfs/data #批量启动和关闭集群 #全部启动 /opt/modules/hadoop/hadoop-1.0.3/bin/start-all.sh #全部关闭 /opt/modules/hadoop/hadoop-1.0.3/bin/stop-all.sh #通过界面查看集群部署部署成功#检查 namenode 和 datanode 是否正常 http://master:50070/ #检查 jobtracker 和 tasktracker 是否正常 http://master:50030/ hadoop fs -ls / hadoop fs -mkdir/data/ #通过执行 Hadoop pi 运行样例检查集群是否成功cd /opt/modules/hadoop/hadoop-1.0.3 bin/hadoop jar hadoop-examples-1.0.3.jar pi10 100 #集群正常效果如下 8. 自动化安装脚本为加快服务器集群的安装和部署,会使用自动化安装脚本安装。以下为自动化部署脚本样例。脚本中#红色部分 具体参考以上配置做具体修改。本脚本里面的安装包用于 64位服务器安装,32位安装包需要单独下载修改。 #master 服务器自动安装脚本#hadoop_install.sh,将以下放入shell脚本中并执行。 vi hadoop_install.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#!/bin/shyum -y install lrzsz gcc gcc-c++ libstdc++-devel ntp #安装gcc 基础环境echo &quot;0 1 * * * root /usr/sbin/ntpdate cn.pool.ntp.org&quot; &gt;&gt; /etc/crontab #配置时间同步/usr/sbin/ntpdate cn.pool.ntp.org #手动同步时间/usr/sbin/groupadd hadoop #新增hadoop 群组/usr/sbin/useradd hadoop -g hadoop #新增Hadoop 用户并绑定到hadoop 群中#安装依赖包并设置hadoop用户mkdir -p /opt/modules/hadoop/mkdir -p /opt/data/hadoop/chown hadoop:hadoop /opt/data/hadoop/#配置 /etc/hosts ip 对应主机名称echo -e &quot;127.0.0.1\\tlocalhost.localdomain localhost#::1\\tlocalhost6.localdomain6 localhost6#机架1192.168.1.100\\thadoopmaster192.168.1.101\\thadoopslave192.168.1.101\\thadoop-node-101&quot; &gt; /etc/hosts#获取服务器外网IP并替换host中127.0.0.1 collect-*IP=`/sbin/ifconfig eth0 | grep &quot;inet addr&quot; | awk -F&quot;:&quot; &apos;&#123;print $2&#125;&apos; | awk -F&quot; &quot; &apos;&#123;print $1&#125;&apos;`sed -i &quot;s/^127.0.0.1\\tcollect/$&#123;IP&#125;\\tcollect/g&quot; /etc/hostsecho &quot;----------------env init finish and prepare su hadoop---------------&quot;HADOOP=/home/hadoopcd $HADOOP#生成密钥sudo -u hadoop mkdir .sshssh-keygen -q -t rsa -N &quot;&quot; -f $HADOOP/.ssh/id_rsaCd$HADOOP/.ssh/ &amp;&amp; echo &quot;#此处需要 cat master id_rsa.pub&quot; &gt; $HADOOP/.ssh/authorized_keyschmod go-rwx $HADOOP/.ssh/authorized_keys #修改文件权限cd $HADOOP#下载已经配置好的 Hadoop 集群包wget http://60.28.110.228/source/package/hadoop/hadoop_gz.tar.gzwget http://60.28.110.228/source/package/hadoop/hadoop_rpm.tar.gzmkdir $HADOOP/hadoopmv *.tar.gz $HADOOP/hadoopcd $HADOOP/hadooptar zxvf hadoop_rpm.tar.gztar zxvf hadoop_gz.tar.gzrpm -ivh jdk-6u21-linux-amd64.rpmrpm -ivh lrzsz-0.12.20-19.x86_64.rpmrpm -ivh lzo-2.04-1.el5.rf.x86_64.rpmrpm -ivh hadoop-gpl-packaging-0.2.8-1.x86_64.rpmtar xzvf lzo-2.06.tar.gzcd lzo-2.06 &amp;&amp; ./configure --enable-shared &amp;&amp; make &amp;&amp; make installcp /usr/local/lib/liblzo2.* /usr/lib/cd ..tar xzvf lzop-1.03.tar.gzcd lzop-1.03./configure &amp;&amp; make &amp;&amp; make install &amp;&amp; cd ..chown -R hadoop:hadoop /opt/modules/hadoop/cp hadoop-node-0.20.203.0.tar.gz /opt/modules/hadoop/cd /opt/modules/hadoop/ &amp;&amp; tar -xzvf hadoop-node-0.20.203.0.tar.gzchown -R hadoop:hadoop /opt/modules/hadoop/chown -R hadoop:hadoop /home/hadoop 9. 开启集群LZO#下载相关 LZO 包wget http://113.11.199.230/resources/lzop-1.03.tar.gz wget http://113.11.199.230/resources/x64/hadoop-gpl-packaging-0.5.3-1.x86_64.rpm wget http://113.11.199.230/resources/lzo-2.06.tar.gz wget http://113.11.199.230/resources/x64/lzo-2.06-1.el5.rf.x86_64.rpm wget http://113.11.199.230/resources/x64/lzo-devel-2.06-1.el5.rf.x86_64.rpm #安装 LZO 相关包rpm -ivhlzo-2.06-1.el5.rf.x86_64.rpm lzo-devel-2.06-1.el5.rf.x86_64.rpmhadoop-gpl-packaging-0.5.3-1.x86_64.rpm #编译 安装 lzo 包12345678910111213tar zxf lzo-2.06.tar.gzcd lzo-2.06./configure &amp;&amp; make &amp;&amp; makeinstallcd ../tar zxf lzop-1.03.tar.gzcd lzop-1.03./configure &amp;&amp; make &amp;&amp; makeinstall #修改hadoop配置文件core-site.xml1234567&lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,org.apache.hadoop.io.compress.BZip2Codec&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt; &lt;/property&gt; #修改hadoop配置文件mapred-site.xml12345678&lt;property&gt; &lt;name&gt;mapred.map.output.compression.codec&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt; &lt;/property&gt;&lt;property&gt;&lt;name&gt;mapred.child.java.opts&lt;/name&gt;&lt;value&gt;-Djava.library.path=/opt/hadoopgpl/native/Linux-amd64-64:/opt/modules/hadoop/hadoop-1.0.3/lib/native/Linux-amd64-64&lt;/value&gt;&lt;/property&gt; 10. 开启任务调度器#修改 mapred-site.xml12345678&lt;property&gt; &lt;name&gt;mapred.jobtracker.taskScheduler&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.CapacityTaskScheduler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.queue.names&lt;/name&gt; &lt;value&gt;default,hive,streaming&lt;/value&gt; &lt;/property&gt; #修改 capacity-scheduler.xml添加 hive streaming 等队列。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245&lt;?xml version=\"1.0\"?&gt; &lt;!-- This is the configuration file for the resource manager in Hadoop. --&gt; &lt;!-- You can configure various scheduling parameters related to queues. --&gt; &lt;!-- The properties for a queue follow a naming convention,such as, --&gt; &lt;!-- mapred.capacity-scheduler.queue.&lt;queue-name&gt;.property-name. --&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.maximum-system-jobs&lt;/name&gt; &lt;value&gt;3000&lt;/value&gt; &lt;description&gt;Maximum number of jobs in the system which can be initialized, concurrently, by the CapacityScheduler. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.default.capacity&lt;/name&gt; &lt;value&gt;10&lt;/value&gt; &lt;description&gt;Percentage of the number of slots in the cluster that are to be available for jobs in this queue. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.default.maximum-capacity&lt;/name&gt; &lt;value&gt;-1&lt;/value&gt; &lt;description&gt; maximum-capacity defines a limit beyond which a queue cannot use the capacity of the cluster. This provides a means to limit how much excess capacity a queue can use. By default, there is no limit. The maximum-capacity of a queue can only be greater than or equal to its minimum capacity. Default value of -1 implies a queue can use complete capacity of the cluster. This property could be to curtail certain jobs which are long running in nature from occupying more than a certain percentage of the cluster, which in the absence of pre-emption, could lead to capacity guarantees of other queues being affected. One important thing to note is that maximum-capacity is a percentage , so based on the cluster's capacity the max capacity would change. So if large no of nodes or racks get added to the cluster , max Capacity in absolute terms would increase accordingly. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.default.supports-priority&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;If true, priorities of jobs will be taken into account in scheduling decisions. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.default.minimum-user-limit-percent&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;description&gt; Each queue enforces a limit on the percentage of resources allocated to a user at any given time, if there is competition for them. This user limit can vary between a minimum and maximum value. The former depends on the number of users who have submitted jobs, and the latter is set to this property value. For example, suppose the value of this property is 25. If two users have submitted jobs to a queue, no single user can use more than 50% of the queue resources. If a third user submits a job, no single user can use more than 33% of the queue resources. With 4 or more users, no user can use more than 25% of the queue's resources. A value of 100 implies no user limits are imposed. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.default.user-limit-factor&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;description&gt;The multiple of the queue capacity which can be configured to allow a single user to acquire more slots. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.default.maximum-initialized-active-tasks&lt;/name&gt; &lt;value&gt;200000&lt;/value&gt; &lt;description&gt;The maximum number of tasks, across all jobs in the queue, which can be initialized concurrently. Once the queue's jobs exceed this limit they will be queued on disk. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.default.maximum-initialized-active-tasks-per-user&lt;/name&gt; &lt;value&gt;100000&lt;/value&gt; &lt;description&gt;The maximum number of tasks per-user, across all the of the user's jobs in the queue, which can be initialized concurrently. Once the user's jobs exceed this limit they will be queued on disk. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.default.init-accept-jobs-factor&lt;/name&gt; &lt;value&gt;10&lt;/value&gt; &lt;description&gt;The multipe of (maximum-system-jobs * queue-capacity) used to determine the number of jobs which are accepted by the scheduler. &lt;/description&gt; &lt;/property&gt; &lt;!-- The default configuration settings for the capacity task scheduler --&gt; &lt;!-- The default values would be applied to all the queues which don't have --&gt; &lt;!-- the appropriate property for the particular queue --&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.default-supports-priority&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;If true, priorities of jobs will be taken into account in scheduling decisions by default in a job queue. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.default-minimum-user-limit-percent&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;description&gt;The percentage of the resources limited to a particular user for the job queue at any given point of time by default. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.default-user-limit-factor&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;description&gt;The default multiple of queue-capacity which is used to determine the amount of slots a single user can consume concurrently. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.default-maximum-active-tasks-per-queue&lt;/name&gt; &lt;value&gt;200000&lt;/value&gt; &lt;description&gt;The default maximum number of tasks, across all jobs in the queue, which can be initialized concurrently. Once the queue's jobs exceed this limit they will be queued on disk. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.default-maximum-active-tasks-per-user&lt;/name&gt; &lt;value&gt;100000&lt;/value&gt; &lt;description&gt;The default maximum number of tasks per-user, across all the of the user's jobs in the queue, which can be initialized concurrently. Once the user's jobs exceed this limit they will be queued on disk. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.default-init-accept-jobs-factor&lt;/name&gt; &lt;value&gt;10&lt;/value&gt; &lt;description&gt;The default multipe of (maximum-system-jobs * queue-capacity) used to determine the number of jobs which are accepted by the scheduler. &lt;/description&gt; &lt;/property&gt; &lt;!-- Capacity scheduler Job Initialization configuration parameters --&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.init-poll-interval&lt;/name&gt; &lt;value&gt;5000&lt;/value&gt; &lt;description&gt;The amount of time in miliseconds which is used to poll the job queues for jobs to initialize. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.init-worker-threads&lt;/name&gt; &lt;value&gt;5&lt;/value&gt; &lt;description&gt;Number of worker threads which would be used by Initialization poller to initialize jobs in a set of queue. If number mentioned in property is equal to number of job queues then a single thread would initialize jobs in a queue. If lesser then a thread would get a set of queues assigned. If the number is greater then number of threads would be equal to number of job queues. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.hive.capacity&lt;/name&gt; &lt;value&gt;60&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.hive.maximum-capacity&lt;/name&gt; &lt;value&gt;80&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.hive.supports-priority&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.hive.minimum-user-limit-percent&lt;/name&gt; &lt;value&gt;20&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.hive.user-limit-factor&lt;/name&gt; &lt;value&gt;10&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.hive.maximum-initialized-active-tasks&lt;/name&gt; &lt;value&gt;200000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.hive.maximum-initialized-active-tasks-per-user&lt;/name&gt; &lt;value&gt;100000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.hive.init-accept-jobs-factor&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;/property&gt; &lt;!--streaming--&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.streaming.capacity&lt;/name&gt; &lt;value&gt;30&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.streaming.maximum-capacity&lt;/name&gt; &lt;value&gt;90&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.streaming.supports-priority&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.streaming.minimum-user-limit-percent&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.streaming.user-limit-factor&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.streaming.maximum-initialized-active-tasks&lt;/name&gt; &lt;value&gt;200000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.streaming.maximum-initialized-active-tasks-per-user&lt;/name&gt; &lt;value&gt;100000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.capacity-scheduler.queue.streamingh.init-accept-jobs-factor&lt;/name&gt; &lt;value&gt;10&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 11. 开启机架感知#修改hadoop配置文件core-site.xml 添加机架感知代码1234&lt;property&gt; &lt;name&gt;topology.script.file.name&lt;/name&gt; &lt;value&gt;/opt/modules/hadoop/hadoop-1.0.3/conf/RackAware.py&lt;/value&gt;&lt;/property&gt; #新建 RackAware.py 文件123456789101112131415#!/usr/bin/python#-*-coding:UTF-8 -*-import sysrack = &#123;\"hadoopnode-101\":\"rack1\", \"hadoopnode-102\":\"rack1\", \"hadoopnode-203\":\"rack2\", \"192.168.1.101\":\"rack1\", \"192.168.1.102\":\"rack1\", \"192.168.1.203\":\"rack2\", &#125;if __name__==\"__main__\": print \"/\" + rack.get(sys.argv[1],\"rack0\") 12. 配置详解#Hadoop系统配置详解#core-site.xml为公共配置,hdfs-site.xml mapred-site.xml 在 hdfs 和mapreduce 启动的时候加载。 hadoop-env.sh name value 含义 JAVA_HOME /usr/java/jdk1.6.0_30 JDK所在路径 JAVA_LIBRARY_PATH /opt/hadoopgpl/native/Linux-amd64-64:/opt/modules/hadoop/hadoop-0.20.203.0/lib/native/Linux-amd64-64 Lzo,Snappy,gzip 等压缩算法库地址 HADOOP_HEAPSIZE 26000 最大 HEAPSIZE 大小,默认 1000M core-site.xml Name value 含义 fs.default.name hdfs://hadoopmaster:9000 指定默认的文件系统，默认端口 8020。 fs.checkpoint.dir /data1/hdfs/secondarynamenode,/data2/hdfs/secondarynamenode 辅助NameNode检查点存储目录，分别存储到各个目录，支持冗余备份。 fs.checkpoint.period 1800 editlog和fsimage,合并触发周期30分钟。 fs.checkpoint.size 33554432 editlog和fsimage,合并触发日志大小32M。 fs.trash.interval 1440 文件清理周期 24小时 io.compression.codecs org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec 压缩类库 io.compression.codec.lzo.class com.hadoop.compression.lzo.LzoCodec LZO 编码类 io.file.buffer.size 65536 指定缓冲区的大小，默认4K太小，64k(65536)或128k(131072)更为常用 topology.script.file.name /opt/modules/hadoop/hadoop-0.20.203.0/conf/RackAware.py 配置 机架感知的代码 hdfs-site.xml Name Value 含义 dfs.name.dir /data1/hadoop/hdfs/name,/data2/hadoop/hdfs/name,/nfs/hadoop/hdfs/name NameNode上持久化存储元数据和事务日志的路径。指定多个目录的话，各个目录内容完全一致。 使用NFS在加载一个远程目录,以便后续主机宕机,快速恢复。 dfs.data.dir /data1/hadoop/hdfs/data /data2/hadoop/hdfs/data,/data3/hadoop/hdfs/data DataNode上存储数据块的地方。如果指定多个目录，则数据库被随机的存放。 dfs.http.address hadoopmaster:50070 HDFS 管理界面 dfs.secondary.http.address hadoopslave:50090 secondary namenode http 地址 dfs.replication 整数 数据复制的份数 dfs.datanode.du.reserved 1073741824 预留文件数量 dfs.block.size 134217728 HDFS 文件块大小,默认128M dfs.datanode.max.xcievers 4096 datanode同时打开的文件上限。默认256太小。 dfs.permissions FALSE 默认是 true，则打开前文所述的权限系统。如果是 false，权限检查 就是关闭的 dfs.support.append FALSE 支持文件append，主要是支持hbase mapred-size.xml Name Value 说明 mapred.job.tracker hadoopmaster:9001 Jobtracker的RPC服务器所在的主机名称和端口。 mapred.local.dir /data1/hadoop/mapred/mrlocal,/data2/hadoop/mapred/mrlocal 存储作业中间数据的目录列表，作业结束后，数据被清楚 mapred.system.dir /data1/hadoop/mapred/mrsystem 作业运行期间的存储共享目录的目录，必须是HDFS之上的目录 mapred.task.tracker.map.tasks.maximum 12 运行在tasktracker之上的最大map任务数 mapred.task.tracker.reduce.tasks.maximum 4 运行在tasktracker之上的最大reduce任务数 (MAP+RED=CPU核心*2) (Map/Red=4/1) mapred.child.java.opts -Xmx1536M JVM选项，默认 -Xmx200m mapred.compress.map.output true Map输出后压缩传输,可以缩短文件传输时间 mapred.map.output.compression.codec com.hadoop.compression.lzo.LzoCodec 使用Lzo库作为压缩算法 mapred.child.java.opts -Djava.library.path=/opt/hadoopgpl/native/Linux-amd64-64 加载Lzo 库 mapred.jobtracker.taskScheduler org.apache.hadoop.mapred.CapacityTaskScheduler 使用能力调度器 mapred.queue.names default,HIVE,ETL 配置能力调度器队列 fs.inmemory.size.mb 300 为reduce阶段合并map输出所需的内存文件系统分配更多的内存 io.sort.mb 300 reduce 排序时的内存上限 mapred.jobtracker.restart.recover true 默认:false mapred.reduce.parallel.copies 10 默认:5 ,reduce 并行 copy的线程数 masters Value 说明 hadoopslave SecondaryNameNode HostName地址 slaves Value 说明 datanode1 DataNode TaskTracker HostName 地址列表 样例 http://hadoop.apache.org/common/docs/r0.20.2/core-default.html http://hadoop.apache.org/common/docs/r0.20.2/hdfs-default.html http://hadoop.apache.org/common/docs/r0.20.0/mapred-default.html #机器配置推荐 编号 机器类型 系统 数量 CPU 内存 硬盘 网卡 推荐机型 说明 网络设备 1 交换机 千兆以太网交换机 华为8512 核心和管理节点 1 NameNode CentOS 5.8 1 2*4核 ECC DDR2 96G SATA 1T+1T*3(RAID 10) 千兆以太网卡*2 Dell(R410),HP(DL160) 配置相同,故障可以快速切换 2 Secondary NameNode CentOS 5.8 1 2*4核 ECC DDR2 96G SATA 1T+1T*3(RAID 10) 千兆以太网卡*2 Dell(R410),HP(DL160) 3 JobTracker CentOS 5.8 1 2*4核 ECC DDR2 32G SATA 1T+1T*3(RAID 10) 千兆以太网卡*2 Dell(R410),HP(DL160) 4 负载均衡(HaProxy) CentOS 5.8 2 2*4核 ECC DDR2 8~16G SATA 500G+(RAID 10) 千兆以太网卡*2 Dell(R410),HP(DL160) 互备 6 监控管理(Cacti Nagios Ganglia) CentOS 5.8 1 2*4核 ECC DDR2 8~16G SATA 500G+(RAID 10) 千兆以太网卡*2 Dell(R410),HP(DL160) 7 Hive MetaStore DB Server (Mysql) CentOS 5.9 2 2*5核 ECC DDR2 8~16G SATA 500G+(RAID 10) 千兆以太网卡*2 Dell(R410),HP(DL160) 压力较小 存储和运算节点 5 DataNode+TaskTracker CentOS 5.8 N 2*4核 ECC DDR2 16~32G SATA 2T+2T*(3~12) 千兆以太网卡*2 Dell(R410,R510) 磁盘不做 RAID 6 HiveServer CentOS 5.8 2 2*4核 ECC DDR2 32G SATA 2T 千兆以太网卡*2 Dell(R410),HP(DL160) EasyHive仓库集群部署入门 1. 名词解释\\1. Hive,由facebook 开源的数据仓库系统,可以基于Hql语句操作Hadoop集群数据。 \\2. Mysql,开源数据库系统,H ive 存储原数据使用。 \\3. FineReport 一个简单易用的快速报表研发工具。 \\4. alexa 国际知名网站排名统计 \\5. Sogou 数据资源,http://www.sogou.com/labs/resources.html 2. Hive的作用和原理说明#Hadoop仓库和传统数据仓库的协作结构图 属于互为补充的关系,相比传统数据仓库技术,Hadoop仓库更合适做非结构化数据分析。 #Hadoop/Hive仓库数据日志分析流向图 #hive内部结构图 3. Hive 部署和安装 #安装Hadoop集群,看EasyHadoop集群部署入门章节。#编译安装Mysql,启动Mysql,检查gc++包。12345678910111213141516171819202122##/bin/shcd /opt/modules/srcwget http://ftp.gnu.org/pub/gnu/ncurses/ncurses-5.6.tar.gztar zxvf ncurses-5.6.tar.gzcd ncurses-5.6./configure -prefix=/usr -with-shared -without-debugmake &amp;&amp; make install cleanwget http://60.28.110.228/source/package/mysql-5.1.42.tar.gztar -xzvf mysql-5.1.42.tar.gzcd mysql-5.1.42./configure '--with-embedded-server' '--with-comment=MySQL Community Server (GPL)' '--with-mysqld-ldflags=-static' '--with-client-ldflags=-static' '--enable-assembler' '--enable-local-infile' '--with-fast-mutexes' '--with-mysqld-user=mysql' '--with-unix-socket-path=/var/lib/mysql/mysql.sock' '--with-pic' '--prefix=/opt/modules/mysql/' '--with-extra-charsets=complex' '--with-ssl' '--sysconfdir=/etc' '--datadir=/opt/data/modules/mysql/' '--enable-thread-safe-client' '--with-readline' '--with-innodb' '--with-plugin-innodb_plugin' '--without-ndbcluster' '--with-archive-storage-engine' '--with-csv-storage-engine' '--with-blackhole-storage-engine' '--with-federated-storage-engine' '--without-plugin-daemon_example' '--without-plugin-ftexample' '--with-partition' '--with-big-tables' '--with-zlib-dir=bundled' '--enable-shared' 'CC=gcc' 'CFLAGS=-O2 -g -pipe' 'LDFLAGS=' 'CXX=gcc' 'CXXFLAGS=-O2 -g -pipe -felide-constructors -fno-exceptions -fno-rtti 'make &amp;&amp; make install /usr/sbin/groupadd mysql /usr/sbin/useradd -g mysql mysqlmkdir -p /opt/data/modules/mysql/logs/mkdir -p /opt/data/modules/mysql/data/./scripts/mysql_install_dbchown -R mysql:mysql /opt/data/modules/mysql/chown -R mysql:mysql /opt/modules/mysql/var/opt/modules/mysql/bin/mysqld_safe &amp; #RPM 安装Mysql server 1234567#http://mirrors.oss.org.cn/mysql/MySQL-6.0/?C=M;O=D 下载地址!#mysql serverrpm -ivh MySQL-server-community-6.0.11-0.rhel5.i386.rpm#mysql client rpm -ivh MySQL-client-community-6.0.11-0.rhel5.i386.rpm #启动mysql服务器 /sbin/service mysql start 123[root@master ~]# /sbin/service mysql restartShutting down MySQL.. [ OK ]Starting MySQL. [ OK ] #登陆Mysql 服务器1mysql -uroot -p #添加Hive用户名和密码123456789101112#2).授权法。例如,你想myuser使用mypassword从任何主机连接到mysql服务器的话。GRANT ALL PRIVILEGES ON *.* TO 'hive'@'%' IDENTIFIED BY 'hive' WITH GRANT OPTION ;#如果你想允许用户myuser从ip为192.168.1.%的主机连接到mysql服务器，并使用mypassword作为密码 GRANT ALL PRIVILEGES ON *.* TO 'hive'@'192.168.1.%' IDENTIFIED BY 'hive' WITH GRANT OPTION;#我用的第一个方法,刚开始发现不行,在网上查了一下,少执行一个语句mysql&gt;FLUSH PRIVILEGES#使修改生效，就可以了#遗留问题[root@master ~]# mysql -uhive -phive -h192.168.1.100ERROR 1130 (HY000): Host '::ffff:192.168.1.100' is not allowed to connect to this MySQL server #解压Hive包并配置JDBC连接地址。12345mkdir -p /opt/modules/hive/cp hive-0.9.0.tar.gz /opt/modules/cd /opt/modules/tar -xzvf hive-0.9.0.tar.gzcp mysql-connector-java-5.1.20-bin.jar /opt/modules/hive/hive-0.9.0/lib/ #编辑配置Hive默认文件cp/opt/modules/hive/hive-0.9.0/conf/hive-env.sh.template/opt/modules/hive/hive-0.9.0/conf/hive-env.sh 1export HADOOP_HEAPSIZE=64 #默认1024m cp /opt/modules/hive/hive-0.9.0/conf/hive-default.xml.template/opt/modules/hive-0.9.0/conf/hive-site.xml vi /opt/modules/hive/hive-0.9.0/conf/hive-site.xml #修改一下hive-site mysql JDBC节点内容1234567891011121314151617181920212223&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.1.100:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt; #添加Mysql Hive用户名和密码,创建Hive 仓库#登陆mysql 服务器/opt/modules/mysql/bin/mysql -uroot -p 12#创建Hive metastorecreate database hive; #启动Hive thrift Server#启动hive server/opt/modules/hive/hive-0.9.0/bin/hive–service hiveserver 10001 #测试netstat -nap |grep 10001 12[root@hadoop-231 bin]#netstat -nap |grep 10001tcp 0 0 :::10001 :::* LISTEN 31166/java #启动内置的Hive UI。/opt/modules/hive/hive-0.9.0/bin/hive–service hwi 4. Hive Cli 的基本用法 cd /opt/modules/hive/hive-0.9.0/bin/ #登陆查询#执行hive 命令到 hive 命令行模式下面 ./hive 12345678[root@hadoop-231 bin]#./hivehive&gt; show databases;OKalexadefaulttestTime taken: 3.103 secondshive&gt; #查询文件方式#编辑一个HQL 文件vi test.q 12use test;select * from test_text limit 30; #读取查询文件查询方式./hive -f test.q 12345678910[root@hadoop-231 bin]#./hive -f test.q Hive history file=/tmp/root/hive_job_log_root_201207281215_1090497680.txtOKTime taken: 3.306 secondsOK12944 28595 3450211412 27522 3497417982 27989 3092317813 26214 38065......................................................... #命令行模式./hive -e “select * fromtest.test_text limit 30” #效果同上 5. HQL基本语法 (创建表,加载表,分析查询,删除表)#快速案例./hive #登陆hive Cli模式 CREATEdatabase alexa; #建库 usealexa; create table uid3(uid string) PARTITIONED BY(dt STRING) ROW FORMAT DELIMITED FIELDSTERMINATED BY ‘\\t’ collectionitems terminated by “\\n” STOREDAS TEXTFILE LOCATION’/data/dw/demo/uid3/‘ 12345CREATE TABLE top100w (id BIGINT,domain STRING COMMENT 'alexa domain') COMMENT 'alexa top100w'ROW FORMAT DELIMITED FIELDS TERMINATED BY ','collection items terminated by \"\\n\" STORED AS TEXTFILE LOADDATA LOCAL INPATH ‘/root/top-1m.csv’OVERWRITE INTO TABLE top100w; #加载 LOADDATA INPATH’/data/dw/top100w/top-1m.csv’ OVERWRITE INTO TABLE top100w; #加载 loaddata inpath ‘/data/uid.txt’ overwrite into table uid; #测试 SELECT* FROM alexa.top100w limit 100; select* from alexa.top100w where domain=”itcast.cn” #创建表 a.内部表创建 1CREATE TABLE top100w (id BIGINT,domain STRING) COMMENT 'alexa top100w' b.外部表创建 123456CREATE EXTERNAL TABLE top100w (id BIGINT,domain STRING COMMENT 'alexa domain') COMMENT 'alexa top100w'ROW FORMAT DELIMITED FIELDS TERMINATED BY ','collection items terminated by \"\\n\" STORED AS TEXTFILELOCATION '/data/dw/alexa/top100w/' #加载数据 #加载数据到外部表使用 put 或者 copyFromLocal Hadoop fs -mkdir /data/dw/alexa/top100w/ hadoop fs -put /root/top-1m.csv /data/dw/alexa/top100w/ #检查 Select * from alexa.top100w limit 100; #加载数据到内部表 LOADDATA LOCAL INPATH ‘/home/hadoop/top100w.csv’ OVERWRITE INTO TABLE log; #分析查询 简单查询 select * from top100w limit 10 分析背后原理 简单查询 select a from top100w limit 10 分析背后原理 3.where 练习 4.group by 练习 5.order by 练习 6.union all 练习 7.INSERT OVERWRITE TABLE 练习 8.INSERT OVERWRITE DIRECTORY ‘/user/facebook/tmp/pv_age_sum’ 9.SELECT a.key, a.value FROM a WHERE a.key in (SELECT b.key FROM B) -&gt; SELECTa.key, a.val FROM a LEFT SEMI JOIN b on (a.key = b.key) 10.Hive 函数查询方法 查询地址: https://cwiki.apache.org/confluence/display/Hive/LanguageManual 11.dfs 操作命令 6. 使用hive到Mysql构建简单数据集市#创建一个数据表使用Hivecli 进行数据分析#使用shell 编写Hsql 并使用HiveCli导出数据,使用Mysql命令加载到数据库中。#使用crontab 新增每日运行任务定时器 crontab -e 150 02 * * * /opt/jobs/top100w.sh 7. 使用FineReport 数据展现数据#安装FineReport,使用注册码#使用FineReport,快速展现数据报表8. 练习\\1. 下载 http://www.sogou.com/labs/dl/q.html完整版(1.9GB) \\2. 计算出网民搜索行为曲线,并用图表展现 \\3. 计算出关键词搜索行为次数排行榜 \\4. 计算出域名访问排行榜 思考题:通过挖掘日志如何改进搜索结果,那个URL才是用户想要的? \\5. 使用日志计算出每个点击URL的用户行为权重 \\6. 下载其他搜狗数据库做练习 搜狗行为库结构: 访问时间\\t用户ID\\t[查询词]\\t该URL在返回结果中的排名\\t用户点击的顺序号\\t用户点击的URL 123456789101112131415161718192021222324252627282930313233343536373839404142434445logtime string user bigintkeyword stringrank intclickrank inturl string CREATE EXTERNAL TABLE sogouq (logtime string,user bigint,keyword string,rank int,clickrank int,url string ) COMMENT 'sogou click log'ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'collection items terminated by \"\\n\" STORED AS TEXTFILELOCATION '/data/dw/sogouq/'CREATE TABLE clicklog2 (logtime string,user bigint,keyword string,rank int,clickrank int,url string ) COMMENT 'sogou click log'PARTITIONED BY(dt STRING)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'collection items terminated by \"\\n\" STORED AS TEXTFILEselect c.hour,count(*) from (select split(logtime,':')[0] hour from clicklog) c group by c.hour;CREATE TABLE `click_stat` ( `stat_hour` varchar(4) DEFAULT NULL, `click_count` int(11) DEFAULT NULL ) ENGINE=MyISAM DEFAULT CHARSET=utf8 mysql -uhive -phive -h192.168.1.100 --local-infile -e \"use test;LOAD DATA LOCAL INFILE 'click_hour.csv' REPLACE INTO TABLE click_stat CHARACTER SET utf8 FIELDS TERMINATED BY '\\t' LINES TERMINATED BY '\\n' ( stat_hour,click_count)\"14:00 23232323232 Hadoop 3 2 easyhadoo.cpm Hadoop apacheHadoop chinaeasyhadoopCsnd hadoop iconv -f GBK -t UTF-8 access_log.20080611.decode.filter &gt; access_log.20080611iconv -f GB18030 -t UTF-8 access_log.20080611.decode.filter &gt; access_log.20080611/opt/modules/hadoop-1.0.3/bin/hadoop fs -copyFromLocal access_log.20080611 /data/dw/sogouq/dt=20080611/alter table sogouq add partition(dt=20080602) location '/data/dw/sogouq/dt=20080602/';访问 http://hive.easyhadoop.com/ 进行在线练习 ==后续课程== Hive 数据仓库编程和高级应用 1.HiveServer HA (使用haproxy提高HiveServer可用性) 2.使用JDBC 连接Hive进行查询和分析 3.使用Lzo压缩优化数据存储容量 3.使用正则表达式加载数据 4.使用Hive分区优化查询 5.HQL高级语法 6.编写UDF函数 7.编写Hive自定义MapReduce脚本优化查询 8 编写UDAF自定义函数 9.Hive内存优化查询性能优化 ———————– 大数据接收 存储 分析 展现方案 ————————- nginx –&gt; scribe(flume) –&gt; Hadoop–&gt; Hive–&gt;sqoop[ETL] –&gt; oozie[工作流] –&gt; inforbirght –&gt; java+Highcharts --&gt;haproxy--&gt; phphiveadmin--&gt; 用户 --&gt;Hbase --&gt; java+Highcharts (大云,用户日志查询) 10.使用Sqoop进行数据分析 11.使用oozie配置工作流 12.使用Inforbright 构建大型数据集市 13.使用Java+Highcharts构建动态报表 14.Hive+phpHiveAdmin部署和安装仓库自助查询系统 a.编译安装nginx+php并运行 b.修改phpHiveAdmin配置文件 -———————– 15.构建基于Hive的OLAP应用 EasyPig数据流分析集群部署入门1.数据测试#从 top100w 中查询出 itcast.cn 的 alexa 排名 top100w= load ‘/data/dw/top100w/top-1m.csv’using PigStorage(‘,’) as (id:int,domain:chararray); fltrd = FILTER top100w by domain==’itcast.cn’ itcast = FOREACH fltrd GENERATE id,domain; store itcast into ‘/data/itcast.out’","raw":null,"content":null,"categories":[{"name":"hadoop","slug":"hadoop","permalink":"https://stanxia.github.io/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://stanxia.github.io/tags/hadoop/"}]},{"title":"Hadoop HA配置","slug":"Hadoop-HA配置","date":"2017-03-20T14:40:01.000Z","updated":"2017-03-20T15:04:01.000Z","comments":true,"path":"2017/03/20/Hadoop-HA配置/","link":"","permalink":"https://stanxia.github.io/2017/03/20/Hadoop-HA配置/","excerpt":"","text":"在这里我们选用4台机器进行示范，各台机器的职责如下表格所示 hadoop0 hadoop1 hadoop2 hadoop3 是NameNode吗? 是，属集群cluster1 是，属集群cluster1 是，属集群cluster2 是，属集群cluster2 是DataNode吗？ 否 是 是 是 是JournalNode吗？ 是 是 是 否 是ZooKeeper吗？ 是 是 是 否 是ZKFC吗? 是 是 是 是 1. 搭建自动HA1.1. 复制编译后的hadoop项目到/usr/local目录下1.2. 修改位于etc/hadoop目录下的配置文件1.2.1. hadoop-env.shexport JAVA_HOME=/usr/local/jdk 1.2.2. core-site.xml123456789101112131415161718192021222324252627282930313233&lt;configuration&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://cluster1&lt;/value&gt;&lt;/property&gt; 【这里的值指的是默认的HDFS路径。当有多个HDFS集群同时工作时，用户如果不写集群名称，那么默认使用哪个哪？在这里指定！该值来自于hdfs-site.xml中的配置。在节点hadoop0和hadoop1中使用cluster1，在节点hadoop2和hadoop3中使用cluster2】&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt;&lt;/property&gt;【这里的路径默认是NameNode、DataNode、JournalNode等存放数据的公共目录。用户也可以自己单独指定这三类节点的目录。】&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;&lt;value&gt;hadoop0:2181,hadoop1:2181,hadoop2:2181&lt;/value&gt;&lt;/property&gt;【这里是ZooKeeper集群的地址和端口。注意，数量一定是奇数，且不少于三个节点】&lt;/configuration&gt; 1.2.3. hdfs-site.xml该文件只配置在hadoop0和hadoop1上。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt;【指定DataNode存储block的副本数量。默认值是3个，我们现在有4个DataNode，该值不大于4即可。】 &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;cluster1,cluster2&lt;/value&gt; &lt;/property&gt;【使用federation时，使用了2个HDFS集群。这里抽象出两个NameService实际上就是给这2个HDFS集群起了个别名。名字可以随便起，相互不重复即可】 &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.cluster1&lt;/name&gt; &lt;value&gt;hadoop0,hadoop1&lt;/value&gt; &lt;/property&gt;【指定NameService是cluster1时的namenode有哪些，这里的值也是逻辑名称，名字随便起，相互不重复即可】 &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.cluster1.hadoop0&lt;/name&gt; &lt;value&gt;hadoop0:9000&lt;/value&gt; &lt;/property&gt;【指定hadoop0的RPC地址】 &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.cluster1.hadoop0&lt;/name&gt; &lt;value&gt;hadoop0:50070&lt;/value&gt; &lt;/property&gt;【指定hadoop0的http地址】 &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.cluster1.hadoop1&lt;/name&gt; &lt;value&gt;hadoop1:9000&lt;/value&gt; &lt;/property&gt;【指定hadoop1的RPC地址】 &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.cluster1.hadoop1&lt;/name&gt; &lt;value&gt;hadoop1:50070&lt;/value&gt; &lt;/property&gt;【指定hadoop1的http地址】 &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop0:8485;hadoop1:8485;hadoop2:8485/cluster1&lt;/value&gt; &lt;/property&gt;【指定cluster1的两个NameNode共享edits文件目录时，使用的JournalNode集群信息】 &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled.cluster1&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;【指定cluster1是否启动自动故障恢复，即当NameNode出故障时，是否自动切换到另一台NameNode】 &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.cluster1&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt;【指定cluster1出故障时，哪个实现类负责执行故障切换】 &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.cluster2&lt;/name&gt; &lt;value&gt;hadoop2,hadoop3&lt;/value&gt; &lt;/property&gt;【指定NameService是cluster2时，两个NameNode是谁，这里是逻辑名称，不重复即可。以下配置与cluster1几乎全部相似，不再添加注释】 &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.cluster2.hadoop2&lt;/name&gt; &lt;value&gt;hadoop2:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.cluster2.hadoop2&lt;/name&gt; &lt;value&gt;hadoop2:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.cluster2.hadoop3&lt;/name&gt; &lt;value&gt;hadoop3:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.cluster2.hadoop3&lt;/name&gt; &lt;value&gt;hadoop3:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop0:8485;hadoop1:8485;hadoop2:8485/cluster2&lt;/value&gt; &lt;/property&gt;【这段代码是注释掉的，不要打开】 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled.cluster2&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.cluster2&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp/journal&lt;/value&gt; &lt;/property&gt;【指定JournalNode集群在对NameNode的目录进行共享时，自己存储数据的磁盘路径】 &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt;【一旦需要NameNode切换，使用ssh方式进行操作】 &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt;【如果使用ssh进行故障切换，使用ssh通信时用的密钥存储的位置】&lt;/configuration&gt; 1.2.4. slaveshadoop1 hadoop2 hadoop3 1.3. 把以上配置的内容复制到hadoop1、hadoop2、hadoop3节点上1.4. 修改hadoop1、hadoop2、hadoop3上的配置文件内容1.4.1. 修改hadoop2上的core-site.xml内容fs.defaultFS的值改为hdfs://cluster2 1234&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://cluster2&lt;/value&gt;&lt;/property&gt; 1.4.2. 修改hadoop2上的hdfs-site.xml内容把cluster1中关于journalnode的配置项删除，增加如下内容 1234567&lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop0:8485;hadoop1:8485;hadoop2:8485/cluster2&lt;/value&gt;&lt;/property&gt; 1.4.3. 开始启动1.4.3.1. 启动journalnode在hadoop0、hadoop1、hadoop2上执行sbin/hadoop-daemon.sh startjournalnode 1sbin/hadoop-daemon.sh startjournalnode 1.4.3.2. 格式化ZooKeeper在hadoop0、hadoop2上执行bin/hdfs zkfc -formatZK 1bin/hdfs zkfc -formatZK 1.4.3.3. 对hadoop0节点进行格式化和启动123bin/hdfs namenode -formatsbin/hadoop-daemon.sh start namenode 1.4.3.4. 对hadoop1节点进行格式化和启动123bin/hdfs namenode -bootstrapStandbysbin/hadoop-daemon.sh start namenode 1.4.3.5. 在hadoop0、hadoop1上启动zkfc1sbin/hadoop-daemon.sh start zkfc 我们的hadoop0、hadoop1有一个节点就会变为active状态。 1.4.3.6. 对于cluster2执行类似操作1.4.4. 启动datanode在hadoop0上执行命令sbin/hadoop-daemons.sh start datanode 1sbin/hadoop-daemons.sh start datanode 1.5. 配置Yarn1.5.1. 修改文件mapred-site.xml1234567&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 1.5.2. 修改文件yarn-site.xml1234567&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop0&lt;/value&gt; &lt;/property&gt; 【自定ResourceManager的地址，还是单点，这是隐患】 1234567&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 1.5.3. 启动yarn在hadoop0上执行sbin/start-yarn.sh 1sbin/start-yarn.sh","raw":null,"content":null,"categories":[{"name":"hadoop","slug":"hadoop","permalink":"https://stanxia.github.io/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://stanxia.github.io/tags/hadoop/"}]},{"title":"兼顾稳定和性能，58大数据平台的技术演进与实践","slug":"兼顾稳定和性能，58大数据平台的技术演进与实践","date":"2017-03-13T14:34:45.000Z","updated":"2017-03-15T17:25:51.000Z","comments":true,"path":"2017/03/13/兼顾稳定和性能，58大数据平台的技术演进与实践/","link":"","permalink":"https://stanxia.github.io/2017/03/13/兼顾稳定和性能，58大数据平台的技术演进与实践/","excerpt":"","text":"讲师｜赵健博 编辑｜尚剑 大家好！我是赵健博，来自58赶集，今天给大家分享一下58大数据这块的经验。我先做个自我介绍，我本科和研究生分别是在北京邮电大学和中国科学院计算技术研究所先后毕业的，之前在百度和360工作，现在是58赶集高级架构师、58大数据平台负责人。我有多年的分布式系统（存储、计算）的实践和研发经验，在我工作的这些年中运营了大大小小的集群，最大单集群也达到了四五千台，在这个过程中做了大量的功能研发、系统优化，当然也淌了大量的坑，今天会给大家介绍一些我认为比较重要的。 接下来我会跟大家分享一下58大数据平台在最近一年半的时间内技术演进的过程。主要内容分为三方面：58大数据平台目前的整体架构是怎么样的；最近一年半的时间内我们面临的问题、挑战以及技术演进过程；以及未来的规划。 整体架构 首先看一下58大数据平台架构。大的方面来说分为三层：数据基础平台层、数据应用平台层、数据应用层，还有两列监控与报警和平台管理。 数据基础平台层又分为四个子层： 接入层，包括了Canal/Sqoop（主要解决数据库数据接入问题）、还有大量的数据采用Flume解决方案； 存储层，典型的系统HDFS（文件存储）、HBase（KV存储）、Kafka（消息缓存）； 再往上就是调度层，这个层次上我们采用了Yarn的统一调度以及Kubernetes的基于容器的管理和调度的技术； 再往上是计算层，包含了典型的所有计算模型的计算引擎，包含了MR、HIVE、Storm、Spark、Kylin以及深度学习平台比如Caffe、Tensorflow等等。 数据应用平台主要包括以下功能： 元信息管理，还有针对所有计算引擎、计算引擎job的作业管理，之后就是交互分析、多维分析以及数据可视化的功能。 再往上是支撑58集团的数据业务，比如说流量统计、用户行为分析、用户画像、搜索、广告等等。 针对业务、数据、服务、硬件要有完备的检测与报警体系。 平台管理方面，需要对流程、权限、配额、升级、版本、机器要有很全面的管理平台。 这个就是目前58大数据平台的整体架构图。 这个图展示的是架构图中所包含的系统数据流动的情况。分为两个部分： 首先是实时流，就是黄色箭头标识的这个路径。数据实时采集过来之后首先会进入到Kafka平台，先做缓存。实时计算引擎比如Spark streaming或storm会实时的从Kafka中取出它们想要计算的数据。经过实时的处理之后结果可能会写回到Kafka或者是形成最终的数据存到MySQL或者HBase，提供给业务系统，这是一个实时路径。 对于离线路径，通过接入层的采集和收集，数据最后会落到HDFS上，然后经过Spark、MR批量计算引擎处理甚至是机器学习引擎的处理。其中大部分的数据要进去数据仓库中，在数据仓库这部分是要经过数据抽取、清洗、过滤、映射、合并汇总，最后聚合建模等等几部分的处理，形成数据仓库的数据。然后通过HIVE、Kylin、SparkSQL这种接口将数据提供给各个业务系统或者我们内部的数据产品，有一部分还会流向MySQL。以上是数据在大数据平台上的流动情况。 在数据流之外还有一套管理平台。包括元信息管理（云窗）、作业管理平台（58dp）、权限审批和流程自动化管理平台（NightFury）。 我们的规模可能不算大，跟BAT比起来有些小，但是也过了一千台，目前有1200台的机器。我们的数据规模目前有27PB，每天增量有50TB。作业规模每天大概有80000个job，核心job（产生公司核心指标的job）有20000个，每天80000个job要处理数据量是2.5PB。 技术平台技术演进与实现接下来我会重点介绍一下在最近一年半时间内我们大数据平台的技术演进过程，共分四个部分：稳定性、平台治理、性能以及异构计算。第一个部分关于稳定性的改进，稳定性是最基础的工作，我们做了比较多的工作。第二个部分是在平台治理方面的内容。第三个方面我们针对性能也做了一些优化。第四个方面，我们针对异构环境，比如说机器的异构、作业的异构，在这种环境下怎么合理地使用资源。 稳定性改进首先看一下稳定性的改进。这块我会举一些例子进行说明。稳定性包含了几个方面，其中第一个方面就是系统的可用性，大家可以采用社区提供的HDFS HA、Yarn HA，Storm HA来解决。另外一个方面是关于扩展性，例如Flume、HDFS，Yarn，Storm的扩展性。这里主要介绍下Flume和HDFS的扩展性相关的一些考虑。此外，有了可用性和扩展性，系统就稳定了吗？实际上不是这样。因为还有很多的突发问题。即使解决了可用性和扩展性，但突发问题还是可能会造成系统不可用，例如由于一些问题造成两台NameNode全部宕机。 首先看一下Flume的扩展性。我们人为的把它定义了两层。一个是FlumeLocal（主要解决一台机器的日志采集问题，简称Local），一个是FlumeCenter（主要从Local上收集数据，然后把数据写到HDFS上，简称Center），Local和Center之间是有一个HA的考虑的，就是Local需要在配置文件里指定两个Center去写入，一旦一个Center出现问题，数据可以马上从另一个Center流向HDFS。此外，我们还开发了一个高可靠的Agent。业务系统中会把数据产生日志写到磁盘上，Agent保证数据从磁盘上实时可靠的收集给本地的Local，其中我们采用了检查点的技术来解决数据可靠性的问题。 这是Flume的典型架构。Local需要在配置文件里面指定死要连到哪几个Center上。如果说10台，可能还OK，100台也OK，如果一千台呢？如果发现两台Flume Center已经达到机器资源的上限，如何做紧急的扩容呢？所以从这个角度看Flume的扩展性是有问题的。 我们的解决方法是在Local和Center中间加了一个ZooKeeper，Local通过ZK动态发现Center，动态的发现下游有什么，就可以达到Center自动扩容的目标了。我们公司Local有两千多台，扩容一台Center仅需一分钟，这种架构实际上可以支持达到万台规模的，这是Flume扩展性的一些改进。 接下来看一下HDFS扩展性的问题。上面这张图展示了hdfs federation的架构，左侧是一个单namespace架构，即整个目录树在一个namespace中，整个集群的文件数规模受限制于单机内存的限制。federation的思想是把目录树拆分，形成不同的namespace，不同namespace由不同namenode管理，这样就打破了单机资源限制，从而达到了可扩展的目标，如右侧图。 但这个方案有一些隐藏的问题，不知道大家有没有注意到，比如这里每个Datanode都会与所有的NameNode去心跳，如果DataNode数量上万台，那么就可能会出现两个问题：第一，从主节点之间的心跳、块汇报成为瓶颈，第二，如果单个部门的数据规模过大那该怎么办？ 针对从主节点之间交互的问题，我们可以进行拆分，控制一个NameNode管理的DateNode的数量，这样就可以避免主从节点交互开销过大的问题。针对单部门数据过大的话可以针对部门内数据进行进一步细拆，就OK了。或者可以考虑百度之前提供的一个方案，即把目录树和inode信息进行抽象，然后分层管理和存储。当然我们目前采用社区federation的方案。如果好好规划的话，也是可以到万台了。 不知道大家有没有在自己运营集群过程中遇到过一些问题，你们是怎么解决的，有些问题可能相当的棘手。突发问题是非常紧急而且重要的，需要在短时间内搞定。接下来我会分享三个例子。 第一个例子是HDFS的Active NN会不定期异常退出，触发HA切换，这就好像一个不定时炸弹一样。这个图展示了HDFS的HA的架构图，客户端进行变更操作（如创建文件）的话会发出请求给namenode，namenode请求处理完之后会进行持久化工作，会在本地磁盘存一份，同时会在共享存储存一份，共享存储是为了active和standby之间同步状态的，standby会周期从共享存储中拉取更新的数据应用到自己的内存和目录树当中，所有的DataNode都是双汇报的，这样两个namenode都会有最新的块信息。最上面的是两个Checker，是为了仲裁究竟谁是Active的。 还有一个过程，Standby NameNode会定期做checkpoint工作，然后在checkpoint做完之后会回传最新的fsimage给active，最终保存在active的磁盘中，默认情况下在回传过程会造成大量的网络和磁盘的压力，导致active的本地磁盘的Util达到100%，此时用户变更请求延迟就会变高。如果磁盘的Util100%持续时间很长就会导致用户请求超时，甚至Checher的检测请求也因排队过长而超时，最终然后触发Checker仲裁HA切换。 切换的过程中在设计上有很重要一点考虑，不能同时有两个Active，所以要成为新Active NameNode，要把原来的Active NameNode停止掉。先会很友好地停止，什么是友好呢？就是发一个RPC，如果成功了就是友好的，如果失败了，就会ssh过去，把原来active namenode进程kill掉，这就是Active NameNode异常退的原因。 当这个原因了解了之后，其实要解决这个问题也非常简单。 第一点要把editlog与fsimage保存的本地目录分离配置，这种分离是磁盘上的分离，物理分离。 第二是checkpoint之后fsimage回传限速。把editlog与fsimage两个磁盘分离，fsimage回传的io压力不会对客户端请求造成影响，另外，回传限速后，也能限制io压力。这是比较棘手的问题。原因看起来很简单，但是从现象找到原因，这个过程并没有那么容易。 第二个案例也是一样，Active NN又出现异常退出，产生HA切换。这次和网络连接数有关，这张图是Active NameNode的所在机器的网络连接数，平时都挺正常，20000到30000之间，忽然有一个点一下打到60000多，然后就打平了，最后降下来，降下来的原因很明显，是服务进程退了。 为什么会出现这个情况呢？在后续分析的过程中我们发现了一个线索，在NameNode日志里报了一个空指针的异常。就顺藤摸瓜发现了一个JDK1.7的BUG，参见上面图片所示，在java select库函数调度路径过程中最终会调用这个函数（setUpdateEvents），大家可以看到，如果fd的个数超过了MAX_UPDATE_ARRAY_SIZE（65535）这个数的话，将会走到else路径，这个路径在if进行不等表达式判断时，将会出发空指针异常。 接下来的问题是，为什么会产生这么多的链接呢？经过分析我们发现，在问题出现的时候，存在一次大目录的DU操作，而DU会锁住整个namespace，这样就导致后续的写请求被阻塞，最终导致请求的堆积，请求的堆积导致了连接数大量堆积，连接数堆积到一定程度就触发JDK1.7的这个BUG。这个问题的解决，从两个方面看，首先我们先把JDK升级到1.8。其次，调整参数dfs.content-summary.limit，限制du操作的持锁时间。该参数默认参数是0。我们现在是设成10000了，大家可以参考。这是第二个非常棘手的问题。 第三个案例关于YARN主节点的，有一天中午，我们收到报警，发现Active RM异常进程退出，触发HA的切换，然而切换后一会新的Active RM节点也会异常退出，这就比较悲剧，我们先进行了恢复。之后我们从当时的日志中发现了原因：一个用户写了一万个文件到分布式缓存里，分布式缓存里数据会同步到ZK上，RM持久化作业状态到ZK时超过Znode单节点最大上限，抛出异常，最终导致ResourceManager进程的异常退出。其实问题的解决方法也非常简单，我们增加了限制逻辑，对于序列化数据量大于Znode节点大小的Job，直接抛异常触发Job的失败。另外我们还适当提升Znode节点大小。 以上是在稳定性方面的一些工作，这三个案例跟大家分享一下，如果有类似的问题建议大家可以尝试一下，这些方案是被我们验证OK的。 平台治理接下来介绍一下平台治理这块。包含几个问题，其中第一问题是关于数据的，一方面，就是大家开发了数据之后，经常找不到，要靠喊，比如说在群里喊一下什么数据在哪，谁能告诉我一下，这个效率很低下。另外一方面是之前的管理数据是共享的，不安全，任何人都可以访问其他人的数据。 第二个问题是关于资源，之前是“大锅饭”模式，大家共享计算资源，相互竞争，这样“能吃的“肯定是挤兑”不能吃的“，经常出现核心任务不能按时按点完成，老板看不到数据，这点很可怕。还有是整个集群资源使用情况没有感知，这样根本不知道资源要怎么分配，是否够用。 第三个问题是关于作业的，开发人员开发大量的作业之后，这些作业要怎么管理，实际上他们可能都不知道。还有就是关于作业之间依赖，经常一个指标计算出来要经历多个作业，作业之间依赖是怎么考虑的，单纯靠时间上的依赖是非常脆弱的，如果前期的job延迟产生了，后续的job必然失败。最后一个问题是数据开发人员的效率不高，所需要做的步骤过多。 针对这四个问题我们做了一些改进，首先是数据与资源治理。数据方面要引入安全策略、元信息管理与基础数仓建设。我们自己开发了一套安全控制策略，主要增加了白名单和权限控制策略。一个HDFS的请求的流程，首先客户端会向NameNode发请求，NameNode接到请求之后首先要做连接解析，读取出请求相关内容做请求处理，再把结果反馈回来，之后客户端向相应的DataNode进行写入数据或者读取数据。从上述流程可以看出，所有HDFS操作全部要经过NameNode这一层。 那么安全策略只要在NameNode的两个点做下控制既可完成：在连接解析后，我们会验证请求方的IP，以及用户是不是在合法配置下面的。如果验证失败，则拒绝请求。如果验证通过，我们会进一步在请求处理过程中验证用户访问的目录和用户在否在合法的配置下。比如说用户A想访问用户B的数据，如果没在允许的情况下会把连接关掉，通过简单的策略调整就能达到灵活的数据的安全控制和数据共享的方式。接下来针对数据找不到的问题，我们开发了全公司层面的基础数据仓库以及针对全公司层面元数据管理平台。 这张图展示了基础数据仓库覆盖度，它覆盖了集团各个公司，又覆盖了多个平台，比如说手机、App端、PC端、微信端等等。数据层次，是数据仓库层、数据集市层还是数据应用层，所属哪个事业群，最后针对数据进行分类标签，比如说帖子数据、用户数据等等都可以通过标签的方式来找到。当想找具体一份数据的时候可以通过这个界面，点一些标签，筛选出一些数据表，甚至在搜索框里面搜数据的关键字。当查到数据表的时候可以在右侧按钮，将显示出表结构，还有表信息，表信息表明了这个表有多少列，这个表的负责人是什么，还有关于数据质量，表的数据量的变化情况等等，如果你想申请可以点击最右边的权限开通。整体开通流程也是自动化的。这是针对数据找不到的问题做的一些改进。 针对资源问题要避免大锅饭，必须要引入账号概念，资源按照账号预留与隔离。我们划分了不同的配额，根据预算、业务需求去申请配额，然后我们调整配额。针对队列这块我们划分多个队列，每个业务线有自己的队列，不同业务线不能跨队列提交任务，每个队列划分出不同资源，资源主要是针对业务线需求而定的。通过这些改进可以达到资源的隔离以及适度的共享。 有了账号的概念之后我们就可以统计每个业务线资源使用情况。我们每天都会有报表。显示了业务线的计算和存储资源的使用情况，甚至是Job的细节情况。 接下来我会介绍一下业务线开发效率低下问题的改进，实际上我们在易用性上也做了很多改进。首先我们开发了云窗平台，它主要解决了元信息查找、数据查询、可是化展示和多维分析这些需求。然后针对任务开发这块我们开发了58DP解决了元信息开发、作业管理与统计等。我们针对实时多维分析开发了飞流，实时作业开发全部配置化、同时支持多种统计算子、自动图表生成等等。还有NightFury，流程自动化管理平台。 这是云窗的界面，上面是一个SQL查询界面，下面是可视化产品界面，这是我们数据可视化的一个结果。 然后关于任务开发的话，我们用58DP来做任务开发，可以支持的不同任务，涵盖目前的所有主流作业以及作业依赖等管理。这是58DP的页面，可以设置基本信息、调度及依赖等。 飞流是支持周期性的统计、全天累计性的统计，大家可以定义统计方法、定义任务的一些基本信息，设置维度、设置度量，设置完之后就展现了图形，也提供了跟昨天的对比情况。当在图里点任何一个点的时候，可以看到不同维度组合下在这个点上的数据分布，点击两个点可以看到不同维度下两个点的分布对比。针对历史数据可以进行对比，我们可以把时间拉的更长，可以查看不同周的实时统计结果，而不是一天。 这是NightFury的界面，这就是我们运维的自动化管理平台，大家可以看到有很多个流程和权限的开通申请，表单的填写、工单审批，审批之后的一些流程全部是自动化的。 性能性能方面，主要分为四个方面： MR作业性能、数据收集性能、SQL查询性能和多维分析的性能。针对MR作业性能，我们引用多租户功能，资源预留，核心作业执行有保障。 第二点小文件合并处理，可以提升任务执行效率，减少调度本身的开销。 第三点我们针对Shuffle阶段参数优化，可以实现并发度提升，IO消耗降低。 经过三个方面的改进之后，我们整体任务的运行时间实际上有一倍左右的提升。数据传输优化方面，我们经过消息合并改进数据传输性能，提升了20倍。在SQL优化方面我们引用内存执行引擎与列存储方案的结合，在同等资源情况下针对线上一百多条SQL进行测试，总体性能大概提升80%。在多维计算这块，我们引入Kylin，针对多维的查询95%以上查询能控制在2s以内。 异构计算异构计算方面我们面临了两个主要问题，一个是作业的异构，我们有多种类型的作业，比如说实时作业强调低时延，而离线作业强调高吞吐，这本身就是矛盾的，怎么解决这个矛盾。第二方面是机器异构，CPU、内存、网络、磁盘配置不同，这种异构环境又要怎么办。 从上面图中可以看出：如果实时作业的task和批处理作业的task被调度到一台机器上了，如果批处理作业把资源占满了（例如网络带宽），则实时作业的task必将收到影响。所以，需要对实时作业和批处理作业做隔离才行。 做资源隔离，我们的思路是采用标签化，给每个NodeManager赋予不同标签，表示不同机器被分配了不同标签；资源队列也赋予不同标签，然后在RM调度时，保证相同标签的队列里容器资源必从相同标签的NodeManager上分配的。这样就可以通过标签的不同达到物理上的资源隔离目标。 这张图是实现图。首先可以看到NodeManager分成了两个集合，一个是实时的，一个是离线的，不同的队列也被赋予了实时或离线的标签，当用户提交一个job的时候它可以指定一个队列，提交到离线队列里就是离线任务，ResourceManager就会把这个作业所需要的资源分配到离线标签的NodeManager上，这样就可以做到物理资源隔离。 未来规划以上主要是介绍了我们最近一年半做的一些工作。接下来我会介绍一下未来的规划。首先就是深度学习。这个概念今年非常火爆，甚至是要爆炸了，深度学习在58这块需求也是蛮强烈的。目前深度学习工具有这么多，caffe、theano、torch等等非常多，怎么做整合，怎么降低使用成本，这是第一个问题。第二个问题，机器是有限的，怎么高效利用资源，需要把机器分配模式变成资源分配模式。还有光有单机的机器学习或者深度学习工具还不够，因为性能太差，所以我们需要将深度学习训练分布式化。我们做了一个初步的测试，针对caffe与Tensorflow工具的分布式化训练做了比较，4卡相对于单卡模型训练性能提升100%~170%，所以分布式化的工作本身意义也是非常大的。 这个图展示的是工具融合方案。我们这里利用的是Kubernetes，支持主流的深度学习工具，每个工具做成镜像形成POD，用户需要的话可以直接把POD分发给他，用户在训练的时候从HDFS上直接拉取样本，并且把训练的参数回写到HDFS上，也就是说通过HDFS做数据的共享，通过这种模式可以很轻松地支持多种深度学习工具，也可以达到按所需资源量进行资源的分配目标。 另外我们会做一个深度学习工具分布式的改造，是针对caffe，我们用的是CaffeOnSpark，即把整个分布式的方案做成模板供用户使用。首先启动多个POD，通过POD启动一个Spark集群，然后再提一个Spark job来做训练，最后在整个训练结束之后再把集群停掉。Tensorflow也是一样的，首先启动tensorflow集群，然后提交任务，任务训练完以后再把集群停掉。其他工具分布式化我们也会采取类似的思路解决。以上是关于深度学习这块我们目前的一些工作。 其次，是关于空间资源利用率的。目前我们有一千多台机器，存储是很大的成本。之前也提到了，我们是属于花钱的部门，所以压力非常大。那怎么节省成本是一个很重要的问题。除了传统压缩之外，还能做什么？HDFS RAID是一个比较好的解决方案。HDFS RAID采用是RC编码，类似RAID6，比如一个文件有m个块，根据m个块生成k个校验块，然后能保证k个块丢失的情况下数据还能找回来，举个例子来说，比如文件2.5G大小，256M一个块，可以分成10个块，根据RC算法再生成4个校验块，可以保证丢了4个块情况下，数据都能找回来。在这个例子中，3副本情况下，一共需要30个块，而采用HDFS RAID，仅需要14个块。但他们的可靠性一样，空间占用情况却差了57%。 具体实施时，第一步对集群数据进行冷热分析，RAID毕竟有些性能问题，一旦数据有问题，你要通过计算才能恢复，势必会造成性能低下，所以针对冷数据做肯定是风险最低的。第二步就是压缩+archive+RAID，通过三方面技术结合把文件数和空间全部节省出来。归档实际上是会变换目录的，为了做适配，我们通过软连接功能，做到对用户透明。最后在数据读取时，如果是RAID数据，就要具备实时RAID修复功能才能保证在数据缺失的情况下不影响数据的访问。 后续我们会对计算资源利用率再做进一步提升。另外也会考虑Storm和YARN扩展性。还有Kubernetes调度优化，比如针对GPU资源管理功能。 总结以上就是我今天想介绍的全部内容。在结束之前请允许我再做一下总结。 首先我介绍了58目前的大数据平台架构是怎么样的，简单来说就是“342”，三个层次、细分为四个子层、旁边两列。所以大家要做大数据平台建设工作，这几个方面是必备的。 第二个方面我重点的介绍了58在一年半的时间内的技术改进。第一点是关于稳定性，主要从Flume和HDFS扩展性方面重点介绍了我们的解决方案，举了三个案例来说明突发问题，不是说有了可用性和扩展性就万事OK了，还要解决突发问题。针对平台治理首先介绍了一下数据和资源的治理方法，接着又介绍了关于易用性方面的改进，我们提供了一系列平台来提高开发人员的开发效率。 第三方面从性能上介绍了我们这边做的优化工作以及优化的结果是怎么样的； 第四方面介绍了在异构环境下如何支持不同特征的作业进行合理调度。 最后我介绍了58深度学习平台建设方面以及存储资源空间利用率优化方面的内容。以上就是我今天的全部内容，希望对大家有帮助。 作者介绍赵健博，58集团高级架构师，技术委员会委员。大数据领域专家，2009年毕业于中国科学院计算所，先后就职于百度、奇虎360、58集团，主要研究领域包括分布式计算与存储系统等。58集团大数据平台负责人，负责大数据平台在集团的研发，应用与发展。 Good Luck!","raw":null,"content":null,"categories":[{"name":"大数据案例","slug":"大数据案例","permalink":"https://stanxia.github.io/categories/大数据案例/"}],"tags":[{"name":"大数据案例","slug":"大数据案例","permalink":"https://stanxia.github.io/tags/大数据案例/"}]},{"title":"Alibaba代码规范","slug":"Alibaba代码规范","date":"2017-03-06T17:24:49.000Z","updated":"2017-03-10T10:44:57.000Z","comments":true,"path":"2017/03/07/Alibaba代码规范/","link":"","permalink":"https://stanxia.github.io/2017/03/07/Alibaba代码规范/","excerpt":"","text":"Good Luck!","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"https://stanxia.github.io/categories/java/"}],"tags":[{"name":"代码规范","slug":"代码规范","permalink":"https://stanxia.github.io/tags/代码规范/"}]},{"title":"经典大数据架构案例：酷狗音乐的大数据平台重构","slug":"经典大数据架构案例：酷狗音乐的大数据平台重构","date":"2017-02-20T16:06:45.000Z","updated":"2017-02-26T05:26:43.000Z","comments":true,"path":"2017/02/21/经典大数据架构案例：酷狗音乐的大数据平台重构/","link":"","permalink":"https://stanxia.github.io/2017/02/21/经典大数据架构案例：酷狗音乐的大数据平台重构/","excerpt":"","text":"前言编者按：本文是酷狗音乐的架构师王劲对酷狗大数据架构重构的总结。酷狗音乐的大数据架构本身很经典，而这篇讲解了对原来的架构上进行重构的工作内容，总共分为重构的原因、新一代的大数据技术架构、踩过的坑、后续持续改进四个部分来给大家谈酷狗音乐大数据平台重构的过程。 眨眼就新的一年了，时间过的真快，趁这段时间一直在写总结的机会，也总结下上一年的工作经验，避免重复踩坑。酷狗音乐大数据平台重构整整经历了一年时间，大头的行为流水数据迁移到新平台稳定运行，在这过程中填过坑，挖过坑，为后续业务的实时计算需求打下了很好的基础。在此感谢酷狗团队成员的不懈努力，大部分从开始只知道大数据这个概念，到现在成为团队的技术支柱，感到很欣慰。 从重构原因，技术架构，踩过的坑，后续持续改进四个方面来描述酷狗音乐大数据平台重构的过程，在此抛砖引玉，这次的内容与6月份在高可用架构群分享的大数据技术实践的有点不同，技术架构做了些调整。 其实大数据平台是一个庞大的系统工程，整个建设周期很长，涉及的生态链很长(包括：数据采集、接入，清洗、存储计算、数据挖掘，可视化等环节，每个环节都可以当做一个复杂的系统来建设)，风险也很大。 一、重构原因在讲重构原因前，先介绍下原有的大数据平台架构，如下图： 从上图可知，主要基于Hadoop1.x+hive做离线计算(T+1)，基于大数据平台的数据采集、数据接入、数据清洗、作业调度、平台监控几个环节存在的一些问题来列举下。 数据采集： 数据收集接口众多，且数据格式混乱，基本每个业务都有自己的上报接口 存在较大的重复开发成本 不能汇总上报，消耗客户端资源，以及网络流量 每个接口收集数据项和格式不统一，加大后期数据统计分析难度 各个接口实现质量并不高，存在被刷，泄密等风险 数据接入: 通过rsync同步文件，很难满足实时流计算的需求 接入数据出现异常后，很难排查及定位问题，需要很高的人力成本排查 业务系统数据通过Kettle每天全量同步到数据中心，同步时间长，导致依赖的作业经常会有延时现象 数据清洗： ETL集中在作业计算前进行处理 存在重复清洗 作业调度： 大部分作业通过crontab调度，作业多了后不利于管理 经常出现作业调度冲突 平台监控： 只有硬件与操作系统级监控 数据平台方面的监控等于空白 基于以上问题，结合在大数据中，数据的时效性越高，数据越有价值(如：实时个性化推荐系统，RTB系统，实时预警系统等)的理念，因此，开始大重构数据平台架构。 二、新一代大数据技术架构在讲新一代大数据技术架构前，先讲下大数据特征与大数据技术要解决的问题。 1.大数据特征：“大量化(Volume)、多样化(Variety)、快速化(Velocity)、价值密度低（Value）”就是“大数据”显著的4V特征，或者说，只有具备这些特点的数据，才是大数据。 2.大数据技术要解决的问题：大数据技术被设计用于在成本可承受的条件下，通过非常快速（velocity）地采集、发现和分析，从大量（volumes）、多类别（variety）的数据中提取价值（value），将是IT领域新一代的技术与架构。 介绍了大数据的特性及大数据技术要解决的问题，我们先看看新一代大数据技术架构的数据流架构图： 从这张图中，可以了解到大数据处理过程可以分为数据源、数据接入、数据清洗、数据缓存、存储计算、数据服务、数据消费等环节，每个环节都有具有高可用性、可扩展性等特性，都为下一个节点更好的服务打下基础。整个数据流过程都被数据质量监控系统监控，数据异常自动预警、告警。 新一代大数据整体技术架构如图： 将大数据计算分为实时计算与离线计算，在整个集群中，奔着能实时计算的，一定走实时计算流处理，通过实时计算流来提高数据的时效性及数据价值，同时减轻集群的资源使用率集中现象。 整体架构从下往上解释下每层的作用： 数据实时采集： 主要用于数据源采集服务，从数据流架构图中，可以知道，数据源分为前端日志，服务端日志，业务系统数据。下面讲解数据是怎么采集接入的。 a.前端日志采集接入： 前端日志采集要求实时，可靠性，高可用性等特性。技术选型时，对开源的数据采集工具flume,scribe,chukwa测试对比，发现基本满足不了我们的业务场景需求。所以，选择基于kafka开发一套数据采集网关，来完成数据采集需求。数据采集网关的开发过程中走了一些弯路，最后采用nginx+lua开发，基于lua实现了kafka生产者协议。有兴趣同学可以去Github上看看，另一同事实现的，现在在github上比较活跃，被一些互联网公司应用于线上环境了。 b.后端日志采集接入： FileCollect,考虑到很多线上环境的环境变量不能改动，为减少侵入式，目前是采用Go语言实现文件采集，年后也准备重构这块。 前端，服务端的数据采集整体架构如下图： c.业务数据接入 利用Canal通过MySQL的binlog机制实时同步业务增量数据。 数据统一接入：为了后面数据流环节的处理规范，所有的数据接入数据中心，必须通过数据采集网关转换统一上报给Kafka集群，避免后端多种接入方式的处理问题。 数据实时清洗(ETL)：为了减轻存储计算集群的资源压力及数据可重用性角度考虑，把数据解压、解密、转义，部分简单的补全，异常数据处理等工作前移到数据流中处理，为后面环节的数据重用打下扎实的基础(实时计算与离线计算)。 数据缓存重用：为了避免大量数据流(400+亿条/天)写入HDFS，导致HDFS客户端不稳定现象及数据实时性考虑，把经过数据实时清洗后的数据重新写入Kafka并保留一定周期，离线计算(批处理)通过KG-Camus拉到HDFS(通过作业调度系统配置相应的作业计划)，实时计算基于Storm/JStorm直接从Kafka消费，有很完美的解决方案storm-kafka组件。 离线计算(批处理)：通过spark，spark SQL实现，整体性能比hive提高5—10倍，hive脚本都在转换为Spark/Spark SQL；部分复杂的作业还是通过Hive/Spark的方式实现。在离线计算中大部分公司都会涉及到数据仓库的问题，酷狗音乐也不例外，也有数据仓库的概念，只是我们在做存储分层设计时弱化了数据仓库概念。数据存储分层模型如下图： 大数据平台数据存储模型分为：数据缓冲层Data Cache Layer（DCL）、数据明细层Data Detail Layer（DDL）、公共数据层（Common）、数据汇总层Data Summary Layer（DSL）、数据应用层Data Application Layer（DAL）、数据分析层（Analysis）、临时提数层（Temp）。 1）数据缓冲层(DCL)：存储业务系统或者客户端上报的，经过解码、清洗、转换后的原始数据，为数据过滤做准备。 2)数据明细层（DDL）：存储接口缓冲层数据经过过滤后的明细数据。 3）公共数据层（Common）：主要存储维表数据与外部业务系统数据。 4）数据汇总层（DSL）：存储对明细数据，按业务主题，与公共数据层数据进行管理后的用户行为主题数据、用户行为宽表数据、轻量汇总数据等。为数据应用层统计计算提供基础数据。数据汇总层的数据永久保存在集群中。 5）数据应用层（DAL）：存储运营分析（Operations Analysis ）、指标体系（Metrics System）、线上服务（Online Service）与用户分析（User Analysis）等。需要对外输出的数据都存储在这一层。主要基于热数据部分对外提供服务，通过一定周期的数据还需要到DSL层装载查询。 6）数据分析层（Analysis）：存储对数据明细层、公共数据层、数据汇总层关联后经过算法计算的、为推荐、广告、榜单等数据挖掘需求提供中间结果的数据。 7）临时提数层（Temp）：存储临时提数、数据质量校验等生产的临时数据。 实时计算：基于Storm/JStorm，Drools,Esper。主要应用于实时监控系统、APM、数据实时清洗平台、实时DAU统计等。 HBase/MySQL：用于实时计算，离线计算结果存储服务。 Redis：用于中间计算结果存储或字典数据等。 Elasticsearch：用于明细数据实时查询及HBase的二级索引存储(这块目前在数据中心还没有大规模使用，有兴趣的同学可以加入我们一起玩ES)。 Druid：目前用于支持大数据集的快速即席查询(ad-hoc)。 数据平台监控系统：数据平台监控系统包括基础平台监控系统与数据质量监控系统，数据平台监控系统分为2大方向，宏观层面和微观层面。宏观角度的理解就是进程级别,拓扑结构级别,拿Hadoop举例，如：DataNode，NameNode，JournalNode，ResourceManager，NodeManager，主要就是这5大组件，通过分析这些节点上的监控数据，一般你能够定位到慢节点，可能某台机器的网络出问题了，或者说某台机器执行的时间总是大于正常机器等等这样类似的问题。刚刚说的另一个监控方向，就是微观层面，就是细粒度化的监控，基于user用户级别，基于单个job，单个task级别的监控，像这类监控指标就是另一大方向，这类的监控指标在实际的使用场景中特别重要，一旦你的集群资源是开放给外面的用户使用，用户本身不了解你的这套机制原理，很容易会乱申请资源，造成严重拖垮集群整体运作效率的事情，所以这类监控的指标就是为了防止这样的事情发生。目前我们主要实现了宏观层面的监控。如：数据质量监控系统实现方案如下。 三、大数据平台重构过程中踩过的坑我们在大数据平台重构过程中踩过的坑，大致可以分为操作系统、架构设计、开源组件三类，下面主要列举些比较典型的，花时间比较长的问题。 1、操作系统级的坑 Hadoop的I/O性能很大程度上依赖于Linux本地文件系统的读写性能。Linux中有多种文件系统可供选择，比如ext3和ext4，不同的文件系统性能有一定的差别。我们主要想利用ext4文件系统的特性，由于之前的操作系统都是CentOS5.9不支持ext4文件格式，所以考虑操作系统升级为CentOS6.3版本，部署Hadoop集群后，作业一启动，就出现CPU内核过高的问题。如下图 经过很长时间的测试验证，发现CentOS6优化了内存申请的效率，引入了THP的特性，而Hadoop是高密集型内存运算系统，这个改动给hadoop带来了副作用。通过以下内核参数优化关闭系统THP特性，CPU内核使用率马上下降，如下图: echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/enabled echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag 2、架构设计的坑 最初的数据流架构是数据采集网关把数据上报给Kafka，再由数据实时清洗平台(ETL)做预处理后直接实时写入HDFS，如下图： 此架构，需要维持HDFS Client的长连接，由于网络等各种原因导致Storm实时写入HDFS经常不稳定，隔三差五的出现数据异常，使后面的计算结果异常不断，当时尝试过很多种手段去优化，如：保证长连接、连接断后重试机制、调整HDFS服务端参数等，都解决的不是彻底。 每天异常不断，旧异常没解决，新异常又来了，在压力山大的情况下，考虑从架构角度调整，不能只从具体的技术点去优化了，在做架构调整时，考虑到我们架构重构的初衷，提高数据的实时性，尽量让计算任务实时化，但重构过程中要考虑现有业务的过渡，所以架构必须支持实时与离线的需求，结合这些需求，在数据实时清洗平台(ETL)后加了一层数据缓存重用层(kafka)，也就是经过数据实时清洗平台后的数据还是写入kafka集群，由于kafka支持重复消费，所以同一份数据可以既满足实时计算也满足离线计算，从上面的整体技术架构也可以看出，如下图： KG-Camus组件也是基于架构调整后，重新实现了一套离线消费Kafka集群数据的组件，此组件是参考LinkedIn的Camus实现的。此方式，使数据消费模式由原来的推方式改为拉模式了，不用维持HDFS Client的长连接等功能了，直接由作业调度系统每隔时间去拉一次数据，不同的业务可以设置不同的时间间隔，从此架构调整上线后，基本没有类似的异常出现了。 这个坑，是我自己给自己挖的，导致我们的重构计划延期2个月，主要原因是由最初技术预研究测试不充分所导致。 3、开源组件的坑 由于整个数据平台涉及到的开源组件很多，踩过的坑也是十个手指数不过来。 1）、当我们的行为数据全量接入到Kafka集群(几百亿/天)，数据采集网卡出现大量连接超时现象，但万兆网卡进出流量使用率并不是很高，只有几百Mbit/s，经过大量的测试排查后，调整以下参数，就是顺利解决了此问题。调整参数后网卡流量如下图： a)、num.network.threads(网络处理线程数)值应该比cpu数略大 b)、num.io.threads(接收网络线程请求并处理线程数)值提高为cpu数两倍 2）、在hive0.14 版本中，利用函数ROW_NUMBER() OVER对数据进行数据处理后，导致大量的作业出现延时很大的现象，经异常排查后，发现在数据记录数没变的情况，数据的存储容量扩大到原来的5倍左右，导致MapReduce执行很慢造成的。改为自己实现类似的函数后，解决了容量扩大为原来几倍的现象。说到这里，也在此请教读到此处的读者一个问题，在海量数据去重中采用什么算法或组件进行比较合适，既能高性能又能高准确性，有好的建议或解决方案可以加happyjim2010微信私我。 3）、在业务实时监控系统中，用OpenTSDB与实时计算系统（storm）结合，用于聚合并存储实时metric数据。在这种实现中，通常需要在实时计算部分使用一个时间窗口（window），用于聚合实时数据，然后将聚合结果写入tsdb。但是，由于在实际情况中，实时数据在采集、上报阶段可能会存在延时，而导致tsdb写入的数据不准确。针对这个问题，我们做了一个改进，在原有tsdb写入api的基础上，增加了一个原子加api。这样，延迟到来的数据会被叠加到之前写入的数据之上，实时的准确性由于不可避免的原因（采集、上报阶段）产生了延迟，到最终的准确性也可以得到保证。另外，添加了这个改进之后，实时计算端的时间窗口就不需要因为考虑延迟问题设置得比较大，这样既节省了内存的消耗，也提高了实时性。 四、后续持续改进数据存储(分布式内存文件系统(Tachyon)、数据多介质分层存储、数据列式存储)、即席查询(OLAP)、资源隔离、数据安全、平台微观层面监控、数据对外服务等。 作者介绍王劲，目前就职酷狗音乐，大数据架构师，负责酷狗大数据技术规划、建设、应用。 11年的IT从业经验，2年分布式应用开发，3年大数据技术实践经验，主要研究方向流式计算、大数据存储计算、分布式存储系统、NoSQL、搜索引擎等。","raw":null,"content":null,"categories":[{"name":"大数据案例","slug":"大数据案例","permalink":"https://stanxia.github.io/categories/大数据案例/"}],"tags":[{"name":"大数据案例","slug":"大数据案例","permalink":"https://stanxia.github.io/tags/大数据案例/"}]},{"title":"CAD破解","slug":"CAD破解","date":"2017-01-26T13:29:49.000Z","updated":"2017-02-26T13:37:53.000Z","comments":true,"path":"2017/01/26/CAD破解/","link":"","permalink":"https://stanxia.github.io/2017/01/26/CAD破解/","excerpt":"","text":"介绍CAD制图软件pojie方式。 步骤点我看看原博 破解 运行破解包中的AutoCAD_2017_English_Win_64bit_dlm_001_002.sfx.exe进行解压安装Autodesk 2017. 安装完重启 运行Autodesk 2017 提示注册，选择“Enter a Serial Number” 使用一下序列号： 666-69696969, 667-98989898, 400-45454545或者066-66666666，产品密钥为001I1 断开网络，点击Next 会提示需要网络连接，选择离线激活（第二个选项） 重点步骤： 8.1 管理员身份运行注册机，点击Patch,此时会看到successfully patched； 8.2 拷贝请求码（Request cod））到注册机中，然后点击Generate，生成激活码（Activation）； 8.3 拷贝激活码（Activation）到软件注册窗口完成注册。 下载地址点我下载 密码: yuux Good Luck!","raw":null,"content":null,"categories":[{"name":"折腾","slug":"折腾","permalink":"https://stanxia.github.io/categories/折腾/"}],"tags":[{"name":"cad","slug":"cad","permalink":"https://stanxia.github.io/tags/cad/"}]},{"title":"linux防火墙开启特定端口访问","slug":"linux防火墙开启特定端口访问","date":"2016-05-22T06:56:40.000Z","updated":"2017-03-22T06:58:00.000Z","comments":true,"path":"2016/05/22/linux防火墙开启特定端口访问/","link":"","permalink":"https://stanxia.github.io/2016/05/22/linux防火墙开启特定端口访问/","excerpt":"","text":"linux开启和关闭端口外网访问的两种方式： 1、命令方式： 12/sbin/iptables -I INPUT -p tcp --dport 3306 -j ACCEPT #开启3306端口/sbin/iptables -A INPUT -p tcp --dport 3306 -j DROP #关闭端口 修改完时记得保存并重启服务 123/etc/rc.d/init.d/iptables save #保存配置 /etc/rc.d/init.d/iptables restart #重启服务 netstat -anp|grep 3306 #查看端口是否已经开放 2、编辑iptables文件，在22端口位置下面添加一行 1vi /etc/sysconfig/iptables 添加好之后如下所示： 123456789101112131415###################################### # Firewall configuration written by system-config-firewall # Manual customization of this file is not recommended. *filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [0:0] -A INPUT -m state –state ESTABLISHED,RELATED -j ACCEPT -A INPUT -p icmp -j ACCEPT -A INPUT -i lo -j ACCEPT -A INPUT -m state –state NEW -m tcp -p tcp –dport 22 -j ACCEPT -A INPUT -m state –state NEW -m tcp -p tcp –dport 3306 -j ACCEPT -A INPUT -j REJECT –reject-with icmp-host-prohibited -A FORWARD -j REJECT –reject-with icmp-host-prohibited COMMIT 最后重启防火墙使配置生效 1/etc/init.d/iptables restart","raw":null,"content":null,"categories":[{"name":"linux","slug":"linux","permalink":"https://stanxia.github.io/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://stanxia.github.io/tags/linux/"}]},{"title":"大数据工作手册","slug":"大数据工作手册","date":"2016-03-19T15:58:06.000Z","updated":"2017-03-19T15:58:50.000Z","comments":true,"path":"2016/03/19/大数据工作手册/","link":"","permalink":"https://stanxia.github.io/2016/03/19/大数据工作手册/","excerpt":"","text":"","raw":null,"content":null,"categories":[{"name":"大数据手册","slug":"大数据手册","permalink":"https://stanxia.github.io/categories/大数据手册/"}],"tags":[{"name":"handbook","slug":"handbook","permalink":"https://stanxia.github.io/tags/handbook/"}]},{"title":"git简单操作","slug":"git简单操作","date":"2016-03-15T15:50:47.000Z","updated":"2017-03-19T14:29:05.000Z","comments":true,"path":"2016/03/15/git简单操作/","link":"","permalink":"https://stanxia.github.io/2016/03/15/git简单操作/","excerpt":"","text":"认识Git就是一个版本控制软件。在进行软件开发时，一个团队的人靠使用Git，就能轻松管理好项目版本，做好项目的追踪和辅助进度控制。确切的讲，Git是一款分布式版本控制系统。这个“分布式”，要和“集中式”放在一起理解。 所谓“集中式版本控制”，就好比这一个团队中，版本库都集中在一台服务器上，每个开发者都要从服务器上获取最新的版本库后才能进行开发，开发完了再把新的版本提交回去。 而“分布式版本控制”，则是这个团队中每个人的电脑上都会有一份完整的版本库，这样，你工作的时候，就不需要联网了，因为版本库就在你自己的电脑上。既然每个人电脑上都有一个完整的版本库，那多个人如何协作呢？比方说你在自己电脑上改了文件A，你的同事也在他的电脑上改了文件A，这时，你们俩之间只需把各自的修改推送给对方，就可以互相看到对方的修改了。 和集中式版本控制系统相比，分布式版本控制系统的安全性要高很多，因为每个人电脑里都有完整的版本库，某一个人的电脑坏掉了不要紧，随便从其他人那里复制一个就可以了。而集中式版本控制系统的中央服务器要是出了问题，所有人都没法干活了。 在实际使用分布式版本控制系统的时候，其实很少在两人之间的电脑上推送版本库的修改，因为可能你们俩不在一个局域网内，两台电脑互相访问不了，也可能今天你的同事病了，他的电脑压根没有开机。因此，分布式版本控制系统通常也有一台充当“中央服务器”的电脑，但这个服务器的作用仅仅是用来方便“交换”大家的修改，没有它大家也一样干活，只是交换修改不方便而已。 名词解释1234Workspace：工作区Index / Stage：暂存区Repository：仓库区（或本地仓库）Remote：远程仓库 操作：新建代码库12345678#在当前目录新建一个Git代码库$ git init#新建一个目录，将其初始化为Git代码库$ git init [project-name]#下载一个项目和它的整个代码历史$ git clone [url] 配置Git的设置文件为.gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。 123456789# 显示当前的Git配置$ git config --list# 编辑Git配置文件$ git config -e [--global]# 设置提交代码时的用户信息$ git config [--global] user.name &quot;[name]&quot;$ git config [--global] user.email &quot;[email address]&quot; 增加/删除文件123456789101112131415161718192021# 添加指定文件到暂存区$ git add [file1] [file2] ...# 添加指定目录到暂存区，包括子目录$ git add [dir]# 添加当前目录的所有文件到暂存区$ git add .# 添加每个变化前，都会要求确认# 对于同一个文件的多处变化，可以实现分次提交$ git add -p# 删除工作区文件，并且将这次删除放入暂存区$ git rm [file1] [file2] ...# 停止追踪指定文件，但该文件会保留在工作区$ git rm --cached [file]# 改名文件，并且将这个改名放入暂存区$ git mv [file-original] [file-renamed] 代码提交123456789101112131415161718# 提交暂存区到仓库区$ git commit -m [message]# 提交暂存区的指定文件到仓库区$ git commit [file1] [file2] ... -m [message]# 提交工作区自上次commit之后的变化，直接到仓库区$ git commit -a# 提交时显示所有diff信息$ git commit -v# 使用一次新的commit，替代上一次提交# 如果代码没有任何新变化，则用来改写上一次commit的提交信息$ git commit --amend -m [message]# 重做上一次commit，并包括指定文件的新变化$ git commit --amend [file1] [file2] ... 分支123456789101112131415161718192021222324252627282930313233343536373839404142# 列出所有本地分支$ git branch# 列出所有远程分支$ git branch -r# 列出所有本地分支和远程分支$ git branch -a# 新建一个分支，但依然停留在当前分支$ git branch [branch-name]# 新建一个分支，并切换到该分支$ git checkout -b [branch]# 新建一个分支，指向指定commit$ git branch [branch] [commit]# 新建一个分支，与指定的远程分支建立追踪关系$ git branch --track [branch] [remote-branch]# 切换到指定分支，并更新工作区$ git checkout [branch-name]# 切换到上一个分支$ git checkout -# 建立追踪关系，在现有分支与指定的远程分支之间$ git branch --set-upstream [branch] [remote-branch]# 合并指定分支到当前分支$ git merge [branch]# 选择一个commit，合并进当前分支$ git cherry-pick [commit]# 删除分支$ git branch -d [branch-name]# 删除远程分支$ git push origin --delete [branch-name]$ git branch -dr [remote/branch] 标签1234567891011121314151617181920212223242526# 列出所有tag$ git tag# 新建一个tag在当前commit$ git tag [tag]# 新建一个tag在指定commit$ git tag [tag] [commit]# 删除本地tag$ git tag -d [tag]# 删除远程tag$ git push origin :refs/tags/[tagName]# 查看tag信息$ git show [tag]# 提交指定tag$ git push [remote] [tag]# 提交所有tag$ git push [remote] --tags# 新建一个分支，指向某个tag$ git checkout -b [branch] [tag] 查看信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 显示有变更的文件$ git status# 显示当前分支的版本历史$ git log# 显示commit历史，以及每次commit发生变更的文件$ git log --stat# 搜索提交历史，根据关键词$ git log -S [keyword]# 显示某个commit之后的所有变动，每个commit占据一行$ git log [tag] HEAD --pretty=format:%s# 显示某个commit之后的所有变动，其&quot;提交说明&quot;必须符合搜索条件$ git log [tag] HEAD --grep feature# 显示某个文件的版本历史，包括文件改名$ git log --follow [file]$ git whatchanged [file]# 显示指定文件相关的每一次diff$ git log -p [file]# 显示过去5次提交$ git log -5 --pretty --oneline# 显示所有提交过的用户，按提交次数排序$ git shortlog -sn# 显示指定文件是什么人在什么时间修改过$ git blame [file]# 显示暂存区和工作区的差异$ git diff# 显示暂存区和上一个commit的差异$ git diff --cached [file]# 显示工作区与当前分支最新commit之间的差异$ git diff HEAD# 显示两次提交之间的差异$ git diff [first-branch]...[second-branch]# 显示今天你写了多少行代码$ git diff --shortstat &quot;@&#123;0 day ago&#125;&quot;# 显示某次提交的元数据和内容变化$ git show [commit]# 显示某次提交发生变化的文件$ git show --name-only [commit]# 显示某次提交时，某个文件的内容$ git show [commit]:[filename]# 显示当前分支的最近几次提交$ git reflog 远程同步1234567891011121314151617181920212223# 下载远程仓库的所有变动$ git fetch [remote]# 显示所有远程仓库$ git remote -v# 显示某个远程仓库的信息$ git remote show [remote]# 增加一个新的远程仓库，并命名$ git remote add [shortname] [url]# 取回远程仓库的变化，并与本地分支合并$ git pull [remote] [branch]# 上传本地指定分支到远程仓库$ git push [remote] [branch]# 强行推送当前分支到远程仓库，即使有冲突$ git push [remote] --force# 推送所有分支到远程仓库$ git push [remote] --all 撤销12345678910111213141516171819202122232425262728293031# 恢复暂存区的指定文件到工作区$ git checkout [file]# 恢复某个commit的指定文件到暂存区和工作区$ git checkout [commit] [file]# 恢复暂存区的所有文件到工作区$ git checkout .# 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变$ git reset [file]# 重置暂存区与工作区，与上一次commit保持一致$ git reset --hard# 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变$ git reset [commit]# 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致$ git reset --hard [commit]# 重置当前HEAD为指定commit，但保持暂存区和工作区不变$ git reset --keep [commit]# 新建一个commit，用来撤销指定commit# 后者的所有变化都将被前者抵消，并且应用到当前分支$ git revert [commit]# 暂时将未提交的变化移除，稍后再移入$ git stash$ git stash pop 简单的应用示范在实际应用中，Git有非常多的用法，而本文是面向Git完全初学者，所以我们要从最基本的开始做。比如，在刚才建好的版本库中，A新建了README文件，并在里面写了东西。写好后他想给项目做个版本，就需要这样： 12$ git add README$ git commit -m &quot;add README&quot; 第一个命令是告诉Git要追踪什么文件，第二个命令是进行提交，并对此次提交做个简答说明。当然，今后他再对README做什么修改，都可以这样做。Git会自动为此次提交生成一个16进制的版本号。 如果此时他查看本地的版本库，就会发现最新的一次提交是在刚才，提交说明为：add README。 然后，他要把项目的版本库更新到GitCafe上，当然这时候项目本身已经在GitCafe上建立好了。他只需要： 1$ git push origin master 这行命令应该这样理解：A已经在本地把项目最新的版本做好了，他要发到GitCafe上，以便团队里其他人都能收到这个新的版本，于是他运行git push；push的目的地是origin，这其实是个名字，意义为该项目在GitCafe上的地址；推送的是本地的master分支。 这个时候，GitCafe上项目的版本号与A本地的最新版本号一致。 分支是版本控制里面的一个概念：在项目做大了之后，如果要在原基础上进行扩展开发，最好新建一个分支，以免影响原项目的正常维护，新的分支开发结束后再与原来的项目分支合并；而在一个项目刚开始的时候，大家一般会在同一个分支下进行开发.这是一种相对安全便捷的开发方式。 此时，小组里成员B对项目其他文件做了一些更改，同样也在本地做了一次提交，然后也想推到GitCafe上面。他运行了git push origin master命令，结果发现提交被拒绝。这要做如何解释？ 仔细想想，最开始的时候，A和B是在同一个版本号上做不同的更改，这就会分别衍生出两个不同的版本号。A先把自己的版本推到GitCafe上，此时GitCafe上的版本库与B本地版本库相比，差异很大，主要在于B这里没有A的版本记录，如果B这时把自己的版本强制同步到GitCafe上，就会把A的版本覆盖掉，这就出问题了。 所以B进行了如下操作： 1$ git pull 这样子，B先把GitCafe上的版本库和自己的版本库做一个合并，这个合并的意义在于：B通过GitCafe，把A刚才添加的版本加了进来，此时B本地的版本库是整个项目最新的，包括项目之前的版本、刚刚A添加的版本和B自己添加的版本。 这之后，B再次运行git push origin master，成功地把自己的版本推到了GitCafe上。如果A想要推送新的版本，也要像B之前这样折腾一番才行。","raw":null,"content":null,"categories":[{"name":"版本控制","slug":"版本控制","permalink":"https://stanxia.github.io/categories/版本控制/"}],"tags":[{"name":"git","slug":"git","permalink":"https://stanxia.github.io/tags/git/"}]},{"title":"mongodb操作指南","slug":"mongodb操作指南","date":"2016-03-15T15:15:33.000Z","updated":"2017-03-19T14:31:16.000Z","comments":true,"path":"2016/03/15/mongodb操作指南/","link":"","permalink":"https://stanxia.github.io/2016/03/15/mongodb操作指南/","excerpt":"","text":"什么是MongoDBMongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。在高负载的情况下，添加更多的节点，可以保证服务器性能。MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。MongoDB 将数据存储为一个文档，数据结构由键值(key=&gt;value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。 主要特点MongoDB的提供了一个面向文档存储，操作起来比较简单和容易。你可以在MongoDB记录中设置任何属性的索引 (如：FirstName=”Sameer”,Address=”8 Gandhi Road”)来实现更快的排序。你可以通过本地或者网络创建数据镜像，这使得MongoDB有更强的扩展性。如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是所谓的分片。Mongo支持丰富的查询表达式。查询指令使用JSON形式的标记，可轻易查询文档中内嵌的对象及数组。MongoDb 使用update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。Mongodb中的Map/reduce主要是用来对数据进行批量处理和聚合操作。Map和Reduce。Map函数调用emit(key,value)遍历集合中所有的记录，将key与value传给Reduce函数进行处理。Map函数和Reduce函数是使用Javascript编写的，并可以通过db.runCommand或mapreduce命令来执行MapReduce操作。GridFS是MongoDB中的一个内置功能，可以用于存放大量小文件。MongoDB允许在服务端执行脚本，可以用Javascript编写某个函数，直接在服务端执行，也可以把函数的定义存储在服务端，下次直接调用即可。MongoDB支持各种编程语言:RUBY，PYTHON，JAVA，C++，PHP，C#等多种语言。MongoDB安装简单。 概念介绍数据库 一个mongodb中可以建立多个数据库。 MongoDB的默认数据库为”db”，该数据库存储在data目录中。 MongoDB的单个实例可以容纳多个独立的数据库，每一个都有自己的集合和权限，不同的数据库也放置在不同的文件中。 “show dbs” 命令可以显示所有数据库的列表。 文档文档是一个键值(key-value)对(即BSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点。 注意： 文档中的键/值对是有序的。 文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。 MongoDB区分类型和大小写。 MongoDB的文档不能有重复的键。 文档的键是字符串。除了少数例外情况，键可以使用任意UTF-8字符。 文档键命名规范： 键不能含有\\0 (空字符)。这个字符用来表示键的结尾。 .和$有特别的意义，只有在特定环境下才能使用。 以下划线”_”开头的键是保留的(不是严格要求的)。 集合集合就是 MongoDB 文档组，类似于 RDBMS （关系数据库管理系统：Relational Database Management System)中的表格。集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性。 合法的集合名 集合名不能是空字符串””。 集合名不能含有\\0字符（空字符)，这个字符表示集合名的结尾。 集合名不能以”system.”开头，这是为系统集合保留的前缀。 用户创建的集合名字不能含有保留字符。有些驱动程序的确支持在集合名里面包含，这是因为某些系统生成的集合中包含该字符。除非你要访问这种系统创建的集合，否则千万不要在名字里出现$。 capped collectionsCapped collections 就是固定大小的collection。它有很高的性能以及队列过期的特性(过期按照插入的顺序). 有点和 “RRD” 概念类似。Capped collections是高性能自动的维护对象的插入顺序。它非常适合类似记录日志的功能 和标准的collection不同，你必须要显式的创建一个capped collection， 指定一个collection的大小，单位是字节。collection的数据存储空间值提前分配的。要注意的是指定的存储大小包含了数据库的头信息。 在capped collection中，你能添加新的对象。能进行更新，然而，对象不会增加存储空间。如果增加，更新就会失败 。数据库不允许进行删除。使用drop()方法删除collection所有的行。注意: 删除之后，你必须显式的重新创建这个collection。在32bit机器中，capped collection最大存储为1e9( 1X10^9)个字节。 操作插入文档：文档的数据结构和JSON基本一样。所有存储在集合中的数据都是BSON格式。BSON是一种类json的一种二进制形式的存储格式,简称Binary JSON。 MongoDB 使用 insert() 或 save() 方法向集合中插入文档。 12345678910111213141516171819202122232425262728293031323334353637383940414243插入文档: db.COLLECTION_NAME.insert(document)开启mongodb服务： $MONGO_HOME/bin/mongod查询当前所在数据库：db查询所有的数据库(不会显示没有数据的数据库)：show dbs使用数据库（如果没有数据库会自动创建）：use database_name 插入数据：db.database_name.insert(&#123;\"\":\"\"&#125;,&#123;&#125;,&#123;&#125;....)删除数据库：use database_namedb.dropDatabase()删除集合：db.collection.drop()显示集合：show tables插入文档(如果该集合不在该数据库中， MongoDB 会自动创建该集合并插入文档)：db.COLLECTION_NAME.insert(document)查看集合中的数据：db.col.find()使用变量：document=(&#123;\"\":\"\"&#125;.....)db.table1.insert(document)更新文档：update() 和 save() 方法db.col.update(&#123;原来的数据&#125;,&#123;$set:&#123;更新后的数据&#125;&#125;)以上语句只会修改第一条发现的文档，如果你要修改多条相同的文档，则需要设置 multi 参数为 true。db.col.update(&#123;原来的数据&#125;,&#123;$set:&#123;更新后的数据&#125;&#125;,&#123;multi:true&#125;)","raw":null,"content":null,"categories":[{"name":"数据库","slug":"数据库","permalink":"https://stanxia.github.io/categories/数据库/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://stanxia.github.io/tags/MongoDB/"}]},{"title":"hadoop 集群模式","slug":"hadoop-集群模式","date":"2016-03-14T02:29:38.000Z","updated":"2017-03-19T14:29:12.000Z","comments":true,"path":"2016/03/14/hadoop-集群模式/","link":"","permalink":"https://stanxia.github.io/2016/03/14/hadoop-集群模式/","excerpt":"","text":"单机模式Single Node Cluster，即伪分布式模式（单机模式），将hadoop安装在一台机器上，通过进程来模拟各主机节点的协作和运行，其可靠性、稳定性都是非常差的，并且具备糟糕的性能效率，没有团队会在生产环境使用它。那么它是否就没有用呢？也不是的，通常使用这种模式进行开发和调试工作。 完全分布式模式完全分布式模式(Full Distributed Cluster)，将hadoop部署在至少两台机子上，数据块副本的数量通常也设置为2以上。该模式的集群，无论规模多大，只拥有1台Namenode节点，且也是唯一Active的工作节点。Namenode（简称NN）相当于hadoop文件系统的管家，对集群的所有文件访问和操作都经由NN统一协调管理。可想，当集群规模越来越庞大时，仅有一台NN，必定是不堪重负，那么它很容易就会挂掉，一旦挂掉，不仅集群立即瘫痪，还很容易造成数据丢失。另外，该模式通常ResourceManager（RM）也仅部署1台，ResourceManager是yarn的管家，主要管理任务的执行，例如MapReduce任务。与NN类似，当集群提交的作业过于繁重时，其同样面临超负载的问题。那么此模式是否也无用武之地呢？也不是的，视业务、资金等情况而定，因为该模式日后也可以安全升级成高可用模式。 高可用模式高可用模式(HA Cluster)，一般来说，分为NN的高可用和RM的高可用。在完全分布式的基础上，增加备用NN和RM节点。NN高可用，也就是集群里面会部署两台NN（最多也只能两台），以形成主备NN节点，达到高可用的目的。RM高可用与NN高可用类似，也是在集群里部署备用RM节点。不过此种模式下集群里面依然只有一台NN/RM处于Active工作状态，另一台则处于Standby的等待状态。当Active的NN/RM出现问题无法工作时，Standby的那台则立即无缝切入，继续保障集群正常运转。这种模式是很多企业都使用的，但是依然有缺陷。什么缺陷呢？虽然集群的可用性问题解决了，但是性能瓶颈依然存在——仅有一台NN/RM，由于无法横向扩展，其很可能会超负载运行。 高可用联邦模式高可用联邦模式(HA + Federation Cluster)，解决了单纯HA模式的性能瓶颈。单纯的HA模式NN和RM之间虽然配置了HA，但是依旧仅有一台NN或RM同时运行，这可能会导致了NN或RM的负载过重，从而造成整个集群的性能瓶颈。而联邦模式将整个HA集群再划分为两个以上的集群，不同的集群之间通过Federation进行连接，不同集群间可以共享数据节点，也可以不共享，可以互相访问和操作数据，也可以不。这样便做到了HA集群的横向扩展，从而移除了单纯HA模式同时仅有1台NN/RM工作所带来的性能瓶颈。Federation模式，相当于在多个集群之上又构建了一个集群层次，从数据访问的角度看，也可以简单的将其理解为一台路由器，而每一个HA集群则是单独的网络，不同网络间通过Federation路由器进行沟通。此模式是目前hadoop生态中最高的一种模式，适用于规模较大的企业。","raw":null,"content":null,"categories":[{"name":"大数据","slug":"大数据","permalink":"https://stanxia.github.io/categories/大数据/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://stanxia.github.io/tags/hadoop/"}]},{"title":"Yarn的ResourceManager配置HA(2.7.1)","slug":"Yarn的ResourceManager配置HA-2-7-1","date":"2016-03-14T02:05:07.000Z","updated":"2017-03-19T14:31:56.000Z","comments":true,"path":"2016/03/14/Yarn的ResourceManager配置HA-2-7-1/","link":"","permalink":"https://stanxia.github.io/2016/03/14/Yarn的ResourceManager配置HA-2-7-1/","excerpt":"","text":"介绍Hadoop 2.4之前的版本，Yarn的ResourceManager是单点的。在Hadoop 2.4版本中，引入了ResourceManager HA。 ResourceManager是主备模式。 可以一个主用RM、一个备用RM。也可以是一个主用RM，多个备用RM。 客户端可以看到多个RM。客户端连接时，需要轮循各个RM，直到找到主用RM。 主备模式切换有两种模式： 自动切换 手工切换 配置在需要配置主备关系的两个节点的yarn-site.xml中，增加如下配置： 以ctrl、data01两个节点上配置主备RM为例。 本例为自动切换模式。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;cluster1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;ctrl&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;data01&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;ctrl:8088&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;data01:8088&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;data01:2181,data02:2181,data03:2181&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.zk-base-path&lt;/name&gt; &lt;value&gt;/yarn-leader-election&lt;/value&gt;&lt;description&gt;Optional setting. The default value is /yarn-leader-election&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;Enable automatic failover; By default, it is enabled only when HA is enabled.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt; &lt;value&gt;ctrl:8132&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt; &lt;value&gt;data01:8132&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt; &lt;value&gt;ctrl:8130&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt; &lt;value&gt;data01:8130&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm1&lt;/name&gt; &lt;value&gt;ctrl:8131&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm2&lt;/name&gt; &lt;value&gt;data01:8131&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;ctrl:8088&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;data01:8088&lt;/value&gt;&lt;/property&gt; 注意：如下配置项需要删除： 1234&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;ctrl,data01&lt;/value&gt;&lt;/property&gt; 启动在ctrl节点：通过start-yarn.sh启动ctrl节点上的ResourceManager以及各节点的NodeManager。 1start-yarn.sh 在data01节点：启用第二个ResourceManager： 12su - hadoopyarn-daemon.sh start resourcemanager 维护查询主备状态 12345$ yarn rmadmin -getServiceState rm1active$ yarn rmadmin -getServiceState rm2standby 对于手工切换模式： 12yarn rmadmin -transitionToActive rm1yarn rmadmin -transitionToStandby rm1 对于自动切换模式，可以强制手工切换： 12yarn rmadmin -transitionToActive rm1 --forcemanualyarn rmadmin -transitionToStandby rm1 --forcemanual Good Luck!","raw":null,"content":null,"categories":[{"name":"大数据","slug":"大数据","permalink":"https://stanxia.github.io/categories/大数据/"}],"tags":[{"name":"yarn","slug":"yarn","permalink":"https://stanxia.github.io/tags/yarn/"}]},{"title":"HDFS机架感知功能原理（rack awareness）","slug":"HDFS机架感知功能原理（rack-awareness）","date":"2016-03-13T16:07:40.000Z","updated":"2017-03-19T14:30:06.000Z","comments":true,"path":"2016/03/14/HDFS机架感知功能原理（rack-awareness）/","link":"","permalink":"https://stanxia.github.io/2016/03/14/HDFS机架感知功能原理（rack-awareness）/","excerpt":"","text":"机架感知HDFS NameNode对文件块复制相关所有事物负责，它周期性接受来自于DataNode的HeartBeat和BlockReport信息，HDFS文件块副本的放置对于系统整体的可靠性和性能有关键性影响。一个简单但非优化的副本放置策略是，把副本分别放在不同机架，甚至不同IDC。这样可以防止整个机架、甚至整个IDC崩溃带来的错误，但是这样文件写必须在多个机架之间、甚至IDC之间传输，增加了副本写的代价。在缺省配置下副本数是3个，通常的策略是：第一个副本放在和Client相同机架的Node里（如果Client不在集群范围，第一个Node是随机选取不太满或者不太忙的Node）；第二个副本放在与第一个Node不同的机架中的Node；第三个副本放在与第二个Node所在机架里不同的Node。Hadoop的副本放置策略在可靠性（副本在不同机架）和带宽（只需跨越一个机架）中做了一个很好的平衡。但是，HDFS如何知道各个DataNode的网络拓扑情况呢？它的机架感知功能需要 topology.script.file.name 属性定义的可执行文件（或者脚本）来实现，文件提供了NodeIP对应RackID的翻译。如果 topology.script.file.name 没有设定，则每个IP都会翻译成/default-rack。 开启机架感知默认情况下，Hadoop机架感知是没有启用的，需要在NameNode机器的hadoop-site.xml里配置一个选项，例如： 1234&lt;property&gt; &lt;name&gt;topology.script.file.name&lt;/name&gt; &lt;value&gt;/path/to/script&lt;/value&gt;&lt;/property&gt; 这个配置选项的value指定为一个可执行程序，通常为一个脚本，该脚本接受一个参数，输出一个值。接受的参数通常为datanode机器的ip地址，而输出的值通常为该ip地址对应的datanode所在的rackID，例如”/rack1”。Namenode启动时，会判断该配置选项是否为空，如果非空，则表示已经启用机架感知的配置，此时namenode会根据配置寻找该脚本，并在接收到每一个datanode的heartbeat时，将该datanode的ip地址作为参数传给该脚本运行，并将得到的输出作为该datanode所属的机架，保存到内存的一个map中。 至于脚本的编写，就需要将真实的网络拓朴和机架信息了解清楚后，通过该脚本能够将机器的ip地址正确的映射到相应的机架上去。Hadoop官方给出的脚本：http://wiki.apache.org/hadoop/topology_rack_awareness_scripts 测试没有开启机架感知当没有配置机架信息时，所有的机器hadoop都默认在同一个默认的机架下，名为 “/default-rack”，这种情况下，任何一台datanode机器，不管物理上是否属于同一个机架，都会被认为是在同一个机架下，此时，就很容易出现之前提到的增添机架间网络负载的情况。在没有机架信息的情况下，namenode默认将所有的slaves机器全部默认为在/default-rack下，此时写block时，三个datanode机器的选择完全是随机的。 开启机架感知当配置了机架感知信息以后，hadoop在选择三个datanode时，就会进行相应的判断： 如果上传本机不是一个datanode，而是一个客户端，那么就从所有slave机器中随机选择一台datanode作为第一个块的写入机器(datanode1)。而此时如果上传机器本身就是一个datanode，那么就将该datanode本身作为第一个块写入机器(datanode1)。 随后在datanode1所属的机架以外的另外的机架上，随机的选择一台，作为第二个block的写入datanode机器(datanode2)。 在写第三个block前，先判断是否前两个datanode是否是在同一个机架上，如果是在同一个机架，那么就尝试在另外一个机架上选择第三个datanode作为写入机器(datanode3)。而如果datanode1和datanode2没有在同一个机架上，则在datanode2所在的机架上选择一台datanode作为datanode3。 得到3个datanode的列表以后，从namenode返回该列表到DFSClient之前，会在namenode端首先根据该写入客户端跟datanode列表中每个datanode之间的“距离”由近到远进行一个排序，客户端根据这个顺序有近到远的进行数据块的写入。 当根据“距离”排好序的datanode节点列表返回给DFSClient以后，DFSClient便会创建Block OutputStream，并向这次block写入pipeline中的第一个节点（最近的节点）开始写入block数据。 写完第一个block以后，依次按照datanode列表中的次远的node进行写入，直到最后一个block写入成功，DFSClient返回成功，该block写入操作结束。 总结开启机架感知，namenode在选择数据块的写入datanode列表时，就充分考虑到了将block副本分散在不同机架下，并同时尽量地避免了之前描述的网络开销。 Good Luck!","raw":null,"content":null,"categories":[{"name":"大数据","slug":"大数据","permalink":"https://stanxia.github.io/categories/大数据/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://stanxia.github.io/tags/hadoop/"}]},{"title":"hadoop集群动态添加删除节点","slug":"hadoop集群动态添加删除节点","date":"2016-03-12T15:25:38.000Z","updated":"2017-03-19T14:29:21.000Z","comments":true,"path":"2016/03/12/hadoop集群动态添加删除节点/","link":"","permalink":"https://stanxia.github.io/2016/03/12/hadoop集群动态添加删除节点/","excerpt":"","text":"动态添加节点准备工作： 准备新节点的操作系统，安装好需要的软件 将hadoop的配置文件scp到新的节点上(复制hadoop目录的时候在新节点上清理下hadoop目录下的日志文件或者数据文件) 实现ssh无密码登录（ssh-copy-id命令实现，可以免去cp *.pub文件后的权限修改） 修改系统hostname（通过hostname和/etc/sysconfig/network进行修改） 修改hosts文件，将集群所有节点hosts配置进去（集群所有节点保持hosts文件统一） 1vi /etc/hosts #追加新添加的节点 修改主节点slave文件，添加新增节点的ip信息（集群重启时使用） 1vi $HADOOP_HOME/etc/hadoop/slaves #追加新添加的节点 添加DataNode 在新增的节点上，运行sbin/hadoop-daemon.sh start datanode即可 1$HADOOP_HOME/sbin/hadoop-daemon.sh start datanode 刷新 1hdfs dfsadmin -refreshNodes 然后在namenode通过hdfs dfsadmin -report查看集群情况 1hdfs dfsadmin -report 最后还需要对hdfs负载设置均衡，因为默认的数据传输带宽比较低，可以设置为64M，即hdfs dfsadmin -setBalancerBandwidth 67108864即可 1hdfs dfsadmin -setBalancerBandwidth 67108864 #设置带宽为 64M/S 默认balancer的threshold为10%，即各个节点与集群总的存储使用率相差不超过10%，我们可将其设置为5% 然后启动Balancer，sbin/start-balancer.sh -threshold 5，等待集群自均衡完成即可 1$HADOOP_HOME/sbin/start-balancer.sh -threshold 5 添加NodeManager 在新增节点，运行sbin/yarn-daemon.sh start nodemanager即可 1$HADOOP_HOME/sbin/yarn-daemon.sh start nodemanager 刷新 1yarn rmadmin -refreshNodes 在ResourceManager，通过yarn node -list查看集群情况 1yarn node -list 说明随时时间推移，各个datanode上的块分布来越来越不均衡，这将降低MR的本地性，导致部分datanode相对更加繁忙。balancer是一个hadoop守护进程，它将块从忙碌的datanode移动相对空闲的datanode，同时坚持块复本放置策略，将复本分散到不同的机器、机架。balancer会促使每个datanode的使用率与整个集群的使用率接近，这个“接近”是通过-threashold参数指定的，默认是10%。（本案例改为5%）不同节点之间复制数据的带宽是受限的，默认是1MB/s，可以通过hdfs-site.xml文件中的dfs.balance.bandwithPerSec属性指定（单位是字节）。（本案例改为64MB/S) 建议定期执行均衡器，如每天或者每周。 动态删除节点步骤 修改Master节点的hdfs-site.xml，增加dfs.hosts.exclude(排除,文本文件)参数 修改Master节点的yarn-site.xml，增加yarn.resourcemanager.nodes.exclude-path参数 修改Master节点的mapred-site.xml，增加mapreduce.jobtracker.hosts.exclude.filename 新建excludes文件，添加需删除的主机名 修改Master节点slaves文件，删除该节点，复制因子等.. 刷新: 12yarn rmadmin -refreshNodeshdfs dfsadmin -refreshNodes 查看状态 1hdfs dfsadmin -report ​","raw":null,"content":null,"categories":[{"name":"大数据","slug":"大数据","permalink":"https://stanxia.github.io/categories/大数据/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://stanxia.github.io/tags/hadoop/"}]},{"title":"几种常见的加密方式","slug":"几种常见的加密方式","date":"2016-03-08T14:36:33.000Z","updated":"2017-03-19T14:27:54.000Z","comments":true,"path":"2016/03/08/几种常见的加密方式/","link":"","permalink":"https://stanxia.github.io/2016/03/08/几种常见的加密方式/","excerpt":"","text":"数据加密(Data encryption)数据加密，是一门历史悠久的技术，指通过加密算法和加密密钥将明文转变为密文，而解密则是通过解密算法和解密密钥将密文恢复为明文。它的核心是密码学。 数据加密目前仍是计算机系统对信息进行保护的一种最可靠的办法。它利用密码技术对信息进行加密，实现信息隐蔽，从而起到保护信息的安全的作用。 Base64加密方式(可逆)什么是Base64Base64编码可以成为密码学的基石。可以将任意的二进制数据进行Base64编码。所有的数据都能被编码为并只用65个字符就能表示的文本文件。（ 65字符：A~Z a~z 0~9 + / = ）等号“=”用来作为后缀用途。编码后的数据~=编码前数据的4/3，会大1/3左右。 在bash中使用Base64:12base64 需要加密的文件 -o 加密后的文件 #加密过程base64 加密后的文件 -o 解密后的文件 -D #解密过程 Base64编码原理1234567891011a.将所有字符转化为ASCII码； b.将ASCII码转化为8位二进制； c.将二进制3个归成一组(不足3个在后边补0)共24位，再拆分成4组，每组6位； d.统一在6位二进制前补两个0凑足8位； e.将补0后的二进制转为十进制； f.从Base64编码表获取十进制对应的Base64编码； Base64编码的说明1234567891011a.转换的时候，将三个byte的数据，先后放入一个24bit的缓冲区中，先来的byte占高位。 b.数据不足3byte的话，于缓冲区中剩下的bit用0补足。然后，每次取出6个bit，按照其值选择查表选择对应的字符作为编码后的输出。 c.不断进行，直到全部输入数据转换完成。 d.如果最后剩下两个输入数据，在编码结果后加1个“=”； e.如果最后剩下一个输入数据，编码结果后加2个“=”； f.如果没有剩下任何数据，就什么都不要加，这样才可以保证资料还原的正确性。 散列函数加密（不可逆）散列函数：散列函数（或散列算法，又称哈希函数，英语：Hash Function）是一种从任何一种数据中创建小的数字“指纹”的方法。散列函数把消息或数据压缩成摘要，使得数据量变小，将数据的格式固定下来。该函数将数据打乱混合，重新创建一个叫做散列值（hash values，hash codes，hash sums，或hashes）的指纹。散列值通常用一个短的随机字母和数字组成的字符串来代表。好的散列函数在输入域中很少出现散列冲突。在散列表和数据处理中，不抑制冲突来区别数据，会使得数据库记录更难找到。 单向散列函数：如果两个散列值是不相同的（根据同一函数），那么这两个散列值的原始输入也是不相同的。这个特性是散列函数具有确定性的结果，具有这种性质的散列函数称为单向散列函数。 哈希碰撞：另一方面，散列函数的输入和输出不是唯一对应关系的，如果两个散列值相同，两个输入值很可能是相同的，但也可能不同，这种情况称为“哈希碰撞”，这通常是两个不同长度的输入值，刻意计算出相同的输出值。 密码散列函数：密码散列函数（英语：Cryptographic hash function），又译为加密散列函数、密码散列函数、加密散列函数，是散列函数的一种。它被认为是一种单向函数，也就是说极其难以由散列函数输出的结果，回推输入的数据是什么。这样的单向函数被称为“现代密码学的驮马”。[1]这种散列函数的输入数据，通常被称为消息（message），而它的输出结果，经常被称为消息摘要（message digest）或摘要（digest）。 一个理想的密码散列函数应该有四个主要的特性： 对于任何一个给定的消息，它都很容易就能运算出散列数值 难以由一个已知的散列数值，去推算出原始的消息 在不更动散列数值的前提下，修改消息内容是不可行的 对于两个不同的消息，它不能给与相同的散列数值 一个工作中的密码散列函数 (特定的, SHA-1)。注意，源输入再微小的变化（“over”这个词）也会使所产生的输出发生急剧变化，通过所谓的雪崩效应)的原理。 碰撞（计算机科学）在计算机科学中，碰撞或冲突是指两个不同的元素具有相同的哈希值，校验和，数字指纹时发生的情况。当数据量足够多（例如将所有可能的人名和计算机文件名映射到一段字符上）时，碰撞是不可避免的。这仅仅是鸽巢原理的一个实例。 消息认证码在密码学中，消息认证码（英语：Message authentication code，缩写为MAC），又译为消息鉴别码、文件消息认证码、讯息鉴别码、信息认证码，是经过特定算法后产生的一小段信息，检查某段消息的完整性，以及作身份验证。它可以用来检查在消息传递过程中，其内容是否被更改过，不管更改的原因是来自意外或是蓄意攻击。同时可以作为消息来源的身份验证，确认消息的来源。 消息认证码的算法中，通常会使用带密钥的散列函数（HMAC），或者块密码的带认证工作模式（如CBC-MAC）。 信息鉴别码不能提供对信息的保密，若要同时实现保密认证，同时需要对信息进行加密。 彩虹表彩虹表是一个用于加密散列函数逆运算的预先计算好的表，常用于破解加密过的密码散列。 查找表常常用于包含有限字符固定长度纯文本密码的加密。这是以空间换时间的典型实践，在每一次尝试都计算的暴力破解中使用更少的计算能力和更多的储存空间，但却比简单的每个输入一条散列的翻查表使用更少的储存空间和更多的计算性能。使用加盐)的KDF函数可以使这种攻击难以实现。 暴力破解法1. 暴力破解法暴力破解法，或称为穷举法，是一种密码分析的方法，即将密码进行逐个推算直到找出真正的密码为止。例如一个已知是四位并且全部由数字组成的密码，其可能共有10000种组合，因此最多尝试9999次就能找到正确的密码。理论上除了具有完善保密性的密码以外，利用这种方法可以破解任何一种密码，问题只在于如何缩短试误时间。有些人运用计算机来增加效率，有些人辅以字典来缩小密码组合的范围。 2. 防护手段最重要的手段是在构建系统时要将系统设计目标定为即便受到暴力破解的攻击也难以被攻破。以下列举了一些常用的防护手段： 增加密码的长度与复杂度 在系统中限制密码试错的次数 密码验证时，将验证结果不是立即返回而是延时若干秒后返回。 限制允许发起请求的客户端的范围 禁止密码输入频率过高的请求 将密码设置为类似安全令牌那样每隔一定时间就发生变化的形式 当同一来源的密码输入出错次数超过一定阈值，立即通过邮件/短信等方式通知系统管理员 人为监视系统，确认有无异常的密码试错。 MD5加密(32位散列值)MD5：MD5消息摘要算法（英语：MD5 Message-Digest Algorithm），一种被广泛使用的密码散列函数，可以产生出一个128位（16字节）的散列值（hash value），用于确保信息传输完整一致。MD5由罗纳德·李维斯特设计，于1992年公开，用以替换MD4算法。这套算法的程序在 RFC 1321 中被加以规范。 1996年后被证实存在弱点，可以被加以破解，对于需要高度安全性的数据，专家一般建议改用其他算法，如SHA-1。2004年，证实MD5算法无法防止碰撞，因此无法适用于安全性认证，如SSL公开密钥认证或是数字签名等用途。 应用：MD5已经广泛使用在为文件传输提供一定的可靠性方面。例如，服务器预先提供一个MD5校验和，用户下载完文件以后，用MD5算法计算下载文件的MD5校验和，然后通过检查这两个校验和是否一致，就能判断下载的文件是否出错。如在一些BitTorrent下载中，软件将通过计算MD5检验下载到的文件片段的完整性。 MD5亦有应用于部分网上赌场以保证赌博的公平性，原理是系统先在玩家下注前已生成该局的结果，将该结果的字符串配合一组随机字符串利用MD5 加密，将该加密字符串于玩家下注前便显示给玩家，再在结果开出后将未加密的字符串显示给玩家，玩家便可利用MD5工具加密验证该字符串是否吻合。 例子: 在玩家下注骰宝前，赌场便先决定该局结果，假设生成的随机结果为4、5、 6大，赌场便会先利用MD5 加密“4, 5, 6”此字符串并于玩家下注前告诉玩家；由于赌场是无法预计玩家会下什么注，所以便能确保赌场不能作弊；当玩家下注完毕后，赌场便告诉玩家该原始字符串，即“4, 5, 6”，玩家便可利用MD5工具加密该字符串是否与下注前的加密字符串吻合。 该字符串一般会加上一组随机字符串 (Random string)，以防止玩家利用碰撞 (Collision) 解密字符串，但如使用超级电脑利用碰撞亦有可能从加上随机字符串的加密字符串中获取游戏结果。随机字符串的长度与碰撞的次数成正比关系，一般网上赌场使用的随机字符串是长于20字，有些网上赌场的随机字符串更长达500字，以增加解密难度。 算法：MD5是输入不定长度信息，输出固定长度128-bits的算法。经过程序流程，生成四个32位数据，最后联合起来成为一个128-bits散列。基本方式为，求余、取余、调整长度、与链接变量进行循环运算。得出结果。 特点： 压缩性：任意长度的数据，算出的MD5值长度都是固定的(32位)。 容易计算：从原数据计算出MD5值很容易。 抗修改性：对原数据进行任何改动，哪怕只修改1个字节，所得到的MD5值都有很大区别。 强抗碰撞：已知原数据和其MD5值，想找到一个具有相同MD5值的数据（即伪造数据）是非常困难的（但也会被碰撞）。 bash中使用MD512md5 -s \"需要md5加密的字符串\" #md5加密字符串md5 文件 #md5加密文件 缺陷：2009年谢涛和冯登国仅用了220.96的碰撞算法复杂度，破解了MD5的碰撞抵抗，该攻击在普通计算机上运行只需要数秒钟。 盐(密码学)加盐：盐（Salt），在密码学中，是指通过在密码任意固定位置插入特定的字符串，让散列后的结果和使用原始密码的散列结果不相符，这种过程称之为“加盐”。 为什么加盐：通常情况下，当字段经过散列处理（如MD5），会生成一段散列值，而散列后的值一般是无法通过特定算法得到原始字段的。但是某些情况，比如一个大型的彩虹表，通过在表中搜索该MD5值，很有可能在极短的时间内找到该散列值对应的真实字段内容。 加盐后的散列值，可以极大的降低由于用户数据被盗而带来的密码泄漏风险，即使通过彩虹表寻找到了散列后的数值所对应的原始内容，但是由于经过了加盐，插入的字符串扰乱了真正的密码，使得获得真实密码的概率大大降低。 实现原理：加盐的实现过程通常是在需要散列的字段的特定位置增加特定的字符，打乱原始的字符串，使其生成的散列结果产生变化。比如，用户使用了一个密码： 1x7faqgjw 经过MD5散列后，可以得出结果： 1455e0e5c2bc109deae749e7ce0cdd397 但是由于用户密码位数不足，短密码的散列结果很容易被彩虹表破解，因此，在用户的密码末尾添加特定字符串（括号内的字体为加盐的字段）： 1x7faqgjw(abcdefghijklmnopqrstuvwxyz) 因此，加盐后的密码位数更长了，散列的结果也发生了变化： 14a1690d5eb6c126ef68606dda68c2f79 以上就是加盐过程的简单描述，在实际使用过程中，还需要通过特定位数插入、倒序或多种方法对原始密码进行固定的加盐处理，使得散列的结果更加不容易被破解或轻易得到原始密码，比如（括号内的字体为加盐字符串）： 1x7(a)fa(b)qg(c)jw 十六进制十六进制（简写为hex或下标16）在数学中是一种逢16进1的进位制。一般用数字0到9和字母A到F（或a~f）表示，其中:A~F表示10~15，这些称作十六进制数字。 SHA-1（40位散列值）SHA-1（英语：Secure Hash Algorithm 1，中文名：安全散列算法1）是一种密码散列函数，美国国家安全局设计，并由美国国家标准技术研究所（NIST）发布为联邦数据处理标准（FIPS）[2]。SHA-1可以生成一个被称为消息摘要的160位（20字节）散列值，散列值通常的呈现形式为40个十六进制数。(现已经被破解，可碰撞) SHA-2特点： 安全，目前为止没有被成功碰撞 快速 应用：SHA-2 + 盐值 共同用于密码的保存。 java实现：通过org.apache.commons.codec.digest.DigestUtils类实现。直接调用静态加密方法。 12345import org.apache.commons.codec.digest.DigestUtils; //导入包String md5=DigestUtils.md5Hex(\"需要加密的数据\"); //md5加密（32位散列值）String sha256=DigestUtils.sha256Hex(\"需要加密的数据\"); //sha256加密（64位）String sha384=DigestUtils.sha384Hex(\"需要加密的数据\"); //sha384加密（96位）String sha512=DigestUtils.sha512Hex(\"需要加密的数据\"); //sha512加密（128位） 钥匙串加密方式iCloud钥匙串,苹果给我们提供的密码保存的解决方案,iOS7之后有的 存沙盒： 1、如果手机越狱，密码容易被窃取。 2、当软件更新时，沙盒里的内容是不被删除的。但是，如果将软件卸载后重装，沙盒里的数据就没有了。 3、每个APP的沙盒是相对独立的，密码无法共用。 存钥匙串里： 1、苹果提供的安全方案，rsa加密，相对安全。 2、无论软件更新或删除，密码都存在，都可以自动登录。 3、同一公司的APP密码是可以共用的。 对称加密算法（可逆）优点：算法公开、计算量小、加密速度快、加密效率高、可逆 缺点：双方使用相同钥匙，安全性得不到保证 现状：对称加密的速度比公钥加密快很多，在很多场合都需要对称加密， 算法: 在对称加密算法中常用的算法有：DES、3DES、TDEA、Blowfish、RC2、RC4、RC5、IDEA、SKIPJACK、AES等。不同算法的实现机制不同，可参考对应算法的详细资料 相较于DES和3DES算法而言，AES算法有着更高的速度和资源使用效率，安全级别也较之更高了，被称为下一代加密标准 nECB ：电子代码本，就是说每个块都是独立加密的 nCBC ：密码块链，使用一个密钥和一个初始化向量 (IV)对数据执行加密转换 ECB和CBC区别：CBC更加复杂更加安全，里面加入了8位的向量（8个0的话结果等于ECB）。在明文里面改一个字母，ECB密文对应的那一行会改变，CBC密文从那一行往后都会改变。 DES(Data Encryption Standard) 对称加密算法DES算法的入口参数有三个：Key、Data、Mode。 Key为8个字节共64位，是DES算法的工作密钥； Data也为8个字节64位，是要被加密或被解密的数据； Mode为DES的工作方式，有两种：加密或解密。 应用：为了网络上信息传输的安全（防止第三方窃取信息看到明文），发送发和接收方分别进行加密和解密，这样信息在网络上传输(比如银行卡号)的时候就是相对安全的。 java实现DES:123456789101112131415161718192021222324252627282930313233343536373839import org.apache.xerces.impl.dv.util.Base64;import java.security.NoSuchAlgorithmException;import java.security.SecureRandom;import javax.crypto.SecretKey;import javax.crypto.Cipher;import javax.crypto.spec.SecretKeySpec;/** * DES加密介绍 DES是一种对称加密算法，所谓对称加密算法即：加密和解密使用相同密钥的算法。DES加密算法出自IBM的研究， * 后来被美国政府正式采用，之后开始广泛流传，但是近些年使用越来越少，因为DES使用56位密钥，以现代计算能力， * 24小时内即可被破解。虽然如此，在某些简单应用中，我们还是可以使用DES加密算法，本文简单讲解DES的JAVA实现 。 * 注意：DES加密和解密过程中，密钥长度都必须是8 */public class DES &#123; private static final String ALGORITHM=\"DES\"; public static void main(String args[]) throws Exception, NoSuchAlgorithmException &#123; // data 待加密内容 String str = \"3456789567890\"; // key 密匙，DES的密钥长度只能是8字节 String password = \"12345678\"; // DES算法要求有一个可信任的随机数源 SecureRandom random = new SecureRandom(); SecretKey securekey = new SecretKeySpec(password.getBytes(), ALGORITHM); // Cipher对象实际完成加密操作 Cipher cipher = Cipher.getInstance(ALGORITHM); // 用密匙初始化Cipher对象 cipher.init(Cipher.ENCRYPT_MODE, securekey, random); // 现在，获取数据并加密 // 正式执行加密操作 byte[] result = cipher.doFinal(str.getBytes()); //转换为String传输 String text = Base64.encode(result); System.out.println(\"暗文：\" + text); //解密 cipher.init(Cipher.DECRYPT_MODE, securekey, random); byte[] de = cipher.doFinal(Base64.decode(text)); System.out.println(\"明文\" + new String(de)); &#125; AES(Advanced Encryption Standard)介绍：这个标准用来替代原先的DES，已经被多方分析且广为全世界所使用。 AES的区块长度固定为128比特，密钥长度则可以是128，192或256比特，对应的也就是16字节、24字节、32字节。 特点：相对安全，推荐使用AES替代DES. java实现AES:123456789101112131415161718192021222324252627282930313233343536373839import org.apache.xerces.impl.dv.util.Base64;import java.security.NoSuchAlgorithmException;import java.security.SecureRandom;import javax.crypto.SecretKey;import javax.crypto.Cipher;import javax.crypto.spec.SecretKeySpec;/** * 这个标准用来替代原先的DES，已经被多方分析且广为全世界所使用。 * AES的区块长度固定为128比特，密钥长度则可以是128，192或256比特，对应的也就是16字节、24字节、32字节。 */public class AES &#123; private static final String ALGORITHM=\"AES\"; // 测试 public static void main(String args[]) throws Exception, NoSuchAlgorithmException &#123; // data 待加密内容 String str = \"3456789567890\"; // key 密匙，AES的密钥长度只能是16字节、24字节、32字节 String password = \"1234567891111111\"; // AES算法要求有一个可信任的随机数源 SecureRandom random = new SecureRandom(); SecretKey securekey = new SecretKeySpec(password.getBytes(), ALGORITHM); // Cipher对象实际完成加密操作 Cipher cipher = Cipher.getInstance(ALGORITHM); // 用密匙初始化Cipher对象 cipher.init(Cipher.ENCRYPT_MODE, securekey, random); // 现在，获取数据并加密 // 正式执行加密操作 byte[] result = cipher.doFinal(str.getBytes()); //转换为String传输 String text = Base64.encode(result); System.out.println(\"暗文：\" + text); //解密 cipher.init(Cipher.DECRYPT_MODE, securekey, random); byte[] de = cipher.doFinal(Base64.decode(text)); System.out.println(\"明文\" + new String(de)); &#125; 非对称加密算法(Secruty.framework系统库)（可逆）非对称加密算法需要两个密钥：公开密钥（publickey）和私有密钥（privatekey） 非对称加密中使用的主要算法有：RSA、Elgamal、背包算法、Rabin、D-H、ECC（椭圆曲线加密算法）等。 公开密钥与私有密钥是一对，如果用公开密钥对数据进行加密，只有用对应的私有密钥才能解密；如果用私有密钥对数据进行加密，那么只有用对应的公开密钥才能解密 特点： 非对称密码体制的特点：算法强度复杂、安全性依赖于算法与密钥但是由于其算法复杂，而使得加密解密速度没有对称加密解密的速度快 对称密码体制中只有一种密钥，并且是非公开的，如果要解密就得让对方知道密钥。所以保证其安全性就是保证密钥的安全，而非对称密钥体制有两种密钥，其中一个是公开的，这样就可以不需要像对称密码那样传输对方的密钥了 但是RSA加密算法效率较差，对大型数据加密时间很长，一般用于小数据。","raw":null,"content":null,"categories":[{"name":"技术向","slug":"技术向","permalink":"https://stanxia.github.io/categories/技术向/"}],"tags":[{"name":"加密","slug":"加密","permalink":"https://stanxia.github.io/tags/加密/"}]},{"title":"hadoop手动合并小文件","slug":"hadoop手动合并小文件","date":"2016-03-06T15:11:30.000Z","updated":"2017-03-19T14:29:29.000Z","comments":true,"path":"2016/03/06/hadoop手动合并小文件/","link":"","permalink":"https://stanxia.github.io/2016/03/06/hadoop手动合并小文件/","excerpt":"","text":"前提下载主要jar包：filecrush-2.0-SNAPSHOT.jar密码: x9mh 执行1234567891011Hadoop jar filecrush-2.0-SNAPSHOT.jar crush.Crush \\-Ddfs.block.size=134217728 \\--input-format=text \\--output-format=text \\--compress=none \\/要合并的目录 /合并到哪里去 时间戳(20170221175612) Good Luck!","raw":null,"content":null,"categories":[{"name":"大数据","slug":"大数据","permalink":"https://stanxia.github.io/categories/大数据/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://stanxia.github.io/tags/hadoop/"}]},{"title":"Spark Streaming 报错:kafka.cluster.BrokerEndPoint cannot be cast to kafka.cluster.Broker'","slug":"Spark-Streaming-报错-kafka-cluster-BrokerEndPoint-cannot-be-cast-to-kafka-cluster-Broker","date":"2016-03-02T08:16:00.000Z","updated":"2017-03-19T14:31:38.000Z","comments":true,"path":"2016/03/02/Spark-Streaming-报错-kafka-cluster-BrokerEndPoint-cannot-be-cast-to-kafka-cluster-Broker/","link":"","permalink":"https://stanxia.github.io/2016/03/02/Spark-Streaming-报错-kafka-cluster-BrokerEndPoint-cannot-be-cast-to-kafka-cluster-Broker/","excerpt":"","text":"问题Spark Streaming 连接kafka报错：kafka.cluster.BrokerEndPoint cannot be cast to kafka.cluster.Broker 解决方案Spark Streaming默认使用的是Kafka 0.8.2.1，将kafka的版本改为0.8.2.1 Good Luck!","raw":null,"content":null,"categories":[{"name":"问题集锦","slug":"问题集锦","permalink":"https://stanxia.github.io/categories/问题集锦/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"}]},{"title":"spark stream无法收取到kafka生产者的消息","slug":"spark-stream-and","date":"2016-03-02T07:26:57.000Z","updated":"2017-03-19T14:31:33.000Z","comments":true,"path":"2016/03/02/spark-stream-and/","link":"","permalink":"https://stanxia.github.io/2016/03/02/spark-stream-and/","excerpt":"","text":"问题spark stream无法收取到kafka发过来的消息 解决方案找到集群中 kafka的server.properties，修改如下： vi server.properties 12listeners=PLAINTEXT://slave2xls:9092 #改为本主机的ip：portport=9092","raw":null,"content":null,"categories":[{"name":"问题集锦","slug":"问题集锦","permalink":"https://stanxia.github.io/categories/问题集锦/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"}]},{"title":"JVM理解","slug":"JVM理解","date":"2016-03-01T06:06:57.000Z","updated":"2017-03-19T14:30:38.000Z","comments":true,"path":"2016/03/01/JVM理解/","link":"","permalink":"https://stanxia.github.io/2016/03/01/JVM理解/","excerpt":"","text":"什么是jvmjvm（java virtual machine）,java虚拟机，就是在计算机的内存中再虚拟出个计算机。为什么java一次编译，随处运行？都归功于jvm的命令集，jvm翻译之后，根据不同的cpu，翻译成不同的机器语言。 jvm的组成部分 Class Loader 类加载器将javac 编译后的 .class文件加载到内存中。（注意：并不会加载所有的.class文件，而是只加载符合class规范的文件。可阅读中的第四章”The Class File Format”） Execution Engine 执行引擎也叫做解释器(Interpreter)，负责解释命令，提交操作系统执行。 Native Interface 本地接口本地接口的作用是融合不同的语言为java所用。 Runtime Data Area 运行数据区所有程序被加载到运行数据区域才能运行。 jvm 的内存管理所有的程序都是被加载到运行数据区域才能执行。运行数据区主要包括： Stack 栈栈也叫做栈内存，与线程同生死。线程创建时创建，线程结束时自动释放栈内存，不需要GC。栈的原则：先进后出。栈中存放的数据格式：栈帧(Stack Frame)栈帧：方法和运行期数据的数据集栈帧包括：a. 本地变量(local variables)，包括输入，输出参数以及方法内的变量b. 栈操作(Operand Stack)，记录进出栈的操作c. 栈帧数据(Frame Data),包括类文件，方法等 ​ java 栈结构图 图示：栈中有两个栈帧，栈帧2是最先被调用的方法，先入栈。方法2又调用方法1，栈帧1入栈，且位于栈顶，栈帧2位于栈底。执行完成后，先弹出栈帧1，再弹出栈帧2.线程结束，释放栈。 Heap 堆内存一个jvm只有一个堆内存，大小可调节。类加载器加载了类文件后，需要把类，方法，常变量放到堆内存中，以方便解释器执行。堆内存结构：a. Permanent Space永久存储区永久存储区是一个常驻内存区域。用于存放jdk自身所携带的Class,Interface的元数据，即存储的是运行环境所必须的类信息，该区域中的数据不会被GC回收，只有关闭jvm才会释放该区域所占内存。b. Young Genaration Space 新生区新生区是类的诞生，成长，消亡的区域。一个类在这里产生，应用，最后被GC收回。新生区分为两个区：伊甸区(Eden Space)和幸存者区(Survivor space)。所有类都在伊甸区被new出来的。幸存区有两个：0区(Survivor 0 space)和1区(Survivor 1 space)。当伊甸区的空间用完时，程序有需要new新的类，这是GC将对伊甸区进行回收，将伊甸区中不再被引用的对象进行销毁，然后将伊甸区中剩余的对象移动到幸存0区。若幸存0区也满了，在对该区域进行垃圾回收，然后将剩余的移动到1区。如果1区也满了，再移动到养老区。c. Tenure Generation Space 养老区养老区用于保存被新生区筛选出来的对象。一般池对象都保存在这个区。 Method Area 方法区方法区是被所有线程共享，该区域保存所有字段和方法字节码，以及一些特殊方法如 构造函数，接口代码也在此定义。 Program Counter Register 程序计数器每个线程都有一个程序计数器，就是一个指针，指向方法区中的方法字节码，由执行引擎读取下一条指令。 Native Method Stack 本地方法栈 JVM相关问题堆和栈的区别？ 堆中存放对象，对象内的临时变量存放在栈中。 栈随线程同生死，堆随 jvm同生死。 堆内存存放什么？对象，包括对象变量以及对象方法。 类变量和实例变量的区别？ 静态变量是类变量，非静态变量是实例变量。 静态变量存放在方法区中，实例变量存放在堆内存中。 为什么产生OutOfMemory?Heap堆内存中没有足够的内存可用。新申请的内存大于堆中的空闲内存。 产生的对象不多，为什么也会出现OutOfMemory?继承的层次太多，Heap堆内存中产生对象是先产生父类，然后才产生子类。","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"https://stanxia.github.io/categories/java/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://stanxia.github.io/tags/JVM/"}]},{"title":"IDE关闭Spark执行日志","slug":"IDE关闭Spark执行日志","date":"2016-02-28T03:25:24.000Z","updated":"2017-03-19T14:30:18.000Z","comments":true,"path":"2016/02/28/IDE关闭Spark执行日志/","link":"","permalink":"https://stanxia.github.io/2016/02/28/IDE关闭Spark执行日志/","excerpt":"","text":"引言用IDE执行spark任务会出现很多的日志信息，当我们不需要看这些日志信息的时候该怎么整，如何优雅的搞掉这些日志，看看下面的，或许对你有帮助。 内容添加以下代码： 123456789import org.apache.log4j.&#123;Level, Logger&#125;//设置spark的日志级别为 warn，才会打印日志 Logger.getLogger(\"org.apache.spark\").setLevel(Level.WARN)//直接关闭 jetty日志 Logger.getLogger(\"org.eclipse.jetty.server\").setLevel(Level.OFF)//直接关闭 spark的运行日志 SparkContext().setLogLevel(\"OFF\") Good Luck!","raw":null,"content":null,"categories":[{"name":"大数据","slug":"大数据","permalink":"https://stanxia.github.io/categories/大数据/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"}]},{"title":"Spark 包冲突error class javax.servlet.FilterRegistration","slug":"Spark-包冲突error-class-javax-servlet-FilterRegistration","date":"2016-02-27T12:04:37.000Z","updated":"2017-03-19T14:31:27.000Z","comments":true,"path":"2016/02/27/Spark-包冲突error-class-javax-servlet-FilterRegistration/","link":"","permalink":"https://stanxia.github.io/2016/02/27/Spark-包冲突error-class-javax-servlet-FilterRegistration/","excerpt":"","text":"问题描述在idea上运行spark程序时，出现以下信息： Spark error class “javax.servlet.FilterRegistration”‘s signer information does not match signer information of other classes in the same package 如图： 看了一圈网上的答案，应该是包冲突，试过了各种方法，终于找到了一种看似很莫名其妙的答案，但却是非常有效。 解决方案右键模块项目==&gt;Open Module Settings ==&gt; 选择Dependencies==&gt;找到javax.servlet:servlet-api:xx==&gt;移动到列表的最末端，如下图： Apply==&gt;Ok==&gt;运行试试！ 总结这个问题的解决方案很奇怪，以后明白了再回来说明下。 Good Luck!","raw":null,"content":null,"categories":[{"name":"问题集锦","slug":"问题集锦","permalink":"https://stanxia.github.io/categories/问题集锦/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"}]},{"title":"spark集群搭建","slug":"spark集群搭建","date":"2016-02-27T07:50:44.000Z","updated":"2017-03-19T14:31:44.000Z","comments":true,"path":"2016/02/27/spark集群搭建/","link":"","permalink":"https://stanxia.github.io/2016/02/27/spark集群搭建/","excerpt":"","text":"前置环境 需要jdk 需要hadoop 下载spark 点我下载spark 安装spark 安装spark 12tar -zxvf spark-1.6.0-bin-hadoop2.6.tgz -C /opt/ #解压到/opt/mv spark-1.6.0-bin-hadoop2.6 spark #重命名 配置环境变量 vi /etc/profile #配置环境变量 12export SPARK_HOME=/opt/sparkexport PATH=$PATH:$SPARK_HOME/bin ​ source /etc/profile 配置生效。 配置spark-env.sh 1234567cp spark-env.sh.template spark-env.sh vi spark-env.sh #修改spark-env.sh#添加如下配置：export JAVA_HOME=/opt/jdk1.8/export SCALA_HOME=/opt/scalaexport SPARK_MASTER_IP=masterexport SPARK_WORKER_MEMORY=4G #worker内存 可随意设置 配置slaves文件 123456cd /opt/spark/conf/ cp slaves.template slavesvi slaves#修改slaves,添加集群中的worker节点slave1slave2 复制spark文件夹到集群中的所有节点 1scp -r /opt/spark hadoop@slave1:/opt/ #复制到其他节点 配置所有节点上的spark环境变量 启动运行 在启动spark之前，先开启hadoop服务 启动脚本都放在${SPARK_HOME}/sbin/下面 1/opt/spark/sbin/start-all.sh #开启spark服务 测试 进入spark操作界面 1spark-shell 跑一个spark自带任务看看 1$SPARK_HOME/bin/run-example SparkPi 检查页面： 12ip:8080ip:4040 #需要进入到spark环境才会有这个页面 Good Luck!","raw":null,"content":null,"categories":[{"name":"集群搭建","slug":"集群搭建","permalink":"https://stanxia.github.io/categories/集群搭建/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"}]},{"title":" IntelliJ IDEA 破解之道（尽量还是支持正版。。）","slug":"IntelliJ-IDEA-破解之道（尽量还是支持正版。。）","date":"2016-02-27T06:04:41.000Z","updated":"2017-03-19T14:30:26.000Z","comments":true,"path":"2016/02/27/IntelliJ-IDEA-破解之道（尽量还是支持正版。。）/","link":"","permalink":"https://stanxia.github.io/2016/02/27/IntelliJ-IDEA-破解之道（尽量还是支持正版。。）/","excerpt":"","text":"话说IntelliJ IDEA这东西做的确实不错，但是对于一般人来说可能是有点不舍得花钱的，有钱还是建议买正版，这里也给出一些皮姐的方式。。。： 适用于 IntelliJ IDEA (v2016.3.4) 开始安装 选择License server 将这个网址添加进去： http://jetbrains.tech 确认 试试行不行 如果是IntelliJ IDEA v15.0.2 使用http://idea.lanyus.com，如果不行，可换为http://nfsgkyi.nrqw46lvomxgg33n.dresk.ru 一手资料消息来源 Good Luck!","raw":null,"content":null,"categories":[{"name":"折腾","slug":"折腾","permalink":"https://stanxia.github.io/categories/折腾/"}],"tags":[{"name":"IDEA","slug":"IDEA","permalink":"https://stanxia.github.io/tags/IDEA/"}]},{"title":"hadoop运行job时卡住在INFO mapreduce.Job: Running job:job _1456252725626_0001","slug":"hadoop运行job时卡住在INFO-mapreduce-Job-Running-job-job-1456252725626-0001","date":"2016-02-25T06:30:49.000Z","updated":"2017-03-19T14:29:51.000Z","comments":true,"path":"2016/02/25/hadoop运行job时卡住在INFO-mapreduce-Job-Running-job-job-1456252725626-0001/","link":"","permalink":"https://stanxia.github.io/2016/02/25/hadoop运行job时卡住在INFO-mapreduce-Job-Running-job-job-1456252725626-0001/","excerpt":"","text":"问题详情：在运行hadoop MapReduce任务时，一直卡在INFO mapreduce.Job: Running job:job 1456252725626 0001的位置。 问题原因：遇到了问题，首先从哪里找？当发现问题的时候，首先去查看相应的日志，查找问题产生的原因，对症下药。 在hadoop2.6以及以上版本中，mapreduce任务都是交给yarn资源管理器 管理的，所以首先去 查看 yarn-hadoop-nodemanager-slave01.log 日志。 终端输入： cat hadoop所在目录/logs/yarn-hadoop-nodemanager-slave01.log 如果显示如下信息： 1INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 字面上的意思每次尝试连接0.0.0.0/0.0.0.0:8031失败，google 百度一下 找到如下办法（不能保证百分百有效果，但也是值得一试的） 解决方案：从问题可以推断出，可能是 配置文件有问题，打开文件 hadoop文件目录/etc/hadoop/yarn-site.xml查看详细配置； 终端中输入： 1vi hadoop目录/etc/hadoop/yarn-site.xml 添加如下 配置到 文件中：（只需要将下面的 monsterxls修改为你集群中的namenode所在的主机名即可 ） 123456789101112&lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;monsterxls:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;monsterxls:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;monsterxls:8031&lt;/value&gt; &lt;/property&gt; 注意：集群中的每台服务器都修改一下。 重启hadoop服务再试试mapreduce重启服务，执行任务试试效果。 Good Luck!","raw":null,"content":null,"categories":[{"name":"问题集锦","slug":"问题集锦","permalink":"https://stanxia.github.io/categories/问题集锦/"}],"tags":[{"name":"hadoop问题","slug":"hadoop问题","permalink":"https://stanxia.github.io/tags/hadoop问题/"}]},{"title":"Hive与Impala","slug":"Hive与Impala","date":"2016-02-23T15:31:13.000Z","updated":"2017-03-19T14:30:13.000Z","comments":true,"path":"2016/02/23/Hive与Impala/","link":"","permalink":"https://stanxia.github.io/2016/02/23/Hive与Impala/","excerpt":"","text":"HiveQl执行过程：==》驱动模块==》编译器进行编译 Antlr==》优化器进行优化==》执行器执行（执行map reduce任务） 全表扫描* 不会执行 map reduce 任务 HiveQL查询的 MapReduce 作业转化流程：==》用户输入sql==》抽象语法树 AST Tree==》查询块 QueryBlock==》逻辑查询计划 OperatorTree==》重写逻辑查询计划==》物理计划==》选择最优的优化查询策略==》输出 Impala与Hive的区别：1.Hive适合长时间的批处理查询分析；Impala适合实时的sql查询2.Hive依赖于MapReduce，执行计划组合成管道形的MapReduce任务模式；Impala执行计划表现为一颗完整的执行计划树3.Hive在查询过程中，内存不够用时会使用外存；Impala在内存不够用时，不会使用外存，所有Impala在查询的时候会存在一定的限制 Impala与Hive的相同点：1.使用相同的存储数据池，都支持 HDFS,HBase2.使用相同的元数据3.对SQL的解释处理比较相似，都是通过 语法分析 生成执行计划","raw":null,"content":null,"categories":[{"name":"大数据","slug":"大数据","permalink":"https://stanxia.github.io/categories/大数据/"}],"tags":[{"name":"hive","slug":"hive","permalink":"https://stanxia.github.io/tags/hive/"}]},{"title":"HBase1.3新版本 新特性预览","slug":"HBase1-3新版本-新特性预览","date":"2016-02-22T05:07:44.000Z","updated":"2017-03-19T14:30:00.000Z","comments":true,"path":"2016/02/22/HBase1-3新版本-新特性预览/","link":"","permalink":"https://stanxia.github.io/2016/02/22/HBase1-3新版本-新特性预览/","excerpt":"","text":"新版本特性2017年1月中旬 发布的 HBase 1.3.0版本，新版本特性如下： 支持分层数据的压缩 多方面的性能提升，如 多预写日志（WAL）一个新的RPC机制，避免大量IO峰值的磁盘刷新吞吐量控制器等 新特性解析分层压缩使用场景：数据被铲除或更新的时候，通常要更频繁的扫描最新的数据，而旧数据则较少被扫描。 解决痛点：使用这种分层压缩策略，可有轻松的记录文件的TTL(生存时间 time-to-live)；当将现有存储文件压缩到单个较大的存储文件中时，过期的记录将被删除。 多预写日志每个region server都有一个预写日志(WAL)，该区域上的所有操作都要写入这个唯一的预写日志。 新特性改进后的 多预写日志(WAL) 支持更高性能的写入操作，这使得复制速度更快而且同步写入的延迟更低。默认情况下，多预写日志 提供了三个区域分组策略来分配 预写日志：每个区域的 预写日志都有 一个身份 标识，轮询调度算法 保证 每个区域映射的预写日志 都有其边界，区域中 不同 命名空间 的表 被映射到 不同的 预写日志 中。 性能测试报告 显示：预写日志 在纯SATA 磁盘中的运行平均延时 减少了 20% ；在SATA-SSD磁盘中运行延时 减少了 40%。 新的RPC调度器新的RPC调度器基于 CoDel算法 ，用于阻止 可用IO无法满足过高清秋频率引起的 长连接队列。 CoDel算法 用可控的延迟 来管理 活动队列，他根据 定义好的 阈值 来裁决队列中的最小延迟。一旦最小延迟超过阈值，该链接便会 被丢弃以便处理其他更有力的 最小延迟。","raw":null,"content":null,"categories":[{"name":"大数据","slug":"大数据","permalink":"https://stanxia.github.io/categories/大数据/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"https://stanxia.github.io/tags/HBase/"}]},{"title":"IntelliJ Idea编译报错：javacTask: 源发行版 1.8 需要目标发行版 1.8","slug":"IntelliJ-Idea编译报错：javacTask-源发行版-1-7-需要目标发行版-1-7","date":"2016-02-21T05:04:48.000Z","updated":"2017-03-19T14:30:33.000Z","comments":true,"path":"2016/02/21/IntelliJ-Idea编译报错：javacTask-源发行版-1-7-需要目标发行版-1-7/","link":"","permalink":"https://stanxia.github.io/2016/02/21/IntelliJ-Idea编译报错：javacTask-源发行版-1-7-需要目标发行版-1-7/","excerpt":"","text":"问题一：java compiler error运行java程序时，编译报错：java compiler error，如下图所示： 解决办法打开idea设置=&gt;&gt;Build,Execution,Deployment=&gt;&gt;Compiler=&gt;&gt;Java Compiler=&gt;&gt;左边框Pre-module bytecode version =&gt;&gt;找到程序所在的模块=&gt;&gt;Target Bytecode version 选择提示中的需要目标发行版本=&gt;&gt;Apply=&gt;&gt;ok,如下图所示： 问题二：Usage of API documented as @since 1.6/1.7/…当使用了一些api之后，idea会提示Usage of API documented as @since 1.6/1.7/…如下图所示： 解决方案：右键项目=&gt;&gt;open module setting=&gt;&gt;Laguage Level =&gt;&gt;选择（大于或等于）提示中@since的版本，如下图所示：","raw":null,"content":null,"categories":[{"name":"问题集锦","slug":"问题集锦","permalink":"https://stanxia.github.io/categories/问题集锦/"}],"tags":[{"name":"idea编译报错","slug":"idea编译报错","permalink":"https://stanxia.github.io/tags/idea编译报错/"}]},{"title":"flume1.7结合kafka0.9.0.1相关配置","slug":"flume1-7结合kafka0-9-0-1相关配置","date":"2016-02-20T08:50:54.000Z","updated":"2017-03-19T14:28:58.000Z","comments":true,"path":"2016/02/20/flume1-7结合kafka0-9-0-1相关配置/","link":"","permalink":"https://stanxia.github.io/2016/02/20/flume1-7结合kafka0-9-0-1相关配置/","excerpt":"","text":"简介利用flume1.7抓取数据，传入到kafka 配置文件设置在flume/conf/新建一个 kafka.conf,修改该文件,相关配置如下： 1234567891011121314151617181920212223242526272829303132333435vi flume/conf/kafka.conf#agent1表示代理名称agent1.sources=source1agent1.channels=channel1agent1.sinks=sink1#Spooling Directory是监控指定文件夹中新文件的变化，一旦新文件出现，就解析该文件内容，然后写入到channle。写入完成后，标记该文件已完成或者删除该文件。#配置source#数据来源类型 spooldir表示 文件夹 ，commandagent1.sources.source1.type=spooldir#指定监控的目录agent1.sources.source1.spoolDir=/home/hadoop/logsagent1.sources.source1.channels=channel1agent1.sources.source1.fileHeader=falseagent1.sources.source1.interceptors=i1agent1.sources.source1.interceptors.i1.type=timestamp#配置channel1agent1.channels.channel1.type=file#channel数据存放的备份目录agent1.channels.channel1.checkpointDir=/home/hadoop/channel_data.backup#channel数据存放目录agent1.channels.channel1.dataDir=/home/hadoop/channel_data#配置sink1agent1.sinks.sink1.type = org.apache.flume.sink.kafka.KafkaSink#新版本开始使用如下配置：agent1.sinks.sink1.kafka.bootstrap.servers=monsterxls:9092,slave1xls:9092,slave2xls:9092#agent1.sinks.sink1.partition.key=0#agent1.sinks.sink1.partitioner.class=org.apache.flume.plugins.SinglePartitionagent1.sinks.sink1.serializer.class=kafka.serializer.StringEncoderagent1.sinks.sink1.max.message.size=1000000agent1.sinks.sink1.producer.type=syncagent1.sinks.sink1.custom.encoding=UTF-8#新版本使用如下配置：agent1.sinks.sink1.topic=stanxlsagent1.sinks.sink1.channel=channel1 小结注意版本的问题。新版本改动了很多，在配置之前多看下帮助文档，了解下各种属性。","raw":null,"content":null,"categories":[{"name":"集群搭建","slug":"集群搭建","permalink":"https://stanxia.github.io/categories/集群搭建/"}],"tags":[{"name":"flume","slug":"flume","permalink":"https://stanxia.github.io/tags/flume/"}]},{"title":"yarn三种调度规则","slug":"yarn三种调度规则","date":"2016-02-16T06:23:32.000Z","updated":"2017-03-19T14:32:02.000Z","comments":true,"path":"2016/02/16/yarn三种调度规则/","link":"","permalink":"https://stanxia.github.io/2016/02/16/yarn三种调度规则/","excerpt":"","text":"yarn三种调度机制 FIFO Scheduler先进先出调度机制 Fair Scheduler公平调度机制 Capacity Scheduler容量机制 FIFO Scheduler按照先进先出的调度机制，所有的application将按照提交的顺序来执行，这些application都放在一个队列里面，顺序执行，执行完一个之后，才会执行下一个。 缺点：如果任务耗时长，后面提交的任务会一直处于等待状态，影响效率。所以只适合单人跑任务。 面对以上缺点，yarn提出了另两种策略，更加适合共享集群。 Capacity Scheduler定位：多人共享调度器。 机制：为每人分配一个队列，每个队列占用集群固定的资源，每个队列占用的资源可以不同，每个队列内部还是按照FIFO的策略。 特性：queue elasticity （弹性队列）根据实际情况分配资源 Capacity Scheduler 的队列时支持层级关系的： 相关配置如下： 队列设置如果是mapreduce任务，通过 mapreduce.job.queuename来设置执行队列。 Fair Scheduler机制：为每一个任务均匀分配资源，一个任务就可以用整个集群资源，两个任务就平分集群资源，依次类推。 开启Fair Scheduler在yarn-site.xml中设置 yarn.resourcemanager.scheduler.class为org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler 。NOTE:CDH默认的就是Faire Scheduler ，CDH并不支持 Capacity Scheduler. 队列设置设置fair-scheduler.xml文件，可参考下图：","raw":null,"content":null,"categories":[{"name":"大数据","slug":"大数据","permalink":"https://stanxia.github.io/categories/大数据/"}],"tags":[{"name":"yarn","slug":"yarn","permalink":"https://stanxia.github.io/tags/yarn/"}]},{"title":"Kafka文件存储机制及partition和offset","slug":"Kafka文件存储机制及partition和offset","date":"2016-02-15T08:05:05.000Z","updated":"2017-03-19T14:31:00.000Z","comments":true,"path":"2016/02/15/Kafka文件存储机制及partition和offset/","link":"","permalink":"https://stanxia.github.io/2016/02/15/Kafka文件存储机制及partition和offset/","excerpt":"","text":"初识kafkakafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。 这种动作是在现代网络上的许多社会功能的一个关键因素。 Kafka是最初由Linkedin公司开发，是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统(也可以当做MQ系统)，常见可以用于web/nginx日志、访问日志，消息服务等等，Linkedin于2010年贡献给了Apache基金会并成为顶级开源项目。 为什么用kafka一个商业化消息队列的性能好坏，其文件存储机制设计是衡量一个消息队列服务技术水平和最关键指标之一。 下面将从Kafka文件存储机制和物理结构角度，分析Kafka是如何实现高效文件存储，及实际应用效果。 kafka名词解释 名词 解释 broker 消息中间件处理结点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群。 topic 一类消息，例如page view日志、click日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发。 partition topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。 segment partition物理上由多个segment组成，下面有详细解释 kafka分析步骤 topic中partition存储分布 partiton中文件存储方式 partiton中segment文件存储结构 在partition中如何通过offset查找message topic中partition存储分布详解假设实验环境中Kafka集群只有一个broker，xxx/message-folder为数据文件存储根目录，在Kafka broker中server.properties文件配置(参数log.dirs=xxx/message-folder)，例如创建2个topic名称分别为report_push、launch_info, partitions数量都为partitions=4 存储路径和目录规则为： xxx/message-folder |–report_push-0 |–report_push-1 |–report_push-2 |–report_push-3 |–launch_info-0 |–launch_info-1 |–launch_info-2 |–launch_info-3 在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。 partiton中文件存储方式 图1 每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。但每个段segment file消息数量不一定相等，这种特性方便old segment file快速被删除。 每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。 这样做的好处就是能快速删除无用文件，有效提高磁盘利用率。 partiton中segment文件存储结构segment file组成：由2大部分组成，分别为index file和data file，此2个文件一一对应，成对出现，后缀”.index”和“.log”分别表示为segment索引文件、数据文件. segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值。数值最大为64位long大小，19位数字字符长度，没有数字用0填充。 下面文件列表是笔者在Kafka broker上做的一个实验，创建一个topicXXX包含1 partition，设置每个segment大小为500MB,并启动producer向Kafka broker写入大量数据,如下图2所示segment文件列表形象说明了上述2个规则： 图2 上图中对segment file文件为例，说明segment中index&lt;—-&gt;data file对应关系物理结构如下： 图3 上图中索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。 其中以索引文件中元数据3,497为例，依次在数据文件中表示第3个message(在全局partiton表示第368772个message)、以及该消息的物理偏移地址为497。 从上图了解到segment data file由许多message组成，下面详细说明message物理结构如下： 图4 参数说明： 参数 说明 8 byte offset 在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息, 在parition(分区)内的位置。即offset表示partiion的第多少message 4 byte message size message大小 4 byte CRC32 用crc32校验message 1 byte “magic” 表示本次发布Kafka服务程序协议版本号 1 byte “attributes” 表示为独立版本、或标识压缩类型、或编码类型。 4 byte key length 表示key的长度,当key为-1时，K byte key字段不填 K byte key 可选 value bytes payload 表示实际消息数据。 在partition中如何通过offset查找message例如读取offset=368776的message，需要通过下面2个步骤查找。 第一步查找segment file： 上述图2为例，其中00000000000000000000.index表示最开始的文件，起始偏移量(offset)为0.第二个文件00000000000000368769.index的消息量起始偏移量为368770 = 368769 + 1.同样，第三个文件00000000000000737337.index的起始偏移量为737338=737337 + 1，其他后续文件依次类推，以起始偏移量命名并排序这些文件，只要根据offset 二分查找文件列表，就可以快速定位到具体文件。 当offset=368776时定位到00000000000000368769.index|log 第二步通过segment file查找message： 通过第一步定位到segment file，当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和 00000000000000368769.log的物理偏移地址，然后再通过00000000000000368769.log顺序查找直到 offset=368776为止。 从上述图3可知这样做的优点，segment index file采取稀疏索引存储方式，它减少索引文件大小，通过mmap可以直接内存操作，稀疏索引为数据文件的每个对应message设置一个元数据指针,它比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间。 Kafka文件存储机制?实际运行效果Kafka运行时很少有大量读磁盘的操作，主要是定期批量写磁盘操作，因此操作磁盘很高效。这跟Kafka文件存储中读写message的设计是息息相关的。Kafka中读写message有如下特点: 写message消息从java堆转入page cache(即物理内存)。 由异步线程刷盘,消息从page cache刷入磁盘。 读message消息直接从page cache转入socket发送出去。 当从page cache没有找到相应数据时，此时会产生磁盘IO,从磁盘Load消息到page cache,然后直接从socket发出去。 总结Kafka高效文件存储设计特点Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。 通过索引信息可以快速定位message和确定response的最大大小。 通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作。 通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。 kafka中的partition和offset,log机制 图5 分区读写日志 首先,kafka是通过log(日志)来记录消息发布的.每当产生一个消息,kafka会记录到本地的log文件中,这个log和我们平时的log有一定的区别. 这个log文件默认的位置在config/server.properties中指定的.默认的位置是log.dirs=/tmp/kafka-logs 分区partitionkafka是为分布式环境设计的,因此如果日志文件,其实也可以理解成消息数据库,放在同一个地方,那么必然会带来可用性的下降,一挂全挂,如果全量拷贝到所有的机器上,那么数据又存在过多的冗余,而且由于每台机器的磁盘大小是有限的,所以即使有再多的机器,可处理的消息还是被磁盘所限制,无法超越当前磁盘大小.因此有了partition的概念. kafka对消息进行一定的计算,通过hash来进行分区.这样,就把一份log文件分成了多份.如上面的分区读写日志图,分成多份以后,在单台broker上,比如快速上手中,如果新建topic的时候,我们选择了–replication-factor 1 –partitions 2,那么在log目录里,我们会看到： test-0目录和test-1目录.就是两个分区了. 图6 kafka分布式分区存储 这是一个topic包含4个Partition，2 Replication(拷贝),也就是说全部的消息被放在了4个分区存储,为了高可用,将4个分区做了2份冗余,然后根据分配算法.将总共8份数据,分配到broker集群上. 结果就是每个broker上存储的数据比全量数据要少,但每份数据都有冗余,这样,一旦一台机器宕机,并不影响使用.比如图中的Broker1,宕机了.那么剩下的三台broker依然保留了全量的分区数据.所以还能使用,如果再宕机一台,那么数据不完整了.当然你可以设置更多的冗余,比如设置了冗余是4,那么每台机器就有了0123完整的数据,宕机几台都行.需要在存储占用和高可用之间做衡量. 宕机后,zookeeper会选出新的partition leader.来提供服务. 偏移offset上面说了分区，分区是一个有序的,不可变的消息队列.新来的commit log持续往后面加数据.这些消息被分配了一个下标(或者偏移),就是offset,用来定位这一条消息. 消费者消费到了哪条消息,是保持在消费者这一端的.消息者也可以控制,消费者可以在本地保存最后消息的offset,并间歇性的向zookeeper注册offset.也可以重置offset. 如何通过offset算出分区partition存储的时候,又分成了多个segment(段),然后通过一个index,索引,来标识第几段. 在磁盘中，每个topic目录下面会有两个文件 index和log. 图7 index文件和log文件 对于某个指定的分区,假设每5个消息,作为一个段大小,当产生了10条消息的情况下,目前有会得到： 0.index (表示这里index是对0-4做的索引) 5.index (表示这里index是对5-9做的索引) 10.index (表示这里index是对10-15做的索引,目前还没满) 和log文件 0.log 5.log 10.log ,当消费者需要读取offset=8的时候,首先kafka对index文件列表进行二分查找,可以算出.应该是在5.index对应的log文件中,然后对对应的5.log文件,进行顺序查找,5-&gt;6-&gt;7-&gt;8,直到顺序找到8就好了.","raw":null,"content":null,"categories":[{"name":"大数据","slug":"大数据","permalink":"https://stanxia.github.io/categories/大数据/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://stanxia.github.io/tags/kafka/"}]},{"title":"Hadoop原生集群添加hive组件","slug":"Hadoop原生集群添加hive组件","date":"2016-02-15T07:45:14.000Z","updated":"2017-03-19T14:29:43.000Z","comments":true,"path":"2016/02/15/Hadoop原生集群添加hive组件/","link":"","permalink":"https://stanxia.github.io/2016/02/15/Hadoop原生集群添加hive组件/","excerpt":"","text":"一：前提 准备MYSQL JDBC驱动 本机已经安装了JDK 基于自己已有的HADOOP集群进行操作 在开启HIve之前，开启HDFS + YARN+ntpd时间同步 二：HIVE下载 HIVE的安装包下载 wget http://mirrors.cnnic.cn/apache/hive/hive-1.2.1/apache-hive-1.2.1-bin.tar.gz 然后解压 tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/ cd /opt/ mv apache-hive-1.2.1-bin hive 配置环境变量 vi /etc/profile 添加以下内容： HIVE_HOME=/opt/hive export PATH=$PATH:$HIVE_HOME/bin 文件生效： source /etc/profile 最好 ROOT用户与 HADOOP用户都执行一次 三：安装依赖包 安装nettools yum -y install net-tools 安装perl yum install -y perl-Module-Install.noarch 四：MySQL安装与配置 安装MYSQL 查看是否已经安装MYSQL执行命令如下： rpm -qa | grep mariadb 如果存在 执行卸载: yum remove mariadb-libs 然后 yum remove mariadb 安装MYSQL 简易版需要安装 unzip工具 yum -y install unzip 下载mysql并解压，建议下载rpm包： 点击MySQL 下载 解压下载的zip： unzip **.zip 进入到解压的MYSQL目录，安装rpm包： rpm –ivh **.rpm 配置MYSQL 修改配置文件路径：cp /usr/share/mysql/my-default.cnf /etc/my.cnf 在配置文件中增加以下配置并保存： vim /etc/my.cnf 123456789default-storage-engine = innodbinnodb_file_per_tablecollation-server = utf8_general_ciinit-connect = 'SET NAMES utf8'character-set-server = utf8 初始化数据库执行： /usr/bin/mysql_install_db 开启MYSQL服务： service mysql restart 查看mysql root初始化密码： cat /root/.mysql_secret 登陆mysql ： mysql -u root –p 复制root的初始密码 （MYSQL下执行）SET PASSWORD=PASSWORD(&#39;123456&#39;); 创建HIVE用户，HIVE数据库 create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci; use mysql; grant all privileges on *.* to hive@&quot;%&quot; identified by &quot;hive&quot; with grant option; flush privileges; 最好添加如下代码（否则可能会有乱码产生）： alter database hive CHARACTER SET latin1 （LINUX下执行）开启开机启动： chkconfig mysql on （LINUX下执行）拷贝mysql驱动包到 hive/lib目录下面,否则hive不能连接上mysql： cp mysql-connector-java-5.1.34-bin.jar /opt/hive/lib 五：解决冲突包查看hadoop目录/share/hadoop/yarn/lib和hive目录/lib，检查jlinexxxx.jar 的版本，将低版本的替换为另一边高版本的。 例如：/opt/Hadoop/share/Hadoop/yarn/lib下的jline为jline 2.xx，而/opt/hive/lib/jiline为jline 0.xxx版本，则将 /opt/Hadoop/share/Hadoop/yarn/lib下的jline 2.xx包复制到/opt/hive/lib/下面，并且删除/opt/hive/lib/jline 0.xxx包。 ls /opt/hadoop/share/hadoop/yarn/lib ls /opt/hive/lib/ 六： 修改配置文件进入到hive的配置文件目录下，找到hive-default.xml.template，cp为hive-default.xml cd /opt/hive/conf/ cp hive-default.xml.template hive-default.xml 另创建hive-site.xml并添加参数 vi hive-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273 &lt;?xml version=\"1.0\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/tmp/hivelog&lt;/value&gt; &lt;description&gt; Location of Hive run time structured log file &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive-$&#123;user.name&#125;&lt;/value&gt; &lt;description&gt;Scratch space for Hive jobs&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://monsterxls:3306/hive?createDatabaseIfNotExist=true&amp; useUnicode=true&amp;characterEncoding=utf-8&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 复制 hive-env.sh.template文件为 hive-env.sh cp hive-env.sh.template hive-env.sh 主要修改以下三行 vi hive-env.sh 12345HADOOP_HOME=/opt/hadoopexport HIVE_CONF_DIR=/opt/hive/confexport HIVE_AUX_JARS_PATH=/opt/hive/lib 七： Hive启动 启动命令如下 hive --service metastore &amp; 查看启动后，进程是否存在 jps 1234567891011121310288 RunJar #多了一个进程9365 NameNode9670 SecondaryNameNode11096 Jps9944 NodeManager9838 ResourceManager9471 DataNode 八：Hive服务器端访问直接在命令控制台输入： hive 即可进入hive的控制台界面 进行一些简单的操作查看hive是否安装成功： 1234567891011121314151617181920212223242526272829hive&gt; show databases;OKdefaultTime taken: 1.332 seconds, Fetched: 2 row(s)hive&gt; use default;OKTime taken: 0.037 secondshive&gt; create table test1(id int);OKTime taken: 0.572 secondshive&gt; show tables;OKtest1Time taken: 0.057 seconds, Fetched: 3 row(s)hive&gt; 创建表 testload 字段包含 id1,id2,id3，以逗号分割： 1hive&gt; CREATE TABLE testload (id1 STRING,id2 STRING,id3 STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;; 使用 Hiveload进入testload: 1hive&gt; LOAD DATA LOCAL INPATH &apos;目标文件&apos; OVERWRITE INTO TABLE testload; 测试能否执行mapreduce任务： 1hive&gt; SELECT count(*) FROM testload; 结束语： 安装集群需要耐心以及细心，否则前面错一步后面会很难找到错误的来源。 出现错误请看日志，一般都会在日志中找到问题的原因。 GOOD LUCK!!","raw":null,"content":null,"categories":[{"name":"集群搭建","slug":"集群搭建","permalink":"https://stanxia.github.io/categories/集群搭建/"}],"tags":[{"name":"hive","slug":"hive","permalink":"https://stanxia.github.io/tags/hive/"}]},{"title":"完善ntp时间同步","slug":"完善ntp时间同步","date":"2016-02-14T08:19:58.000Z","updated":"2017-03-19T14:28:19.000Z","comments":true,"path":"2016/02/14/完善ntp时间同步/","link":"","permalink":"https://stanxia.github.io/2016/02/14/完善ntp时间同步/","excerpt":"","text":"问题1ntp同步时间过长 解决方案修改 /etc/ntp.conf 主节点配置： 12server ntp7.aliyun.com iburstrestrict ntp7.aliyun.com nomodify notrap noquery 从节点配置： 12restrict hadoop1(主机名) nomodify notrap noqueryserver hadoop1(主机名) iburst 问题2ntp时间同步之后，显示非中国时区 解决方案1cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime","raw":null,"content":null,"categories":[{"name":"集群搭建","slug":"集群搭建","permalink":"https://stanxia.github.io/categories/集群搭建/"}],"tags":[{"name":"ntp","slug":"ntp","permalink":"https://stanxia.github.io/tags/ntp/"}]},{"title":"zookeeper启动时数组越界异常","slug":"zookeeper启动时数组越界异常","date":"2016-02-14T05:39:29.000Z","updated":"2017-03-19T14:32:14.000Z","comments":true,"path":"2016/02/14/zookeeper启动时数组越界异常/","link":"","permalink":"https://stanxia.github.io/2016/02/14/zookeeper启动时数组越界异常/","excerpt":"","text":"问题启动zookeeper时，出现以下异常信息： 解决方案修改 ／zookeeper/conf/zoo.cfg文件修改服务器id和ip映射时注意格式为：12vi /zookeeper/conf/zoo.cfgserver.1=host:port:port或者host:port或者host:port:port:type","raw":null,"content":null,"categories":[{"name":"问题集锦","slug":"问题集锦","permalink":"https://stanxia.github.io/categories/问题集锦/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"https://stanxia.github.io/tags/zookeeper/"}]},{"title":"kafka-server-properties参数详解","slug":"kafka-server-properties参数详解","date":"2016-02-13T06:01:39.000Z","updated":"2017-03-19T14:30:45.000Z","comments":true,"path":"2016/02/13/kafka-server-properties参数详解/","link":"","permalink":"https://stanxia.github.io/2016/02/13/kafka-server-properties参数详解/","excerpt":"","text":"server.properties参数说明 参数 说明 broker.id=0 每一个broker在集群中的唯一表示，要求是正数。当该服务器的IP地址发生改变时，broker.id没有变化，则不会影响consumers的消息情况 log.dirs=/data/kafka-logs kafka数据的存放地址，多个地址的话用逗号分割,多个目录分布在不同磁盘上可以提高读写性能 /data/kafka-logs-1，/data/kafka-logs-2 port =9092 broker server服务端口 message.max.bytes =6525000 表示消息体的最大大小，单位是字节 num.network.threads =4 broker处理消息的最大线程数，一般情况下数量为cpu核数 num.io.threads =8 broker处理磁盘IO的线程数，数值为cpu核数2倍 background.threads =4 一些后台任务处理的线程数，例如过期消息文件的删除等，一般情况下不需要去做修改 queued.max.requests =500 等待IO线程处理的请求队列最大数，若是等待IO的请求超过这个数值，那么会停止接受外部消息，应该是一种自我保护机制。 host.name broker的主机地址，若是设置了，那么会绑定到这个地址上，若是没有，会绑定到所有的接口上，并将其中之一发送到ZK socket.send.buffer.bytes=100*1024 socket的发送缓冲区，socket的调优参数SO_SNDBUFF socket.receive.buffer.bytes =100*1024 socket的接受缓冲区，socket的调优参数SO_RCVBUFF socket.request.max.bytes =10010241024 socket请求的最大数值，防止serverOOM，message.max.bytes必然要小于socket.request.max.bytes，会被topic创建时的指定参数覆盖 log.segment.bytes =102410241024 topic的分区是以一堆segment文件存储的，这个控制每个segment的大小，会被topic创建时的指定参数覆盖 log.roll.hours =24*7 这个参数会在日志segment没有达到log.segment.bytes设置的大小，也会强制新建一个segment会被 topic创建时的指定参数覆盖 log.cleanup.policy = delete 日志清理策略选择有：delete和compact主要针对过期数据的处理，或是日志文件达到限制的额度，会被 topic创建时的指定参数覆盖 log.retention.bytes=-1 topic每个分区的最大文件大小，一个topic的大小限制 = 分区数*log.retention.bytes。-1没有大小限log.retention.bytes和log.retention.minutes任意一个达到要求，都会执行删除，会被topic创建时的指定参数覆盖 log.retention.check.interval.ms=5minutes 文件大小检查的周期时间，是否处罚 log.cleanup.policy中设置的策略 log.cleaner.enable=false 是否开启日志清理 log.cleaner.threads = 2 日志清理运行的线程数 log.cleaner.io.max.bytes.per.second=None 日志清理时候处理的最大大小 log.cleaner.dedupe.buffer.size=50010241024 日志清理去重时候的缓存空间，在空间允许的情况下，越大越好 log.cleaner.io.buffer.size=512*1024 日志清理时候用到的IO块大小一般不需要修改 log.cleaner.io.buffer.load.factor =0.9 日志清理中hash表的扩大因子一般不需要修改 log.cleaner.backoff.ms =15000 检查是否处罚日志清理的间隔 log.cleaner.min.cleanable.ratio=0.5 日志清理的频率控制，越大意味着更高效的清理，同时会存在一些空间上的浪费，会被topic创建时的指定参数覆盖 log.cleaner.delete.retention.ms =1day 对于压缩的日志保留的最长时间，也是客户端消费消息的最长时间，同log.retention.minutes的区别在于一个控制未压缩数据，一个控制压缩后的数据。会被topic创建时的指定参数覆盖 log.index.size.max.bytes =1010241024 对于segment日志的索引文件大小限制，会被topic创建时的指定参数覆盖 log.index.interval.bytes =4096 当执行一个fetch操作后，需要一定的空间来扫描最近的offset大小，设置越大，代表扫描速度越快，但是也更好内存，一般情况下不需要搭理这个参数 log.flush.interval.messages=None log文件”sync”到磁盘之前累积的消息条数,因为磁盘IO操作是一个慢操作,但又是一个”数据可靠性”的必要手段,所以此参数的设置,需要在“**数据可靠性**”与”性能”之间做必要的权衡.如果此值过大,将会导致每次”fsync”的时间较长(IO阻塞),如果此值过小,将会导致“fsync”的次数较多,这也意味着整体的client请求有一定的延迟.物理server故障,将会导致没有fsync的消息丢失. log.flush.scheduler.interval.ms =3000 检查是否需要固化到硬盘的时间间隔 log.flush.interval.ms = None 仅仅通过interval来控制消息的磁盘写入时机,是不足的.此参数用于控制“fsync”的时间间隔,如果消息量始终没有达到阀值,但是离上一次磁盘同步的时间间隔达到阀值,也将触发. log.delete.delay.ms =60000 文件在索引中清除后保留的时间一般不需要去修改 log.flush.offset.checkpoint.interval.ms =60000 控制上次固化硬盘的时间点，以便于数据恢复一般不需要去修改 auto.create.topics.enable =true 是否允许自动创建topic，若是false，就需要通过命令创建topic default.replication.factor =1 默认副本因子 num.partitions =1 每个topic的分区个数，若是在topic创建时候没有指定的话会被topic创建时的指定参数覆盖 以下是Leader，replicas配置 参数 说明 controller.message.queue.size=10 partition leader与replicas数据同步时,消息的队列尺寸 controller.socket.timeout.ms =30000 partition leader与replicas之间通讯时,socket的超时时间 replica.lag.time.max.ms =10000 replicas响应partition leader的最长等待时间，若是超过这个时间，就将replicas列入ISR(in-sync replicas)，并认为它是死的，不会再加入管理中 replica.lag.max.messages =4000 如果follower落后与leader太多,将会认为此follower[或者说partition relicas]已经失效， 通常,在follower与leader通讯时,因为网络延迟或者链接断开,总会导致replicas中消息同步滞后， 如果消息之后太多,leader将认为此follower网络延迟较大或者消息吞吐能力有限,将会把此replicas迁移， 到其他follower中， 在broker数量较少,或者网络不足的环境中,建议提高此值. replica.socket.timeout.ms=30*1000 follower与leader之间的socket超时时间 replica.socket.receive.buffer.bytes=64*1024 leader复制时候的socket缓存大小 replica.fetch.max.bytes =1024*1024 replicas每次获取数据的最大大小 replica.fetch.wait.max.ms =500 replicas同leader之间通信的最大等待时间，失败了会重试 replica.fetch.min.bytes =1 fetch的最小数据尺寸,如果leader中尚未同步的数据不足此值,将会阻塞,直到满足条件 num.replica.fetchers=1 leader进行复制的线程数，增大这个数值会增加follower的IO replica.high.watermark.checkpoint.interval.ms =5000 每个replica检查是否将最高水位进行固化的频率 controlled.shutdown.enable =false 是否允许控制器关闭broker ,若是设置为true,会关闭所有在这个broker上的leader，并转移到其他broker controlled.shutdown.max.retries =3 控制器关闭的尝试次数 controlled.shutdown.retry.backoff.ms =5000 每次关闭尝试的时间间隔 leader.imbalance.per.broker.percentage =10 leader的不平衡比例，若是超过这个数值，会对分区进行重新的平衡 leader.imbalance.check.interval.seconds =300 检查leader是否不平衡的时间间隔 offset.metadata.max.bytes 客户端保留offset信息的最大空间大小 kafka中zookeeper参数配置 参数 说明 zookeeper.connect = localhost:2181 zookeeper集群的地址，可以是多个，多个之间用逗号分割hostname1:port1,hostname2:port2,hostname3:port3 zookeeper.session.timeout.ms=6000 ZooKeeper的最大超时时间，就是心跳的间隔，若是没有反映，那么认为已经死了，不易过大 zookeeper.connection.timeout.ms =6000 ZooKeeper的连接超时时间 zookeeper.sync.time.ms =2000 ZooKeeper集群中leader和follower之间的同步时间","raw":null,"content":null,"categories":[{"name":"大数据","slug":"大数据","permalink":"https://stanxia.github.io/categories/大数据/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://stanxia.github.io/tags/kafka/"}]},{"title":"kafka安装与简单的应用","slug":"kafka安装与简单的应用","date":"2016-02-13T06:01:39.000Z","updated":"2017-03-19T14:30:52.000Z","comments":true,"path":"2016/02/13/kafka安装与简单的应用/","link":"","permalink":"https://stanxia.github.io/2016/02/13/kafka安装与简单的应用/","excerpt":"","text":"一、安装环境 多台Linux服务器 已经安装好zookeeper的集群（安装zookeeper的步骤可以查看前篇文章） 下载kafka 点击选择需要下载的kafka版本 或者直接在服务器上面下载： 1wget http://apache.opencas.org/kafka/0.9.0.1/kafka_2.11-0.9.0.1.tgz 二、安装kafka创建项目目录：创建kafka项目目录，最好是将多有的集群项目都放在一个目录下面，方便管理各类项目。博主是将所有的集群项目都放在/opt下面。 12mkdir /opt/kafka #创建kafka项目目录mkdir /opt/kafka/kafkalogs #创建kafka项目的日志目录 安装kafka：123tar xzvf kafka-0.8.0-beta1-src.tgz -C /opt/kafka/ #解压kafka到指定目录下cd /opt/kafka/ #到解压kafka的目录mv kafka-0.8.0-beta1-src kafka #重命名 三、修改配置文件配置文件目录：kafka的配置文件都存放在/opt/kafka/kafka/config/ 12cd /opt/kafka/kafka/config/ ll #查看kafka所有的配置文件 修改配置文件：主要修改server.properties： 123456789101112131415161718broker.id=0 #当前机器在集群中的唯一标识，和zookeeper的myid性质一样port=19092 #当前kafka对外提供服务的端口默认是9092host.name=192.168.7.100 #这个参数默认是关闭的，在0.8.1有个bug，DNS解析问题，失败率的问题。num.network.threads=3 #这个是borker进行网络处理的线程数num.io.threads=8 #这个是borker进行I/O处理的线程数log.dirs=/opt/kafka/kafkalogs/ #消息存放的目录，这个目录可以配置为“，”逗号分割的表达式，上面的num.io.threads要大于这个目录的个数这个目录，如果配置多个目录，新创建的topic他把消息持久化的地方是，当前以逗号分割的目录中，那个分区数最少就放那一个socket.send.buffer.bytes=102400 #发送缓冲区buffer大小，数据不是一下子就发送的，存储到缓冲区到达一定的大小后再发送，能提高性能socket.receive.buffer.bytes=102400 #kafka接收缓冲区大小，当数据到达一定大小后在序列化到磁盘socket.request.max.bytes=104857600 #这个参数是向kafka请求消息或者向kafka发送消息的请请求的最大数，这个值不能超过java的堆栈大小num.partitions=1 #默认的分区数，一个topic默认1个分区数log.retention.hours=168 #默认消息的最大持久化时间，168小时，7天message.max.byte=5242880 #消息保存的最大值5Mdefault.replication.factor=2 #kafka保存消息的副本数，如果一个副本失效了，另一个还可以继续提供服务replica.fetch.max.bytes=5242880 #取消息的最大直接数log.segment.bytes=1073741824 #这个参数是：因为kafka的消息是以追加的形式落地到文件，当超过这个值的时候，kafka会新建一个文件log.retention.check.interval.ms=300000 #每隔300000毫秒去检查上面配置的log失效时间（log.retention.hours=168 ），到目录查看是否有过期的消息如果有，删除log.cleaner.enable=false #是否启用log压缩，一般不用启用，启用的话可以提高性能zookeeper.connect=192.168.7.100:12181,192.168.7.101:12181,192.168.7.107:1218 #设置zookeeper的连接端口，与zookeeper的zoo.cfg文件中的clientPort保持一致 三、开启并使用kafka开启kafka服务： 首先要确保已经开启了zookeeper服务: 1/opt/zookeeper/zookeeper/bin/zkServer.sh start #开启zookeeper服务 后台开启kafka： 12nohup /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties &amp; #后台挂起kafka服务 nohup &amp;ps -ef | grep java | grep -v grep #查看当前的java进程，zookeeper与kafka都是基于java kafka基本操作： 创建topics 1/opt/kafka/bin/kafka-topics.sh --zookeeper 192.168.221.138:2181 --create --topic test --replication-factor 1 --partition 1 #新建主题 连接zookeeper --create 创建主题 --topic 主题名 --replication-factor 副本因子 --partitions 分为几个区 发消息 1bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test &gt;/dev/null #producer发送消息 发送给broker 收消息 1bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning 2&gt;/dev/null #consumer接收消息 连接zookeeper服务器 --from-beginning 接收历史消息 四、总结Note: 开启kafka之前必须要开启zookeeper 注意生产者连接broker 端口号默认9092；消费者连接zookeeper 端口号默认2181 创建主题时，设置分区为集群服务器数的两倍或多倍，可有效避免消息发送和接收的读写热点 Good Luck!","raw":null,"content":null,"categories":[{"name":"集群搭建","slug":"集群搭建","permalink":"https://stanxia.github.io/categories/集群搭建/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://stanxia.github.io/tags/kafka/"}]},{"title":"zookeeper集群搭建","slug":"zookeeper集群搭建","date":"2016-02-12T06:22:07.000Z","updated":"2017-03-19T14:32:07.000Z","comments":true,"path":"2016/02/12/zookeeper集群搭建/","link":"","permalink":"https://stanxia.github.io/2016/02/12/zookeeper集群搭建/","excerpt":"","text":"一、环境准备服务器准备：Linxu服务器2*n+1台，最好是奇数台服务器，因为 zookeeper集群的机制是选举制度，需要超过半数才能对外提供服务。 jdk环境：zookeeper底层是用java写的，所以需要jdk环境。jdk环境的安装在之前几篇文章中已经说过，这里就不赘述了。 下载zookeeper：点我下载zookeeper 或者直接在服务器上面下载： 1wget http://mirrors.cnnic.cn/apache/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz 二、安装zookeeper首先确定好zookeeper的目录结构，避免在项目过多的时候找不到所需的项目。 在这里博主统一把所有组件都安装在/opt下的。 创建zookeeper 项目目录：123mkdir -r /opt/zookeeper #创建zookeeper项目目录mkdir -r /opt/zookeeper/zkdata #存放快照日志mkdir -r /opt/zookeeper/zkdatalog #存放事务日志 解压：12tar -zxvf zookeeper-3.4.6.tar.gz -C /opt/zookeeper #解压到/opt/zookeeper下mv zookeeper-3.4.6 zookeeper #重命名 三、修改配置文件zookeeper的相关配置都在zoo.cfg文件中。 123cd /opt/zookeeper/zookeeper/conf/ #进入conf目录ll #查看配置文件cp zoo_sample.cfg zoo.cfg #复制并更名为zoo.cfg，zookeeper指定的命名规范为zoo.cfg 修改zoo.cfg:vi zoo.cfg #设置如下属性： 12345678910tickTime=2000 #用于配置zookeeper中的最小时间单元的长度。默认为3000ms，很多运行时的时间间隔都是tickTime的倍数。例如：zookeeper中会话的最小超时时间默认为2*tickTime.initLimit=10 #默认为10，表示参数tickTime的10倍。用于配置Leader服务器等待Follower启动并完成数据同步的时间。Leader允许Follower在initLimit时间内与Leader完成连接并数据同步。syncLimit=5 #默认5，表示参数tickTime的5倍。用于配置Leader与Follower之间心跳连接的最长延时时间。如果Leader在syncLimit时间内无法获取到Follower的心跳检测相应，则会认为该Follower已经脱离了和自己的同步。dataDir=/opt/zookeeper/zkdata #无默认值，必须配置。用于配置存放快照文件的目录。如果没有配置参数dataLogDir属性，那么会默认把日志文件存在该目录下。所以最好设置dataLogDir参数。dataLogDir=/opt/zookeeper/zkdatalog #zookeeper存放日志的目录。clientPort=2181 #必须配置。用于配置该服务器对外的服务端口。客户端会通过该服务端口语zookeeper服务器创建连接，一般设置为2181。每台服务器都可以随意设置该端口号，同个集群中的每个服务器也可以设置不同的端口号。#server.id=host:port:port 无默认值。用于配置集群中的服务器列表。id为ServerID,与myid文件中的保持一致，用于辨识这是哪一台服务器，所以必须要唯一。第一个端口用于指定Follower与Leader之间通信和数据同步的端口。第二个端口专门用于Leader选举的投票端口。server.1=192.168.7.100:12888:13888 server.2=192.168.7.101:12888:13888server.3=192.168.7.107:12888:13888 创建myid文件：myid文件用于存放当前服务器的ServerID，即当前服务器的唯一标识，必须唯一。 1echo \"ServerId\"&gt;&gt;/opt/zookeeper/zkdata/myid #存放在zkdata下，ServerID必须与zoo.cfg中的id保持一致。 四、启动zookeeper启动服务：进入到zookeeper/bin目录下： 12cd /opt/zookeeper/zookeeper/bin/ #进入zookeeper/bin目录下./zkServer.sh start #启动zookeeper服务，集群所有的服务器都需要开启 检查服务器状态：1./zkServer.sh status #查看zookeeper服务器状态 五、总结注意： 注意创建zkdata与zkdatalog文件夹用于存放数据与日志。如果没有设置zkdatalog，zookeeper会默认把日志都存放在zkdata中，但是这样会严重影响zookeeper的性能。作为性能调优的地方，最好将zkdatalog设置在单独的磁盘中。 注意各个端口的设置，如果不使用默认的端口，尽量设置大端口号，以免端口冲突。TCP能设置的最大端口号：65535 注意设置ServerID的时候，一定要保证唯一，否则将不能识别该服务器。 Good Luck!","raw":null,"content":null,"categories":[{"name":"集群搭建","slug":"集群搭建","permalink":"https://stanxia.github.io/categories/集群搭建/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"https://stanxia.github.io/tags/zookeeper/"}]},{"title":"cdh集群搭建","slug":"cdh集群搭建","date":"2016-02-11T03:03:55.000Z","updated":"2017-03-19T14:28:40.000Z","comments":true,"path":"2016/02/11/cdh集群搭建/","link":"","permalink":"https://stanxia.github.io/2016/02/11/cdh集群搭建/","excerpt":"","text":"1.如果存在jdk：卸载方式：rpm -qa | grep jdkrpm -e —nodeps 加上上面返回的结构 2.安装jdk：rpm -ivh jdk-7u80-linux-x64.rpm 3.配置hostnamevi /etc/sysconfig/networkNETWORKING=yesHOSTNAME=master 4.vi /etc/hostname #删除文件内容 ,然后输入master 5.修改host映射vi /etc/hosts 10.211.55.9 master #ipDress1为master服务器的IP地址 6.selinux 关闭vi /etc/sysconfig/selinuxSELINUX=disable 7.重启reboot 8.更改防火墙systemctl stop firewalldsystemctl disable firewalldsystemctl status firewalld 9.安装时间同步服务yum -y install ntpvi /etc/ntp.conf #注释掉所有的server..* 的指向 ，新添加一条可连接的ntp服务器server ntp.sjtu.edu.cn iburst #启动时间同步服务service ntpd start #执行命令ntpdate -u 1.asia.pool.ntp.org #重启时间同步服务service ntpd restart 10.ssh无密码登陆配置ssh-keygen -t rsa #一直使用默认 11.安装mysql #查看mysql是否意境安装：rpm -qa | grep mariadb #如果存在：cd #安装mysql依赖：yum install -y perl-Module-Install.noarch unzip .ziprpm -ivh .rpm #修改配置文件目录cp /usr/share/mysql/my-default.cnf /etc/my.cnf #在配置文件中增加以下配置并保存：vi /etc/my.cnfdefault-storage-engine = innodbinnodb_file_per_tablecollation-server = utf8_general_ciinit-connect = ‘SET NAMES utf8’character-set-server=utf8 #初始化数据库执行：/usr/bin/mysql_install_db #开启mysql服务：service mysql restart #查看mysql root 初始化密码：cat /root/.mysql_secret T1STjiM6A1TXQB5p #登陆mysql：mysql -u root -pSET PASSWORD=PASSWORD(‘123456’)#复制root的初始密码mysql下面执行：SET PASSWORDcd /=PASSWORD(‘123456’) #linux开启开机启动：chkconfig mysql on #linux下面执行 拷贝mysql-connector-java-5.1.25-bin.jar 到/usr/share/java/mysql-connector-java.jar #创建数据库：mysqlcreate database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database monitor DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci; use mysql;grant all on . to root@‘master’ Identified by ‘123456’;flush privileges; 12.安装cloudera-manager #解压cm tar 包到指定目录mkdir /opt/cloudera-managertar -zxvf cloudier-manager-centos7-cm5.6.0_x86_64.tar.gz -C/opt/cloudera-manager #创建cloudera-scm用户：[root@master cloudera-manager]# useradd –system –home=/opt/cloudera-manager/cm-5.6.0/run/cloudera-scm-server–no-create-home –shell=/bin/false –comment “Cloudera SCM User” cloudera-scm #在注解点创建cloudera-manager-server的本地元数据保存目录mkdir /var/cloudera-scm-serverchown cloudera-scm:cloudera-scm /var/cloudera-scm-serverchown cloudera-scm:cloudera-scm /opt/cloudera-manager #配置从节点cloudera-manager-agent 指向注解点服务器vi /opt/cloudera-manager/cm-5.6.0/etc/cloudera-scm-agent/config.ini #将server host改为CMS所在的主机名即master #注解点中创建parcel-repo 仓库目录：mkdir -p /opt/cloudera/parcel-repochown cloudera-scm:cloudera-scm/opt/cloudera/parcel-repocp CDH-5.6.0-1.cdh5.6.0.p0.18-el7.parcel CDH-5.6.0-1.cdh5.6.0.p0.18-el7.parcel.sha manifest.json /opt/cloudera/parcel-repo #所有节点创建parcel目录：mkdir -p /opt/cloudera/parcelschown cloudera-scm:cloudera-scm/opt/cloudera/parcels 13.初始化脚本配置数据库：/opt/cloudera-manager/cm-5.6.0/share/cmf/schema/scm_prepare_database.sh mysql -hmaster -uroot -p123456 —sim-host master scmdbn scmdbu scmdbp 14.启动注解点cloudera scm servercp /opt/cloudera-manager/cm-5.6.0/etc/init.d/cloudera-scm-server /etc/init.d/cloudera-scm-server #修改变量路径：vi /etc/init.d/cloudera-scm-server 将CMF_DEFAULTS=${CMF_DEFAULTS:-/etc/default}改为=/opt/cloudera-manager/cm-5.6.0/etc/default chkconfig cloudera-scm-server on #启动注解点cloudera scm server mkdir /opt/cloudera-manager/cm-5.6.0/run/cloudera-scm-agentcp /opt/cloudera-manager/cm-5.6.0/etc/init.d/cloudera-scm-agent /etc/init.d/cloudera-scm-agent #修改变量路径：vi /etc/init.d/cloudera-scm-agent #将CMF_DEFAULTS=${CMF_DEFAULTS:-/etc/default}改为=/opt/cloudera-manager/cm-5.6.0/etc/default chkconfig cloudera-scm-agent on service cloudera-scm-server start","raw":null,"content":null,"categories":[{"name":"集群搭建","slug":"集群搭建","permalink":"https://stanxia.github.io/categories/集群搭建/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://stanxia.github.io/tags/hadoop/"}]},{"title":"hadoop原生集群搭建","slug":"hadoop原生集群搭建","date":"2016-02-11T02:55:39.000Z","updated":"2017-03-19T14:29:36.000Z","comments":true,"path":"2016/02/11/hadoop原生集群搭建/","link":"","permalink":"https://stanxia.github.io/2016/02/11/hadoop原生集群搭建/","excerpt":"","text":"一：解压hadoop.tar.gz和jdk安装包 将hadoop和jdk解压在/opt目录下面 12tar -zxvf jdk1.8.gz -C /opt/ #解压jdk到/opt下tar -zxvf hadoop-2.6.0.tar.gz -C /opt/ #解压hadoop到/opt 在/opt目录下面修改 Hadoop和jdk 名字（非必需，只是为了后续的操作方便） 12mv hadoop-2.6.0 hadoop #hadoop文件夹重命名mv jdk1.8 jdk #jdk文件夹重命名 二：设置SSH互信： 在每台服务器上面设置SSH无密码登录： 1ssh-keygen -t rsa -p '' #-t rsa 表示通过rsa 算法处理 ；-p '' 设置密码为‘’ 即为空 拷贝每台服务器上面的 idrsa.pub ： 12345cd ~ #到当前用户目录cd .ssh #到存放idrsa.pub 的目录scp idrsa.pub hadoop@master:/home/hadoop/.ssh/ #所有服务器上面的idrsa.pub都传给mastertouch authorized_keys #matsr新建authorized_keys文件，存放所有服务器的idrsa.pub公匙scp authorized_keys hadoop@slave1:/home/hadoop/.ssh/ #将authorized_keys复制给集群中的所有服务器 试试效果： 1ssh slave1 #如果不需要输密码，则证明配置成功；若配置失败，再进行一次第二步操作 三：修改配置文件： 修改/etc/profile配置java hadoop环境变量 vi /etc/profile 添加如下代码： 123export JAVA_HOME=/opt/jdk1.8export HADOOP_HOME=/opt/hadoopexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin 配置/etc/hostname（重启生效） vi /etc/hostname 添加该机的服务器名（例如namenode所在的服务器就写 master）： 1master 配置 /etc/hosts（重启生效） vi /etc/hosts 配置集群所有服务器的IP与hostname映射关系： 123192.168.1.121 master192.168.1.122 slave1192.168.1.123 slave2 配置/etc/sysconfig/network vi /etc/sysconfig/network 添加如下代码： 12NETWORKING=yesHOSTNAME=master 关闭防火墙(必须关闭防火墙，否则会出现很多问题) 直接在命令行执行以下代码： 123systemctl stop firewalld #关闭防火墙systemctl disable firewalld #防火墙下线systemctl status firewalld #查看防火墙状态（dead为已关闭） 添加hadoop用户 root用户执行以下代码： 123adduser hadoop #添加hadoop用户passwd hadoop #修改密码usermod -g root hadoop #将hadoop用户放在root组 配置yarn-site.xml 12345678910111213141516&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;monsterxls:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;monsterxls:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;monsterxls:8031&lt;/value&gt; &lt;/property&gt; 配置mapred-site.xml 123456789101112&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;monsterxls:10020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;monsterxls:19888&lt;/value&gt;&lt;/property&gt; 配置hdfs-site.xml 123456789101112&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.ipc.address&lt;/name&gt; &lt;value&gt;0.0.0.0:50020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.http.address&lt;/name&gt; &lt;value&gt;0.0.0.0:50075&lt;/value&gt;&lt;/property&gt; 配置core-site.xml 12345678&lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://monsterxls:9000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/tmp&lt;/value&gt;&lt;/property&gt; 配置hadoop-env.sh export JAVA_HOME=/opt/jdk1.8 配置yarn-env.sh 1export HADOOP_YARN_USER=/opt/hadoop 三：总结安装集群要细心，理解着意思去安装，特别是配置文件，想想为什么要这么设置，出了问题多看看配置文件，检查是否有误设置的地方。 Good Luck!","raw":null,"content":null,"categories":[{"name":"集群搭建","slug":"集群搭建","permalink":"https://stanxia.github.io/categories/集群搭建/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://stanxia.github.io/tags/hadoop/"}]},{"title":"ssh互信","slug":"ssh互信","date":"2016-02-11T01:58:28.000Z","updated":"2017-03-19T14:31:50.000Z","comments":true,"path":"2016/02/11/ssh互信/","link":"","permalink":"https://stanxia.github.io/2016/02/11/ssh互信/","excerpt":"","text":"1.ssh-keygen -t rsa -P ‘’-t rsa表示通过rsa算法-P表示设置密码 cd .ssh :包含文件 idrsa为密匙 idrsa.pub为公钥 如果当前使用的用户时hadoop，当使用ssh切换时默认的是到hadoop用户 ，可以使用ssh root@hadoop 2.跨机器传输：scp 文件 hadoop@hadoop1:/目标路径 scp idrsa.pub hadoop@hadoop1:/home/hadoop/文件夹为：scp -r …","raw":null,"content":null,"categories":[{"name":"集群搭建","slug":"集群搭建","permalink":"https://stanxia.github.io/categories/集群搭建/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://stanxia.github.io/tags/hadoop/"}]},{"title":"chrome离线恐龙快跑游戏","slug":"chrome离线恐龙快跑游戏","date":"2016-01-18T15:36:40.000Z","updated":"2017-03-19T14:28:52.000Z","comments":true,"path":"2016/01/18/chrome离线恐龙快跑游戏/","link":"","permalink":"https://stanxia.github.io/2016/01/18/chrome离线恐龙快跑游戏/","excerpt":"","text":"游戏介绍来源自Google chrome 浏览器没有网络状态下的小彩蛋。 安装指南 右键这里存储连接下载源码 将下载的js文件放置在source/js/下面 在页面上添加如下代码即可： 12345&lt;div id=\"container\"&gt;&lt;/div&gt;&lt;script src=\"/js/runner.js\"&gt;&lt;/script&gt;&lt;script&gt; initRunner('#container');&lt;/script&gt; 食用指南手机端：点触屏幕即可开始和操作。 电脑端：点击小恐龙，按空格键即可开始和操作。 initRunner('#container');","raw":null,"content":null,"categories":[{"name":"折腾","slug":"折腾","permalink":"https://stanxia.github.io/categories/折腾/"}],"tags":[{"name":"折腾","slug":"折腾","permalink":"https://stanxia.github.io/tags/折腾/"}]},{"title":"这个杀手不太冷","slug":"这个杀手不太冷","date":"2016-01-16T16:29:57.000Z","updated":"2017-03-19T14:28:29.000Z","comments":true,"path":"2016/01/17/这个杀手不太冷/","link":"","permalink":"https://stanxia.github.io/2016/01/17/这个杀手不太冷/","excerpt":"","text":"","raw":null,"content":null,"categories":[{"name":"电影","slug":"电影","permalink":"https://stanxia.github.io/categories/电影/"}],"tags":[{"name":"电影","slug":"电影","permalink":"https://stanxia.github.io/tags/电影/"}]},{"title":"海贼王856“少骗人了”","slug":"海贼王856“少骗人了”","date":"2016-01-16T15:06:34.000Z","updated":"2017-03-19T14:51:36.000Z","comments":true,"path":"2016/01/16/海贼王856“少骗人了”/","link":"","permalink":"https://stanxia.github.io/2016/01/16/海贼王856“少骗人了”/","excerpt":"","text":"不定期转更！12345&lt;center&gt;&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=86 src=&quot;http://music.163.com/outchain/player?type=2&amp;id=25706282&amp;auto=0&amp;height=66&quot;&gt;&lt;/iframe&gt; &lt;/center&gt; if (\"xls123456\"==prompt(\"通关密码\")) {alert(\"芝麻开门\"); }else{alert(\"容我再想想\"); location=\"https://stanxia.github.io/\"; }","raw":null,"content":null,"categories":[{"name":"漫画","slug":"漫画","permalink":"https://stanxia.github.io/categories/漫画/"}],"tags":[{"name":"one-piece","slug":"one-piece","permalink":"https://stanxia.github.io/tags/one-piece/"}]},{"title":"接入bilibili视频播放","slug":"测试视频播放","date":"2016-01-15T12:49:37.000Z","updated":"2017-03-19T14:51:42.000Z","comments":true,"path":"2016/01/15/测试视频播放/","link":"","permalink":"https://stanxia.github.io/2016/01/15/测试视频播放/","excerpt":"","text":"测试视频播放接入bilibili的视频，只需要在md文档中添加如下代码即可： 1&lt;iframe src=\"https://www.bilibili.com/html/html5player.html?aid=3521416&amp;cid=6041635\" width=\"640\" height=\"480\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt; 其中aid和cid在bilibili网页上都可以爬出来。 效果如下：","raw":null,"content":null,"categories":[{"name":"折腾","slug":"折腾","permalink":"https://stanxia.github.io/categories/折腾/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://stanxia.github.io/tags/hexo/"}]},{"title":"敦煌展","slug":"敦煌展","date":"2016-01-14T15:42:32.000Z","updated":"2017-03-19T14:51:49.000Z","comments":true,"path":"2016/01/14/敦煌展/","link":"","permalink":"https://stanxia.github.io/2016/01/14/敦煌展/","excerpt":"","text":"","raw":null,"content":null,"categories":[{"name":"生活","slug":"生活","permalink":"https://stanxia.github.io/categories/生活/"}],"tags":[]},{"title":"海賊王 第 859 话 四皇暗殺作戰","slug":"海賊王-第-859-话-四皇暗殺作戰","date":"2015-03-16T15:45:13.000Z","updated":"2017-03-19T15:02:30.000Z","comments":true,"path":"2015/03/16/海賊王-第-859-话-四皇暗殺作戰/","link":"","permalink":"https://stanxia.github.io/2015/03/16/海賊王-第-859-话-四皇暗殺作戰/","excerpt":"","text":"","raw":null,"content":null,"categories":[{"name":"漫画","slug":"漫画","permalink":"https://stanxia.github.io/categories/漫画/"}],"tags":[{"name":"onepiece","slug":"onepiece","permalink":"https://stanxia.github.io/tags/onepiece/"}]},{"title":"MAC应用无法打开或文件损坏的处理方法","slug":"MAC应用无法打开或文件损坏的处理方法","date":"2015-02-15T06:17:24.000Z","updated":"2017-03-19T14:31:07.000Z","comments":true,"path":"2015/02/15/MAC应用无法打开或文件损坏的处理方法/","link":"","permalink":"https://stanxia.github.io/2015/02/15/MAC应用无法打开或文件损坏的处理方法/","excerpt":"","text":"问题下载了一些程序之后，却发现无法在MAC中安装，安装时会弹出下图所示警告框：“打不开 xxx，因为它来自身份不明的开发者” 原因在MAC下安装一些软件时提示”来自身份不明开发者”，其实这是MAC新系统启用了新的安全机制。默认只信任 Mac App Store 下载的软件和拥有开发者 ID 签名的应用程序。换句话说就是 MAC 系统默认只能安装靠谱渠道（有苹果审核的 Mac App Store）下载的软件或被认可的人开发的软件。 这当然是为了用户不会稀里糊涂安装流氓软件中招，但没有开发者签名的 “老实软件” 也受影响了，安装就会弹出下图所示警告框：“打不开 xxx，因为它来自身份不明的开发者”。 解决方案 最简单的方式：按住Control后，再次点击软件图标，即可。 修改系统配置：系统偏好设置… -&gt; 安全性与隐私… -&gt;通用… -&gt;选择任何来源。 macOs Sierra 10.2以上版本，打开终端，执行:sudo spctl --master-disable 就可以啦。","raw":null,"content":null,"categories":[{"name":"生活","slug":"生活","permalink":"https://stanxia.github.io/categories/生活/"}],"tags":[{"name":"mac","slug":"mac","permalink":"https://stanxia.github.io/tags/mac/"}]}]}