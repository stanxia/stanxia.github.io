{"meta":{"title":"东篱下","subtitle":"斯坦@森","description":"采菊东篱下，悠然见南山","author":"森","url":"https://stanxia.github.io"},"posts":[{"title":"JavaRDDLike.scala","slug":"JavaRDDLike-scala","date":"2017-11-21T06:21:23.000Z","updated":"2017-11-21T06:31:58.000Z","comments":true,"path":"2017/11/21/JavaRDDLike-scala/","link":"","permalink":"https://stanxia.github.io/2017/11/21/JavaRDDLike-scala/","excerpt":"使用Java开发Spark程序，JavaRDD的功能算子中英文注释\nAs a workaround for https://issues.scala-lang.org/browse/SI-8905, implementations of JavaRDDLike should extend this dummy abstract class instead of directly inheriting from the trait. See SPARK-3266 for additional details.作为https://issues.scala-lang.org/browse/SI-8905的一个解决方案，JavaRDDLike的实现应该扩展这个虚拟抽象类，而不是直接继承这个特性。更多细节见spark - 3266。","text":"使用Java开发Spark程序，JavaRDD的功能算子中英文注释 As a workaround for https://issues.scala-lang.org/browse/SI-8905, implementations of JavaRDDLike should extend this dummy abstract class instead of directly inheriting from the trait. See SPARK-3266 for additional details.作为https://issues.scala-lang.org/browse/SI-8905的一个解决方案，JavaRDDLike的实现应该扩展这个虚拟抽象类，而不是直接继承这个特性。更多细节见spark - 3266。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894package org.apache.spark.api.javaprivate[spark] abstract class AbstractJavaRDDLike[T, This &lt;: JavaRDDLike[T, This]] extends JavaRDDLike[T, This]/** * Defines operations common to several Java RDD implementations. * * 定义几个Java RDD实现的常见操作。 * * * @note This trait is not intended to be implemented by user code. * * 该特性不打算由用户代码实现。 */trait JavaRDDLike[T, This &lt;: JavaRDDLike[T, This]] extends Serializable &#123; def wrapRDD(rdd: RDD[T]): This implicit val classTag: ClassTag[T] def rdd: RDD[T] /** Set of partitions in this RDD. * 在这个RDD中设置的分区。 * */ def partitions: JList[Partition] = rdd.partitions.toSeq.asJava /** Return the number of partitions in this RDD. * 返回该RDD中的分区数。 * */ @Since(\"1.6.0\") def getNumPartitions: Int = rdd.getNumPartitions /** The partitioner of this RDD. * 这个RDD的分区。 * */ def partitioner: Optional[Partitioner] = JavaUtils.optionToOptional(rdd.partitioner) /** The [[org.apache.spark.SparkContext]] that this RDD was created on. * * 这个RDD是在[[org.apache.spark.SparkContext]]上面创建的。 * */ def context: SparkContext = rdd.context /** A unique ID for this RDD (within its SparkContext). * 这个RDD的惟一ID(在它的SparkContext内)。 * */ def id: Int = rdd.id /** Get the RDD's current storage level, or StorageLevel.NONE if none is set. * 获取RDD的当前存储级别，或StorageLevel。如果没有设置就没有。 * */ def getStorageLevel: StorageLevel = rdd.getStorageLevel /** * Internal method to this RDD; will read from cache if applicable, or otherwise compute it. * This should ''not'' be called by users directly, but is available for implementors of custom * subclasses of RDD. * 内部方法的RDD;将从缓存读取，如果适用的话，或者计算它。 * 这应该“不是”直接由用户调用，而是用于RDD的自定义子类的实现者 * */ def iterator(split: Partition, taskContext: TaskContext): JIterator[T] = rdd.iterator(split, taskContext).asJava // Transformations (return a new RDD) /** * Return a new RDD by applying a function to all elements of this RDD. * 将一个函数应用于这个RDD的所有元素，返回一个新的RDD。 * */ def map[R](f: JFunction[T, R]): JavaRDD[R] = new JavaRDD(rdd.map(f)(fakeClassTag))(fakeClassTag) /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * 通过在RDD的每个分区上应用一个函数来返回一个新的RDD，同时跟踪原始分区的索引。 * */ def mapPartitionsWithIndex[R]( f: JFunction2[jl.Integer, JIterator[T], JIterator[R]], preservesPartitioning: Boolean = false): JavaRDD[R] = new JavaRDD(rdd.mapPartitionsWithIndex((a, b) =&gt; f.call(a, b.asJava).asScala, preservesPartitioning)(fakeClassTag))(fakeClassTag) /** * Return a new RDD by applying a function to all elements of this RDD. * 将一个函数应用于这个RDD的所有元素，返回一个新的RDD。 */ def mapToDouble[R](f: DoubleFunction[T]): JavaDoubleRDD = &#123; new JavaDoubleRDD(rdd.map(f.call(_).doubleValue())) &#125; /** * Return a new RDD by applying a function to all elements of this RDD. * 将一个函数应用于这个RDD的所有元素，返回一个新的RDD。 * */ def mapToPair[K2, V2](f: PairFunction[T, K2, V2]): JavaPairRDD[K2, V2] = &#123; def cm: ClassTag[(K2, V2)] = implicitly[ClassTag[(K2, V2)]] new JavaPairRDD(rdd.map[(K2, V2)](f)(cm))(fakeClassTag[K2], fakeClassTag[V2]) &#125; /** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. * 返回一个新的RDD，首先将一个函数应用于这个RDD的所有元素，然后将结果扁平化。 * */ def flatMap[U](f: FlatMapFunction[T, U]): JavaRDD[U] = &#123; def fn: (T) =&gt; Iterator[U] = (x: T) =&gt; f.call(x).asScala JavaRDD.fromRDD(rdd.flatMap(fn)(fakeClassTag[U]))(fakeClassTag[U]) &#125; /** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. * 返回一个新的RDD，首先将一个函数应用于这个RDD的所有元素，然后将结果扁平化。 * */ def flatMapToDouble(f: DoubleFlatMapFunction[T]): JavaDoubleRDD = &#123; def fn: (T) =&gt; Iterator[jl.Double] = (x: T) =&gt; f.call(x).asScala new JavaDoubleRDD(rdd.flatMap(fn).map(_.doubleValue())) &#125; /** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. * 返回一个新的RDD，首先将一个函数应用于这个RDD的所有元素，然后将结果扁平化。 * */ def flatMapToPair[K2, V2](f: PairFlatMapFunction[T, K2, V2]): JavaPairRDD[K2, V2] = &#123; def fn: (T) =&gt; Iterator[(K2, V2)] = (x: T) =&gt; f.call(x).asScala def cm: ClassTag[(K2, V2)] = implicitly[ClassTag[(K2, V2)]] JavaPairRDD.fromRDD(rdd.flatMap(fn)(cm))(fakeClassTag[K2], fakeClassTag[V2]) &#125; /** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */ def mapPartitions[U](f: FlatMapFunction[JIterator[T], U]): JavaRDD[U] = &#123; def fn: (Iterator[T]) =&gt; Iterator[U] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; JavaRDD.fromRDD(rdd.mapPartitions(fn)(fakeClassTag[U]))(fakeClassTag[U]) &#125; /** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */ def mapPartitions[U](f: FlatMapFunction[JIterator[T], U], preservesPartitioning: Boolean): JavaRDD[U] = &#123; def fn: (Iterator[T]) =&gt; Iterator[U] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; JavaRDD.fromRDD( rdd.mapPartitions(fn, preservesPartitioning)(fakeClassTag[U]))(fakeClassTag[U]) &#125; /** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */ def mapPartitionsToDouble(f: DoubleFlatMapFunction[JIterator[T]]): JavaDoubleRDD = &#123; def fn: (Iterator[T]) =&gt; Iterator[jl.Double] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; new JavaDoubleRDD(rdd.mapPartitions(fn).map(_.doubleValue())) &#125; /** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */ def mapPartitionsToPair[K2, V2](f: PairFlatMapFunction[JIterator[T], K2, V2]): JavaPairRDD[K2, V2] = &#123; def fn: (Iterator[T]) =&gt; Iterator[(K2, V2)] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; JavaPairRDD.fromRDD(rdd.mapPartitions(fn))(fakeClassTag[K2], fakeClassTag[V2]) &#125; /** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */ def mapPartitionsToDouble(f: DoubleFlatMapFunction[JIterator[T]], preservesPartitioning: Boolean): JavaDoubleRDD = &#123; def fn: (Iterator[T]) =&gt; Iterator[jl.Double] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; new JavaDoubleRDD(rdd.mapPartitions(fn, preservesPartitioning) .map(_.doubleValue())) &#125; /** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */ def mapPartitionsToPair[K2, V2](f: PairFlatMapFunction[JIterator[T], K2, V2], preservesPartitioning: Boolean): JavaPairRDD[K2, V2] = &#123; def fn: (Iterator[T]) =&gt; Iterator[(K2, V2)] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; JavaPairRDD.fromRDD( rdd.mapPartitions(fn, preservesPartitioning))(fakeClassTag[K2], fakeClassTag[V2]) &#125; /** * Applies a function f to each partition of this RDD. * 将函数f应用于该RDD的每个分区。 * */ def foreachPartition(f: VoidFunction[JIterator[T]]): Unit = &#123; rdd.foreachPartition(x =&gt; f.call(x.asJava)) &#125; /** * Return an RDD created by coalescing all elements within each partition into an array. * 返回一个RDD，它将每个分区中的所有元素合并到一个数组中。 * */ def glom(): JavaRDD[JList[T]] = new JavaRDD(rdd.glom().map(_.toSeq.asJava)) /** * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of * elements (a, b) where a is in `this` and b is in `other`. * 返回这个RDD和另一个的笛卡尔乘积，即所有元素对的RDD(a,b) ：a在该RDD中，b在另一个RDD中 * */ def cartesian[U](other: JavaRDDLike[U, _]): JavaPairRDD[T, U] = JavaPairRDD.fromRDD(rdd.cartesian(other.rdd)(other.classTag))(classTag, other.classTag) /** * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements * mapping to that key. * 返回分组元素的RDD。 * 每个组由一个键和一个映射到该键的元素序列组成。 * */ def groupBy[U](f: JFunction[T, U]): JavaPairRDD[U, JIterable[T]] = &#123; // The type parameter is U instead of K in order to work around a compiler bug; see SPARK-4459 // 类型参数是U而不是K，是为了绕过编译器错误 implicit val ctagK: ClassTag[U] = fakeClassTag implicit val ctagV: ClassTag[JList[T]] = fakeClassTag JavaPairRDD.fromRDD(groupByResultToJava(rdd.groupBy(f)(fakeClassTag))) &#125; /** * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements * mapping to that key. * 返回分组元素的RDD。 * 每个组由一个键和一个映射到该键的元素序列组成。 */ def groupBy[U](f: JFunction[T, U], numPartitions: Int): JavaPairRDD[U, JIterable[T]] = &#123; // The type parameter is U instead of K in order to work around a compiler bug; see SPARK-4459 implicit val ctagK: ClassTag[U] = fakeClassTag implicit val ctagV: ClassTag[JList[T]] = fakeClassTag JavaPairRDD.fromRDD(groupByResultToJava(rdd.groupBy(f, numPartitions)(fakeClassTag[U]))) &#125; /** * Return an RDD created by piping elements to a forked external process. * 返回由管道元素调用外部程序返回新的RDD * */ def pipe(command: String): JavaRDD[String] = &#123; rdd.pipe(command) &#125; /** * Return an RDD created by piping elements to a forked external process. * 返回由管道元素调用外部程序返回新的RDD * */ def pipe(command: JList[String]): JavaRDD[String] = &#123; rdd.pipe(command.asScala) &#125; /** * Return an RDD created by piping elements to a forked external process. * 返回由管道元素调用外部程序返回新的RDD * */ def pipe(command: JList[String], env: JMap[String, String]): JavaRDD[String] = &#123; rdd.pipe(command.asScala, env.asScala) &#125; /** * Return an RDD created by piping elements to a forked external process. * 返回由管道元素调用外部程序返回新的RDD * */ def pipe(command: JList[String], env: JMap[String, String], separateWorkingDir: Boolean, bufferSize: Int): JavaRDD[String] = &#123; rdd.pipe(command.asScala, env.asScala, null, null, separateWorkingDir, bufferSize) &#125; /** * Return an RDD created by piping elements to a forked external process. * 返回由管道元素调用外部程序返回新的RDD * */ def pipe(command: JList[String], env: JMap[String, String], separateWorkingDir: Boolean, bufferSize: Int, encoding: String): JavaRDD[String] = &#123; rdd.pipe(command.asScala, env.asScala, null, null, separateWorkingDir, bufferSize, encoding) &#125; /** * Zips this RDD with another one, returning key-value pairs with the first element in each RDD, * second element in each RDD, etc. Assumes that the two RDDs have the *same number of * partitions* and the *same number of elements in each partition* (e.g. one was made through * a map on the other). * 将此RDD与另一个RDD进行Zips，返回键值对，每个RDD中的第一个元素，每个RDD中的第二个元素，等等。 * 假设两个RDDs拥有相同数量的分区和每个分区中相同数量的元素 * (例如，一个是通过另一个map的)。 */ def zip[U](other: JavaRDDLike[U, _]): JavaPairRDD[T, U] = &#123; JavaPairRDD.fromRDD(rdd.zip(other.rdd)(other.classTag))(classTag, other.classTag) &#125; /** * Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by * applying a function to the zipped partitions. Assumes that all the RDDs have the * *same number of partitions*, but does *not* require them to have the same number * of elements in each partition. * 用一个(或多个)RDD(或多个)来压缩这个RDD的分区，并返回一个新的RDD将函数应用于压缩分区。 * 假设所有RDDs拥有相同数量的分区，但不要求它们在每个分区中拥有相同数量的元素。 */ def zipPartitions[U, V]( other: JavaRDDLike[U, _], f: FlatMapFunction2[JIterator[T], JIterator[U], V]): JavaRDD[V] = &#123; def fn: (Iterator[T], Iterator[U]) =&gt; Iterator[V] = &#123; (x: Iterator[T], y: Iterator[U]) =&gt; f.call(x.asJava, y.asJava).asScala &#125; JavaRDD.fromRDD( rdd.zipPartitions(other.rdd)(fn)(other.classTag, fakeClassTag[V]))(fakeClassTag[V]) &#125; /** * Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k, * 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method * won't trigger a spark job, which is different from [[org.apache.spark.rdd.RDD#zipWithIndex]]. * 用生成的唯一长的id来压缩这个RDD。 * 第k个分区的项将得到id k,n + k,2 *n+ k，…，其中n是分区数。 * 因此，可能存在差距，但这种方法不会触发spark作业，它与[org .apache.spark. spark.rdd. rdd. rdd # zipWithIndex]不同。 */ def zipWithUniqueId(): JavaPairRDD[T, jl.Long] = &#123; JavaPairRDD.fromRDD(rdd.zipWithUniqueId()).asInstanceOf[JavaPairRDD[T, jl.Long]] &#125; /** * Zips this RDD with its element indices. The ordering is first based on the partition index * and then the ordering of items within each partition. So the first item in the first * partition gets index 0, and the last item in the last partition receives the largest index. * This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type. * This method needs to trigger a spark job when this RDD contains more than one partitions. * * 用它的元素索引来压缩这个RDD。 * 排序首先基于分区索引，然后是每个分区中的条目的排序。 * 因此，第一个分区中的第一个项的索引值为0，最后一个分区中的最后一个项得到最大的索引。 * 这类似于Scala的zipWithIndex，但它使用的是Long而不是Int作为索引类型。 * 当这个RDD包含多个分区时，这个方法需要触发一个spark作业。 */ def zipWithIndex(): JavaPairRDD[T, jl.Long] = &#123; JavaPairRDD.fromRDD(rdd.zipWithIndex()).asInstanceOf[JavaPairRDD[T, jl.Long]] &#125; // Actions (launch a job to return a value to the user program) // 操作(启动作业以返回用户程序的值) /** * Applies a function f to all elements of this RDD. * 将函数f应用于该RDD的所有元素。 */ def foreach(f: VoidFunction[T]) &#123; rdd.foreach(x =&gt; f.call(x)) &#125; /** * Return an array that contains all of the elements in this RDD. * 返回包含该RDD中所有元素的数组。 * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。 */ def collect(): JList[T] = rdd.collect().toSeq.asJava /** * Return an iterator that contains all of the elements in this RDD. * 返回包含该RDD中所有元素的迭代器。 * * The iterator will consume as much memory as the largest partition in this RDD. * 迭代器将消耗与此RDD中最大的分区一样多的内存。 */ def toLocalIterator(): JIterator[T] = asJavaIteratorConverter(rdd.toLocalIterator).asJava /** * Return an array that contains all of the elements in a specific partition of this RDD. * 返回包含该RDD的特定分区中的所有元素的数组。 */ def collectPartitions(partitionIds: Array[Int]): Array[JList[T]] = &#123; // This is useful for implementing `take` from other language frontends // like Python where the data is serialized. // 这有助于从其他语言的前沿实现“take”，如Python，数据被序列化。 val res = context.runJob(rdd, (it: Iterator[T]) =&gt; it.toArray, partitionIds) res.map(_.toSeq.asJava) &#125; /** * Reduces the elements of this RDD using the specified commutative and associative binary * operator. * 使用指定的交换和关联二元运算符来减少该RDD的元素。 */ def reduce(f: JFunction2[T, T, T]): T = rdd.reduce(f) /** * Reduces the elements of this RDD in a multi-level tree pattern. * 将此RDD的元素简化为多层树模式。 * * @param depth suggested depth of the tree 建议树的深度 * @see [[org.apache.spark.api.java.JavaRDDLike#reduce]] */ def treeReduce(f: JFunction2[T, T, T], depth: Int): T = rdd.treeReduce(f, depth) /** * [[org.apache.spark.api.java.JavaRDDLike#treeReduce]] 建议深度 2 . */ def treeReduce(f: JFunction2[T, T, T]): T = treeReduce(f, 2) /** * Aggregate the elements of each partition, and then the results for all the partitions, using a * given associative function and a neutral \"zero value\". The function * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object * allocation; however, it should not modify t2. * 对每个分区的元素进行聚合，然后使用给定的关联函数和中立的“零值”，对所有分区进行结果。 * 函数op(t1,t2)被允许修改t1，并将其作为其结果值返回，以避免对象分配;但是，它不应该修改t2。 * * This behaves somewhat differently from fold operations implemented for non-distributed * collections in functional languages like Scala. This fold operation may be applied to * partitions individually, and then fold those results into the final result, rather than * apply the fold to each element sequentially in some defined ordering. For functions * that are not commutative, the result may differ from that of a fold applied to a * non-distributed collection. * 这与在Scala等函数式语言中实现非分布式集合的折叠操作有一定的不同。 * 这个折叠操作可以单独应用于分区，然后将这些结果折叠到最终结果中，而不是在某些定义的排序中顺序地对每个元素进行折叠。 * 对于非交换的函数，结果可能与应用于非分布式集合的函数不同。 * */ def fold(zeroValue: T)(f: JFunction2[T, T, T]): T = rdd.fold(zeroValue)(f) /** * Aggregate the elements of each partition, and then the results for all the partitions, using * given combine functions and a neutral \"zero value\". This function can return a different result * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are * allowed to modify and return their first argument instead of creating a new U to avoid memory * allocation. * 对每个分区的元素进行聚合，然后使用给定的组合函数和一个中立的“零值”，对所有分区进行结果。 * 这个函数可以返回一个不同的结果类型U，而不是这个RDD的类型。 * 因此，我们需要一个操作来将一个T合并到一个U和一个合并两个U的操作，就像在scala . traversableonce中那样。 * 这两个函数都可以修改和返回第一个参数，而不是创建一个新的U，以避免内存分配。 * */ def aggregate[U](zeroValue: U)(seqOp: JFunction2[U, T, U], combOp: JFunction2[U, U, U]): U = rdd.aggregate(zeroValue)(seqOp, combOp)(fakeClassTag[U]) /** * Aggregates the elements of this RDD in a multi-level tree pattern. * 将此RDD的元素聚集在多层树模式中。 * * @param depth suggested depth of the tree 建议的树的深度 * @see [[org.apache.spark.api.java.JavaRDDLike#aggregate]] */ def treeAggregate[U]( zeroValue: U, seqOp: JFunction2[U, T, U], combOp: JFunction2[U, U, U], depth: Int): U = &#123; rdd.treeAggregate(zeroValue)(seqOp, combOp, depth)(fakeClassTag[U]) &#125; /** * [[org.apache.spark.api.java.JavaRDDLike#treeAggregate]] with suggested depth 2. * 建议的树的深度为 2 */ def treeAggregate[U]( zeroValue: U, seqOp: JFunction2[U, T, U], combOp: JFunction2[U, U, U]): U = &#123; treeAggregate(zeroValue, seqOp, combOp, 2) &#125; /** * Return the number of elements in the RDD. * 返回RDD中元素的数量。 * */ def count(): Long = rdd.count() /** * Approximate version of count() that returns a potentially incomplete result * within a timeout, even if not all tasks have finished. * 近似版本的count()，即使不是所有的任务都完成了，也会在一个超时中返回一个潜在的不完整的结果。 * * * The confidence is the probability that the error bounds of the result will * contain the true value. That is, if countApprox were called repeatedly * with confidence 0.9, we would expect 90% of the results to contain the * true count. The confidence must be in the range [0,1] or an exception will * be thrown. * 置信值是结果的误差边界包含真实值的概率。 * 也就是说，如果countApprox被反复调用，confidence 0.9，我们将期望90%的结果包含真实的计数。 * confidence必须在范围[0,1]中，否则将抛出异常。 * * * @param timeout maximum time to wait for the job, in milliseconds * 等待工作的最大时间，以毫秒为单位 * @param confidence the desired statistical confidence in the result * 对结果的期望的统计信心 * @return a potentially incomplete result, with error bounds * 一个可能不完整的结果，有错误界限 */ def countApprox(timeout: Long, confidence: Double): PartialResult[BoundedDouble] = rdd.countApprox(timeout, confidence) /** * Approximate version of count() that returns a potentially incomplete result * within a timeout, even if not all tasks have finished. * 近似版本的count()，即使不是所有的任务都完成了，也会在一个超时中返回一个潜在的不完整的结果。 * * * @param timeout maximum time to wait for the job, in milliseconds * 等待工作的最大时间，以毫秒为单位 */ def countApprox(timeout: Long): PartialResult[BoundedDouble] = rdd.countApprox(timeout) /** * Return the count of each unique value in this RDD as a map of (value, count) pairs. The final * combine step happens locally on the master, equivalent to running a single reduce task. * 将此RDD中的每个惟一值的计数作为(值、计数)对的映射。 * 最后的联合步骤在master的本地发生，相当于运行一个reduce任务。 * */ def countByValue(): JMap[T, jl.Long] = mapAsSerializableJavaMap(rdd.countByValue()).asInstanceOf[JMap[T, jl.Long]] /** * Approximate version of countByValue(). * countByValue()近似的版本。 * * The confidence is the probability that the error bounds of the result will * contain the true value. That is, if countApprox were called repeatedly * with confidence 0.9, we would expect 90% of the results to contain the * true count. The confidence must be in the range [0,1] or an exception will * be thrown. * 置信值是结果的误差边界包含真实值的概率。 * 也就是说，如果countApprox被反复调用，confidence 0.9，我们将期望90%的结果包含真实的计数。 * confidence必须在范围[0,1]中，否则将抛出异常。 * * * @param timeout maximum time to wait for the job, in milliseconds * 等待工作的最大时间，毫秒为单位。 * @param confidence the desired statistical confidence in the result * 对结果的期望的统计信心 * @return a potentially incomplete result, with error bounds * 一个可能不完整的结果，有错误界限 */ def countByValueApprox( timeout: Long, confidence: Double ): PartialResult[JMap[T, BoundedDouble]] = rdd.countByValueApprox(timeout, confidence).map(mapAsSerializableJavaMap) /** * Approximate version of countByValue(). * countByValue().的近似版本. * * @param timeout maximum time to wait for the job, in milliseconds * 等待工作的最大时间，毫秒为单位。 * @return a potentially incomplete result, with error bounds * 一个可能不完整的结果，有错误界限 */ def countByValueApprox(timeout: Long): PartialResult[JMap[T, BoundedDouble]] = rdd.countByValueApprox(timeout).map(mapAsSerializableJavaMap) /** * Take the first num elements of the RDD. This currently scans the partitions *one by one*, so * it will be slow if a lot of partitions are required. In that case, use collect() to get the * whole RDD instead. * 获取RDD的第一个num元素。 * 这将会一次一个地扫描分区，所以如果需要很多分区，它将会很慢。 * 在这种情况下，使用collect()来获得整个RDD。 * * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。 * */ def take(num: Int): JList[T] = rdd.take(num).toSeq.asJava def takeSample(withReplacement: Boolean, num: Int): JList[T] = takeSample(withReplacement, num, Utils.random.nextLong) def takeSample(withReplacement: Boolean, num: Int, seed: Long): JList[T] = rdd.takeSample(withReplacement, num, seed).toSeq.asJava /** * Return the first element in this RDD. * 返回这个RDD中的第一个元素。 */ def first(): T = rdd.first() /** * @return true if and only if the RDD contains no elements at all. Note that an RDD * may be empty even when it has at least 1 partition. * 当且仅当RDD不包含任何元素，则为真。 * 请注意，即使在至少有一个分区的情况下，RDD也可能是空的。 */ def isEmpty(): Boolean = rdd.isEmpty() /** * Save this RDD as a text file, using string representations of elements. * 将此RDD保存为文本文件，使用元素的字符串表示形式。 */ def saveAsTextFile(path: String): Unit = &#123; rdd.saveAsTextFile(path) &#125; /** * Save this RDD as a compressed text file, using string representations of elements. * 将此RDD保存为一个压缩文本文件，使用元素的字符串表示形式。 */ def saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec]): Unit = &#123; rdd.saveAsTextFile(path, codec) &#125; /** * Save this RDD as a SequenceFile of serialized objects. * 将此RDD保存为序列化对象的序列文件。 */ def saveAsObjectFile(path: String): Unit = &#123; rdd.saveAsObjectFile(path) &#125; /** * Creates tuples of the elements in this RDD by applying `f`. * 通过应用“f”创建这个RDD中元素的元组。 */ def keyBy[U](f: JFunction[T, U]): JavaPairRDD[U, T] = &#123; // The type parameter is U instead of K in order to work around a compiler bug; see SPARK-4459 // 类型参数用U替代K，为了绕过编译器错误; implicit val ctag: ClassTag[U] = fakeClassTag JavaPairRDD.fromRDD(rdd.keyBy(f)) &#125; /** * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint * directory set with SparkContext.setCheckpointDir() and all references to its parent * RDDs will be removed. This function must be called before any job has been * executed on this RDD. It is strongly recommended that this RDD is persisted in * memory, otherwise saving it on a file will require recomputation. * 将此RDD标记为检查点。 * 它将被保存到由SparkContext.setCheckpointDir()设置的检查点目录下的文件中。 * 所有对其父RDDs的引用将被删除。 * 在此RDD上执行任何作业之前，必须调用此函数。 * 强烈建议将此RDD保存在内存中，否则将其保存在文件中需要重新计算。 * */ def checkpoint(): Unit = &#123; rdd.checkpoint() &#125; /** * Return whether this RDD has been checkpointed or not * 返回 RDD是否已被检查过 */ def isCheckpointed: Boolean = rdd.isCheckpointed /** * Gets the name of the file to which this RDD was checkpointed * 获取该RDD所指向的checkpointed文件的名称 */ def getCheckpointFile(): Optional[String] = &#123; JavaUtils.optionToOptional(rdd.getCheckpointFile) &#125; /** A description of this RDD and its recursive dependencies for debugging. * 对该RDD及其对调试的递归依赖的描述。 * */ def toDebugString(): String = &#123; rdd.toDebugString &#125; /** * Returns the top k (largest) elements from this RDD as defined by * the specified Comparator[T] and maintains the order. * 根据指定的比较器[T]，从这个RDD中返回最大的k(最大)元素，并维护顺序。 * * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。 * * @param num k, the number of top elements to return 返回的元素数量 * @param comp the comparator that defines the order 定义排序的比较器 * @return an array of top elements 返回最大元素的数组 */ def top(num: Int, comp: Comparator[T]): JList[T] = &#123; rdd.top(num)(Ordering.comparatorToOrdering(comp)).toSeq.asJava &#125; /** * Returns the top k (largest) elements from this RDD using the * natural ordering for T and maintains the order. * 使用T的自然顺序，从这个RDD中返回最大的k(最大)元素，并维护顺序。 * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。 * * @param num k, the number of top elements to return 返回的元素数量 * @return an array of top elements 最大元素的数组 */ def top(num: Int): JList[T] = &#123; val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[T]] top(num, comp) &#125; /** * Returns the first k (smallest) elements from this RDD as defined by * the specified Comparator[T] and maintains the order. * 从这个RDD中返回第一个k(最小)元素，由指定的Comparator[T]定义，并维护该顺序。 * * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。 * * @param num k, the number of elements to return 返回的元素数量 * @param comp the comparator that defines the order 排序比较器 * @return an array of top elements 元素数组 */ def takeOrdered(num: Int, comp: Comparator[T]): JList[T] = &#123; rdd.takeOrdered(num)(Ordering.comparatorToOrdering(comp)).toSeq.asJava &#125; /** * Returns the maximum element from this RDD as defined by the specified * Comparator[T]. * 按照指定比较器[T]定义的RDD， * 返回最大元素。 * * @param comp the comparator that defines ordering 指定的比较器 * @return the maximum of the RDD 最大值 */ def max(comp: Comparator[T]): T = &#123; rdd.max()(Ordering.comparatorToOrdering(comp)) &#125; /** * Returns the minimum element from this RDD as defined by the specified * Comparator[T]. * 按照指定比较器[T]定义的RDD， * 返回最小元素。 * * @param comp the comparator that defines ordering 指定的比较器 * @return the minimum of the RDD 最小值 */ def min(comp: Comparator[T]): T = &#123; rdd.min()(Ordering.comparatorToOrdering(comp)) &#125; /** * Returns the first k (smallest) elements from this RDD using the * natural ordering for T while maintain the order. * 使用原生的 T排序比较器，返回 k个 最小值，并维护这个顺序 * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 尽量应用于小的数组，因为会加载到driver内存中。 * @param num k, the number of top elements to return * @return an array of top elements */ def takeOrdered(num: Int): JList[T] = &#123; val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[T]] takeOrdered(num, comp) &#125; /** * Return approximate number of distinct elements in the RDD. * 返回RDD中不重复元素的数量近似数。 * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * &lt;a href=\"http://dx.doi.org/10.1145/2452376.2452456\"&gt;here&lt;/a&gt;. * 所使用的算法是基于streamlib在实践中的“HyperLogLog”的实现: * “一种艺术基数估计算法状态的算法工程”， * * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017. * 相对精度。 * 较小的值创建需要更多空间的计数器。 * 它必须大于0.000017。 */ def countApproxDistinct(relativeSD: Double): Long = rdd.countApproxDistinct(relativeSD) def name(): String = rdd.name /** * The asynchronous version of `count`, which returns a * future for counting the number of elements in this RDD. * “count”的异步版本，它为计算这个RDD中元素的数量返回一个未来。 */ def countAsync(): JavaFutureAction[jl.Long] = &#123; new JavaFutureActionWrapper[Long, jl.Long](rdd.countAsync(), jl.Long.valueOf) &#125; /** * The asynchronous version of `collect`, which returns a future for * retrieving an array containing all of the elements in this RDD. * “collect”的异步版本， * 它返回一个用于检索包含该RDD中所有元素的数组的未来。 * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 尽量应用于小数量数组。 */ def collectAsync(): JavaFutureAction[JList[T]] = &#123; new JavaFutureActionWrapper(rdd.collectAsync(), (x: Seq[T]) =&gt; x.asJava) &#125; /** * The asynchronous version of the `take` action, which returns a * future for retrieving the first `num` elements of this RDD. * “take”操作的异步版本， * 它将返回用于检索此RDD的第一个“num”元素的未来。 * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. */ def takeAsync(num: Int): JavaFutureAction[JList[T]] = &#123; new JavaFutureActionWrapper(rdd.takeAsync(num), (x: Seq[T]) =&gt; x.asJava) &#125; /** * The asynchronous version of the `foreach` action, which * applies a function f to all the elements of this RDD. * “foreach”操作的异步版本， * 它将函数f应用于这个RDD的所有元素。 * */ def foreachAsync(f: VoidFunction[T]): JavaFutureAction[Void] = &#123; new JavaFutureActionWrapper[Unit, Void](rdd.foreachAsync(x =&gt; f.call(x)), &#123; x =&gt; null.asInstanceOf[Void] &#125;) &#125; /** * The asynchronous version of the `foreachPartition` action, which * applies a function f to each partition of this RDD. * “foreachPartition”操作的异步版本， * 它将函数f应用于该RDD的每个分区。 */ def foreachPartitionAsync(f: VoidFunction[JIterator[T]]): JavaFutureAction[Void] = &#123; new JavaFutureActionWrapper[Unit, Void](rdd.foreachPartitionAsync(x =&gt; f.call(x.asJava)), &#123; x =&gt; null.asInstanceOf[Void] &#125;) &#125;&#125;","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"},{"name":"源码","slug":"源码","permalink":"https://stanxia.github.io/tags/源码/"}]},{"title":"spark取样函数分析","slug":"spark取样函数分析","date":"2017-11-08T09:30:25.000Z","updated":"2017-11-10T04:41:25.000Z","comments":true,"path":"2017/11/08/spark取样函数分析/","link":"","permalink":"https://stanxia.github.io/2017/11/08/spark取样函数分析/","excerpt":"\nSpark取样操作\n无法获取随机样本的解决方案","text":"Spark取样操作 无法获取随机样本的解决方案 背景Dataset中sample函数源码如下： 12345678910111213141516171819202122232425262728293031323334353637/** * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate. 每行数据被抽样的概率 * @param seed Seed for sampling.随机生成器的种子，起始值。如果seed一样，则每次的随机数都一样。 * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]].并不能保证提供按照分数得出的结果，经过实际验证，确实如此。 * * @group typedrel * @since 1.6.0 */ def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = &#123; require(fraction &gt;= 0, s\"Fraction must be nonnegative, but got $&#123;fraction&#125;\") withTypedPlan &#123; Sample(0.0, fraction, withReplacement, seed, logicalPlan)() &#125; &#125; /** * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate. * * @note This is NOT guaranteed to provide exactly the fraction of the total count * of the given [[Dataset]]. * 这里seed直接用生成的随机数代替。 * @group typedrel * @since 1.6.0 */ def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = &#123; sample(withReplacement, fraction, Utils.random.nextLong) &#125; 问题结果数据的行数一般在（fraction*总数）左右。没有一个固定的值，如果需要得到固定行数的随机数据的话不建议采用该方法。 办法获取随机取样的替代方法： 123456df.createOrReplaceTempView(\"test_sample\"); // 生成临时表df.sqlContext() // 添加随机数列，并根据其进行排序 .sql(\"select * ,rand() as random from test_sample order by random\") .limit(2) // 根据参数的fraction计算需要获取的取样结果 .drop(\"random\") // 删除掉添加的随机列 .show();","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"}]},{"title":"spark源码注释翻译","slug":"spark源码注释翻译","date":"2017-11-06T08:57:05.000Z","updated":"2017-11-07T00:57:49.000Z","comments":true,"path":"2017/11/06/spark源码注释翻译/","link":"","permalink":"https://stanxia.github.io/2017/11/06/spark源码注释翻译/","excerpt":"\n版本：spark2.1.1\n目的：方便中文用户阅读源码，把时间花在理解而不是翻译上","text":"版本：spark2.1.1 目的：方便中文用户阅读源码，把时间花在理解而不是翻译上 初衷开始立项进行翻译，一方面方便日后阅读源码，另一方面先粗粒度的熟悉下spark框架和组件。优化完之后希望能帮助更多的中文用户，节省翻译时间。 进度已完成： 正在作：spark core模块 模块名 模块介绍 完成度 api broadcast deploy executor 执行器：用于启动线程池，是真正负责执行task的部件 已完成 input internal io launcher mapred memory metrics network partial rdd rpc scheduler 调度器：spark应用程序的任务调度器 正在作 security serializer shuffle status.api.v1 storage util","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"}]},{"title":"spark关于parquet的优化","slug":"spark关于parquet的优化","date":"2017-11-01T06:53:13.000Z","updated":"2017-11-08T04:20:04.000Z","comments":true,"path":"2017/11/01/spark关于parquet的优化/","link":"","permalink":"https://stanxia.github.io/2017/11/01/spark关于parquet的优化/","excerpt":"\nParquet列式存储在sparkSQL中的应用","text":"Parquet列式存储在sparkSQL中的应用 Parquet的简介parquet是一种列式存储。可以提供面向列的存储和查询。 Parquet的优势在sparkSQL程序中使用parquet格式存储文件，在存储空间和查询性能方面都有很高的效率。 存储方面因为是面向列的存储，同一列的类型相同，因而在存储的过程中可以使用更高效的压缩方案，可以节省大量的存储空间。 查询方面在执行查询任务时，只会扫描需要的列，而不是全部，高度灵活性使查询变得非常高效。 实例测试 测试数据大小 存储类型 存储所占空间 查询性能 1T TEXTFILE 897.9G 698s 1T Parquet 231.4G 21s Parquet的使用使用parquet的简单demo： 12345678910111213141516171819202122// Encoders for most common types are automatically provided by importing spark.implicits._import spark.implicits._val peopleDF = spark.read.json(\"examples/src/main/resources/people.json\")// DataFrames can be saved as Parquet files, maintaining the schema informationpeopleDF.write.parquet(\"people.parquet\")// Read in the parquet file created above// Parquet files are self-describing so the schema is preserved// The result of loading a Parquet file is also a DataFrameval parquetFileDF = spark.read.parquet(\"people.parquet\")// Parquet files can also be used to create a temporary view and then used in SQL statementsparquetFileDF.createOrReplaceTempView(\"parquetFile\")val namesDF = spark.sql(\"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19\")namesDF.map(attributes =&gt; \"Name: \" + attributes(0)).show()// +------------+// | value|// +------------+// |Name: Justin|// +------------+","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"},{"name":"parquet","slug":"parquet","permalink":"https://stanxia.github.io/tags/parquet/"}]},{"title":"三步走战略","slug":"三步走战略","date":"2017-11-01T02:45:09.000Z","updated":"2017-11-05T04:20:40.000Z","comments":true,"path":"2017/11/01/三步走战略/","link":"","permalink":"https://stanxia.github.io/2017/11/01/三步走战略/","excerpt":"\n设定中长期规划\n稳扎稳打，逐个击破，实现技术上的重大突破","text":"设定中长期规划 稳扎稳打，逐个击破，实现技术上的重大突破 第一步深刻了解spark运行机制第二步深度剖析sparkSQL和sparkStreaming第三步实现对spark机器学习的深度掌握","raw":null,"content":null,"categories":[{"name":"规划","slug":"规划","permalink":"https://stanxia.github.io/categories/规划/"}],"tags":[{"name":"规划","slug":"规划","permalink":"https://stanxia.github.io/tags/规划/"}]},{"title":"手把手搭建vps和shadowsocks","slug":"手把手搭建vps和shadowsocks","date":"2017-10-30T16:16:22.000Z","updated":"2017-11-05T04:22:43.000Z","comments":true,"path":"2017/10/31/手把手搭建vps和shadowsocks/","link":"","permalink":"https://stanxia.github.io/2017/10/31/手把手搭建vps和shadowsocks/","excerpt":"\n何来的枷锁\n没有人应该被困在牢笼之中","text":"何来的枷锁 没有人应该被困在牢笼之中 背景记性不好，做个记录，日后有需要时难得费神。 名词解释了解一些原理，熟悉一些名词，也方便理解接下来安装过程中的操作。 vpsVPS(Virtual private server) 译作虚拟专用伺服器。你可以把它简单地理解为一台在远端的强劲电脑。当你租用了它以后，可以给它安装操作系统、软件，并通过一些工具连接和远程操控它。 vultrVultr 是一家 VPS 服务器提供商，有美国、亚洲、欧洲等多地的 VPS。它家的服务器以性价比高闻名，按时间计费，最低的资费为每月 $2.5。 linuxLinux 是免费开源的操作系统，大概被世界上过半服务器所采用。有大量优秀的开源软件可以安装，上述 Shadowsocks 就是其一。你可以通过命令行来直接给 Linux 操作系统「下命令」，比如 $ cd ~/Desktop 就是进入你根目录下的 Desktop 文件夹。 ssh SSH 是一种网络协议，作为每一台 Linux 电脑的标准配置，用于计算机之间的加密登录。当你为租用的 VPS 安装 Linux 系统后，只要借助一些工具，就可以用 SSH 在你自己的 Mac/PC 电脑上远程登录该 VPS 了。 shadowsocksShadowsocks(ss) 是由 Clowwindy 开发的一款软件，其作用本来是加密传输资料。当然，也正因为它加密传输资料的特性，使得 GFW 没法将由它传输的资料和其他普通资料区分开来，也就不能干扰我们访问那些「不存在」的网站了。 搭建vps目的就是搭建梯子。无建站的需求。推荐vultr，最便宜的有2.5美元一个月。500g流量完全够用了。且现在支持支付宝付款，颇为方便。现阶段的优惠活动是新注册的用户完成指定的任务会获得3美元的奖励。（详细情况可依参见官网。） 注册首先点击右侧注册链接：https://www.vultr.com/2017Promo，然后会来到下图所示的注册页面。 第一个框中填写注册邮箱，第二个框中填写注册密码（至少包含1个小写字母、1个大写字母和1个数字），最后点击Create Account创建账户。 创建账户后注册邮箱会收到一封验证邮件，我们需要点击Verify Your E-mail来验证邮箱。 如果注册邮箱收不到验证邮件请更换注册邮箱后重复第一步。 验证邮箱后我们会来到下图所示的登录界面，按下图中指示填写信息，然后点击Login登录。 登陆后我们会来到充值界面。Vultr要求新账户充值后才可以正常创建服务器。Vultr已经支持支付宝了，在这里推荐大家使用支付宝充值，最低金额为10美元。 购买充值完毕后点击右上角的蓝色加号按钮进入创建服务器界面。 首先需要选择Server Location即机房位置，从左到右、从上到下依次为东京、新加坡、伦敦、法兰克福、巴黎、阿姆斯特丹、迈阿密、亚特兰大、芝加哥、硅谷、达拉斯、洛杉矶、纽约、西雅图、悉尼。 然后需要选择Server Type即服务类型，这里大家需要选择安装Debian 7 x64系统，因为这个系统折腾起来比较容易，搭建东西也简单便捷。 然后需要选择Server Size即方案类型，这里大家可以按照需要自行选择，如果只是普通使用那么选择第二个5美元方案即可。 然后Additional Features、Startup Script、SSH Keys以及Server Hostname &amp; Label等四部分大家保持默认即可，最后点击右下方的蓝色Deploy Now按钮确认创建服务器。 创建服务器后我们会看到下图所示界面。 上图中我们需要耐心等待3~4分钟，等红色Installing字变为绿色Running字后，点击Cloud Instance即可进入服务器详细信息界面，如下图所示。 左侧红框内四行信息依次为机房位置、IP地址、登录用户名、登录密码。IP地址后面的按钮为复制IP地址，登录密码后面的按钮为复制密码及显示/隐藏密码。右上角红框内后面四个按钮分别是关闭服务器、重启服务器、重装系统、删除服务器。 远程登录安装远程登录软件。这里以windos端的xshell为例。使用mac的同学可以下载iTerm。 下载安装后打开软件。根据下图中的指示，我们点击会话框中的新建按钮。 点击新建按钮后会弹出下图所示界面。根据图中指示，我们首先填写IP地址，然后点击确定按钮。 点击确定按钮后我们会回到下图所示界面。根据图中指示，我们双击打开新建会话或者点击下方连接按钮打开新建会话。 开新建会话后会弹出下图所示界面。根据图中指示，我们点击接受并保存按钮。 点击接受并保存按钮会弹出下图所示界面。根据图中指示，我们首先填写SSH连接密码，然后打钩记住密码，最后点击确定按钮。 如果提示需要输入用户名（登录名），那么请输入root！ 点击确定按钮后服务器会自动连接，连接完毕后我们会来到下图所示界面 部署shadowsocks这里采用网上整理的一键部署的方案。简单方便操作。 首先复制以下内容： 1wget -N --no-check-certificate https://0123.cool/download/55r.sh &amp;&amp; chmod +x 55r.sh &amp;&amp; ./55r.sh 然后回到Xshell软件，右击选择粘贴，粘贴完毕后回车继续。 回车后系统会自行下载脚本文件并运行。根据下图图中指示，我们依次输入SSR的各项连接信息，最后回车继续。 安装完成后会出现下图所示界面。根据图中指示，我们将红框圈中的信息保存到记事本内。 配置锐意加速根据下图图中指示，我们继续复制下列信息： 1wget -N --no-check-certificate https://0123.cool/download/rs.sh &amp;&amp; bash rs.sh install 然后回到Xshell软件，右击选择粘贴，粘贴完毕后回车继续。 回车后系统会自行下载脚本文件并运行。根据下图图中指示，我们依次输入锐速的各项配置信息，最后回车继续。 回车后，系统自动执行命令完成破解版锐速安装，如下图所示。 我们首先输入： 1reboot 然后回车，Xshell会断开连接，系统会在1分钟后重启完毕，此时可以关闭Xshell软件了。 搭建教程到此结束，亲测成功。如果不能连接的，请检查自己的每一步操作。","raw":null,"content":null,"categories":[{"name":"vps","slug":"vps","permalink":"https://stanxia.github.io/categories/vps/"}],"tags":[{"name":"vps","slug":"vps","permalink":"https://stanxia.github.io/tags/vps/"}]},{"title":"spark报错集","slug":"spark报错集","date":"2017-10-30T05:58:58.000Z","updated":"2017-11-05T05:11:40.000Z","comments":true,"path":"2017/10/30/spark报错集/","link":"","permalink":"https://stanxia.github.io/2017/10/30/spark报错集/","excerpt":"\n魔高一尺\n道高一丈","text":"魔高一尺 道高一丈 有话要说针对一个老毛病：有些错误屡犯屡改，屡改屡犯，没有引起根本上的注意，或者没有从源头理解错误发生的底层原理，导致做很多无用功。 总结历史，并从中吸取教训，减少无用功造成的时间浪费。特此将从目前遇到的spark问题全部记录在这里，搞清楚问题，自信向前。 问题汇总关键词：spark-hive概述：1Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Unable to instantiate SparkSession with Hive support because Hive classes are not found. 场景：1在本地调试spark程序，连接虚拟机上的集群，尝试执行sparkSQL时，启动任务就报错。 原理：1缺少sparkSQL连接hive的必要和依赖jar包 办法：123456789在项目／模块的pom.xml中添加相关的spark-hive依赖jar包。&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive_2.11 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;重新编译项目／模块即可。","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"}]},{"title":"life","slug":"life","date":"2017-10-29T03:00:42.000Z","updated":"2017-11-05T04:25:41.000Z","comments":true,"path":"2017/10/29/life/","link":"","permalink":"https://stanxia.github.io/2017/10/29/life/","excerpt":"\nLife is simple &amp;&amp; funny.","text":"Life is simple &amp;&amp; funny. var dplayer0 = new DPlayer({\"element\":document.getElementById(\"dplayer0\"),\"autoplay\":false,\"theme\":\"#FADFA3\",\"loop\":true,\"video\":{\"url\":\"xx\",\"pic\":\"xx\"}});","raw":null,"content":null,"categories":[{"name":"movie","slug":"movie","permalink":"https://stanxia.github.io/categories/movie/"}],"tags":[{"name":"movie","slug":"movie","permalink":"https://stanxia.github.io/tags/movie/"}]},{"title":"杂乱无章","slug":"杂乱无章","date":"2017-10-28T17:31:31.000Z","updated":"2017-11-05T04:24:21.000Z","comments":true,"path":"2017/10/29/杂乱无章/","link":"","permalink":"https://stanxia.github.io/2017/10/29/杂乱无章/","excerpt":"\n时光的机器，加足马力冲回过去\n历史的长河，丝丝涟漪涌向未来","text":"时光的机器，加足马力冲回过去 历史的长河，丝丝涟漪涌向未来 道不清楚，说不明白，夜深人静的时候，说一些想到的废话。窗外隆隆作响，不知疲倦的机器不知疲倦的执行着不知疲倦的动作。窗内屏幕暗淡，双眼干涩，思索着宇宙外的回想。 小时候，望向星空，那时的天空群星闪烁，哪像现在，嘿，享受了大城市的霓虹，哪里再给你无垠的星空，贪。 躺在草地，微风轻拂脸颊，初秋的夜晚，有点微凉。 仰望星河，也想着外面的世界，多精彩。 揣摩着无垠的宇宙，翻过地球，越过银河，驶向无限拓展的星际，身上的烦恼，微风一吹，全散了。 风轻拂，静静望着天空，思考着外面的朋友或许也在渴望着远方的我，伸手触摸这天空，抓一把星辰贪婪的放入梦。 深邃的夜空，望不尽的远方，是光明中的无尽黑暗，也似黑暗道路的一束亮光，洒向我，思绪跟着遨游，呵，世界与我万千美好，我与世界却念念叨叨，琐琐碎碎，麻麻烦烦。心里是想放飞的。 夜深，车水呼啸，诉说着城市的不眠，可我困，关窗，闷。开窗，嘿，不知疲倦的机器又开始不知疲倦的执行不知疲倦的动作。这样的夜晚，眠难。 深夜思考，写作。夜使我宁静，内心的宁静，这白天的大城市给予不了。感谢夜的馈赠，接收这无上的加冕，驰骋在思绪的星空，痛快，精彩，精彩。 杂乱无章，呵，可以。","raw":null,"content":null,"categories":[{"name":"think","slug":"think","permalink":"https://stanxia.github.io/categories/think/"}],"tags":[{"name":"think","slug":"think","permalink":"https://stanxia.github.io/tags/think/"}]},{"title":"闲谈","slug":"闲谈","date":"2017-10-28T03:07:14.000Z","updated":"2017-11-05T04:23:25.000Z","comments":true,"path":"2017/10/28/闲谈/","link":"","permalink":"https://stanxia.github.io/2017/10/28/闲谈/","excerpt":"\n九九登高忆重阳","text":"九九登高忆重阳","raw":null,"content":null,"categories":[{"name":"think","slug":"think","permalink":"https://stanxia.github.io/categories/think/"}],"tags":[{"name":"think","slug":"think","permalink":"https://stanxia.github.io/tags/think/"}]},{"title":"这个杀手不太冷","slug":"这个杀手不太冷","date":"2017-10-27T17:18:04.000Z","updated":"2017-11-05T04:25:09.000Z","comments":true,"path":"2017/10/28/这个杀手不太冷/","link":"","permalink":"https://stanxia.github.io/2017/10/28/这个杀手不太冷/","excerpt":"\nIs life always this hard,or is it just when you’re a kid?\nAlways like this.","text":"Is life always this hard,or is it just when you’re a kid? Always like this.","raw":null,"content":null,"categories":[{"name":"movie","slug":"movie","permalink":"https://stanxia.github.io/categories/movie/"}],"tags":[{"name":"movie","slug":"movie","permalink":"https://stanxia.github.io/tags/movie/"}]},{"title":"mac使用小技巧","slug":"mac使用小技巧","date":"2017-10-27T17:13:44.000Z","updated":"2017-11-05T04:26:26.000Z","comments":true,"path":"2017/10/28/mac使用小技巧/","link":"","permalink":"https://stanxia.github.io/2017/10/28/mac使用小技巧/","excerpt":"\n记录mac使用的小技巧\n持续更新ing ","text":"记录mac使用的小技巧 持续更新ing 隐藏小技巧开启充电提示音（类似于iphone充电提示音，默认关闭）终端输入（开启）： 1defaults write com.apple.PowerChime ChimeOnAllHardware -bool true; open /System/Library/CoreServices/PowerChime.app &amp; 关闭： 1defaults write com.apple.PowerChime ChimeOnAllHardware -bool false;killall PowerChime 隐藏文件夹更好的保护学习资料，有时候需要设置隐藏文件夹： 1mv foldername .foldername 查看隐藏文件夹mac最新版本： 1⌘⇧.(Command + Shift + .) #隐藏 和显示 Macbook Pro 用外接显示器时，如何关闭笔记本屏幕，同时开盖使用12sudo nvram boot-args=&quot;iog=0x0&quot; #(10.10以前版本)sudo nvram boot-args=&quot;niog=1&quot; #(10.10及以后版本)这个命令的意思就是外接显示器时关闭自身屏幕，重启生效 开机流程：连上电源和外接显示器，按开机键，立即合盖，等外接显示器有信号时开盖即可如果报错 (已知 10.11/10.12 会报错)nvram: Error setting variable - ‘boot-args’: (iokit/common) general error 重启，按住command + r 进入恢复界面 左上角菜单里面找到终端，输入nvram boot-args=”niog=1”，回车问题解决。重启生效","raw":null,"content":null,"categories":[{"name":"mac","slug":"mac","permalink":"https://stanxia.github.io/categories/mac/"}],"tags":[{"name":"mac","slug":"mac","permalink":"https://stanxia.github.io/tags/mac/"}]},{"title":"火之意志","slug":"火之意志","date":"2017-10-26T16:54:07.000Z","updated":"2017-11-08T17:28:39.000Z","comments":true,"path":"2017/10/27/火之意志/","link":"","permalink":"https://stanxia.github.io/2017/10/27/火之意志/","excerpt":"\n只要有木叶飞舞的地方\n就会有火燃烧","text":"只要有木叶飞舞的地方 就会有火燃烧","raw":null,"content":null,"categories":[{"name":"火影","slug":"火影","permalink":"https://stanxia.github.io/categories/火影/"}],"tags":[{"name":"动漫","slug":"动漫","permalink":"https://stanxia.github.io/tags/动漫/"}]}]}