{"meta":{"title":"东篱下","subtitle":"斯坦@森","description":"采菊东篱下，悠然见南山","author":"森","url":"https://stanxia.github.io"},"posts":[{"title":"火之意志","slug":"火之意志","date":"2022-10-26T16:54:07.000Z","updated":"2017-12-01T06:47:47.000Z","comments":false,"path":"2022/10/27/火之意志/","link":"","permalink":"https://stanxia.github.io/2022/10/27/火之意志/","excerpt":"","text":"","raw":null,"content":null,"categories":[{"name":"photo","slug":"photo","permalink":"https://stanxia.github.io/categories/photo/"}],"tags":[{"name":"photo","slug":"photo","permalink":"https://stanxia.github.io/tags/photo/"}]},{"title":"Hive事务管理","slug":"Hive事务管理","date":"2017-12-01T03:30:06.000Z","updated":"2017-12-06T07:19:00.000Z","comments":true,"path":"2017/12/01/Hive事务管理/","link":"","permalink":"https://stanxia.github.io/2017/12/01/Hive事务管理/","excerpt":"简介Hive作为Hadoop家族历史最悠久的组件之一，一直以其优秀的兼容性支持和稳定性而著称，越来越多的企业将业务数据从传统数据库迁移至Hadoop平台，并通过Hive来进行数据分析。但是我们在迁移的过程中难免会碰到如何将传统数据库的功能也迁移到Hadoop的问题，比如说事务。事务作为传统数据库很重要的一个功能，在Hive中是如何实现的呢？Hive的实现有什么不一样的地方呢？我们将传统数据库的应用迁移到Hive如果有事务相关的场景我们该如何去转换并要注意什么问题呢？\n本文会通过很多真实测试案例来比较Hive与传统数据库事务的区别，并在文末给出一些在Hive平台上使用事务相关的功能时的指导和建议。\nACID与实现原理为了方便解释和说明后面的一些问题，这里重提传统数据库事务相关的概念，以下内容来源于网络。","text":"简介Hive作为Hadoop家族历史最悠久的组件之一，一直以其优秀的兼容性支持和稳定性而著称，越来越多的企业将业务数据从传统数据库迁移至Hadoop平台，并通过Hive来进行数据分析。但是我们在迁移的过程中难免会碰到如何将传统数据库的功能也迁移到Hadoop的问题，比如说事务。事务作为传统数据库很重要的一个功能，在Hive中是如何实现的呢？Hive的实现有什么不一样的地方呢？我们将传统数据库的应用迁移到Hive如果有事务相关的场景我们该如何去转换并要注意什么问题呢？ 本文会通过很多真实测试案例来比较Hive与传统数据库事务的区别，并在文末给出一些在Hive平台上使用事务相关的功能时的指导和建议。 ACID与实现原理为了方便解释和说明后面的一些问题，这里重提传统数据库事务相关的概念，以下内容来源于网络。 ACID说明何为事务？就是一组单元化操作，这些操作要么都执行，要么都不执行，是一个不可分割的工作单位。 事务（transaction）所应该具有的四个要素：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。这四个基本要素通常称为ACID特性。 原子性（Atomicity）:一个事务是一个不可再分割的工作单位，事务中的所有操作要么都发生，要么都不发生。 一致性（Consistency）:事务开始之前和事务结束以后，数据库的完整性约束没有被破坏。这是说数据库事务不能破坏关系数据的完整性以及业务逻辑上的一致性。 隔离性（Isolation）:多个事务并发访问，事务之间是隔离的，一个事务不影响其它事务运行效果。这指的是在并发环境中，当不同的事务同时操作相同的数据时，每个事务都有各自完整的数据空间。事务查看数据更新时，数据所处的状态要么是另一事务修改它之前的状态，要么是另一事务修改后的状态，事务不会查看到中间状态的数据。事务之间的相应影响，分别为：脏读、不可重复读、幻读、丢失更新。 持久性（Durability）:意味着在事务完成以后，该事务锁对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。 ACID的实现原理事务可以保证ACID原则的操作，那么事务是如何保证这些原则的？解决ACID问题的两大技术点是： 预写日志（Write-ahead logging）保证原子性和持久性 锁（locking）保证隔离性 这里并没有提到一致性，是因为一致性是应用相关的话题，它的定义一个由业务系统来定义，什么样的状态才是一致？而实现一致性的代码通常在业务逻辑的代码中得以体现。 注：锁是指在并发环境中通过读写锁来保证操作的互斥性。根据隔离程度不同，锁的运用也不同。 测试环境 操作系统 CentOS 6.5 JDK jdk1.7.0_67 CDH 5.9 Hadoop 2.6.0 Hive 1.1.0 Hive的ACID测试Hive中的锁（不开启事务）Hive中定义了两种锁的模式：共享锁（S）和排它锁（X），顾名思义，多个共享锁(S)可以同时获取，但是排它锁(X)会阻塞其它所有锁。在本次测试中，CDH5.9的Concurrency参数是默认开启的（hive.support.concurrency=true），以下分别对开启Concurrency和关闭进行相关测试。 首先在测试之前，创建一个普通的hive表： 1create table test_notransaction(user_id Int,name String); 向test_transaction表中插入测试数据： 1insert into test_notransaction values(1,'peach1'),(2,'peach2'),(3, 'peach3'),(4, 'peach4'); 查看插入的数据： 开启Concurrency1、对catalog_sales表进行并发select操作 执行的sql语句：select count(*) from catalog_sales; 执行单条sql查询时，获取一个共享锁（S），sql语句正常执行 同时执行两条sql查询是，获取两个共享锁，并且sql语句均正常执行 分析：由此对比可得出hive在执行sql查询时获取Share锁，在并发的情况下可获取多个共享锁。 2、对test表进行并发Insert操作 创建表： 1create table test(name string, id int); 执行sql语句： 12insert into test values('test11aaa1',1252); insert into test values('test1',52); 执行单条insert语句时，获取一个X锁，sql语句正常执行 同时执行两条insert语句时，只能获取一个test表X锁，第一条insert语句正常执行，第二条insert语句处于等待状态，在第一条insert语句释放test表的X锁，第二条sql语句正常执行. 分析：由此对比可得出hive在执行insert操作时，只能获取一个X锁且锁不能共享，只能在sql执行完成释放锁后，后续sql方可继续执行。 3、对test表执行select的同时执行insert操作 执行sql语句： 12select count(*) from test; insert into test values(&quot;test123&quot;,123); 步骤： 1) 执行select语句，在select未运行完时，在新的窗口同时执行insert语句观察两条sql执行情况，select语句正常执行，insert语句处于等待状态。 2) 此时查看test表锁状态 在步骤1的执行过程中，获取到test表的锁为共享锁（S） 3) 在select语句执行完成后，观察insert语句开始正常执行，此时获取test表锁为排它锁（X）。注意：在select语句执行完成后，大概过40s左右insert语句才正常执行，这是由hive.lock.sleep.between.retries参数控制，默认60 分析： 由上述操作可得出，hive中一个表只能有一个排它锁(X)且锁不能共享，在获取排它锁时，表上不能有其它锁包括共享锁(S)，只有在表上所有的锁都释放后，insert操作才能继续，否则处于等待状态。 对注意部分进行参数调整，将hive.lock.sleep.between.retries设置为10s，再次进行测试发现，在select语句执行完成后，大概过6s左右insert语句开始执行,通过两次测试发现，等待时间均在10s以内，由此可以得出此参数影响sql操作获取锁的间隔（在未获取到锁的情况下），如果此时未到获取锁触发周期，执行其它sql则，该sql会优于等待的sql执行。 4、对test表执行insert的同时执行select操作 执行sql语句： 12insert into test values(&quot;test123&quot;,123); select count(*) from test; 操作步骤： 1) 在命令窗口执行insert语句，在insert操作未执行完成时，在新的命令窗口执行select语句，观察两个窗口的sql执行情况，insert语句正常执行，select语句处于等待状态。 2) 此时查看test表锁状态，只有insert操作获取的排它锁（X） 3) 在insert语句执行完成后，观察select语句开始正常执行，此时查看test表锁状态为共享锁（S），之前的insert操作获取的排它锁（X）已被释放 分析：在test表锁状态为排它锁(X)时，所有的操作均被阻塞处于等待状态，只有在排它锁(X)释放其它操作可继续进行。 5、测试update和delete修改test表数据 sql语句： 12update test set name=&apos;aaaa&apos; where id=1252; delete test set name=&apos;bbbb&apos; where id=123; 1) 表中数据，更新前 2) 在beeline窗口执行update操作 执行update操作报错，异常提示“Attempt to do update or delete using transaction manager that does not support these operations”，在非事务模式下不支持update 和 delete。 关闭Concurrency1、执行insert操作的同时执行select操作 sql语句： 12insert into test_notransaction values(1,&apos;peach1&apos;),(2,&apos;peach2&apos;),(3, &apos;peach3&apos;),(4, &apos;peach4&apos;); select count(*) from test_notransaction; 操作sql前，查看表数据 查看test_notransaction表获取情况，show locks; hive在未开启concurrency 的情况下,show locks不能正常获取表的锁，同时对同一张表执行insert和select操作时并发执行，获取数据取决于sql执行速度，因此在select 的时候未获取到插入数据。 2、执行select操作的同时执行insert操作 sql语句： 12select count(*) from test_notransaction; insert into test_notransaction values(1,&apos;peach1&apos;),(2,&apos;peach2&apos;),(3, &apos;peach3&apos;),(4, &apos;peach4&apos;); 在执行select的同时执行insert操作，操作可以同时并行操作，未产生阻塞等待的过程。 3、同时执行多条insert操作 sql语句： 12insert into test_notransaction values(1,&apos;peach1&apos;),(2,&apos;peach2&apos;),(3, &apos;peach3&apos;),(4, &apos;peach4&apos;); insert into test_notransaction values(1,&apos;peach1&apos;),(2,&apos;peach2&apos;),(3, &apos;peach3&apos;),(4, &apos;peach4&apos;); 同时执行insert操作时，可同时执行未产生阻塞等待的过程。 4、执行update操作，将表中user_id为2的用户名修改为peach22 sql语句： 1update test_notransaction set name=&apos;peach22&apos; where user_id=2; 执行update操作，执行结果如下： 在未配置hive的Transaction和ACID时，不支持update操作。 5、执行delete操作，将表中user_id为1信息删除 sql语句： 1delete from test_notransaction where user_id=1; 执行delete操作，执行结果如下： hive未配置Transaction和ACID，不支持delete操作。 6、查看表获取锁类型 1show locks; 无法正常执行； Hive的事务Hive的事务配置Hive从0.13开始加入了事务支持，在行级别提供完整的ACID特性，Hive在0.14时加入了对INSERT…VALUES,UPDATE,and DELETE的支持。对于在Hive中使用ACID和Transactions，主要有以下限制： 不支持BEGIN,COMMIT和ROLLBACK 只支持ORC文件格式 表必须分桶 不允许从一个非ACID连接写入/读取ACID表 为了使Hive支持事务操作，需将以下参数加入到hive-site.xml文件中。 123456789101112131415161718192021222324&lt;property&gt;&lt;name&gt;hive.support.concurrency&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.enforce.bucketing&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.exec.dynamic.partition.mode&lt;/name&gt;&lt;value&gt;nonstrict&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.txn.manager&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hive.ql.lockmgr.DbTxnManager&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.compactor.initiator.on&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.compactor.worker.threads &lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 可以在Cloudera Manager进行以下配置： 为了让beeline支持还需要配置： Hive事务测试环境准备1、创建一个支持ACID的表 建表语句： 123create table test_trancaction (user_id Int,name String) clustered by (user_id) into 3 buckets stored as orc TBLPROPERTIES ('transactional'='true'); 将表名修改为test_transaction 1alter table test_trancaction rename to test_transaction; 2、准备测试数据，向数据库中插入数据 1insert into test_transaction values(1,'peach'),(2,'peach2'),(3,'peach3'),(4,'peach4'),(5,'peach5'); 用例测试1、执行update操作，将user_id的name修改为peach_update sql语句： 1update test_transaction set name='peach_update' where user_id=1; 执行修改操作，查看表获取锁类型 数据修改成功，且不影响其它数据。 2、同时修改同一条数据，将userid为1的用户名字修改为peach，另一条sql将名字修改为peach sql语句： 12update test_transaction set name='peach' where user_id=1;update test_transaction set name='peach_' where user_id=1; sql执行顺序为peach，其次为peach_ 此时查看表获取到的锁 通过获取到锁分析，在同时修改同一条数据时，优先执行的sql获取到了SHARED_WRITE，而后执行的sql获取锁的状态为WAITING状态，表示还未获取到SHARED_WRITE锁，等待第一条sql执行结束后方可获取到锁对数据进行操作。 通过上不执行操作分析，数据userid为1的用户名字应被修改为peach 3、同时修改不同数据，修改id为2的name为peachtest，修改id为3的name为peach_test sql语句： 12update test_transaction set name='peachtest' where user_id=2; update test_transaction set name='peach_test' where user_id=3; sql执行顺序为peachtest，其次为peach_test 此时查看表获取到的锁 通过sql操作获取锁分析，在同时修改不同数据时，优先执行的sql获取到了SHARED_WRITE，而后执行的sql获取锁的状态为WAITING状态，表示还未获取到SHARED_WRITE锁，等待第一条sql执行结束后方可获取到锁对数据进行操作。 4、执行select操作的同时执行insert操作 sql语句： 12select count(*) from test_transaction; insert into test_transaction values(3,'peach3'); 步骤： 先执行select操作，再执行insert操作，执行完成后查看表获取到的锁 由于select和insert操作均获取的是SHARED_READ锁，读锁为并行，所以select查询和insert同时执行，互不影响。 5、update同一条数据的同时select该条数据 sql语句： 12update test_transaction set name='peach_update' where user_id=1; select * from test_transaction where user_id=1; 步骤： 先执行update操作，再执行select操作，获取此时表获取到的锁 通过获取锁的情况分析， 在update操作时，获取到SHARED_WRITE锁，执行select操作时获取到SHARED_READ锁，在进行修改数据时未阻塞select查询操作，update未执行完成时，select查询到的数据为未修改的数据。 6、执行delete操作，将user_id为3的数据删除 sql语句： 1delete from test_transaction where user_id=3; 步骤： 执行delete操作，获取此时表获取到的锁 删除操作获取到的是SHARED_WRITE锁 执行成功后数据 7、同时delete同一条数据 sql语句： 12delete from test_transaction where user_id=3;delete from test_transaction where user_id=3; 步骤： 按顺序执行两条delete操作，查看此时表获取到的锁： 通过查看delete操作获取到的锁，优先执行的操作获取到SHARED_WRITE锁，后执行的delete操作未获取到SHARED_WRITE锁，处于WAITING状态。 执行删除后结果 8、同时delete两条不同的数据 sql语句： 12delete from test_transaction where user_id=1; delete from test_transaction where user_id=5; 步骤： 按顺序执行两条delete操作，查看此时表获取到的锁： 通过查看delete操作获取到的锁，优先执行的操作获取到SHARED_WRITE锁，后执行的delete操作未获取到SHARED_WRITE锁，处于WAITING状态。 执行删除后结果 9、执行delete的同时对删除的数据进行update操作 sql语句： 12delete from test_transaction where user_id=3; update test_transaction set name='test' where user_id=3; 步骤： 按顺序执行两条sql，查看此时获取到表的锁： 通过查看delete和update操作获取到的锁，优先执行的操作获取到SHARED_WRITE锁，后执行的操作未获取到SHARED_WRITE锁，处于WAITING状态。 执行delete和update后结果 注意：此处在delete优先于update执行，但执行结果为update的结果，执行异常。 10、执行delete的同时对不同的数据进行update操作 sql语句： 12delete from test_transaction where user_id=2; update test_transaction set name='test' where user_id=4; 步骤： 按顺序执行上面两条sql，查看表锁获取情况 通过查看delete和update操作获取到的锁，优先执行的操作获取到SHARED_WRITE锁，后执行的操作未获取到SHARED_WRITE锁，处于WAITING状态。 执行delete和update后结果,执行结果正常 11、执行delete的同时执行select操作 sql语句： 12delete from test_transaction where user_id=4; select count(*) from test_transaction; 步骤： 按顺序执行上面两条sql，查看表锁获取情况 在操作delete的同时执行select操作，两个操作均同时获取到SHARED_RED和SHARED_WRITE锁，操作并行进行未出现阻塞。 总结对比 Hive事务使用建议 传统数据库中有三种模型隐式事务、显示事务和自动事务。在目前Hive对事务仅支持自动事务，因此Hive无法通过显示事务的方式对一个操作序列进行事务控制。 传统数据库事务在遇到异常情况可自动进行回滚，目前Hive无法支持ROLLBACK。 传统数据库中支持事务并发，而Hive对事务无法做到完全并发控制,多个操作均需要获取WRITE的时候则这些操作为串行模式执行（在测试用例中”delete同一条数据的同时update该数据”，操作是串行的且操作完成后数据未被删除且数据被修改）未保证数据一致性。 Hive的事务功能尚属于实验室功能，并不建议用户直接上生产系统，因为目前它还有诸多的限制，如只支持ORC文件格式，建表必须分桶等，使用起来没有那么方便，另外该功能的稳定性还有待进一步验证。 CDH默认开启了Hive的Concurrency功能，主要是对并发读写的的时候通过锁进行了控制。所以为了防止用户在使用Hive的时候，报错提示该表已经被lock，对于用户来说不友好，建议在业务侧控制一下写入和读取，比如写入同一个table或者partition的时候保证是单任务写入，其他写入需控制写完第一个任务了，后面才继续写，并且控制在写的时候不让用户进行查询。另外需要控制在查询的时候不要允许有写入操作。 如果对于数据一致性不在乎，可以完全关闭Hive的Concurrency功能关闭，即设置hive.support.concurrency为false，这样Hive的并发读写将没有任何限制。","raw":null,"content":null,"categories":[{"name":"hive","slug":"hive","permalink":"https://stanxia.github.io/categories/hive/"}],"tags":[{"name":"hive","slug":"hive","permalink":"https://stanxia.github.io/tags/hive/"},{"name":"acid","slug":"acid","permalink":"https://stanxia.github.io/tags/acid/"}]},{"title":"SparkSQL-Catalyst优化理解","slug":"SparkSQL-Catalyst优化理解","date":"2017-11-30T03:32:26.000Z","updated":"2017-11-30T05:34:31.000Z","comments":true,"path":"2017/11/30/SparkSQL-Catalyst优化理解/","link":"","permalink":"https://stanxia.github.io/2017/11/30/SparkSQL-Catalyst优化理解/","excerpt":"前言本文主要介绍SparkSQL的优化器系统Catalyst，其设计思路基本都来自于传统型数据库，而且和大多数当前的大数据SQL处理引擎设计基本相同（Impala、Presto、Hive（Calcite）等）。\nSQL优化器核心执行策略主要分为两个大的方向：\n\n基于规则优化（RBO）：是一种经验式、启发式地优化思路，更多地依靠前辈总结出来的优化规则，简单易行且能够覆盖到大部分优化逻辑，但是对于核心优化算子Join却显得有点力不从心。\n基于代价优化 (CBO)：根据代价估算确定一种代价最小的方案。\n\n举个简单的例子，两个表执行Join到底应该使用BroadcastHashJoin还是SortMergeJoin？当前SparkSQL的方式是通过手工设定参数来确定，如果一个表的数据量小于这个值就使用BroadcastHashJoin，但是这种方案显得很不优雅，很不灵活。基于代价优化就是为了解决这类问题，它会针对每个Join评估当前两张表使用每种Join策略的代价，根据代价估算确定一种代价最小的方案。","text":"前言本文主要介绍SparkSQL的优化器系统Catalyst，其设计思路基本都来自于传统型数据库，而且和大多数当前的大数据SQL处理引擎设计基本相同（Impala、Presto、Hive（Calcite）等）。 SQL优化器核心执行策略主要分为两个大的方向： 基于规则优化（RBO）：是一种经验式、启发式地优化思路，更多地依靠前辈总结出来的优化规则，简单易行且能够覆盖到大部分优化逻辑，但是对于核心优化算子Join却显得有点力不从心。 基于代价优化 (CBO)：根据代价估算确定一种代价最小的方案。 举个简单的例子，两个表执行Join到底应该使用BroadcastHashJoin还是SortMergeJoin？当前SparkSQL的方式是通过手工设定参数来确定，如果一个表的数据量小于这个值就使用BroadcastHashJoin，但是这种方案显得很不优雅，很不灵活。基于代价优化就是为了解决这类问题，它会针对每个Join评估当前两张表使用每种Join策略的代价，根据代价估算确定一种代价最小的方案。 Tree&amp;Rule在介绍SQL优化器工作原理之前，有必要首先介绍两个重要的数据结构：Tree和Rule。SQL语法树就是SQL语句通过编译器之后会被解析成一棵树状结构。这棵树会包含很多节点对象，每个节点都拥有特定的数据类型，同时会有0个或多个孩子节点（节点对象在代码中定义为TreeNode对象），下图是个简单的示例：如上图所示，箭头左边表达式有3种数据类型（Literal表示常量、Attribute表示变量、Add表示动作），表示x+(1+2)。映射到右边树状结构后，每一种数据类型就会变成一个节点。另外，Tree还有一个非常重要的特性，可以通过一定的规则进行等价变换，如下图： 上图定义了一个等价变换规则(Rule)：两个Integer类型的常量相加可以等价转换为一个Integer常量，这个规则其实很简单，对于上文中提到的表达式x+(1+2)来说就可以转变为x+3。对于程序来讲，如何找到两个Integer常量呢？其实就是简单的二叉树遍历算法，每遍历到一个节点，就模式匹配当前节点为Add、左右子节点是Integer常量的结构，定位到之后将此三个节点替换为一个Literal类型的节点。 上面用一个最简单的示例来说明等价变换规则以及如何将规则应用于语法树。在任何一个SQL优化器中，通常会定义大量的Rule（后面会讲到），SQL优化器会遍历语法树中每个节点，针对遍历到的节点模式匹配所有给定规则（Rule），如果有匹配成功的，就进行相应转换，如果所有规则都匹配失败，就继续遍历下一个节点。 Catalyst工作流程任何一个优化器工作原理都大同小异：SQL语句首先通过Parser模块被解析为语法树，此棵树称为Unresolved Logical Plan；Unresolved Logical Plan通过Analyzer模块借助于数据元数据解析为Logical Plan；此时再通过各种基于规则的优化策略进行深入优化，得到Optimized Logical Plan；优化后的逻辑执行计划依然是逻辑的，并不能被Spark系统理解，此时需要将此逻辑执行计划转换为Physical Plan；为了更好的对整个过程进行理解，下文通过一个简单示例进行解释。 ParserParser简单来说是将SQL字符串切分成一个一个Token，再根据一定语义规则解析为一棵语法树。Parser模块目前基本都使用第三方类库ANTLR进行实现，比如Hive、 Presto、SparkSQL等。下图是一个示例性的SQL语句（有两张表，其中people表主要存储用户基本信息，score表存储用户的各种成绩），通过Parser解析后的AST语法树如图所示： Analyzer通过解析后的逻辑执行计划基本有了骨架，但是系统并不知道score、sum这些都是些什么鬼，此时需要基本的元数据信息来表达这些词素，最重要的元数据信息主要包括两部分：表的Scheme和基本函数信息，表的scheme主要包括表的基本定义（列名、数据类型）、表的数据格式（Json、Text）、表的物理位置等，基本函数信息主要指类信息。 Analyzer会再次遍历整个语法树，对树上的每个节点进行数据类型绑定以及函数绑定，比如people词素会根据元数据表信息解析为包含age、id以及name三列的表，people.age会被解析为数据类型为int的变量，sum会被解析为特定的聚合函数，如下图所示： SparkSQL中Analyzer定义了各种解析规则，可以查看Analyzer类，其中定义了基本的解析规则，如下： Optimizer优化器是整个Catalyst的核心，上文提到优化器分为基于规则优化和基于代价优化两种，当前SparkSQL 2.1依然没有很好的支持基于代价优化，此处只介绍基于规则的优化策略，基于规则的优化策略实际上就是对语法树进行一次遍历，模式匹配能够满足特定规则的节点，再进行相应的等价转换。因此，基于规则优化说到底就是一棵树等价地转换为另一棵树。SQL中经典的优化规则有很多，下文结合示例介绍三种比较常见的规则： 谓词下推（Predicate Pushdown） 常量累加（Constant Folding） 列值裁剪（Column Pruning） 上图左边是经过Analyzer解析后的语法树，语法树中两个表先做join，之后再使用age&gt;10对结果进行过滤。大家知道join算子通常是一个非常耗时的算子，耗时多少一般取决于参与join的两个表的大小，如果能够减少参与join两表的大小，就可以大大降低join算子所需时间。谓词下推就是这样一种功能，它会将过滤操作下推到join之前进行，上图中过滤条件age&gt;0以及id!=null两个条件就分别下推到了join之前。这样，系统在扫描数据的时候就对数据进行了过滤，参与join的数据量将会得到显著的减少，join耗时必然也会降低。 常量累加其实很简单，就是上文中提到的规则 x+(1+2) -&gt; x+3，虽然是一个很小的改动，但是意义巨大。示例如果没有进行优化的话，每一条结果都需要执行一次100+80的操作，然后再与变量math_score以及english_score相加，而优化后就不需要再执行100+80操作。 列值裁剪是另一个经典的规则，示例中对于people表来说，并不需要扫描它的所有列值，而只需要列值id，所以在扫描people之后需要将其他列进行裁剪，只留下列id。这个优化一方面大幅度减少了网络、内存数据量消耗，另一方面对于列存数据库（Parquet）来说大大提高了扫描效率。 除此之外，Catalyst还定义了很多其他优化规则，可以查看Optimizer类，下图简单的截取一部分规则： 至此，逻辑执行计划已经得到了比较完善的优化，然而，逻辑执行计划依然没办法真正执行，他们只是逻辑上可行，实际上Spark并不知道如何去执行这个东西。比如Join只是一个抽象概念，代表两个表根据相同的id进行合并，然而具体怎么实现这个合并，逻辑执行计划并没有说明。 此时就需要将逻辑执行计划转换为物理执行计划，将逻辑上可行的执行计划变为Spark可以真正执行的计划。比如Join算子，Spark根据不同场景为该算子制定了不同的算法策略，有BroadcastHashJoin、ShuffleHashJoin以及SortMergeJoin等（可以将Join理解为一个接口，BroadcastHashJoin是其中一个具体实现），物理执行计划实际上就是在这些具体实现中挑选一个耗时最小的算法实现，这个过程涉及到基于代价优化策略。 查看SparkSQL执行计划至此，通过一个简单的示例完整的介绍了Catalyst的整个工作流程，包括Parser阶段、Analyzer阶段、Optimize阶段以及Physical Planning阶段。有同学可能会比较感兴趣Spark环境下如何查看一条具体的SQL的整个过程，在此介绍两种方法： 查看逻辑执行计划使用queryExecution方法查看逻辑执行计划，如下所示： 查看物理执行计划使用explain方法查看物理执行计划： Spark WebUI进行查看","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"},{"name":"sparkSQL","slug":"sparkSQL","permalink":"https://stanxia.github.io/tags/sparkSQL/"},{"name":"catalyst","slug":"catalyst","permalink":"https://stanxia.github.io/tags/catalyst/"}]},{"title":"spark容错机制","slug":"spark容错机制","date":"2017-11-29T09:16:32.000Z","updated":"2017-12-05T03:34:46.000Z","comments":true,"path":"2017/11/29/spark容错机制/","link":"","permalink":"https://stanxia.github.io/2017/11/29/spark容错机制/","excerpt":"引言一般来说，分布式数据集的容错性有两种方式： 数据检查点 和 记录数据的更新 。面向大规模数据分析，数据检查点操作成本很高，需要通过数据中心的网络连接在机器之间复制庞大的数据集，而网络带宽往往比内存带宽低得多，同时还需要消耗更多的存储资源。因此，Spark选择记录更新的方式。\n但是，如果更新粒度太细太多，那么记录更新成本也不低。因此，RDD只支持粗粒度转换，即只记录单个块上执行的单个操作，然后将创建RDD的一系列变换序列（每个RDD都包含了他是如何由其他RDD变换过来的以及如何重建某一块数据的信息。因此RDD的容错机制又称“血统(Lineage)”容错）记录下来，以便恢复丢失的分区。 \nLineage本质上很类似于数据库中的重做日志（Redo Log），只不过这个重做日志粒度很大，是对全局数据做同样的重做进而恢复数据。\nLineage机制Lineage简介相比其他系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的特定数据Transformation操作（如filter、map、join等）行为。当这个RDD的部分分区数据丢失时，它可以通过Lineage获取足够的信息来重新运算和恢复丢失的数据分区。因为这种粗颗粒的数据模型，限制了Spark的运用场合，所以Spark并不适用于所有高性能要求的场景，但同时相比细颗粒度的数据模型，也带来了性能的提升。","text":"引言一般来说，分布式数据集的容错性有两种方式： 数据检查点 和 记录数据的更新 。面向大规模数据分析，数据检查点操作成本很高，需要通过数据中心的网络连接在机器之间复制庞大的数据集，而网络带宽往往比内存带宽低得多，同时还需要消耗更多的存储资源。因此，Spark选择记录更新的方式。 但是，如果更新粒度太细太多，那么记录更新成本也不低。因此，RDD只支持粗粒度转换，即只记录单个块上执行的单个操作，然后将创建RDD的一系列变换序列（每个RDD都包含了他是如何由其他RDD变换过来的以及如何重建某一块数据的信息。因此RDD的容错机制又称“血统(Lineage)”容错）记录下来，以便恢复丢失的分区。 Lineage本质上很类似于数据库中的重做日志（Redo Log），只不过这个重做日志粒度很大，是对全局数据做同样的重做进而恢复数据。 Lineage机制Lineage简介相比其他系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的特定数据Transformation操作（如filter、map、join等）行为。当这个RDD的部分分区数据丢失时，它可以通过Lineage获取足够的信息来重新运算和恢复丢失的数据分区。因为这种粗颗粒的数据模型，限制了Spark的运用场合，所以Spark并不适用于所有高性能要求的场景，但同时相比细颗粒度的数据模型，也带来了性能的提升。 两种依赖关系RDD在Lineage依赖方面分为两种：窄依赖(Narrow Dependencies)与宽依赖(Wide Dependencies,源码中称为Shuffle Dependencies)，用来解决数据容错的高效性。 窄依赖：是指父RDD的每一个分区最多被一个子RDD的分区所用，表现为一个父RDD的分区对应于一个子RDD的分区，或多个父RDD的分区对应于一个子RDD的分区，也就是说一个父RDD的一个分区不可能对应一个子RDD的多个分区。 1个父RDD分区对应1个子RDD分区，这其中又分两种情况：1个子RDD分区对应1个父RDD分区（如map、filter等算子），1个子RDD分区对应N个父RDD分区（如co-paritioned（协同划分）过的Join）。 宽依赖：是指子RDD的分区依赖于父RDD的多个分区或所有分区，即存在一个父RDD的一个分区对应一个子RDD的多个分区。 1个父RDD分区对应多个子RDD分区，这其中又分两种情况：1个父RDD对应所有子RDD分区（未经协同划分的Join）或者1个父RDD对应非全部的多个RDD分区（如groupByKey）。 本质理解根据父RDD分区是对应1个还是多个子RDD分区来区分窄依赖（父分区对应一个子分区，多个父分区对应一个子分区）和宽依赖（父分区对应多个子分区）。如果对应多个，则当容错重算分区时，因为父分区数据只有一部分是需要重算子分区的，其余数据重算就造成了冗余计算。 对于宽依赖，Stage计算的输入和输出在不同的节点上，对于输入节点完好，而输出节点死机的情况，通过重新计算恢复数据这种情况下，这种方法容错是有效的，否则无效，因为无法重试，需要向上追溯其祖先看是否可以重试（这就是lineage，血统的意思），窄依赖对于数据的重算开销要远小于宽依赖的数据重算开销。 窄依赖和宽依赖的概念主要用在两个地方：一个是容错中相当于Redo日志的功能；另一个是在调度中构建DAG作为不同Stage的划分点。 依赖关系的特性第一，窄依赖可以在某个计算节点上直接通过计算父RDD的某块数据计算得到子RDD对应的某块数据；宽依赖则要等到父RDD所有数据都计算完成之后，并且父RDD的计算结果进行hash并传到对应节点上之后才能计算子RDD。 第二，数据丢失时，对于窄依赖只需要重新计算丢失的那一块数据来恢复；对于宽依赖则要将祖先RDD中的所有数据块全部重新计算来恢复。所以在长“血统”链特别是有宽依赖的时候，需要在适当的时机设置数据检查点。也是这两个特性要求对于不同依赖关系要采取不同的任务调度机制和容错恢复机制。 容错原理在容错机制中，如果一个节点死机了，而且运算窄依赖，则只要把丢失的父RDD分区重算即可，不依赖于其他节点。而宽依赖需要父RDD的所有分区都存在，重算就很昂贵了。 可以这样理解开销的经济与否：在窄依赖中，在子RDD的分区丢失、重算父RDD分区时，父RDD相应分区的所有数据都是子RDD分区的数据，并不存在冗余计算。在宽依赖情况下，丢失一个子RDD分区重算的每个父RDD的每个分区的所有数据并不是都给丢失的子RDD分区用的，会有一部分数据相当于对应的是未丢失的子RDD分区中需要的数据，这样就会产生冗余计算开销，这也是宽依赖开销更大的原因。 因此如果使用Checkpoint算子来做检查点，不仅要考虑Lineage是否足够长，也要考虑是否有宽依赖，对宽依赖加Checkpoint是最物有所值的。 Checkpoint机制通过上述分析可以看出在以下两种情况下，RDD需要加检查点。 DAG中的Lineage过长，如果重算，则开销太大（如在PageRank中）。 在宽依赖上做Checkpoint获得的收益更大。 由于RDD是只读的，所以Spark的RDD计算中一致性不是主要关心的内容，内存相对容易管理，这也是设计者很有远见的地方，这样减少了框架的复杂性，提升了性能和可扩展性，为以后上层框架的丰富奠定了强有力的基础。 在RDD计算中，通过检查点机制进行容错，传统做检查点有两种方式：通过冗余数据和日志记录更新操作。在RDD中的doCheckPoint方法相当于通过冗余数据来缓存数据，而之前介绍的血统就是通过相当粗粒度的记录更新操作来实现容错的。 检查点（本质是通过将RDD写入Disk做检查点）是为了通过lineage做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"},{"name":"原理","slug":"原理","permalink":"https://stanxia.github.io/tags/原理/"}]},{"title":"ORC与PARQUET文件类型的比较","slug":"ORC与PARQUET文件类型的比较","date":"2017-11-29T02:29:04.000Z","updated":"2017-11-29T03:29:58.000Z","comments":true,"path":"2017/11/29/ORC与PARQUET文件类型的比较/","link":"","permalink":"https://stanxia.github.io/2017/11/29/ORC与PARQUET文件类型的比较/","excerpt":"列式存储由于OLAP查询的特点，列式存储可以提升其查询性能，但是它是如何做到的呢？这就要从列式存储的原理说起，从图1中可以看到，相对于关系数据库中通常使用的行式存储，在使用列式存储时每一列的所有元素都是顺序存储的。由此特点可以给查询带来如下的优化：\n123查询的时候不需要扫描全部的数据，而只需要读取每次查询涉及的列，这样可以将I/O消耗降低N倍，另外可以保存每一列的统计信息(min、max、sum等)，实现部分的谓词下推。由于每一列的成员都是同构的，可以针对不同的数据类型使用更高效的数据压缩算法，进一步减小I/O。由于每一列的成员的同构性，可以使用更加适合CPU pipeline的编码方式，减小CPU的缓存失效。\n图1 行式存储VS列式存储","text":"列式存储由于OLAP查询的特点，列式存储可以提升其查询性能，但是它是如何做到的呢？这就要从列式存储的原理说起，从图1中可以看到，相对于关系数据库中通常使用的行式存储，在使用列式存储时每一列的所有元素都是顺序存储的。由此特点可以给查询带来如下的优化： 123查询的时候不需要扫描全部的数据，而只需要读取每次查询涉及的列，这样可以将I/O消耗降低N倍，另外可以保存每一列的统计信息(min、max、sum等)，实现部分的谓词下推。由于每一列的成员都是同构的，可以针对不同的数据类型使用更高效的数据压缩算法，进一步减小I/O。由于每一列的成员的同构性，可以使用更加适合CPU pipeline的编码方式，减小CPU的缓存失效。 图1 行式存储VS列式存储 嵌套数据格式通常我们使用关系数据库存储结构化数据，而关系数据库支持的数据模型都是扁平式的，而遇到诸如List、Map和自定义Struct的时候就需要用户自己解析，但是在大数据环境下，数据的来源多种多样，例如埋点数据，很可能需要把程序中的某些对象内容作为输出的一部分，而每一个对象都可能是嵌套的，所以如果能够原生的支持这种数据，查询的时候就不需要额外的解析便能获得想要的结果。例如在Twitter，他们一个典型的日志对象（一条记录）有87个字段，其中嵌套了7层，如下图。 图2 嵌套数据模型 随着嵌套格式的数据的需求日益增加，目前Hadoop生态圈中主流的查询引擎都支持更丰富的数据类型，例如Hive、SparkSQL、Impala等都原生的支持诸如struct、map、array这样的复杂数据类型，这样促使各种存储格式都需要支持嵌套数据格式。 Parquet存储格式Apache Parquet是Hadoop生态圈中一种新型列式存储格式，它可以兼容Hadoop生态圈中大多数计算框架(Mapreduce、Spark等)，被多种查询引擎支持（Hive、Impala、Drill等），并且它是语言和平台无关的。Parquet最初是由Twitter和Cloudera合作开发完成并开源，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。 Parquet最初的灵感来自Google于2010年发表的Dremel论文，文中介绍了一种支持嵌套结构的存储格式，并且使用了列式存储的方式提升查询性能，在Dremel论文中还介绍了Google如何使用这种存储格式实现并行查询的，如果对此感兴趣可以参考论文和开源实现Drill。 数据模型Parquet支持嵌套的数据模型，类似于Protocol Buffers，每一个数据模型的schema包含多个字段，每一个字段有三个属性：重复次数、数据类型和字段名，重复次数可以是以下三种：required(只出现1次)，repeated(出现0次或多次)，optional(出现0次或1次)。每一个字段的数据类型可以分成两种：group(复杂类型)和primitive(基本类型)。例如Dremel中提供的Document的schema示例，它的定义如下： 1234567891011121314message Document &#123; required int64 DocId; optional group Links &#123; repeated int64 Backward; repeated int64 Forward; &#125; repeated group Name &#123; repeated group Language &#123; required string Code; optional string Country; &#125; optional string Url; &#125;&#125; 可以把这个Schema转换成树状结构，根节点可以理解为repeated类型，如图3。 图3 Parquet的schema结构 可以看出在Schema中所有的基本类型字段都是叶子节点，在这个Schema中一共存在6个叶子节点，如果把这样的Schema转换成扁平式的关系模型，就可以理解为该表包含六个列。Parquet中没有Map、Array这样的复杂数据结构，但是可以通过repeated和group组合来实现的。由于一条记录中某一列可能出现零次或者多次，需要标示出哪些列的值构成一条完整的记录。这是由Striping/Assembly算法实现的。 由于Parquet支持的数据模型比较松散，可能一条记录中存在比较深的嵌套关系，如果为每一条记录都维护一个类似的树状结可能会占用较大的存储空间，因此Dremel论文中提出了一种高效的对于嵌套数据格式的压缩算法：Striping/Assembly算法。它的原理是每一个记录中的每一个成员值有三部分组成：Value、Repetition level和Definition level。value记录了该成员的原始值，可以根据特定类型的压缩算法进行压缩，两个level值用于记录该值在整个记录中的位置。对于repeated类型的列，Repetition level值记录了当前值属于哪一条记录以及它处于该记录的什么位置；对于repeated和optional类型的列，可能一条记录中某一列是没有值的，假设我们不记录这样的值就会导致本该属于下一条记录的值被当做当前记录的一部分，从而造成数据的错误，因此对于这种情况需要一个占位符标示这种情况。 通过Striping/Assembly算法，parquet可以使用较少的存储空间表示复杂的嵌套格式，并且通常Repetition level和Definition level都是较小的整数值，可以通过RLE算法对其进行压缩，进一步降低存储空间。 文件结构Parquet文件是以二进制方式存储的，是不可以直接读取和修改的，Parquet文件是自解析的，文件中包括该文件的数据和元数据。在HDFS文件系统和Parquet文件中存在如下几个概念： HDFS块(Block)：它是HDFS上的最小的副本单位，HDFS会把一个Block存储在本地的一个文件并且维护分散在不同的机器上的多个副本，通常情况下一个Block的大小为256M、512M等。 HDFS文件(File)：一个HDFS的文件，包括数据和元数据，数据分散存储在多个Block中。 行组(Row Group)：按照行将数据物理上划分为多个单元，每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，Parquet读写的时候会将整个行组缓存在内存中，所以如果每一个行组的大小是由内存大的小决定的。 列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。不同的列块可能使用不同的算法进行压缩。 页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。 通常情况下，在存储Parquet数据的时候会按照HDFS的Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如下图所示。 图4 Parquet文件结构 上图展示了一个Parquet文件的结构，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length存储了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和当前文件的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页，但是在后面的版本中增加。 数据访问说到列式存储的优势，Project下推是无疑最突出的，它意味着在获取表中原始数据时只需要扫描查询中需要的列，由于每一列的所有值都是连续存储的，避免扫描整个表文件内容。 在Parquet中原生就支持Project下推，执行查询的时候可以通过Configuration传递需要读取的列的信息，这些列必须是Schema的子集，Parquet每次会扫描一个Row Group的数据，然后一次性得将该Row Group里所有需要的列的Cloumn Chunk都读取到内存中，每次读取一个Row Group的数据能够大大降低随机读的次数，除此之外，Parquet在读取的时候会考虑列是否连续，如果某些需要的列是存储位置是连续的，那么一次读操作就可以把多个列的数据读取到内存。 在数据访问的过程中，Parquet还可以利用每一个row group生成的统计信息进行谓词下推，这部分信息包括该Column Chunk的最大值、最小值和空值个数。通过这些统计值和该列的过滤条件可以判断该Row Group是否需要扫描。另外Parquet未来还会增加诸如Bloom Filter和Index等优化数据，更加有效的完成谓词下推。 ORC文件格式ORC文件格式是一种Hadoop生态圈中的列式存储格式，它的产生早在2013年初，最初产生自Apache Hive，用于降低Hadoop数据存储空间和加速Hive查询速度。和Parquet类似，它并不是一个单纯的列式存储格式，仍然是首先根据行组分割整个表，在每一个行组内进行按列存储。ORC文件是自描述的，它的元数据使用Protocol Buffers序列化，并且文件中的数据尽可能的压缩以降低存储空间的消耗，目前也被Spark SQL、Presto等查询引擎支持，但是Impala对于ORC目前没有支持，仍然使用Parquet作为主要的列式存储格式。2015年ORC项目被Apache项目基金会提升为Apache顶级项目。 数据模型和Parquet不同，ORC原生是不支持嵌套数据格式的，而是通过对复杂数据类型特殊处理的方式实现嵌套格式的支持，例如对于如下的hive表： 12345CREATE TABLE `orcStructTable`( `name` string, `course` struct&lt;course:string,score:int&gt;, `score` map&lt;string,int&gt;, `work_locations` array&lt;string&gt;) 图5 ORC的schema结构 在ORC的结构中这个schema包含10个column，其中包含了复杂类型列和原始类型的列，前者包括LIST、STRUCT、MAP和UNION类型，后者包括BOOLEAN、整数、浮点数、字符串类型等，其中STRUCT的孩子节点包括它的成员变量，可能有多个孩子节点，MAP有两个孩子节点，分别为key和value，LIST包含一个孩子节点，类型为该LIST的成员类型，UNION一般不怎么用得到。每一个Schema树的根节点为一个Struct类型，所有的column按照树的中序遍历顺序编号。 ORC只需要存储schema树中叶子节点的值，而中间的非叶子节点只是做一层代理，它们只需要负责孩子节点值得读取，只有真正的叶子节点才会读取数据，然后交由父节点封装成对应的数据结构返回。 文件结构和Parquet类似，ORC文件也是以二进制方式存储的，所以是不可以直接读取，ORC文件也是自解析的，它包含许多的元数据，这些元数据都是同构ProtoBuffer进行序列化的。ORC的文件结构入图6，其中涉及到如下的概念： ORC文件：保存在文件系统上的普通二进制文件，一个ORC文件中可以包含多个stripe，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。 文件级元数据：包括文件的描述信息PostScript、文件meta信息（包括整个文件的统计信息）、所有stripe的信息和文件schema信息。 stripe：一组行形成一个stripe，每次读取文件是以行组为单位的，一般为HDFS的块大小，保存了每一列的索引和数据。 stripe元数据：保存stripe的位置、每一个列的在该stripe的统计信息以及所有的stream类型和位置。 row group：索引的最小单位，一个stripe中包含多个row group，默认为10000个值组成。 stream：一个stream表示文件中一段有效的数据，包括索引和数据两类。索引stream保存每一个row group的位置和统计信息，数据stream包括多种类型的数据，具体需要哪几种是由该列类型和编码方式决定。 图6 ORC文件结构 在ORC文件中保存了三个层级的统计信息，分别为文件级别、stripe级别和row group级别的，他们都可以用来根据Search ARGuments（谓词下推条件）判断是否可以跳过某些数据，在统计信息中都包含成员数和是否有null值，并且对于不同类型的数据设置一些特定的统计信息。 数据访问读取ORC文件是从尾部开始的，第一次读取16KB的大小，尽可能的将Postscript和Footer数据都读入内存。文件的最后一个字节保存着PostScript的长度，它的长度不会超过256字节，PostScript中保存着整个文件的元数据信息，它包括文件的压缩格式、文件内部每一个压缩块的最大长度(每次分配内存的大小)、Footer长度，以及一些版本信息。在Postscript和Footer之间存储着整个文件的统计信息(上图中未画出)，这部分的统计信息包括每一个stripe中每一列的信息，主要统计成员数、最大值、最小值、是否有空值等。 接下来读取文件的Footer信息，它包含了每一个stripe的长度和偏移量，该文件的schema信息(将schema树按照schema中的编号保存在数组中)、整个文件的统计信息以及每一个row group的行数。 处理stripe时首先从Footer中获取每一个stripe的其实位置和长度、每一个stripe的Footer数据(元数据，记录了index和data的的长度)，整个striper被分为index和data两部分，stripe内部是按照row group进行分块的(每一个row group中多少条记录在文件的Footer中存储)，row group内部按列存储。每一个row group由多个stream保存数据和索引信息。每一个stream的数据会根据该列的类型使用特定的压缩算法保存。在ORC中存在如下几种stream类型： PRESENT：每一个成员值在这个stream中保持一位(bit)用于标示该值是否为NULL，通过它可以只记录部位NULL的值 DATA：该列的中属于当前stripe的成员值。 LENGTH：每一个成员的长度，这个是针对string类型的列才有的。 DICTIONARY_DATA：对string类型数据编码之后字典的内容。 SECONDARY：存储Decimal、timestamp类型的小数或者纳秒数等。 ROW_INDEX：保存stripe中每一个row group的统计信息和每一个row group起始位置信息。 在初始化阶段获取全部的元数据之后，可以通过includes数组指定需要读取的列编号，它是一个boolean数组，如果不指定则读取全部的列，还可以通过传递SearchArgument参数指定过滤条件，根据元数据首先读取每一个stripe中的index信息，然后根据index中统计信息以及SearchArgument参数确定需要读取的row group编号，再根据includes数据决定需要从这些row group中读取的列，通过这两层的过滤需要读取的数据只是整个stripe多个小段的区间，然后ORC会尽可能合并多个离散的区间尽可能的减少I/O次数。然后再根据index中保存的下一个row group的位置信息调至该stripe中第一个需要读取的row group中。 由于ORC中使用了更加精确的索引信息，使得在读取数据时可以指定从任意一行开始读取，更细粒度的统计信息使得读取ORC文件跳过整个row group，ORC默认会对任何一块数据和索引信息使用ZLIB压缩，因此ORC文件占用的存储空间也更小，这点在后面的测试对比中也有所印证。 在新版本的ORC中也加入了对Bloom Filter的支持，它可以进一步提升谓词下推的效率，在Hive 1.2.0版本以后也加入了对此的支持。 性能测试为了对比测试两种存储格式，我选择使用TPC-DS数据集并且对它进行改造以生成宽表、嵌套和多层嵌套的数据。使用最常用的Hive作为SQL引擎进行测试。 测试环境 Hadoop集群：物理测试集群，四台DataNode/NodeManager机器，每个机器32core+128GB，测试时使用整个集群的资源。 Hive：Hive 1.2.1版本，使用hiveserver2启动，本机MySql作为元数据库，jdbc方式提交查询SQL 数据集：100GB TPC-DS数据集，选取其中的Store_Sales为事实表的模型作为测试数据 查询SQL：选择TPC-DS中涉及到上述模型的10条SQL并对其进行改造。 测试场景和结果整个测试设置了四种场景，每一种场景下对比测试数据占用的存储空间的大小和相同查询执行消耗的时间对比，除了场景一基于原始的TPC-DS数据集外，其余的数据都需要进行数据导入，同时对比这几个场景的数据导入时间。 场景一：一个事实表、多个维度表，复杂的join查询。基于原始的TPC-DS数据集。 Store_Sales表记录数：287,997,024，表大小为： 原始Text格式，未压缩 : 38.1 G ORC格式，默认压缩（ZLIB）,一共1800+个分区 : 11.5 G Parquet格式，默认压缩（Snappy），一共1800+个分区 ： 14.8 G 查询测试结果： 场景二：维度表和事实表join之后生成的宽表，只在一个表上做查询。整个测试设置了四种场景，每一种场景下对比测试数据占用的存储空间的大小和相同查询执行消耗的时间对比，除了场景一基于原始的TPC-DS数据集外，其余的数据都需要进行数据导入，同时对比这几个场景的数据导入时间。选取数据模型中的store_sales, household_demographics, customer_address, date_dim, store表生成一个扁平式宽表(store_sales_wide_table)，基于这个表执行查询，由于场景一种选择的query大多数不能完全match到这个宽表，所以对场景1中的SQL进行部分改造。 store_sales_wide_table表记录数：263,704,266，表大小为： 原始Text格式，未压缩 ： 149.0 G ORC格式，默认压缩 ： 10.6 G PARQUET格式，默认压缩 ： 12.5 G 查询测试结果： 场景三：复杂的数据结构组成的宽表，struct、list、map等（1层）整个测试设置了四种场景，每一种场景下对比测试数据占用的存储空间的大小和相同查询执行消耗的时间对比，除了场景一基于原始的TPC-DS数据集外，其余的数据都需要进行数据导入，同时对比这几个场景的数据导入时间。在场景二的基础上，将维度表（除了store_sales表）转换成一个struct或者map对象，源store_sales表中的字段保持不变。生成有一层嵌套的新表（store_sales_wide_table_one_nested），使用的查询逻辑相同。 store_sales_wide_table_one_nested表记录数：263,704,266，表大小为： 原始Text格式，未压缩 ： 245.3 G ORC格式，默认压缩 ： 10.9 G PARQUET格式，默认压缩 ： 29.8 G 查询测试结果： 场景四：复杂的数据结构，多层嵌套。（3层）整个测试设置了四种场景，每一种场景下对比测试数据占用的存储空间的大小和相同查询执行消耗的时间对比，除了场景一基于原始的TPC-DS数据集外，其余的数据都需要进行数据导入，同时对比这几个场景的数据导入时间。在场景三的基础上，将部分维度表的struct内的字段再转换成struct或者map对象，只存在struct中嵌套map的情况，最深的嵌套为三层。生成一个多层嵌套的新表（store_sales_wide_table_more_nested），使用的查询逻辑相同。 该场景中只涉及一个多层嵌套的宽表，没有任何分区字段，store_sales_wide_table_more_nested表记录数：263,704,266，表大小为： 原始Text格式，未压缩 ： 222.7 G ORC格式，默认压缩 ： 10.9 G PARQUET格式，默认压缩 ： 23.1 G 比一层嵌套表store_sales_wide_table_one_nested要小？ 查询测试结果： 结果分析从上述测试结果来看，星状模型对于数据分析场景并不是很合适，多个表的join会大大拖慢查询速度，并且不能很好的利用列式存储带来的性能提升，在使用宽表的情况下，列式存储的性能提升明显，ORC文件格式在存储空间上要远优于Text格式，较之于PARQUET格式有一倍的存储空间提升，在导数据（insert into table select 这样的方式）方面ORC格式也要优于PARQUET，在最终的查询性能上可以看到，无论是无嵌套的扁平式宽表，或是一层嵌套表，还是多层嵌套的宽表，两者的查询性能相差不多，较之于Text格式有2到3倍左右的提升。 另外，通过对比场景二和场景三的测试结果，可以发现扁平式的表结构要比嵌套式结构的查询性能有所提升，所以如果选择使用大宽表，则设计宽表的时候尽可能的将表设计的扁平化，减少嵌套数据。 通过这三种文件存储格式的测试对比，ORC文件存储格式无论是在空间存储、导数据速度还是查询速度上表现的都较好一些，并且ORC可以一定程度上支持ACID操作，社区的发展目前也是Hive中比较提倡使用的一种列式存储格式，另外，本次测试主要针对的是Hive引擎，所以不排除存在Hive与ORC的敏感度比PARQUET要高的可能性。 总结本文主要从数据模型、文件格式和数据访问流程等几个方面详细介绍了Hadoop生态圈中的两种列式存储格式——Parquet和ORC，并通过大数据量的测试对两者的存储和查询性能进行了对比。对于大数据场景下的数据分析需求，使用这两种存储格式总会带来存储和性能上的提升，但是在实际使用时还需要针对实际的数据进行选择。另外由于不同开源产品可能对不同的存储格式有特定的优化，所以选择时还需要考虑查询引擎的因素。 拓展阅读：spark关于parquet的优化","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"},{"name":"orc","slug":"orc","permalink":"https://stanxia.github.io/tags/orc/"},{"name":"parquet","slug":"parquet","permalink":"https://stanxia.github.io/tags/parquet/"}]},{"title":"spark分层取样","slug":"spark分层取样","date":"2017-11-27T08:57:10.000Z","updated":"2017-12-05T03:34:22.000Z","comments":true,"path":"2017/11/27/spark分层取样/","link":"","permalink":"https://stanxia.github.io/2017/11/27/spark分层取样/","excerpt":"先将总体的单位按某种特征分为若干次级总体（层），然后再从每一层内进行单纯随机抽样，组成一个样本的统计学计算方法叫做分层抽样。在spark.mllib中，用key来分层。\n与存在于spark.mllib中的其它统计函数不同，分层采样方法sampleByKey和sampleByKeyExact可以在key-value对的RDD上执行。在分层采样中，可以认为key是一个标签，value是特定的属性。例如，key可以是男人或者女人或者文档id,它相应的value可能是一组年龄或者是文档中的词。sampleByKey方法通过掷硬币的方式决定是否采样一个观察数据，因此它需要我们传递（pass over）数据并且提供期望的数据大小(size)。sampleByKeyExact比每层使用sampleByKey随机抽样需要更多的有意义的资源，但是它能使样本大小的准确性达到了99.99%。\nsampleByKeyExact()允许用户准确抽取f_k * n_k个样本，这里f_k表示期望获取键为k的样本的比例，n_k表示键为k的键值对的数量。下面是一个使用的例子：","text":"先将总体的单位按某种特征分为若干次级总体（层），然后再从每一层内进行单纯随机抽样，组成一个样本的统计学计算方法叫做分层抽样。在spark.mllib中，用key来分层。 与存在于spark.mllib中的其它统计函数不同，分层采样方法sampleByKey和sampleByKeyExact可以在key-value对的RDD上执行。在分层采样中，可以认为key是一个标签，value是特定的属性。例如，key可以是男人或者女人或者文档id,它相应的value可能是一组年龄或者是文档中的词。sampleByKey方法通过掷硬币的方式决定是否采样一个观察数据，因此它需要我们传递（pass over）数据并且提供期望的数据大小(size)。sampleByKeyExact比每层使用sampleByKey随机抽样需要更多的有意义的资源，但是它能使样本大小的准确性达到了99.99%。 sampleByKeyExact()允许用户准确抽取f_k * n_k个样本，这里f_k表示期望获取键为k的样本的比例，n_k表示键为k的键值对的数量。下面是一个使用的例子： 123456789import org.apache.spark.SparkContextimport org.apache.spark.SparkContext._import org.apache.spark.rdd.PairRDDFunctionsval sc: SparkContext = ...val data = ... // an RDD[(K, V)] of any key value pairsval fractions: Map[K, Double] = ... // specify the exact fraction desired from each key// Get an exact sample from each stratumval approxSample = data.sampleByKey(withReplacement = false, fractions)val exactSample = data.sampleByKeyExact(withReplacement = false, fractions) 当withReplacement为true时，采用PoissonSampler取样器，当withReplacement为false使，采用BernoulliSampler取样器。 123456789101112131415161718192021def sampleByKey(withReplacement: Boolean, fractions: Map[K, Double], seed: Long = Utils.random.nextLong): RDD[(K, V)] = self.withScope &#123; val samplingFunc = if (withReplacement) &#123; StratifiedSamplingUtils.getPoissonSamplingFunction(self, fractions, false, seed) &#125; else &#123; StratifiedSamplingUtils.getBernoulliSamplingFunction(self, fractions, false, seed) &#125; self.mapPartitionsWithIndex(samplingFunc, preservesPartitioning = true) &#125;def sampleByKeyExact( withReplacement: Boolean, fractions: Map[K, Double], seed: Long = Utils.random.nextLong): RDD[(K, V)] = self.withScope &#123; val samplingFunc = if (withReplacement) &#123; StratifiedSamplingUtils.getPoissonSamplingFunction(self, fractions, true, seed) &#125; else &#123; StratifiedSamplingUtils.getBernoulliSamplingFunction(self, fractions, true, seed) &#125; self.mapPartitionsWithIndex(samplingFunc, preservesPartitioning = true) &#125; 下面我们分别来看sampleByKey和sampleByKeyExact的实现。 sampleByKey的实现当我们需要不重复抽样时，我们需要用泊松抽样器来抽样。当需要重复抽样时，用伯努利抽样器抽样。sampleByKey的实现比较简单，它就是统一的随机抽样。 泊松抽样器我们首先看泊松抽样器的实现。 12345678910111213141516171819def getPoissonSamplingFunction[K: ClassTag, V: ClassTag](rdd: RDD[(K, V)], fractions: Map[K, Double], exact: Boolean, seed: Long): (Int, Iterator[(K, V)]) =&gt; Iterator[(K, V)] = &#123; (idx: Int, iter: Iterator[(K, V)]) =&gt; &#123; //初始化随机生成器 val rng = new RandomDataGenerator() rng.reSeed(seed + idx) iter.flatMap &#123; item =&gt; //获得下一个泊松值 val count = rng.nextPoisson(fractions(item._1)) if (count == 0) &#123; Iterator.empty &#125; else &#123; Iterator.fill(count)(item) &#125; &#125; &#125;&#125; getPoissonSamplingFunction返回的是一个函数，传递给mapPartitionsWithIndex处理每个分区的数据。这里RandomDataGenerator是一个随机生成器，它用于同时生成均匀值(uniform values)和泊松值(Poisson values)。 伯努利抽样器1234567891011121314def getBernoulliSamplingFunction[K, V](rdd: RDD[(K, V)], fractions: Map[K, Double], exact: Boolean, seed: Long): (Int, Iterator[(K, V)]) =&gt; Iterator[(K, V)] = &#123; var samplingRateByKey = fractions (idx: Int, iter: Iterator[(K, V)]) =&gt; &#123; //初始化随机生成器 val rng = new RandomDataGenerator() rng.reSeed(seed + idx) // Must use the same invoke pattern on the rng as in getSeqOp for without replacement // in order to generate the same sequence of random numbers when creating the sample iter.filter(t =&gt; rng.nextUniform() &lt; samplingRateByKey(t._1)) &#125; &#125; sampleByKeyExact的实现sampleByKeyExact获取更准确的抽样结果，它的实现也分为两种情况，重复抽样和不重复抽样。前者使用泊松抽样器，后者使用伯努利抽样器。 泊松抽样器12345678910111213141516171819202122232425val counts = Some(rdd.countByKey())//计算立即接受的样本数量，并且为每层生成候选名单val finalResult = getAcceptanceResults(rdd, true, fractions, counts, seed)//决定接受样本的阈值，生成准确的样本大小val thresholdByKey = computeThresholdByKey(finalResult, fractions)(idx: Int, iter: Iterator[(K, V)]) =&gt; &#123; val rng = new RandomDataGenerator() rng.reSeed(seed + idx) iter.flatMap &#123; item =&gt; val key = item._1 val acceptBound = finalResult(key).acceptBound // Must use the same invoke pattern on the rng as in getSeqOp for with replacement // in order to generate the same sequence of random numbers when creating the sample val copiesAccepted = if (acceptBound == 0) 0L else rng.nextPoisson(acceptBound) //候选名单 val copiesWaitlisted = rng.nextPoisson(finalResult(key).waitListBound) val copiesInSample = copiesAccepted + (0 until copiesWaitlisted).count(i =&gt; rng.nextUniform() &lt; thresholdByKey(key)) if (copiesInSample &gt; 0) &#123; Iterator.fill(copiesInSample.toInt)(item) &#125; else &#123; Iterator.empty &#125; &#125;&#125; 伯努利抽样1234567891011121314151617def getBernoulliSamplingFunction[K, V](rdd: RDD[(K, V)], fractions: Map[K, Double], exact: Boolean, seed: Long): (Int, Iterator[(K, V)]) =&gt; Iterator[(K, V)] = &#123; var samplingRateByKey = fractions //计算立即接受的样本数量，并且为每层生成候选名单 val finalResult = getAcceptanceResults(rdd, false, fractions, None, seed) //决定接受样本的阈值，生成准确的样本大小 samplingRateByKey = computeThresholdByKey(finalResult, fractions) (idx: Int, iter: Iterator[(K, V)]) =&gt; &#123; val rng = new RandomDataGenerator() rng.reSeed(seed + idx) // Must use the same invoke pattern on the rng as in getSeqOp for without replacement // in order to generate the same sequence of random numbers when creating the sample iter.filter(t =&gt; rng.nextUniform() &lt; samplingRateByKey(t._1)) &#125; &#125;","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"},{"name":"源码","slug":"源码","permalink":"https://stanxia.github.io/tags/源码/"},{"name":"取样","slug":"取样","permalink":"https://stanxia.github.io/tags/取样/"}]},{"title":"Dataset.scala","slug":"Dataset-scala","date":"2017-11-23T01:34:44.000Z","updated":"2017-12-05T03:33:42.000Z","comments":true,"path":"2017/11/23/Dataset-scala/","link":"","permalink":"https://stanxia.github.io/2017/11/23/Dataset-scala/","excerpt":"前言Dataset 是一种强类型的领域特定对象集合，可以在使用功能或关系操作的同时进行转换。每个 Dataset 也有一个名为 “DataFrame” 的无类型视图，它是 [[Row]] 的 Dataset。Dataset 上可用的操作分为转换和动作:\n\n转换：产生新的 Dataset ；包括 map, filter, select, and aggregate (groupBy).动作：触发计算并返回结果 ；包括 count, show, or 写数据到文件系统。\n\nDataset是懒加载的，例如：只有提交动作的时候才会触发计算。在内部，Datasets表示一个逻辑计划，它描述生成数据所需的计算。当提交动作时，Spark的查询优化器会优化逻辑计划，并以并行和分布式的方式生成有效执行的物理计划。请使用explain 功能，探索逻辑计划和优化的物理计划。\n为了有效地支持特定于领域的对象，需要[[Encoder]]。编码器将特定类型的“T”映射到Spark的内部类型系统。例如：给一个 Person 类，并带有两个属性：name (string) and age (int),编码器告诉Spark在运行时生成代码，序列化 Person 对象为二进制结构。\n通常有两种创建Dataset的方法:\n\n使用 SparkSession 上可用的 read 方法读取 Spark 指向的存储系统上的文件。用现存的 Datasets 转换而来。\n\nDataset操作也可以是无类型的，通过多种领域专用语言（DSL）方法定义：这些操作非常类似于 R或Python语言中的 数据框架抽象中可用的操作。","text":"前言Dataset 是一种强类型的领域特定对象集合，可以在使用功能或关系操作的同时进行转换。每个 Dataset 也有一个名为 “DataFrame” 的无类型视图，它是 [[Row]] 的 Dataset。Dataset 上可用的操作分为转换和动作: 转换：产生新的 Dataset ；包括 map, filter, select, and aggregate (groupBy).动作：触发计算并返回结果 ；包括 count, show, or 写数据到文件系统。 Dataset是懒加载的，例如：只有提交动作的时候才会触发计算。在内部，Datasets表示一个逻辑计划，它描述生成数据所需的计算。当提交动作时，Spark的查询优化器会优化逻辑计划，并以并行和分布式的方式生成有效执行的物理计划。请使用explain 功能，探索逻辑计划和优化的物理计划。 为了有效地支持特定于领域的对象，需要[[Encoder]]。编码器将特定类型的“T”映射到Spark的内部类型系统。例如：给一个 Person 类，并带有两个属性：name (string) and age (int),编码器告诉Spark在运行时生成代码，序列化 Person 对象为二进制结构。 通常有两种创建Dataset的方法: 使用 SparkSession 上可用的 read 方法读取 Spark 指向的存储系统上的文件。用现存的 Datasets 转换而来。 Dataset操作也可以是无类型的，通过多种领域专用语言（DSL）方法定义：这些操作非常类似于 R或Python语言中的 数据框架抽象中可用的操作。 basic-基础方法toDF123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * Converts this strongly typed collection of data to generic Dataframe. In contrast to the * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]] * objects that allow fields to be accessed by ordinal or name. * 将这种强类型的数据集合转换为一般的Dataframe。 * 与Dataset操作所使用的强类型对象相反， * Dataframe返回泛型[[Row]]对象，这些对象允许通过序号或名称访问字段 * * @group basic * @since 1.6.0 */// This is declared with parentheses to prevent the Scala compiler from treating// `ds.toDF(\"1\")` as invoking this toDF and then apply on the returned DataFrame.// 这是用括号声明的，以防止Scala编译器处理ds.toDF(“1”)调用这个toDF，然后在返回的DataFrame上应用。def toDF(): DataFrame = new Dataset[Row](sparkSession, queryExecution, RowEncoder(schema)) /** * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed. * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with * meaningful names. For example: * * 将这种强类型的数据集合转换为通用的“DataFrame”，并将列重命名。 * 在将tuple的RDD转换为富有含义的名称的“DataFrame”时，这是非常方便的，如： * * &#123;&#123;&#123; * val rdd: RDD[(Int, String)] = ... * rdd.toDF() // 隐式转换创建了 DataFrame ，列名为： `_1` and `_2` * rdd.toDF(\"id\", \"name\") // 创建了 DataFrame ，列名为： \"id\" and \"name\" * &#125;&#125;&#125; * * @group basic * @since 2.0.0 */@scala.annotation.varargsdef toDF(colNames: String*): DataFrame = &#123; require(schema.size == colNames.size, \"The number of columns doesn't match.\\n\" + s\"Old column names ($&#123;schema.size&#125;): \" + schema.fields.map(_.name).mkString(\", \") + \"\\n\" + s\"New column names ($&#123;colNames.size&#125;): \" + colNames.mkString(\", \")) val newCols = logicalPlan.output.zip(colNames).map &#123; case (oldAttribute, newName) =&gt; Column(oldAttribute).as(newName) &#125; select(newCols: _*)&#125; as123456789101112131415161718192021222324252627282930313233/** * :: Experimental :: * Returns a new Dataset where each record has been mapped on to the specified type. The * method used to map columns depend on the type of `U`: * * 返回一个新的Dataset，其中每个记录都被映射到指定的类型。用于映射列的方法取决于“U”的类型: * * - When `U` is a class, fields for the class will be mapped to columns of the same name * (case sensitivity is determined by `spark.sql.caseSensitive`). * * 当“U”是类时：类的属性将映射到相同名称的列 * * - When `U` is a tuple, the columns will be be mapped by ordinal (i.e. the first column will * be assigned to `_1`). * * 当“U”是元组时：列将由序数映射 （例如，第一列将为 \"_1\"） * * - When `U` is a primitive type (i.e. String, Int, etc), then the first column of the * `DataFrame` will be used. * * 当“U”是 基本类型（如 String，Int等）：然后将使用“DataFrame”的第一列。 * * If the schema of the Dataset does not match the desired `U` type, you can use `select` * along with `alias` or `as` to rearrange or rename as required. * * 如果数据集的模式与所需的“U”类型不匹配，您可以使用“select”和“alias”或“as”来重新排列或重命名。 * * @group basic * @since 1.6.0 */ @Experimental @InterfaceStability.Evolving def as[U: Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan) schema12345678/** * Returns the schema of this Dataset. * 返回该Dataset的模版 * * @group basic * @since 1.6.0 */def schema: StructType = queryExecution.analyzed.schema printSchema12345678910/** * Prints the schema to the console in a nice tree format. * * 以一种漂亮的树格式将模式打印到控制台。 * * @group basic * @since 1.6.0 */// scalastyle:off printlndef printSchema(): Unit = println(schema.treeString) explain1234567891011121314151617181920212223242526/** * Prints the plans (logical and physical) to the console for debugging purposes. * * 将计划(逻辑和物理)打印到控制台以进行调试。 * 参数：extended = false 为物理计划 * * @group basic * @since 1.6.0 */def explain(extended: Boolean): Unit = &#123; val explain = ExplainCommand(queryExecution.logical, extended = extended) sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach &#123; // scalastyle:off println r =&gt; println(r.getString(0)) // scalastyle:on println &#125;&#125; /** * Prints the physical plan to the console for debugging purposes. * 将物理计划打印到控制台以进行调试。 * * @group basic * @since 1.6.0 */def explain(): Unit = explain(extended = false) dtypes12345678910/** * Returns all column names and their data types as an array. * 以数组的形式返回所有列名称和它们的数据类型 * * @group basic * @since 1.6.0 */def dtypes: Array[(String, String)] = schema.fields.map &#123; field =&gt; (field.name, field.dataType.toString)&#125; columns12345678/** * Returns all column names as an array. * 以数组的形式返回 所有列名 * * @group basic * @since 1.6.0 */def columns: Array[String] = schema.fields.map(_.name) isLocal123456789/** * Returns true if the `collect` and `take` methods can be run locally * (without any Spark executors). * 如果`collect` and `take` 方法能在本地运行，则返回true * * @group basic * @since 1.6.0 */def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation] checkpoint12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/** * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * 急切地检查一个数据集并返回新的数据集。 * 检查点能用来清除Dataset的逻辑计划，尤其是在可能生成指数级别的迭代算法中尤其有用。 * 将会在检查点目录中保存检查文件。可以在`SparkContext#setCheckpointDir`中设置。 * * @group basic * @since 2.1.0 */@Experimental@InterfaceStability.Evolvingdef checkpoint(): Dataset[T] = checkpoint(eager = true) /** * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the * logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * 返回Dataset 之前检查过的版本。 * 检查点能用来清除Dataset的逻辑计划，尤其是在可能生成指数级别的迭代算法中尤其有用。 * 将会在检查点目录中保存检查文件。可以在`SparkContext#setCheckpointDir`中设置。 * * @group basic * @since 2.1.0 */@Experimental@InterfaceStability.Evolvingdef checkpoint(eager: Boolean): Dataset[T] = &#123; val internalRdd = queryExecution.toRdd.map(_.copy()) internalRdd.checkpoint() if (eager) &#123; internalRdd.count() &#125; val physicalPlan = queryExecution.executedPlan // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the // size of `PartitioningCollection` may grow exponentially for queries involving deep inner // joins. // 每当我们看到“PartitioningCollection”时，就采用第一个叶子分区 // 否则，用于涉及深度内连接的查询，“PartitioningCollection”的大小可能会以指数形式增长。 def firstLeafPartitioning(partitioning: Partitioning): Partitioning = &#123; partitioning match &#123; case p: PartitioningCollection =&gt; firstLeafPartitioning(p.partitionings.head) case p =&gt; p &#125; &#125; val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning) Dataset.ofRows( sparkSession, LogicalRDD( logicalPlan.output, internalRdd, outputPartitioning, physicalPlan.outputOrdering )(sparkSession)).as[T]&#125; persist1234567891011121314151617181920212223242526272829303132333435/** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * 持久化。 * 根据默认的 存储级别 (`MEMORY_AND_DISK`) 持久化Dataset。 * * @group basic * @since 1.6.0 */def persist(): this.type = &#123; sparkSession.sharedState.cacheManager.cacheQuery(this) this&#125; /** * Persist this Dataset with the given storage level. * * 根据指定的 存储级别 持久化 Dataset。 * * @param newLevel One of: * `MEMORY_ONLY`, * `MEMORY_AND_DISK`, * `MEMORY_ONLY_SER`, * `MEMORY_AND_DISK_SER`, * `DISK_ONLY`, * `MEMORY_ONLY_2`, 与MEMORY_ONLY的区别是会备份数据到其他节点上 * `MEMORY_AND_DISK_2`, 与MEMORY_AND_DISK的区别是会备份数据到其他节点上 * etc. * @group basic * @since 1.6.0 */def persist(newLevel: StorageLevel): this.type = &#123; sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel) this&#125; cache1234567891011/** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * 持久化。 * 根据默认的 存储级别 (`MEMORY_AND_DISK`) 持久化Dataset。 * 和 persist 一致。 * * @group basic * @since 1.6.0 */def cache(): this.type = persist() storageLevel12345678910111213/** * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted. * * 获取当前Dataset的当前存储级别。如果没有缓存则 StorageLevel.NONE。 * * @group basic * @since 2.1.0 */def storageLevel: StorageLevel = &#123; sparkSession.sharedState.cacheManager.lookupCachedData(this).map &#123; cachedData =&gt; cachedData.cachedRepresentation.storageLevel &#125;.getOrElse(StorageLevel.NONE)&#125; unpersist1234567891011121314151617181920212223242526/** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * * 解除持久化。 * 将Dataset标记为非持久化，并从内存和磁盘中移除所有的块。 * * @param blocking Whether to block until all blocks are deleted. * 是否阻塞，直到删除所有的块。 * @group basic * @since 1.6.0 */def unpersist(blocking: Boolean): this.type = &#123; sparkSession.sharedState.cacheManager.uncacheQuery(this, blocking) this&#125; /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * * 解除持久化。 * 将Dataset标记为非持久化，并从内存和磁盘中移除所有的块。 * * @group basic * @since 1.6.0 */def unpersist(): this.type = unpersist(blocking = false) rdd123456789101112131415/** * Represents the content of the Dataset as an `RDD` of [[T]]. * * 转换为[[T]]的“RDD”，表示Dataset的内容 * * @group basic * @since 1.6.0 */lazy val rdd: RDD[T] = &#123; val objectType = exprEnc.deserializer.dataType val deserialized = CatalystSerde.deserialize[T](logicalPlan) sparkSession.sessionState.executePlan(deserialized).toRdd.mapPartitions &#123; rows =&gt; rows.map(_.get(0, objectType).asInstanceOf[T]) &#125;&#125; toJavaRDD12345678910111213141516171819/** * Returns the content of the Dataset as a `JavaRDD` of [[T]]s. * * 转换为JavaRDD * * @group basic * @since 1.6.0 */def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD() /** * Returns the content of the Dataset as a `JavaRDD` of [[T]]s. * * 转换为JavaRDD * * @group basic * @since 1.6.0 */def javaRDD: JavaRDD[T] = toJavaRDD registerTempTable1234567891011121314/** * Registers this Dataset as a temporary table using the given name. The lifetime of this * temporary table is tied to the [[SparkSession]] that was used to create this Dataset. * * 根据指定的表名，注册临时表。 * 生命周期为[[SparkSession]]的生命周期。 * * @group basic * @since 1.6.0 */@deprecated(\"Use createOrReplaceTempView(viewName) instead.\", \"2.0.0\")def registerTempTable(tableName: String): Unit = &#123; createOrReplaceTempView(tableName)&#125; createTempView123456789101112131415161718192021/** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * 用指定的名字创建本地临时表。 * 与[[SparkSession]] 同生命周期。 * * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that * created it, i.e. it will be automatically dropped when the session terminates. It's not * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view. * * 本地临时表是 session范围内的。当创建它的session停止的时候，该表也随之停止。 * * @throws AnalysisException if the view name already exists * @group basic * @since 2.0.0 */@throws[AnalysisException]def createTempView(viewName: String): Unit = withPlan &#123; createTempViewCommand(viewName, replace = false, global = false)&#125; createOrReplaceTempView123456789101112/** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * 用指定的名字创建本地临时表。如果已经有了则替换。 * * @group basic * @since 2.0.0 */def createOrReplaceTempView(viewName: String): Unit = withPlan &#123; createTempViewCommand(viewName, replace = true, global = false)&#125; createGlobalTempView1234567891011121314151617181920212223/** * Creates a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * 创建全局临时表。 * 生命周期为整个Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `_global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM _global_temp.view1`. * * 全局临时表是跨session的。属于 _global_temp 数据库。e.g. `SELECT * FROM _global_temp.view1`. * * @throws AnalysisException if the view name already exists * 如果表已经存在，则报错。 * @group basic * @since 2.1.0 */@throws[AnalysisException]def createGlobalTempView(viewName: String): Unit = withPlan &#123; createTempViewCommand(viewName, replace = false, global = true)&#125; write123456789101112131415/** * Interface for saving the content of the non-streaming Dataset out into external storage. * * 将非流Dataset的内容保存到外部存储中的接口。 * * @group basic * @since 1.6.0 */def write: DataFrameWriter[T] = &#123; if (isStreaming) &#123; logicalPlan.failAnalysis( \"'write' can not be called on streaming Dataset/DataFrame\") &#125; new DataFrameWriter[T](this)&#125; writeStream123456789101112131415161718/** * :: Experimental :: * Interface for saving the content of the streaming Dataset out into external storage. * * 将流Dataset保存在外部存储。 * * @group basic * @since 2.0.0 */@Experimental@InterfaceStability.Evolvingdef writeStream: DataStreamWriter[T] = &#123; if (!isStreaming) &#123; logicalPlan.failAnalysis( \"'writeStream' can be called only on streaming Dataset/DataFrame\") &#125; new DataStreamWriter[T](this)&#125; toJSON1234567891011121314151617181920212223242526272829303132333435/** * Returns the content of the Dataset as a Dataset of JSON strings. * * 将Dataset转换为JSON。 * * @since 2.0.0 */def toJSON: Dataset[String] = &#123; val rowSchema = this.schema val rdd: RDD[String] = queryExecution.toRdd.mapPartitions &#123; iter =&gt; val writer = new CharArrayWriter() // create the Generator without separator inserted between 2 records val gen = new JacksonGenerator(rowSchema, writer) new Iterator[String] &#123; override def hasNext: Boolean = iter.hasNext override def next(): String = &#123; gen.write(iter.next()) gen.flush() val json = writer.toString if (hasNext) &#123; writer.reset() &#125; else &#123; gen.close() &#125; json &#125; &#125; &#125; import sparkSession.implicits.newStringEncoder sparkSession.createDataset(rdd)&#125; inputFiles12345678910111213141516171819202122/** * Returns a best-effort snapshot of the files that compose this Dataset. This method simply * asks each constituent BaseRelation for its respective files and takes the union of all results. * Depending on the source relations, this may not find all input files. Duplicates are removed. * * 返回组成这个Dataset的所有文件的最佳快照。 * 该方法简单地要求每个组件BaseRelation对其各自的文件进行处理，并联合所有结果。 * 基于源关系，应该可以找到所有的输入文件。 * 重复的也会被移除。 * * @group basic * @since 2.0.0 */def inputFiles: Array[String] = &#123; val files: Seq[String] = queryExecution.optimizedPlan.collect &#123; case LogicalRelation(fsBasedRelation: FileRelation, _, _) =&gt; fsBasedRelation.inputFiles case fr: FileRelation =&gt; fr.inputFiles &#125;.flatten files.toSet.toArray&#125; streamingisStreaming123456789101112131415161718/** * Returns true if this Dataset contains one or more sources that continuously * return data as it arrives. A Dataset that reads data from a streaming source * must be executed as a `StreamingQuery` using the `start()` method in * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or * `collect()`, will throw an [[AnalysisException]] when there is a streaming * source present. * * 如果Dataset包含一个或多个持续返回数据的源，则返回true； * 如果Dataset从streaming源读取数据，则必须像 `StreamingQuery` 一样执行：使用 `DataStreamWriter` 中的 `start()`方法。 * 返回单个值的方法，例如： `count()` or `collect()`，当存在streaming源时，将会抛出[[AnalysisException]]。 * * @group streaming * @since 2.0.0 */@Experimental@InterfaceStability.Evolvingdef isStreaming: Boolean = logicalPlan.isStreaming withWatermark123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * :: Experimental :: 实验性的 * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time * before which we assume no more late data is going to arrive. * * 为这个[[Dataset]]定义事件时间水印。 * 我们假设没有更多的晚期数据将到达之前，一个水印跟踪一个时间点。 * * Spark will use this watermark for several purposes: * Spark用水印有几个目的： * - To know when a given time window aggregation can be finalized and thus can be emitted when * using output modes that do not allow updates. * * 可以知道何时完成给定的时间窗口聚合能够完成，因此当使用不允许更新的输出模式时能够被放出。 * - To minimize the amount of state that we need to keep for on-going aggregations. * 为了最小化我们需要持续不断的聚合的状态数量。 * * * The current watermark is computed by looking at the `MAX(eventTime)` seen across * all of the partitions in the query minus a user specified `delayThreshold`. Due to the cost * of coordinating this value across partitions, the actual watermark used is only guaranteed * to be at least `delayThreshold` behind the actual event time. In some cases we may still * process records that arrive more than `delayThreshold` late. * * 当前的水印 = 查看查询中所有分区上看到的`MAX(eventTime)` - 用户指定的`delayThreshold` * 由于在分区之间协调这个值的花销，实际使用的水印只保证在实际事件时间后至少是“delayThreshold”。 * 在某些情况下，我们可能还会处理比“delayThreshold”晚些时候到达的记录。 * * @param eventTime the name of the column that contains the event time of the row. * 包含行的事件时间的列名 * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest * record that has been processed in the form of an interval * (e.g. \"1 minute\" or \"5 hours\"). * 等待晚到数据的最少延迟，相对于以间隔形式处理的最新记录 * @group streaming * @since 2.1.0 */@Experimental@InterfaceStability.Evolving// We only accept an existing column name, not a derived column here as a watermark that is// defined on a derived column cannot referenced elsewhere in the plan.// 我们只接受一个现有的列名，而不是作为一个在派生列上定义的水印的派生列，而不能在该计划的其他地方引用。def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan &#123; val parsedDelay = Option(CalendarInterval.fromString(\"interval \" + delayThreshold)) .getOrElse(throw new AnalysisException(s\"Unable to parse time delay '$delayThreshold'\")) EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan)&#125; actionshow123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990/** * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated, * and all cells will be aligned right. For example: * * 以表格形式显示数据集。 * 字符串超过20个字符将被截断， * 所有单元格将被对齐。 * &#123;&#123;&#123; * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * &#125;&#125;&#125; * * @param numRows Number of rows to show 要显示的行数 * @group action * @since 1.6.0 */def show(numRows: Int): Unit = show(numRows, truncate = true) /** * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters * will be truncated, and all cells will be aligned right. * 显示头20行 * * @group action * @since 1.6.0 */def show(): Unit = show(20)/** * Displays the top 20 rows of Dataset in a tabular form. * 显示头20行 * * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * 是否截断长字符串。如果 true：超过20个字符就会被截断 * @group action * @since 1.6.0 */def show(truncate: Boolean): Unit = show(20, truncate)/** * Displays the Dataset in a tabular form. For example: * &#123;&#123;&#123; * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * &#125;&#125;&#125; * * @param numRows Number of rows to show 显示的行数 * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * 是否截断长字符串 * @group action * @since 1.6.0 */// scalastyle:off printlndef show(numRows: Int, truncate: Boolean): Unit = if (truncate) &#123; println(showString(numRows, truncate = 20))&#125; else &#123; println(showString(numRows, truncate = 0))&#125;// scalastyle:on println/** * Displays the Dataset in a tabular form. For example: * &#123;&#123;&#123; * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * &#125;&#125;&#125; * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * 设置 触发截断字符串的阈值 * @group action * @since 1.6.0 */// scalastyle:off printlndef show(numRows: Int, truncate: Int): Unit = println(showString(numRows, truncate)) reduce1234567891011121314151617181920212223242526272829/** * :: Experimental :: * (Scala-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * 使用指定的二进制函数减少这个数据集的元素。给定的“func”必须是可交换的和关联的，否则结果可能是不确定性的。 * * @group action * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef reduce(func: (T, T) =&gt; T): T = rdd.reduce(func)/** * :: Experimental :: * (Java-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * 使用指定的二进制函数减少这个数据集的元素。给定的“func”必须是可交换的和关联的，否则结果可能是不确定性的。 * * @group action * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _)) describe123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * Computes statistics for numeric and string columns, including count, mean, stddev, min, and * max. If no columns are given, this function computes statistics for all numerical or string * columns. * * 计算数字和字符串列的统计数据，包括count、mean、stddev、min和max。 * 如果没有给出任何列，该函数计算所有数值或字符串列的统计信息。 * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * 这个函数用于探索性的数据分析，因为我们不能保证生成数据集的模式的向后兼容性。 * 如果您想通过编程计算汇总统计信息，可以使用“agg”函数。 * * &#123;&#123;&#123; * ds.describe(\"age\", \"height\").show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // max 92.0 192.0 * &#125;&#125;&#125; * * @group action * @since 1.6.0 */ @scala.annotation.varargs def describe(cols: String*): DataFrame = withPlan &#123; // The list of summary statistics to compute, in the form of expressions. val statistics = List[(String, Expression =&gt; Expression)]( \"count\" -&gt; ((child: Expression) =&gt; Count(child).toAggregateExpression()), \"mean\" -&gt; ((child: Expression) =&gt; Average(child).toAggregateExpression()), \"stddev\" -&gt; ((child: Expression) =&gt; StddevSamp(child).toAggregateExpression()), \"min\" -&gt; ((child: Expression) =&gt; Min(child).toAggregateExpression()), \"max\" -&gt; ((child: Expression) =&gt; Max(child).toAggregateExpression())) val outputCols = (if (cols.isEmpty) aggregatableColumns.map(usePrettyExpression(_).sql) else cols).toList val ret: Seq[Row] = if (outputCols.nonEmpty) &#123; val aggExprs = statistics.flatMap &#123; case (_, colToAgg) =&gt; outputCols.map(c =&gt; Column(Cast(colToAgg(Column(c).expr), StringType)).as(c)) &#125; val row = groupBy().agg(aggExprs.head, aggExprs.tail: _*).head().toSeq // Pivot the data so each summary is one row row.grouped(outputCols.size).toSeq.zip(statistics).map &#123; case (aggregation, (statistic, _)) =&gt; Row(statistic :: aggregation.toList: _*) &#125; &#125; else &#123; // If there are no output columns, just output a single column that contains the stats. statistics.map &#123; case (name, _) =&gt; Row(name) &#125; &#125; // All columns are string type val schema = StructType( StructField(\"summary\", StringType) :: outputCols.map(StructField(_, StringType))).toAttributes // `toArray` forces materialization to make the seq serializable LocalRelation.fromExternalRows(schema, ret.toArray.toSeq) &#125; head123456789101112131415161718192021222324/** * Returns the first `n` rows. * * 返回前n行 * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 仅适用于结果很少的时候使用，因为会将结果加载进内存中 * @group action * @since 1.6.0 */def head(n: Int): Array[T] = withTypedCallback(\"head\", limit(n)) &#123; df =&gt; df.collect(needCallback = false)&#125;/** * Returns the first row. * * 返回第一行（默认1） * * @group action * @since 1.6.0 */def head(): T = head(1).head first123456789/** * Returns the first row. Alias for head(). * * 返回第一行 ，与head()一样 * * @group action * @since 1.6.0 */def first(): T = head() foreach12345678910111213141516171819202122/** * Applies a function `f` to all rows. * * 对所有行应用函数f。 * * @group action * @since 1.6.0 */def foreach(f: T =&gt; Unit): Unit = withNewExecutionId &#123; rdd.foreach(f)&#125;/** * (Java-specific) * Runs `func` on each element of this Dataset. * * 在这个数据集的每个元素上运行“func”。 * * @group action * @since 1.6.0 */def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_)) foreachPartition1234567891011121314151617181920212223/** * Applies a function `f` to each partition of this Dataset. * * 对这个数据集的每个分区应用一个函数f。 * * @group action * @since 1.6.0 */def foreachPartition(f: Iterator[T] =&gt; Unit): Unit = withNewExecutionId &#123; rdd.foreachPartition(f)&#125;/** * (Java-specific) * Runs `func` on each partition of this Dataset. * * 对这个数据集的每个分区应用一个函数f。 * * @group action * @since 1.6.0 */def foreachPartition(func: ForeachPartitionFunction[T]): Unit = foreachPartition(it =&gt; func.call(it.asJava)) take123456789101112131415/** * Returns the first `n` rows in the Dataset. * * 返回数据集中的前“n”行。 * 同head(n) * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * take在driver端执行，n太大会造成oom * * @group action * @since 1.6.0 */def take(n: Int): Array[T] = head(n) takeAsList1234567891011121314/** * Returns the first `n` rows in the Dataset as a list. * * 以List形式返回 前n行 * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * take在driver端执行，n太大会造成oom * * @group action * @since 1.6.0 */def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n): _*) collect12345678910111213141516/** * Returns an array that contains all of [[Row]]s in this Dataset. * * 返回包含所有Row的 一个数组 * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * 会将所有数据移动到driver，所以可能会造成oom * * For Java API, use [[collectAsList]]. * * @group action * @since 1.6.0 */def collect(): Array[T] = collect(needCallback = true) collectAsList12345678910111213141516171819/** * Returns a Java list that contains all of [[Row]]s in this Dataset. * * 返回包含所有Row的一个Java List * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * 会将所有数据移动到driver，所以可能会造成oom * * @group action * @since 1.6.0 */def collectAsList(): java.util.List[T] = withCallback(\"collectAsList\", toDF()) &#123; _ =&gt; withNewExecutionId &#123; val values = queryExecution.executedPlan.executeCollect().map(boundEnc.fromRow) java.util.Arrays.asList(values: _*) &#125;&#125; toLocalIterator12345678910111213141516171819202122/** * Return an iterator that contains all of [[Row]]s in this Dataset. * * 返回包含所有Row的一个迭代器 * * The iterator will consume as much memory as the largest partition in this Dataset. * * 迭代器将消耗与此数据集中最大的分区一样多的内存。 * * @note this results in multiple Spark jobs, and if the input Dataset is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input Dataset should be cached first. * 这将导致多个Spark作业，如果输入数据集是宽依赖转换的结果(例如，与不同的分区连接)， * 那么为了避免重新计算输入数据，应该首先缓存输入数据集。 * @group action * @since 2.0.0 */def toLocalIterator(): java.util.Iterator[T] = withCallback(\"toLocalIterator\", toDF()) &#123; _ =&gt; withNewExecutionId &#123; queryExecution.executedPlan.executeToIterator().map(boundEnc.fromRow).asJava &#125;&#125; count1234567891011/** * Returns the number of rows in the Dataset. * * 返回总行数 * * @group action * @since 1.6.0 */def count(): Long = withCallback(\"count\", groupBy().count()) &#123; df =&gt; df.collect(needCallback = false).head.getLong(0)&#125; untypedrel-无类型转换na123456789101112/** * Returns a [[DataFrameNaFunctions]] for working with missing data. * 返回一个用于处理丢失数据的[[DataFrameNaFunctions]]。 * &#123;&#123;&#123; * // Dropping rows containing any null values. 删除包含任何null 值的行 * ds.na.drop() * &#125;&#125;&#125; * * @group untypedrel * @since 1.6.0 */def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF()) stat123456789101112/** * Returns a [[DataFrameStatFunctions]] for working statistic functions support. * 返回用于支持统计功能的[[DataFrameStatFunctions]]。 * &#123;&#123;&#123; * // Finding frequent items in column with name 'a'. 查询列名为\"a\"中的频繁数据。 * ds.stat.freqItems(Seq(\"a\")) * &#125;&#125;&#125; * * @group untypedrel * @since 1.6.0 */def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF()) join123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180/** * Join with another `DataFrame`. * 和 另一个 `DataFrame` jion * * Behaves as an INNER JOIN and requires a subsequent join predicate. * 作为一个内部连接，并需要一个后续的连接谓词。 * * @param right Right side of the join operation. join操作的右侧 * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_]): DataFrame = withPlan &#123; Join(logicalPlan, right.logicalPlan, joinType = Inner, None) &#125; /** * Inner equi-join with another `DataFrame` using the given column. * 给定列名的内部等值连接 * * Different from other join functions, the join column will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * &#123;&#123;&#123; * // Joining df1 and df2 using the column \"user_id\" 用\"user_id\" 连接 df1 和df2 * df1.join(df2, \"user_id\") * &#125;&#125;&#125; * * @param right Right side of the join operation. join连接右侧 * @param usingColumn Name of the column to join on. This column must exist on both sides. * 列名。必须在两边都存在 * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * 自连接的时候，请指定 表别名。不然干不了事 * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumn: String): DataFrame = &#123; join(right, Seq(usingColumn)) &#125; /** * Inner equi-join with another `DataFrame` using the given columns. * 根据指定多个列进行join * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * &#123;&#123;&#123; * // Joining df1 and df2 using the columns \"user_id\" and \"user_name\" * df1.join(df2, Seq(\"user_id\", \"user_name\")) * &#125;&#125;&#125; * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = &#123; join(right, usingColumns, \"inner\") &#125; /** * Equi-join with another `DataFrame` using the given columns. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * @param joinType One of: `inner`, `outer`, `left_outer`, `right_outer`, `leftsemi`. * 连接类型：内连接，外连接，左外连接，右外连接，左内连接 * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = &#123; // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right // by creating a new instance for one of the branch. // 自连接的时候，为其中一个分支创建一个新实例来消除左vs右的歧义。 val joined = sparkSession.sessionState.executePlan( Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None)) .analyzed.asInstanceOf[Join] withPlan &#123; Join( joined.left, joined.right, UsingJoin(JoinType(joinType), usingColumns), None) &#125; &#125; /** * Inner join with another `DataFrame`, using the given join expression. * 用给定的表达式进行join * &#123;&#123;&#123; * // The following two are equivalent: * df1.join(df2, $\"df1Key\" === $\"df2Key\") * df1.join(df2).where($\"df1Key\" === $\"df2Key\") * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, \"inner\") /** * Join with another `DataFrame`, using the given join expression. The following performs * a full outer join between `df1` and `df2`. * * &#123;&#123;&#123; * // Scala: * import org.apache.spark.sql.functions._ * df1.join(df2, $\"df1Key\" === $\"df2Key\", \"outer\") * * // Java: * import static org.apache.spark.sql.functions.*; * df1.join(df2, col(\"df1Key\").equalTo(col(\"df2Key\")), \"outer\"); * &#125;&#125;&#125; * * @param right Right side of the join. * @param joinExprs Join expression. * @param joinType One of: `inner`, `outer`, `left_outer`, `right_outer`, `leftsemi`. * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = &#123; // Note that in this function, we introduce a hack in the case of self-join to automatically // resolve ambiguous join conditions into ones that might make sense [SPARK-6231]. // Consider this case: df.join(df, df(\"key\") === df(\"key\")) // Since df(\"key\") === df(\"key\") is a trivially true condition, this actually becomes a // cartesian join. However, most likely users expect to perform a self join using \"key\". // With that assumption, this hack turns the trivially true condition into equality on join // keys that are resolved to both sides. // Trigger analysis so in the case of self-join, the analyzer will clone the plan. // After the cloning, left and right side will have distinct expression ids. // 针对自连接的优化：正常情况下，自连接如果使用 df.join(df, df(\"key\") === df(\"key\")) // 会造成 笛卡尔积 // 这种情况下，分析器会 克隆计划，克隆完成后，左右两边则有不同的 id val plan = withPlan( Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr))) .queryExecution.analyzed.asInstanceOf[Join] // If auto self join alias is disabled, return the plan. if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) &#123; return withPlan(plan) &#125; // If left/right have no output set intersection, return the plan. val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) &#123; return withPlan(plan) &#125; // Otherwise, find the trivially true predicates and automatically resolves them to both sides. // By the time we get here, since we have already run analysis, all attributes should've been // resolved and become AttributeReference. val cond = plan.condition.map &#123; _.transform &#123; case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference) if a.sameRef(b) =&gt; catalyst.expressions.EqualTo( withPlan(plan.left).resolve(a.name), withPlan(plan.right).resolve(b.name)) &#125; &#125; withPlan &#123; plan.copy(condition = cond) &#125; &#125; crossJoin12345678910111213/** * Explicit cartesian join with another `DataFrame`. * 显式笛卡尔积join * * @param right Right side of the join operation. * @note Cartesian joins are very expensive without an extra filter that can be pushed down. * 如果没有额外的过滤器，笛卡尔连接非常昂贵。 * @group untypedrel * @since 2.1.0 */def crossJoin(right: Dataset[_]): DataFrame = withPlan &#123; Join(logicalPlan, right.logicalPlan, joinType = Cross, None)&#125; apply123456789101112/** * Selects column based on the column name and return it as a [[Column]]. * * 选择基于列名的列，并将其作为[[Column]]返回。 * * @note The column name can also reference to a nested column like `a.b`. * * 列名也可以引用像“a.b”这样的嵌套列。 * @group untypedrel * @since 2.0.0 */def apply(colName: String): Column = col(colName) col123456789101112131415161718/** * Selects column based on the column name and return it as a [[Column]]. * * 选择基于列名的列，并将其作为[[Column]]返回。 * * @note The column name can also reference to a nested column like `a.b`. * * 列名也可以引用像“a.b”这样的嵌套列。 * @group untypedrel * @since 2.0.0 */def col(colName: String): Column = colName match &#123; case \"*\" =&gt; Column(ResolvedStar(queryExecution.analyzed.output)) case _ =&gt; val expr = resolve(colName) Column(expr)&#125; select12345678910111213141516171819202122232425262728293031/** * Selects a set of column based expressions. * &#123;&#123;&#123; * ds.select($\"colA\", $\"colB\" + 1) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */@scala.annotation.varargsdef select(cols: Column*): DataFrame = withPlan &#123; Project(cols.map(_.named), logicalPlan)&#125;/** * Selects a set of columns. This is a variant of `select` that can only select * existing columns using column names (i.e. cannot construct expressions). * * 只能是已经存在的列名 * * &#123;&#123;&#123; * // The following two are equivalent: * ds.select(\"colA\", \"colB\") * ds.select($\"colA\", $\"colB\") * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */@scala.annotation.varargsdef select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)): _*) selectExpr12345678910111213141516171819202122/** * Selects a set of SQL expressions. This is a variant of `select` that accepts * SQL expressions. * * 接受SQL表达式 * * &#123;&#123;&#123; * // The following are equivalent: * 以下是等价的: * ds.selectExpr(\"colA\", \"colB as newName\", \"abs(colC)\") * ds.select(expr(\"colA\"), expr(\"colB as newName\"), expr(\"abs(colC)\")) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */@scala.annotation.varargsdef selectExpr(exprs: String*): DataFrame = &#123; select(exprs.map &#123; expr =&gt; Column(sparkSession.sessionState.sqlParser.parseExpression(expr)) &#125;: _*)&#125; groupBy123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * Groups the Dataset using the specified columns, so we can run aggregation on them. See * [[RelationalGroupedDataset]] for all the available aggregate functions. * * 使用指定的列对数据集进行分组，这样我们就可以对它们进行聚合。 * 查看[[RelationalGroupedDataset]]为所有可用的聚合函数。 * * * &#123;&#123;&#123; * // Compute the average for all numeric columns grouped by department. * * 计算按部门分组的所有数字列的平均值。 * * ds.groupBy($\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -&gt; \"avg\", * \"age\" -&gt; \"max\" * )) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */@scala.annotation.varargsdef groupBy(cols: Column*): RelationalGroupedDataset = &#123; RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)&#125; /** * Groups the Dataset using the specified columns, so that we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of groupBy that can only group by existing columns using column names * (i.e. cannot construct expressions). * * &#123;&#123;&#123; * // Compute the average for all numeric columns grouped by department. * ds.groupBy(\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -&gt; \"avg\", * \"age\" -&gt; \"max\" * )) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */@scala.annotation.varargsdef groupBy(col1: String, cols: String*): RelationalGroupedDataset = &#123; val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.GroupByType)&#125; rollup1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * 使用指定的列为当前数据集创建多维的汇总，因此我们可以在它们上运行聚合。 * * * &#123;&#123;&#123; * // Compute the average for all numeric columns rolluped by department and group. * * 汇总后 求平均值 * * ds.rollup($\"department\", $\"group\").avg() * * // Compute the max age and average salary, rolluped by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -&gt; \"avg\", * \"age\" -&gt; \"max\" * )) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def rollup(cols: Column*): RelationalGroupedDataset = &#123; RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType) &#125; /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * 使用指定的列为当前数据集创建多维的rollup，因此我们可以在它们上运行聚合。 * rollup可以实现 从右到左一次递减的多级统计，显示统计某一层次结构的聚合 * 例如 rollup(a,b,c,d) =结果=&gt; (a,b,c,d),(a,b,c),(a,b),a * * This is a variant of rollup that can only group by existing columns using column names * (i.e. cannot construct expressions). * * &#123;&#123;&#123; * // Compute the average for all numeric columns rolluped by department and group. * ds.rollup(\"department\", \"group\").avg() * * // Compute the max age and average salary, rolluped by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -&gt; \"avg\", * \"age\" -&gt; \"max\" * )) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def rollup(col1: String, cols: String*): RelationalGroupedDataset = &#123; val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.RollupType) &#125; cube123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * 使用指定的列为当前数据集创建多维数据集，因此我们可以在它们上运行聚合。 * * * &#123;&#123;&#123; * // Compute the average for all numeric columns cubed by department and group. * ds.cube($\"department\", $\"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -&gt; \"avg\", * \"age\" -&gt; \"max\" * )) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def cube(cols: Column*): RelationalGroupedDataset = &#123; RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType) &#125; /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * 魔方 例如：cube(a,b,c) =结果=&gt; (a,b),(a,c),a,(b,c),b,c 结果为所有的维度 * 使用指定的列为当前数据集创建多维多维数据集，因此我们可以在它们上运行聚合。 * * This is a variant of cube that can only group by existing columns using column names * (i.e. cannot construct expressions). * * 这是一个多维数据集的变体，它只能通过使用列名的现有列来分组 * * &#123;&#123;&#123; * // Compute the average for all numeric columns cubed by department and group. * ds.cube(\"department\", \"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -&gt; \"avg\", * \"age\" -&gt; \"max\" * )) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def cube(col1: String, cols: String*): RelationalGroupedDataset = &#123; val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.CubeType) &#125; agg123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * (Scala-specific) Aggregates on the entire Dataset without groups. * 对整个数据集进行聚合，无需分组。 * &#123;&#123;&#123; * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(\"age\" -&gt; \"max\", \"salary\" -&gt; \"avg\") * ds.groupBy().agg(\"age\" -&gt; \"max\", \"salary\" -&gt; \"avg\") * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = &#123; groupBy().agg(aggExpr, aggExprs: _*) &#125; /** * (Scala-specific) Aggregates on the entire Dataset without groups. * 对整个数据集进行聚合，无需分组。 * * &#123;&#123;&#123; * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -&gt; \"max\", \"salary\" -&gt; \"avg\")) * ds.groupBy().agg(Map(\"age\" -&gt; \"max\", \"salary\" -&gt; \"avg\")) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs) /** * (Java-specific) Aggregates on the entire Dataset without groups. * * 对整个数据集进行聚合，无需分组。 * * &#123;&#123;&#123; * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -&gt; \"max\", \"salary\" -&gt; \"avg\")) * ds.groupBy().agg(Map(\"age\" -&gt; \"max\", \"salary\" -&gt; \"avg\")) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs) /** * Aggregates on the entire Dataset without groups. * * 对整个数据集进行聚合，无需分组。 * * &#123;&#123;&#123; * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(max($\"age\"), avg($\"salary\")) * ds.groupBy().agg(max($\"age\"), avg($\"salary\")) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs: _*) explode12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091/** * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of * the input row are implicitly joined with each row that is output by the function. * * 根据提供的方法，该数据集的每一行都被扩展为零个或更多的行，返回一个新的数据集。 * 这类似于HiveQL的“LATERAL VIEW”。 * 输入行的列 隐式地加入了由函数输出的每一行。 * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count * the number of books that contain a given word: * * 考虑到这已经被弃用，作为替代，您可以使用“functions.explode()”或“flatMap()”来引爆列。 * 下面的示例使用这些替代方法来计算包含给定单词的图书的数量: * * &#123;&#123;&#123; * case class Book(title: String, words: String) * val ds: Dataset[Book] * * val allWords = ds.select('title, explode(split('words, \" \")).as(\"word\")) * * val bookCountPerWord = allWords.groupBy(\"word\").agg(countDistinct(\"title\")) * &#125;&#125;&#125; * * Using `flatMap()` this can similarly be exploded as: * * &#123;&#123;&#123; * ds.flatMap(_.words.split(\" \")) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 已经过时，用 flatMap() 或 functions.explode() 代替 */@deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\")def explode[A &lt;: Product : TypeTag](input: Column*)(f: Row =&gt; TraversableOnce[A]): DataFrame = &#123; val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType] val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema) val rowFunction = f.andThen(_.map(convert(_).asInstanceOf[InternalRow])) val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr)) withPlan &#123; Generate(generator, join = true, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) &#125;&#125;/** * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All * columns of the input row are implicitly joined with each value that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()`: * * &#123;&#123;&#123; * ds.select(explode(split('words, \" \")).as(\"word\")) * &#125;&#125;&#125; * * or `flatMap()`: * * &#123;&#123;&#123; * ds.flatMap(_.words.split(\" \")) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */@deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\")def explode[A, B: TypeTag](inputColumn: String, outputColumn: String)(f: A =&gt; TraversableOnce[B]): DataFrame = &#123; val dataType = ScalaReflection.schemaFor[B].dataType val attributes = AttributeReference(outputColumn, dataType)() :: Nil // TODO handle the metadata? val elementSchema = attributes.toStructType def rowFunction(row: Row): TraversableOnce[InternalRow] = &#123; val convert = CatalystTypeConverters.createToCatalystConverter(dataType) f(row(0).asInstanceOf[A]).map(o =&gt; InternalRow(convert(o))) &#125; val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil) withPlan &#123; Generate(generator, join = true, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) &#125;&#125; withColumn1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * Returns a new Dataset by adding a column or replacing the existing column that has * the same name. * 通过添加一个列或替换具有相同名称的现有列返回新的数据集。 * * @group untypedrel * @since 2.0.0 */def withColumn(colName: String, col: Column): DataFrame = &#123; val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val shouldReplace = output.exists(f =&gt; resolver(f.name, colName)) if (shouldReplace) &#123; val columns = output.map &#123; field =&gt; if (resolver(field.name, colName)) &#123; col.as(colName) &#125; else &#123; Column(field) &#125; &#125; select(columns: _*) &#125; else &#123; select(Column(\"*\"), col.as(colName)) &#125;&#125;/** * Returns a new Dataset by adding a column with metadata. * 通过添加带有元数据的列返回一个新的数据集。 */private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame = &#123; val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val shouldReplace = output.exists(f =&gt; resolver(f.name, colName)) if (shouldReplace) &#123; val columns = output.map &#123; field =&gt; if (resolver(field.name, colName)) &#123; col.as(colName, metadata) &#125; else &#123; Column(field) &#125; &#125; select(columns: _*) &#125; else &#123; select(Column(\"*\"), col.as(colName, metadata)) &#125;&#125; withColumnRenamed1234567891011121314151617181920212223242526/** * Returns a new Dataset with a column renamed. * This is a no-op if schema doesn't contain existingName. * 返回一个重命名的列的新数据集。 * 如果模式不包含存在名称，那么这是不操作的。 * * @group untypedrel * @since 2.0.0 */def withColumnRenamed(existingName: String, newName: String): DataFrame = &#123; val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val shouldRename = output.exists(f =&gt; resolver(f.name, existingName)) if (shouldRename) &#123; val columns = output.map &#123; col =&gt; if (resolver(col.name, existingName)) &#123; Column(col).as(newName) &#125; else &#123; Column(col) &#125; &#125; select(columns: _*) &#125; else &#123; toDF() &#125;&#125; drop1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain * column name. * * 返回删除指定列之后的新Dataset * * This method can only be used to drop top level columns. the colName string is treated * literally without further interpretation. * * 仅用于删除顶层的列 * * @group untypedrel * @since 2.0.0 */def drop(colName: String): DataFrame = &#123; drop(Seq(colName): _*)&#125;/** * Returns a new Dataset with columns dropped. * This is a no-op if schema doesn't contain column name(s). * * 删除指定的多个列，并返回新的dataset * * This method can only be used to drop top level columns. the colName string is treated literally * without further interpretation. * * @group untypedrel * @since 2.0.0 */@scala.annotation.varargsdef drop(colNames: String*): DataFrame = &#123; val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output val remainingCols = allColumns.filter &#123; attribute =&gt; colNames.forall(n =&gt; !resolver(attribute.name, n)) &#125;.map(attribute =&gt; Column(attribute)) if (remainingCols.size == allColumns.size) &#123; toDF() &#125; else &#123; this.select(remainingCols: _*) &#125;&#125;/** * Returns a new Dataset with a column dropped. * This version of drop accepts a [[Column]] rather than a name. * This is a no-op if the Dataset doesn't have a column * with an equivalent expression. * * 删除指定的 列（根据Column） * * @group untypedrel * @since 2.0.0 */def drop(col: Column): DataFrame = &#123; val expression = col match &#123; case Column(u: UnresolvedAttribute) =&gt; queryExecution.analyzed.resolveQuoted( u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u) case Column(expr: Expression) =&gt; expr &#125; val attrs = this.logicalPlan.output val colsAfterDrop = attrs.filter &#123; attr =&gt; attr != expression &#125;.map(attr =&gt; Column(attr)) select(colsAfterDrop: _*)&#125; typedrel-有类型的转换joinWith123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108/** * :: Experimental :: 实验的 * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to * true. * 连接这个数据集返回一个“Tuple2”对每一对的“条件”计算为true。 * * This is similar to the relation `join` function with one important difference in the * result schema. Since `joinWith` preserves objects present on either side of the join, the * result schema is similarly nested into a tuple under the column names `_1` and `_2`. * 这类似于关系“join”函数，在结果模式中有一个重要的区别。 * 由于“joinWith”保存了连接的任何一边的对象，因此结果模式类似地嵌套在列名称“_1”和“_2”下面的tuple中。 * * This type of join can be useful both for preserving type-safety with the original object * types as well as working with relational data where either side of the join has column * names in common. * 这种类型的联接既可以用于保存与原始对象类型的类型安全性， * 也可以用于处理连接的任何一端都有列名的关系数据。 * * @param other Right side of the join. * @param condition Join expression. * @param joinType One of: `inner`, `outer`, `left_outer`, `right_outer`, `leftsemi`. * @group typedrel * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = &#123; // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved, // 创建一个联接节点并首先解析它，使Join条件得到解析，self - Join解析， // etc. val joined = sparkSession.sessionState.executePlan( Join( this.logicalPlan, other.logicalPlan, JoinType(joinType), Some(condition.expr))).analyzed.asInstanceOf[Join] // For both join side, combine all outputs into a single column and alias it with \"_1\" or \"_2\", // to match the schema for the encoder of the join result. // 对于这两个连接，将所有输出合并为一个列，并将其别名为“_1”或“_2”，以匹配连接结果的编码器的模式。 // Note that we do this before joining them, to enable the join operator to return null for one // side, in cases like outer-join. // 请注意，在join它们之前，我们这样做，使join操作符在像outer - join这样的情况下返回null。 val left = &#123; val combined = if (this.exprEnc.flat) &#123; assert(joined.left.output.length == 1) Alias(joined.left.output.head, \"_1\")() &#125; else &#123; Alias(CreateStruct(joined.left.output), \"_1\")() &#125; Project(combined :: Nil, joined.left) &#125; val right = &#123; val combined = if (other.exprEnc.flat) &#123; assert(joined.right.output.length == 1) Alias(joined.right.output.head, \"_2\")() &#125; else &#123; Alias(CreateStruct(joined.right.output), \"_2\")() &#125; Project(combined :: Nil, joined.right) &#125; // Rewrites the join condition to make the attribute point to correct column/field, after we // combine the outputs of each join side. // 在将每个连接的输出组合在一起之后,重写联接条件，使属性指向正确的列/字段。 val conditionExpr = joined.condition.get transformUp &#123; case a: Attribute if joined.left.outputSet.contains(a) =&gt; if (this.exprEnc.flat) &#123; left.output.head &#125; else &#123; val index = joined.left.output.indexWhere(_.exprId == a.exprId) GetStructField(left.output.head, index) &#125; case a: Attribute if joined.right.outputSet.contains(a) =&gt; if (other.exprEnc.flat) &#123; right.output.head &#125; else &#123; val index = joined.right.output.indexWhere(_.exprId == a.exprId) GetStructField(right.output.head, index) &#125; &#125; implicit val tuple2Encoder: Encoder[(T, U)] = ExpressionEncoder.tuple(this.exprEnc, other.exprEnc) withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))&#125;/** * :: Experimental :: * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair * where `condition` evaluates to true. * * 使用内部的等连接加入这个数据集，为每一对返回一个“Tuple2”，其中“条件”的计算结果为true。 * * @param other Right side of the join. * @param condition Join expression. * @group typedrel * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = &#123; joinWith(other, condition, \"inner\")&#125; sortWithinPartitions123456789101112131415161718192021222324252627282930313233/** * Returns a new Dataset with each partition sorted by the given expressions. * * 返回一个新的数据集，每个分区按照给定的表达式排序。 * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * 这与SQL(Hive QL)中“SORT BY”的操作相同。 * * @group typedrel * @since 2.0.0 */@scala.annotation.varargsdef sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = &#123; sortWithinPartitions((sortCol +: sortCols).map(Column(_)): _*)&#125;/** * Returns a new Dataset with each partition sorted by the given expressions. * * 返回一个新的数据集，每个分区按照给定的表达式排序。 * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * 这与SQL(Hive QL)中“SORT BY”的操作相同。 * * @group typedrel * @since 2.0.0 */@scala.annotation.varargsdef sortWithinPartitions(sortExprs: Column*): Dataset[T] = &#123; sortInternal(global = false, sortExprs)&#125; sort1234567891011121314151617181920212223242526272829303132333435/** * Returns a new Dataset sorted by the specified column, all in ascending order. * 排序 升序 * &#123;&#123;&#123; * // The following 3 are equivalent * 下面3个是等价的 * ds.sort(\"sortcol\") * ds.sort($\"sortcol\") * ds.sort($\"sortcol\".asc) * &#125;&#125;&#125; * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sort(sortCol: String, sortCols: String*): Dataset[T] = &#123; sort((sortCol +: sortCols).map(apply): _*) &#125; /** * Returns a new Dataset sorted by the given expressions. For example: * * 返回一个由给定表达式排序的新数据集。例如: * * &#123;&#123;&#123; * ds.sort($\"col1\", $\"col2\".desc) * &#125;&#125;&#125; * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sort(sortExprs: Column*): Dataset[T] = &#123; sortInternal(global = true, sortExprs) &#125; orderBy123456789101112131415161718192021/** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * 这是“sort”函数的别名。 * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols: _*) /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * 这是“sort”函数的别名。 * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs: _*) as12345678910111213141516171819/** * Returns a new Dataset with an alias set. * * 返回一个具有别名集的新数据集。 * * @group typedrel * @since 1.6.0 */ def as(alias: String): Dataset[T] = withTypedPlan &#123; SubqueryAlias(alias, logicalPlan, None) &#125; /** * (Scala-specific) Returns a new Dataset with an alias set. * * @group typedrel * @since 2.0.0 */ def as(alias: Symbol): Dataset[T] = as(alias.name) alias12345678910111213141516/** * Returns a new Dataset with an alias set. Same as `as`. * 返回一个具有别名集的新数据集。与“as”相同。 * * @group typedrel * @since 2.0.0 */ def alias(alias: String): Dataset[T] = as(alias) /** * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0 */ def alias(alias: Symbol): Dataset[T] = as(alias) select1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/** * :: Experimental :: * Returns a new Dataset by computing the given [[Column]] expression for each element. * * 通过计算每个元素的给定[[列]]表达式返回一个新的数据集。 * * &#123;&#123;&#123; * val ds = Seq(1, 2, 3).toDS() * val newDS = ds.select(expr(\"value + 1\").as[Int]) * &#125;&#125;&#125; * * @group typedrel * @since 1.6.0 */ @Experimental @InterfaceStability.Evolving def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = &#123; implicit val encoder = c1.encoder val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan) if (encoder.flat) &#123; new Dataset[U1](sparkSession, project, encoder) &#125; else &#123; // Flattens inner fields of U1 // 使U1的内部区域变平 new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1) &#125; &#125; /** * :: Experimental :: * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ @Experimental @InterfaceStability.Evolving def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] = selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]] /** * :: Experimental :: * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ @Experimental @InterfaceStability.Evolving def select[U1, U2, U3]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] = selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]] /** * :: Experimental :: * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ @Experimental @InterfaceStability.Evolving def select[U1, U2, U3, U4]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] = selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]] /** * :: Experimental :: * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ @Experimental @InterfaceStability.Evolving def select[U1, U2, U3, U4, U5]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4], c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] = selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]] filter12345678910111213141516171819202122232425262728293031323334/** * Filters rows using the given condition. * * 用给定的条件过滤rows * * &#123;&#123;&#123; * // The following are equivalent: * 以下是等价的： * peopleDs.filter($\"age\" &gt; 15) * peopleDs.where($\"age\" &gt; 15) * &#125;&#125;&#125; * * @group typedrel * @since 1.6.0 */ def filter(condition: Column): Dataset[T] = withTypedPlan &#123; Filter(condition.expr, logicalPlan) &#125; /** * Filters rows using the given SQL expression. * * 用给定的 SQL 表达式 过滤rows * * &#123;&#123;&#123; * peopleDs.filter(\"age &gt; 15\") * &#125;&#125;&#125; * * @group typedrel * @since 1.6.0 */ def filter(conditionExpr: String): Dataset[T] = &#123; filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) &#125; where1234567891011121314151617181920212223242526272829303132/** * Filters rows using the given condition. This is an alias for `filter`. * * 使用给定条件过滤行。 * 这是“filter”的别名。 * * &#123;&#123;&#123; * // The following are equivalent: * peopleDs.filter($\"age\" &gt; 15) * peopleDs.where($\"age\" &gt; 15) * &#125;&#125;&#125; * * @group typedrel * @since 1.6.0 */ def where(condition: Column): Dataset[T] = filter(condition) /** * Filters rows using the given SQL expression. * * 使用给定的 SQL 表达式 过滤 rows * * &#123;&#123;&#123; * peopleDs.where(\"age &gt; 15\") * &#125;&#125;&#125; * * @group typedrel * @since 1.6.0 */ def where(conditionExpr: String): Dataset[T] = &#123; filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) &#125; groupByKey12345678910111213141516171819202122232425262728293031323334353637/** * :: Experimental :: * (Scala-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * 返回一个[[KeyValueGroupedDataset]]，数据由给定键' func '分组。 * * @group typedrel * @since 2.0.0 */@Experimental@InterfaceStability.Evolvingdef groupByKey[K: Encoder](func: T =&gt; K): KeyValueGroupedDataset[K, T] = &#123; val inputPlan = logicalPlan val withGroupingKey = AppendColumns(func, inputPlan) val executed = sparkSession.sessionState.executePlan(withGroupingKey) new KeyValueGroupedDataset( encoderFor[K], encoderFor[T], executed, inputPlan.output, withGroupingKey.newColumns)&#125;/** * :: Experimental :: * (Java-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * 返回一个[[KeyValueGroupedDataset]]，数据由给定键' func '分组。 * * @group typedrel * @since 2.0.0 */@Experimental@InterfaceStability.Evolvingdef groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] = groupByKey(func.call(_))(encoder) limit123456789101112131415/** * Returns a new Dataset by taking the first `n` rows. The difference between this function * and `head` is that `head` is an action and returns an array (by triggering query execution) * while `limit` returns a new Dataset. * * 通过使用第一个“n”行返回一个新的数据集。 * 这个函数和“head”的区别在于“head”是一个动作， * 并返回一个数组(通过触发查询执行)，而“limit”则返回一个新的数据集。 * * @group typedrel * @since 2.0.0 */def limit(n: Int): Dataset[T] = withTypedPlan &#123; Limit(Literal(n), logicalPlan)&#125; unionAll-已过时1234567891011121314151617/** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * This is equivalent to `UNION ALL` in SQL. * * 返回一个新的数据集，该数据集包含该数据集中的行和另一个数据集。 * 这相当于SQL中的“UNION ALL”。 * * To do a SQL-style set union (that does deduplication of elements), use this function followed * by a [[distinct]]. * * 如果需要去重的话，在该方法后继续直接 [[distinct]] * * @group typedrel * @since 2.0.0 已经过时 */@deprecated(\"use union()\", \"2.0.0\")def unionAll(other: Dataset[T]): Dataset[T] = union(other) union123456789101112131415161718192021/** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * This is equivalent to `UNION ALL` in SQL. * * 返回一个新的数据集，该数据集包含该数据集中的行和另一个数据集。 * 这相当于SQL中的“UNION ALL”。 * * To do a SQL-style set union (that does deduplication of elements), use this function followed * by a [[distinct]]. * * 如果需要去重的话，在该方法后继续直接 [[distinct]] * * @group typedrel * @since 2.0.0 */def union(other: Dataset[T]): Dataset[T] = withSetOperator &#123; // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. // 这打破了缓存，但通常是可以的，因为它解决了一个非常具体的用例:使用union来联合许多文件或分区。 CombineUnions(Union(logicalPlan, other.logicalPlan))&#125; intersect-交集123456789101112131415161718/** * Returns a new Dataset containing rows only in both this Dataset and another Dataset. * This is equivalent to `INTERSECT` in SQL. * * 返回一个新的数据集，只包含该数据集和另一个数据集相同的行. * 这相当于在SQL中“INTERSECT”。 * 会去重. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * 等式检查直接执行数据的编码表示，因此不受定义为“T”的自定义“equals”函数的影响。 * @group typedrel * @since 1.6.0 */def intersect(other: Dataset[T]): Dataset[T] = withSetOperator &#123; Intersect(logicalPlan, other.logicalPlan)&#125; except-只显示另个Dataset中没有的值12345678910111213141516/** * Returns a new Dataset containing rows in this Dataset but not in another Dataset. * This is equivalent to `EXCEPT` in SQL. * * 返回一个新的数据集，该数据集包含该数据集中的行，而不是在另一个数据集。 * 这等价于SQL中的“EXCEPT”。 * 会去重. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * @group typedrel * @since 2.0.0 */def except(other: Dataset[T]): Dataset[T] = withSetOperator &#123; Except(logicalPlan, other.logicalPlan)&#125; sample-随机抽样1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed. * * 通过使用用户提供的种子，通过抽样的方式返回一个新的[[Dataset]]。 * * @param withReplacement Sample with replacement or not. * 样本已经取过的值是否放回 * @param fraction Fraction of rows to generate. * 每一行数据被取样的概率 * @param seed Seed for sampling. * 取样种子（与随机数生成有关） * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * 不能保证准确的按照给定的分数取样。（一般结果会在概率值*总数左右） * @group typedrel * @since 1.6.0 */def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = &#123; require(fraction &gt;= 0, s\"Fraction must be nonnegative, but got $&#123;fraction&#125;\") withTypedPlan &#123; Sample(0.0, fraction, withReplacement, seed, logicalPlan)() &#125;&#125;/** * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed. * * 通过程序随机的种子，抽样返回新的DataSet * * @param withReplacement Sample with replacement or not. * 取样结果是否放回 * @param fraction Fraction of rows to generate. * 每行数据被取样的概率 * @note This is NOT guaranteed to provide exactly the fraction of the total count * of the given [[Dataset]]. * 不能保证准确的按照给定的分数取样。（一般结果会在概率值*总数左右） * @group typedrel * @since 1.6.0 */def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = &#123; sample(withReplacement, fraction, Utils.random.nextLong)&#125; randomSplit-按照权重分割1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Randomly splits this Dataset with the provided weights. * * 随机将此数据集按照所提供的权重进行分割。 * * @param weights weights for splits, will be normalized if they don't sum to 1. * 切分的权重。如果和不为1就会被标准化。 * @param seed Seed for sampling. * 取样的种子（影响随机数生成器） * * For Java API, use [[randomSplitAsList]]. * Java API 使用 [[randomSplitAsList]]. * @group typedrel * @since 2.0.0 */ def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = &#123; require(weights.forall(_ &gt;= 0), s\"Weights must be nonnegative, but got $&#123;weights.mkString(\"[\", \",\", \"]\")&#125;\") require(weights.sum &gt; 0, s\"Sum of weights must be positive, but got $&#123;weights.mkString(\"[\", \",\", \"]\")&#125;\") // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its // constituent partitions each time a split is materialized which could result in // overlapping splits. To prevent this, we explicitly sort each input partition to make the // ordering deterministic. // MapType cannot be sorted. val sorted = Sort(logicalPlan.output.filterNot(_.dataType.isInstanceOf[MapType]) .map(SortOrder(_, Ascending)), global = false, logicalPlan) val sum = weights.sum // scanLeft 从右到右依次累计算 scanLeft(0.0d)(_+_): (0.0,(0.0+0.2),(0.0+0.2+0.8)) val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) // sliding(n) 每次取n个值，以步长为1向右滑动，如：(0.0,0.2,0.8).sliding(2)=(0.0,0.2),(0.2,0.8) normalizedCumWeights.sliding(2).map &#123; x =&gt; new Dataset[T]( sparkSession, Sample(x(0), x(1), withReplacement = false, seed, sorted)(), encoder) &#125;.toArray &#125; /** * Randomly splits this Dataset with the provided weights. * * 程序自动生成随机数种子，随机将此数据集按照所提供的权重进行分割。 * * @param weights weights for splits, will be normalized if they don't sum to 1. * 切分的权重。如果和不为1就会被标准化。 * @group typedrel * @since 2.0.0 */ def randomSplit(weights: Array[Double]): Array[Dataset[T]] = &#123; randomSplit(weights, Utils.random.nextLong) &#125; /** * Randomly splits this Dataset with the provided weights. Provided for the Python Api. * Python 使用该方法 * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. */ private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = &#123; randomSplit(weights.toArray, seed) &#125; randomSplitAsList12345678910111213141516/** * Returns a Java list that contains randomly split Dataset with the provided weights. * * 根据提供的权重分割DataFrames，返回Java list * * @param weights weights for splits, will be normalized if they don't sum to 1. * 切分的权重。如果和不为1就会被标准化。 * @param seed Seed for sampling. * 取样的种子（影响随机数生成器） * @group typedrel * @since 2.0.0 */def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = &#123; val values = randomSplit(weights, seed) java.util.Arrays.asList(values: _*)&#125; dropDuplicates-去重123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `distinct`. * * 删除重复的row数据，是distinct的别名 * * @group typedrel * @since 2.0.0 */def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)/** * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * 只删除指定列的重复数据 * * @group typedrel * @since 2.0.0 */def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan &#123; val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output val groupCols = colNames.flatMap &#123; colName =&gt; // It is possibly there are more than one columns with the same name, // so we call filter instead of find. val cols = allColumns.filter(col =&gt; resolver(col.name, colName)) if (cols.isEmpty) &#123; throw new AnalysisException( s\"\"\"Cannot resolve column name \"$colName\" among ($&#123;schema.fieldNames.mkString(\", \")&#125;)\"\"\") &#125; cols &#125; val groupColExprIds = groupCols.map(_.exprId) val aggCols = logicalPlan.output.map &#123; attr =&gt; if (groupColExprIds.contains(attr.exprId)) &#123; attr &#125; else &#123; // Removing duplicate rows should not change output attributes. We should keep // the original exprId of the attribute. Otherwise, to select a column in original // dataset will cause analysis exception due to unresolved attribute. // 删除重复行不应该更改输出属性。 // 我们应该保留这个属性的原始属性。 // 否则，在原始数据集中选择一个列将导致分析异常，原因是未解析的属性。 Alias(new First(attr).toAggregateExpression(), attr.name)(exprId = attr.exprId) &#125; &#125; Aggregate(groupCols, aggCols, logicalPlan)&#125;/** * Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * 只针对特定列做去重 * * @group typedrel * @since 2.0.0 */def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)/** * Returns a new [[Dataset]] with duplicate rows removed, considering only * the subset of columns. * * 只针对特定多列做去重 * * @group typedrel * @since 2.0.0 */@scala.annotation.varargsdef dropDuplicates(col1: String, cols: String*): Dataset[T] = &#123; val colNames: Seq[String] = col1 +: cols dropDuplicates(colNames)&#125; transform-自定义转换1234567891011121314151617/** * Concise syntax for chaining custom transformations. * * 用于链接自定义转换的简明语法。 * * &#123;&#123;&#123; * def featurize(ds: Dataset[T]): Dataset[U] = ... * * ds * .transform(featurize) * .transform(...) * &#125;&#125;&#125; * * @group typedrel * @since 1.6.0 */def transform[U](t: Dataset[T] =&gt; Dataset[U]): Dataset[U] = t(this) filter-过滤12345678910111213141516171819202122232425262728293031/** * :: Experimental :: * (Scala-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * 该数据集只包含“func”返回“true”的元素。 * * @group typedrel * @since 1.6.0 */ @Experimental @InterfaceStability.Evolving def filter(func: T =&gt; Boolean): Dataset[T] = &#123; withTypedPlan(TypedFilter(func, logicalPlan)) &#125; /** * :: Experimental :: * (Java-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * 返回一个新数据集，该数据集只包含“func”返回“true”的元素。 * * @group typedrel * @since 1.6.0 */ @Experimental @InterfaceStability.Evolving def filter(func: FilterFunction[T]): Dataset[T] = &#123; withTypedPlan(TypedFilter(func, logicalPlan)) &#125; map1234567891011121314151617181920212223242526272829303132/** * :: Experimental :: * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * 返回一个新的数据集，该数据集包含对每个元素应用“func”的结果。 * * @group typedrel * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef map[U: Encoder](func: T =&gt; U): Dataset[U] = withTypedPlan &#123; MapElements[T, U](func, logicalPlan)&#125;/** * :: Experimental :: * (Java-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * 返回一个新的数据集，该数据集包含对每个元素应用“func”的结果。 * * @group typedrel * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = &#123; implicit val uEnc = encoder withTypedPlan(MapElements[T, U](func, logicalPlan))&#125; mapPartitions1234567891011121314151617181920212223242526272829303132333435/** * :: Experimental :: * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each partition. * * 返回一个新的数据集，该数据集包含对每个分区应用“func”的结果。 * * @group typedrel * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef mapPartitions[U: Encoder](func: Iterator[T] =&gt; Iterator[U]): Dataset[U] = &#123; new Dataset[U]( sparkSession, MapPartitions[T, U](func, logicalPlan), implicitly[Encoder[U]])&#125;/** * :: Experimental :: * (Java-specific) * Returns a new Dataset that contains the result of applying `f` to each partition. * * 返回一个新的数据集，该数据集包含对每个分区应用“f”的结果。 * * @group typedrel * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = &#123; val func: (Iterator[T]) =&gt; Iterator[U] = x =&gt; f.call(x.asJava).asScala mapPartitions(func)(encoder)&#125; flatMap-将map结果flat扁平化123456789101112131415161718192021222324252627282930313233/** * :: Experimental :: * (Scala-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * 返回一个新的数据集，首先对该数据集的所有元素应用一个函数，然后将结果扁平化。 * * @group typedrel * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef flatMap[U: Encoder](func: T =&gt; TraversableOnce[U]): Dataset[U] = mapPartitions(_.flatMap(func))/** * :: Experimental :: * (Java-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * 返回一个新的数据集，首先对该数据集的所有元素应用一个函数，然后将结果扁平化。 * * @group typedrel * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = &#123; val func: (T) =&gt; Iterator[U] = x =&gt; f.call(x).asScala flatMap(func)(encoder)&#125; repartition-重分区123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * Returns a new Dataset that has exactly `numPartitions` partitions. * * 返回一个 给定分区数量的新DataSet * * @group typedrel * @since 1.6.0 */ def repartition(numPartitions: Int): Dataset[T] = withTypedPlan &#123; Repartition(numPartitions, shuffle = true, logicalPlan) &#125; /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is hash partitioned. * * 返回一个由给定的分区表达式划分为“num分区”的新数据集。 * 生成的Dataset是哈希分区的。 * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * 和 SQL (Hive QL) 中的 \"DISTRIBUTE BY\" 作用相同 * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = withTypedPlan &#123; RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, Some(numPartitions)) &#125; /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is hash partitioned. * * 根据指定的分区表达式进行重分区。 * 分区数量由`spark.sql.shuffle.partitions` 获得。 * 结果Dataset 是哈希分区的。 * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * 和 SQL (Hive QL) 中的 \"DISTRIBUTE BY\" 作用相同 * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def repartition(partitionExprs: Column*): Dataset[T] = withTypedPlan &#123; RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions = None) &#125; coalesce-合并分区1234567891011121314151617/** * Returns a new Dataset that has exactly `numPartitions` partitions. * Similar to coalesce defined on an `RDD`, this operation results in a narrow dependency, e.g. * if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of * the 100 new partitions will claim 10 of the current partitions. * * 合并。 * 返回确定分区数量的Dataset。 * 和RDD中的合并方法类似，这个操作导致了一个窄依赖。 * 例如：将1000个分区合并为100个分区，这个过程没有shuffle，而是100个新分区中的每个分区将声明当前的10个分区。 * * @group typedrel * @since 1.6.0 */ def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan &#123; Repartition(numPartitions, shuffle = false, logicalPlan) &#125; distinct-去重1234567891011121314/** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `dropDuplicates`. * * 去重。 * 返回去重后的Dataset。 * 和 `dropDuplicates` 方法一致。 * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * @group typedrel * @since 2.0.0 */ def distinct(): Dataset[T] = dropDuplicates()","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"},{"name":"源码","slug":"源码","permalink":"https://stanxia.github.io/tags/源码/"}]},{"title":"JavaRDDLike.scala","slug":"JavaRDDLike-scala","date":"2017-11-21T06:21:23.000Z","updated":"2017-12-05T03:32:48.000Z","comments":true,"path":"2017/11/21/JavaRDDLike-scala/","link":"","permalink":"https://stanxia.github.io/2017/11/21/JavaRDDLike-scala/","excerpt":"使用Java开发Spark程序，JavaRDD的功能算子中英文注释JavaRDDLike的实现应该扩展这个虚拟抽象类，而不是直接继承这个特性。\n\nJavaRDD1234567891011121314151617181920package org.apache.spark.api.javaprivate[spark] abstract class AbstractJavaRDDLike[T, This &lt;: JavaRDDLike[T, This]]  extends JavaRDDLike[T, This]/**  * Defines operations common to several Java RDD implementations.  *  * 定义几个Java RDD实现的常见操作。  *  * @note This trait is not intended to be implemented by user code.  *  *       该特性不打算由用户代码实现。  */trait JavaRDDLike[T, This &lt;: JavaRDDLike[T, This]] extends Serializable &#123;  def wrapRDD(rdd: RDD[T]): This  implicit val classTag: ClassTag[T]  def rdd: RDD[T]","text":"使用Java开发Spark程序，JavaRDD的功能算子中英文注释JavaRDDLike的实现应该扩展这个虚拟抽象类，而不是直接继承这个特性。 JavaRDD1234567891011121314151617181920package org.apache.spark.api.javaprivate[spark] abstract class AbstractJavaRDDLike[T, This &lt;: JavaRDDLike[T, This]] extends JavaRDDLike[T, This]/** * Defines operations common to several Java RDD implementations. * * 定义几个Java RDD实现的常见操作。 * * @note This trait is not intended to be implemented by user code. * * 该特性不打算由用户代码实现。 */trait JavaRDDLike[T, This &lt;: JavaRDDLike[T, This]] extends Serializable &#123; def wrapRDD(rdd: RDD[T]): This implicit val classTag: ClassTag[T] def rdd: RDD[T] partitions1234/** Set of partitions in this RDD. * 在这个RDD中设置的分区。 * */def partitions: JList[Partition] = rdd.partitions.toSeq.asJava getNumPartitions12345/** Return the number of partitions in this RDD. * 返回该RDD中的分区数。 * */@Since(\"1.6.0\")def getNumPartitions: Int = rdd.getNumPartitions partitioner1234/** The partitioner of this RDD. * 这个RDD的分区。 * */def partitioner: Optional[Partitioner] = JavaUtils.optionToOptional(rdd.partitioner) context12345/** The [[org.apache.spark.SparkContext]] that this RDD was created on. * * 这个RDD是在[[org.apache.spark.SparkContext]]上面创建的。 * */def context: SparkContext = rdd.context id1234/** A unique ID for this RDD (within its SparkContext). * 这个RDD的惟一ID(在它的SparkContext内)。 * */def id: Int = rdd.id name1def name(): String = rdd.name getStorageLevel1234/** Get the RDD's current storage level, or StorageLevel.NONE if none is set. * 获取RDD的当前存储级别，或StorageLevel。如果没有设置就没有。 * */def getStorageLevel: StorageLevel = rdd.getStorageLevel iterator12345678910/** * Internal method to this RDD; will read from cache if applicable, or otherwise compute it. * This should ''not'' be called by users directly, but is available for implementors of custom * subclasses of RDD. * 内部方法的RDD;将从缓存读取，如果适用的话，或者计算它。 * 这应该“不是”直接由用户调用，而是用于RDD的自定义子类的实现者 * */def iterator(split: Partition, taskContext: TaskContext): JIterator[T] = rdd.iterator(split, taskContext).asJavs Transformations (return a new RDD)map1234567/** * Return a new RDD by applying a function to all elements of this RDD. * 将一个函数应用于这个RDD的所有元素，返回一个新的RDD。 * */def map[R](f: JFunction[T, R]): JavaRDD[R] = new JavaRDD(rdd.map(f)(fakeClassTag))(fakeClassTag) mapPartitionsWithIndex1234567891011/** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * 通过在RDD的每个分区上应用一个函数来返回一个新的RDD，同时跟踪原始分区的索引。 * */def mapPartitionsWithIndex[R]( f: JFunction2[jl.Integer, JIterator[T], JIterator[R]], preservesPartitioning: Boolean = false): JavaRDD[R] = new JavaRDD(rdd.mapPartitionsWithIndex((a, b) =&gt; f.call(a, b.asJava).asScala, preservesPartitioning)(fakeClassTag))(fakeClassTag) mapToDouble1234567/** * Return a new RDD by applying a function to all elements of this RDD. * 将一个函数应用于这个RDD的所有元素，返回一个新的RDD。 */def mapToDouble[R](f: DoubleFunction[T]): JavaDoubleRDD = &#123; new JavaDoubleRDD(rdd.map(f.call(_).doubleValue()))&#125; mapToPair123456789/** * Return a new RDD by applying a function to all elements of this RDD. * 将一个函数应用于这个RDD的所有元素，返回一个新的RDD。 * */def mapToPair[K2, V2](f: PairFunction[T, K2, V2]): JavaPairRDD[K2, V2] = &#123; def cm: ClassTag[(K2, V2)] = implicitly[ClassTag[(K2, V2)]] new JavaPairRDD(rdd.map[(K2, V2)](f)(cm))(fakeClassTag[K2], fakeClassTag[V2])&#125; flatMap12345678910/** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. * 返回一个新的RDD，首先将一个函数应用于这个RDD的所有元素，然后将结果扁平化。 * */def flatMap[U](f: FlatMapFunction[T, U]): JavaRDD[U] = &#123; def fn: (T) =&gt; Iterator[U] = (x: T) =&gt; f.call(x).asScala JavaRDD.fromRDD(rdd.flatMap(fn)(fakeClassTag[U]))(fakeClassTag[U])&#125; flatMapToDouble12345678910/** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. * 返回一个新的RDD，首先将一个函数应用于这个RDD的所有元素，然后将结果扁平化。 * */def flatMapToDouble(f: DoubleFlatMapFunction[T]): JavaDoubleRDD = &#123; def fn: (T) =&gt; Iterator[jl.Double] = (x: T) =&gt; f.call(x).asScala new JavaDoubleRDD(rdd.flatMap(fn).map(_.doubleValue()))&#125; flatMapToPair1234567891011/** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. * 返回一个新的RDD，首先将一个函数应用于这个RDD的所有元素，然后将结果扁平化。 * */def flatMapToPair[K2, V2](f: PairFlatMapFunction[T, K2, V2]): JavaPairRDD[K2, V2] = &#123; def fn: (T) =&gt; Iterator[(K2, V2)] = (x: T) =&gt; f.call(x).asScala def cm: ClassTag[(K2, V2)] = implicitly[ClassTag[(K2, V2)]] JavaPairRDD.fromRDD(rdd.flatMap(fn)(cm))(fakeClassTag[K2], fakeClassTag[V2])&#125; mapPartitions12345678910111213141516171819202122232425/** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */def mapPartitions[U](f: FlatMapFunction[JIterator[T], U]): JavaRDD[U] = &#123; def fn: (Iterator[T]) =&gt; Iterator[U] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; JavaRDD.fromRDD(rdd.mapPartitions(fn)(fakeClassTag[U]))(fakeClassTag[U])&#125;/** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */def mapPartitions[U](f: FlatMapFunction[JIterator[T], U], preservesPartitioning: Boolean): JavaRDD[U] = &#123; def fn: (Iterator[T]) =&gt; Iterator[U] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; JavaRDD.fromRDD( rdd.mapPartitions(fn, preservesPartitioning)(fakeClassTag[U]))(fakeClassTag[U])&#125; mapPartitionsToDouble12345678910111213141516171819202122232425/** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */def mapPartitionsToDouble(f: DoubleFlatMapFunction[JIterator[T]]): JavaDoubleRDD = &#123; def fn: (Iterator[T]) =&gt; Iterator[jl.Double] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; new JavaDoubleRDD(rdd.mapPartitions(fn).map(_.doubleValue()))&#125;/** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */def mapPartitionsToDouble(f: DoubleFlatMapFunction[JIterator[T]], preservesPartitioning: Boolean): JavaDoubleRDD = &#123; def fn: (Iterator[T]) =&gt; Iterator[jl.Double] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; new JavaDoubleRDD(rdd.mapPartitions(fn, preservesPartitioning) .map(_.doubleValue()))&#125; mapPartitionsToPair1234567891011121314151617181920212223242526/** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */def mapPartitionsToPair[K2, V2](f: PairFlatMapFunction[JIterator[T], K2, V2]):JavaPairRDD[K2, V2] = &#123; def fn: (Iterator[T]) =&gt; Iterator[(K2, V2)] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; JavaPairRDD.fromRDD(rdd.mapPartitions(fn))(fakeClassTag[K2], fakeClassTag[V2])&#125;/** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */def mapPartitionsToPair[K2, V2](f: PairFlatMapFunction[JIterator[T], K2, V2], preservesPartitioning: Boolean): JavaPairRDD[K2, V2] = &#123; def fn: (Iterator[T]) =&gt; Iterator[(K2, V2)] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; JavaPairRDD.fromRDD( rdd.mapPartitions(fn, preservesPartitioning))(fakeClassTag[K2], fakeClassTag[V2])&#125; foreachPartition12345678/** * Applies a function f to each partition of this RDD. * 将函数f应用于该RDD的每个分区。 * */def foreachPartition(f: VoidFunction[JIterator[T]]): Unit = &#123; rdd.foreachPartition(x =&gt; f.call(x.asJava))&#125; glom1234567/** * Return an RDD created by coalescing all elements within each partition into an array. * 返回一个RDD，它将每个分区中的所有元素合并到一个数组中。 * */def glom(): JavaRDD[JList[T]] = new JavaRDD(rdd.glom().map(_.toSeq.asJava)) cartesian12345678/** * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of * elements (a, b) where a is in `this` and b is in `other`. * 返回这个RDD和另一个的笛卡尔乘积，即所有元素对的RDD(a,b) ：a在该RDD中，b在另一个RDD中 * */def cartesian[U](other: JavaRDDLike[U, _]): JavaPairRDD[T, U] = JavaPairRDD.fromRDD(rdd.cartesian(other.rdd)(other.classTag))(classTag, other.classTag) groupBy123456789101112131415161718192021222324252627/** * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements * mapping to that key. * 返回分组元素的RDD。 * 每个组由一个键和一个映射到该键的元素序列组成。 * */def groupBy[U](f: JFunction[T, U]): JavaPairRDD[U, JIterable[T]] = &#123; // The type parameter is U instead of K in order to work around a compiler bug; see SPARK-4459 // 类型参数是U而不是K，是为了绕过编译器错误 implicit val ctagK: ClassTag[U] = fakeClassTag implicit val ctagV: ClassTag[JList[T]] = fakeClassTag JavaPairRDD.fromRDD(groupByResultToJava(rdd.groupBy(f)(fakeClassTag)))&#125;/** * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements * mapping to that key. * 返回分组元素的RDD。 * 每个组由一个键和一个映射到该键的元素序列组成。 */def groupBy[U](f: JFunction[T, U], numPartitions: Int): JavaPairRDD[U, JIterable[T]] = &#123; // The type parameter is U instead of K in order to work around a compiler bug; see SPARK-4459 implicit val ctagK: ClassTag[U] = fakeClassTag implicit val ctagV: ClassTag[JList[T]] = fakeClassTag JavaPairRDD.fromRDD(groupByResultToJava(rdd.groupBy(f, numPartitions)(fakeClassTag[U])))&#125; pipe123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * Return an RDD created by piping elements to a forked external process. * 返回由管道元素调用外部程序返回新的RDD * */def pipe(command: String): JavaRDD[String] = &#123; rdd.pipe(command)&#125;/** * Return an RDD created by piping elements to a forked external process. * 返回由管道元素调用外部程序返回新的RDD * */def pipe(command: JList[String]): JavaRDD[String] = &#123; rdd.pipe(command.asScala)&#125;/** * Return an RDD created by piping elements to a forked external process. * 返回由管道元素调用外部程序返回新的RDD * */def pipe(command: JList[String], env: JMap[String, String]): JavaRDD[String] = &#123; rdd.pipe(command.asScala, env.asScala)&#125;/** * Return an RDD created by piping elements to a forked external process. * 返回由管道元素调用外部程序返回新的RDD * */def pipe(command: JList[String], env: JMap[String, String], separateWorkingDir: Boolean, bufferSize: Int): JavaRDD[String] = &#123; rdd.pipe(command.asScala, env.asScala, null, null, separateWorkingDir, bufferSize)&#125;/** * Return an RDD created by piping elements to a forked external process. * 返回由管道元素调用外部程序返回新的RDD * */def pipe(command: JList[String], env: JMap[String, String], separateWorkingDir: Boolean, bufferSize: Int, encoding: String): JavaRDD[String] = &#123; rdd.pipe(command.asScala, env.asScala, null, null, separateWorkingDir, bufferSize, encoding)&#125; zip123456789101112/** * Zips this RDD with another one, returning key-value pairs with the first element in each RDD, * second element in each RDD, etc. Assumes that the two RDDs have the *same number of * partitions* and the *same number of elements in each partition* (e.g. one was made through * a map on the other). * 将此RDD与另一个RDD进行Zips，返回键值对，每个RDD中的第一个元素，每个RDD中的第二个元素，等等。 * 假设两个RDDs拥有相同数量的分区和每个分区中相同数量的元素 * (例如，一个是通过另一个map的)。 */def zip[U](other: JavaRDDLike[U, _]): JavaPairRDD[T, U] = &#123; JavaPairRDD.fromRDD(rdd.zip(other.rdd)(other.classTag))(classTag, other.classTag)&#125; zipPartitions1234567891011121314151617/** * Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by * applying a function to the zipped partitions. Assumes that all the RDDs have the * *same number of partitions*, but does *not* require them to have the same number * of elements in each partition. * 用一个(或多个)RDD(或多个)来压缩这个RDD的分区，并返回一个新的RDD将函数应用于压缩分区。 * 假设所有RDDs拥有相同数量的分区，但不要求它们在每个分区中拥有相同数量的元素。 */def zipPartitions[U, V]( other: JavaRDDLike[U, _], f: FlatMapFunction2[JIterator[T], JIterator[U], V]): JavaRDD[V] = &#123; def fn: (Iterator[T], Iterator[U]) =&gt; Iterator[V] = &#123; (x: Iterator[T], y: Iterator[U]) =&gt; f.call(x.asJava, y.asJava).asScala &#125; JavaRDD.fromRDD( rdd.zipPartitions(other.rdd)(fn)(other.classTag, fakeClassTag[V]))(fakeClassTag[V])&#125; zipWithUniqueId1234567891011/** * Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k, * 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method * won't trigger a spark job, which is different from [[org.apache.spark.rdd.RDD#zipWithIndex]]. * 用生成的唯一长的id来压缩这个RDD。 * 第k个分区的项将得到id k,n + k,2 *n+ k，…，其中n是分区数。 * 因此，可能存在差距，但这种方法不会触发spark作业，它与[org .apache.spark. spark.rdd. rdd. rdd # zipWithIndex]不同。 */def zipWithUniqueId(): JavaPairRDD[T, jl.Long] = &#123; JavaPairRDD.fromRDD(rdd.zipWithUniqueId()).asInstanceOf[JavaPairRDD[T, jl.Long]]&#125; zipWithIndex12345678910111213141516/** * Zips this RDD with its element indices. The ordering is first based on the partition index * and then the ordering of items within each partition. So the first item in the first * partition gets index 0, and the last item in the last partition receives the largest index. * This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type. * This method needs to trigger a spark job when this RDD contains more than one partitions. * * 用它的元素索引来压缩这个RDD。 * 排序首先基于分区索引，然后是每个分区中的条目的排序。 * 因此，第一个分区中的第一个项的索引值为0，最后一个分区中的最后一个项得到最大的索引。 * 这类似于Scala的zipWithIndex，但它使用的是Long而不是Int作为索引类型。 * 当这个RDD包含多个分区时，这个方法需要触发一个spark作业。 */def zipWithIndex(): JavaPairRDD[T, jl.Long] = &#123; JavaPairRDD.fromRDD(rdd.zipWithIndex()).asInstanceOf[JavaPairRDD[T, jl.Long]]&#125; Actions (launch a job to return a value to the user program)foreach1234567/** * Applies a function f to all elements of this RDD. * 将函数f应用于该RDD的所有元素。 */def foreach(f: VoidFunction[T]) &#123; rdd.foreach(x =&gt; f.call(x))&#125; collect12345678910/** * Return an array that contains all of the elements in this RDD. * 返回包含该RDD中所有元素的数组。 * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。 */def collect(): JList[T] = rdd.collect().toSeq.asJava toLocalIterator123456789/** * Return an iterator that contains all of the elements in this RDD. * 返回包含该RDD中所有元素的迭代器。 * * The iterator will consume as much memory as the largest partition in this RDD. * 迭代器将消耗与此RDD中最大的分区一样多的内存。 */def toLocalIterator(): JIterator[T] = asJavaIteratorConverter(rdd.toLocalIterator).asJava collectPartitions1234567891011 /** * Return an array that contains all of the elements in a specific partition of this RDD. * 返回包含该RDD的特定分区中的所有元素的数组。 */def collectPartitions(partitionIds: Array[Int]): Array[JList[T]] = &#123; // This is useful for implementing `take` from other language frontends // like Python where the data is serialized. // 这有助于从其他语言的前沿实现“take”，如Python，数据被序列化。 val res = context.runJob(rdd, (it: Iterator[T]) =&gt; it.toArray, partitionIds) res.map(_.toSeq.asJava)&#125; reduce123456/** * Reduces the elements of this RDD using the specified commutative and associative binary * operator. * 使用指定的交换和关联二元运算符来减少该RDD的元素。 */def reduce(f: JFunction2[T, T, T]): T = rdd.reduce(f) treeReduce12345678910111213/** * Reduces the elements of this RDD in a multi-level tree pattern. * 将此RDD的元素简化为多层树模式。 * * @param depth suggested depth of the tree 建议树的深度 * @see [[org.apache.spark.api.java.JavaRDDLike#reduce]] */def treeReduce(f: JFunction2[T, T, T], depth: Int): T = rdd.treeReduce(f, depth)/** * [[org.apache.spark.api.java.JavaRDDLike#treeReduce]] 建议深度 2 . */def treeReduce(f: JFunction2[T, T, T]): T = treeReduce(f, 2) fold123456789101112131415161718192021 /** * Aggregate the elements of each partition, and then the results for all the partitions, using a * given associative function and a neutral \"zero value\". The function * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object * allocation; however, it should not modify t2. * 对每个分区的元素进行聚合，然后使用给定的关联函数和中立的“零值”，对所有分区进行结果。 * 函数op(t1,t2)被允许修改t1，并将其作为其结果值返回，以避免对象分配;但是，它不应该修改t2。 * * This behaves somewhat differently from fold operations implemented for non-distributed * collections in functional languages like Scala. This fold operation may be applied to * partitions individually, and then fold those results into the final result, rather than * apply the fold to each element sequentially in some defined ordering. For functions * that are not commutative, the result may differ from that of a fold applied to a * non-distributed collection. * 这与在Scala等函数式语言中实现非分布式集合的折叠操作有一定的不同。 * 这个折叠操作可以单独应用于分区，然后将这些结果折叠到最终结果中，而不是在某些定义的排序中顺序地对每个元素进行折叠。 * 对于非交换的函数，结果可能与应用于非分布式集合的函数不同。 * */def fold(zeroValue: T)(f: JFunction2[T, T, T]): T = rdd.fold(zeroValue)(f) aggregate12345678910111213141516 /** * Aggregate the elements of each partition, and then the results for all the partitions, using * given combine functions and a neutral \"zero value\". This function can return a different result * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are * allowed to modify and return their first argument instead of creating a new U to avoid memory * allocation. * 对每个分区的元素进行聚合，然后使用给定的组合函数和一个中立的“零值”，对所有分区进行结果。 * 这个函数可以返回一个不同的结果类型U，而不是这个RDD的类型。 * 因此，我们需要一个操作来将一个T合并到一个U和一个合并两个U的操作，就像在scala . traversableonce中那样。 * 这两个函数都可以修改和返回第一个参数，而不是创建一个新的U，以避免内存分配。 * */def aggregate[U](zeroValue: U)(seqOp: JFunction2[U, T, U], combOp: JFunction2[U, U, U]): U = rdd.aggregate(zeroValue)(seqOp, combOp)(fakeClassTag[U]) treeAggregate12345678910111213141516171819202122232425 /** * Aggregates the elements of this RDD in a multi-level tree pattern. * 将此RDD的元素聚集在多层树模式中。 * * @param depth suggested depth of the tree 建议的树的深度 * @see [[org.apache.spark.api.java.JavaRDDLike#aggregate]] */def treeAggregate[U]( zeroValue: U, seqOp: JFunction2[U, T, U], combOp: JFunction2[U, U, U], depth: Int): U = &#123; rdd.treeAggregate(zeroValue)(seqOp, combOp, depth)(fakeClassTag[U])&#125;/** * [[org.apache.spark.api.java.JavaRDDLike#treeAggregate]] with suggested depth 2. * 建议的树的深度为 2 */def treeAggregate[U]( zeroValue: U, seqOp: JFunction2[U, T, U], combOp: JFunction2[U, U, U]): U = &#123; treeAggregate(zeroValue, seqOp, combOp, 2)&#125; count123456/** * Return the number of elements in the RDD. * 返回RDD中元素的数量。 * */def count(): Long = rdd.count() countApprox12345678910111213141516171819202122232425262728293031323334353637 /** * Approximate version of count() that returns a potentially incomplete result * within a timeout, even if not all tasks have finished. * 近似版本的count()，即使不是所有的任务都完成了，也会在一个超时中返回一个潜在的不完整的结果。 * * * The confidence is the probability that the error bounds of the result will * contain the true value. That is, if countApprox were called repeatedly * with confidence 0.9, we would expect 90% of the results to contain the * true count. The confidence must be in the range [0,1] or an exception will * be thrown. * 置信值是结果的误差边界包含真实值的概率。 * 也就是说，如果countApprox被反复调用，confidence 0.9，我们将期望90%的结果包含真实的计数。 * confidence必须在范围[0,1]中，否则将抛出异常。 * * * @param timeout maximum time to wait for the job, in milliseconds * 等待工作的最大时间，以毫秒为单位 * @param confidence the desired statistical confidence in the result * 对结果的期望的统计信心 * @return a potentially incomplete result, with error bounds * 一个可能不完整的结果，有错误界限 */def countApprox(timeout: Long, confidence: Double): PartialResult[BoundedDouble] = rdd.countApprox(timeout, confidence)/** * Approximate version of count() that returns a potentially incomplete result * within a timeout, even if not all tasks have finished. * 近似版本的count()，即使不是所有的任务都完成了，也会在一个超时中返回一个潜在的不完整的结果。 * * * @param timeout maximum time to wait for the job, in milliseconds * 等待工作的最大时间，以毫秒为单位 */def countApprox(timeout: Long): PartialResult[BoundedDouble] = rdd.countApprox(timeout) countByValue123456789/** * Return the count of each unique value in this RDD as a map of (value, count) pairs. The final * combine step happens locally on the master, equivalent to running a single reduce task. * 将此RDD中的每个惟一值的计数作为(值、计数)对的映射。 * 最后的联合步骤在master的本地发生，相当于运行一个reduce任务。 * */def countByValue(): JMap[T, jl.Long] = mapAsSerializableJavaMap(rdd.countByValue()).asInstanceOf[JMap[T, jl.Long]] countByValueApprox1234567891011121314151617181920212223242526272829303132333435363738 /** * Approximate version of countByValue(). * countByValue()近似的版本。 * * The confidence is the probability that the error bounds of the result will * contain the true value. That is, if countApprox were called repeatedly * with confidence 0.9, we would expect 90% of the results to contain the * true count. The confidence must be in the range [0,1] or an exception will * be thrown. * 置信值是结果的误差边界包含真实值的概率。 * 也就是说，如果countApprox被反复调用，confidence 0.9，我们将期望90%的结果包含真实的计数。 * confidence必须在范围[0,1]中，否则将抛出异常。 * * * @param timeout maximum time to wait for the job, in milliseconds * 等待工作的最大时间，毫秒为单位。 * @param confidence the desired statistical confidence in the result * 对结果的期望的统计信心 * @return a potentially incomplete result, with error bounds * 一个可能不完整的结果，有错误界限 */def countByValueApprox( timeout: Long, confidence: Double ): PartialResult[JMap[T, BoundedDouble]] = rdd.countByValueApprox(timeout, confidence).map(mapAsSerializableJavaMap)/** * Approximate version of countByValue(). * countByValue().的近似版本. * * @param timeout maximum time to wait for the job, in milliseconds * 等待工作的最大时间，毫秒为单位。 * @return a potentially incomplete result, with error bounds * 一个可能不完整的结果，有错误界限 */def countByValueApprox(timeout: Long): PartialResult[JMap[T, BoundedDouble]] = rdd.countByValueApprox(timeout).map(mapAsSerializableJavaMap) take12345678910111213141516 /** * Take the first num elements of the RDD. This currently scans the partitions *one by one*, so * it will be slow if a lot of partitions are required. In that case, use collect() to get the * whole RDD instead. * 获取RDD的第一个num元素。 * 这将会一次一个地扫描分区，所以如果需要很多分区，它将会很慢。 * 在这种情况下，使用collect()来获得整个RDD。 * * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。 * */def take(num: Int): JList[T] = rdd.take(num).toSeq.asJava takeSample12345def takeSample(withReplacement: Boolean, num: Int): JList[T] = takeSample(withReplacement, num, Utils.random.nextLong)def takeSample(withReplacement: Boolean, num: Int, seed: Long): JList[T] = rdd.takeSample(withReplacement, num, seed).toSeq.asJava first12345 /** * Return the first element in this RDD. * 返回这个RDD中的第一个元素。 */def first(): T = rdd.first() isEmpty1234567/** * @return true if and only if the RDD contains no elements at all. Note that an RDD * may be empty even when it has at least 1 partition. * 当且仅当RDD不包含任何元素，则为真。 * 请注意，即使在至少有一个分区的情况下，RDD也可能是空的。 */def isEmpty(): Boolean = rdd.isEmpty() saveAsTextFile123456789101112131415/** * Save this RDD as a text file, using string representations of elements. * 将此RDD保存为文本文件，使用元素的字符串表示形式。 */def saveAsTextFile(path: String): Unit = &#123; rdd.saveAsTextFile(path)&#125;/** * Save this RDD as a compressed text file, using string representations of elements. * 将此RDD保存为一个压缩文本文件，使用元素的字符串表示形式。 */def saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec]): Unit = &#123; rdd.saveAsTextFile(path, codec)&#125; saveAsObjectFile1234567/** * Save this RDD as a SequenceFile of serialized objects. * 将此RDD保存为序列化对象的序列文件。 */def saveAsObjectFile(path: String): Unit = &#123; rdd.saveAsObjectFile(path)&#125; keyBy12345678910/** * Creates tuples of the elements in this RDD by applying `f`. * 通过应用“f”创建这个RDD中元素的元组。 */def keyBy[U](f: JFunction[T, U]): JavaPairRDD[U, T] = &#123; // The type parameter is U instead of K in order to work around a compiler bug; see SPARK-4459 // 类型参数用U替代K，为了绕过编译器错误; implicit val ctag: ClassTag[U] = fakeClassTag JavaPairRDD.fromRDD(rdd.keyBy(f))&#125; checkpoint12345678910111213141516/** * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint * directory set with SparkContext.setCheckpointDir() and all references to its parent * RDDs will be removed. This function must be called before any job has been * executed on this RDD. It is strongly recommended that this RDD is persisted in * memory, otherwise saving it on a file will require recomputation. * 将此RDD标记为检查点。 * 它将被保存到由SparkContext.setCheckpointDir()设置的检查点目录下的文件中。 * 所有对其父RDDs的引用将被删除。 * 在此RDD上执行任何作业之前，必须调用此函数。 * 强烈建议将此RDD保存在内存中，否则将其保存在文件中需要重新计算。 * */def checkpoint(): Unit = &#123; rdd.checkpoint()&#125; isCheckpointed12345/** * Return whether this RDD has been checkpointed or not * 返回 RDD是否已被检查过 */def isCheckpointed: Boolean = rdd.isCheckpointed getCheckpointFile1234567/** * Gets the name of the file to which this RDD was checkpointed * 获取该RDD所指向的checkpointed文件的名称 */def getCheckpointFile(): Optional[String] = &#123; JavaUtils.optionToOptional(rdd.getCheckpointFile)&#125; toDebugString123456/** A description of this RDD and its recursive dependencies for debugging. * 对该RDD及其对调试的递归依赖的描述。 * */def toDebugString(): String = &#123; rdd.toDebugString&#125; top12345678910111213141516171819202122232425262728293031323334/** * Returns the top k (largest) elements from this RDD as defined by * the specified Comparator[T] and maintains the order. * 根据指定的比较器[T]，从这个RDD中返回最大的k(最大)元素，并维护顺序。 * * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。 * * @param num k, the number of top elements to return 返回的元素数量 * @param comp the comparator that defines the order 定义排序的比较器 * @return an array of top elements 返回最大元素的数组 */def top(num: Int, comp: Comparator[T]): JList[T] = &#123; rdd.top(num)(Ordering.comparatorToOrdering(comp)).toSeq.asJava&#125;/** * Returns the top k (largest) elements from this RDD using the * natural ordering for T and maintains the order. * 使用T的自然顺序，从这个RDD中返回最大的k(最大)元素，并维护顺序。 * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。 * * @param num k, the number of top elements to return 返回的元素数量 * @return an array of top elements 最大元素的数组 */def top(num: Int): JList[T] = &#123; val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[T]] top(num, comp)&#125; takeOrdered123456789101112131415161718192021222324252627282930313233/** * Returns the first k (smallest) elements from this RDD as defined by * the specified Comparator[T] and maintains the order. * 从这个RDD中返回第一个k(最小)元素，由指定的Comparator[T]定义，并维护该顺序。 * * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。 * * @param num k, the number of elements to return 返回的元素数量 * @param comp the comparator that defines the order 排序比较器 * @return an array of top elements 元素数组 */def takeOrdered(num: Int, comp: Comparator[T]): JList[T] = &#123; rdd.takeOrdered(num)(Ordering.comparatorToOrdering(comp)).toSeq.asJava&#125;/** * Returns the first k (smallest) elements from this RDD using the * natural ordering for T while maintain the order. * 使用原生的 T排序比较器，返回 k个 最小值，并维护这个顺序 * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 尽量应用于小的数组，因为会加载到driver内存中。 * @param num k, the number of top elements to return * @return an array of top elements */def takeOrdered(num: Int): JList[T] = &#123; val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[T]] takeOrdered(num, comp)&#125; max123456789101112/** * Returns the maximum element from this RDD as defined by the specified * Comparator[T]. * 按照指定比较器[T]定义的RDD， * 返回最大元素。 * * @param comp the comparator that defines ordering 指定的比较器 * @return the maximum of the RDD 最大值 */def max(comp: Comparator[T]): T = &#123; rdd.max()(Ordering.comparatorToOrdering(comp))&#125; min123456789101112/** * Returns the minimum element from this RDD as defined by the specified * Comparator[T]. * 按照指定比较器[T]定义的RDD， * 返回最小元素。 * * @param comp the comparator that defines ordering 指定的比较器 * @return the minimum of the RDD 最小值 */def min(comp: Comparator[T]): T = &#123; rdd.min()(Ordering.comparatorToOrdering(comp))&#125; countApproxDistinct123456789101112131415161718/*** Return approximate number of distinct elements in the RDD.* 返回RDD中不重复元素的数量近似数。** The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice:* Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * &lt;a href=\"http://dx.doi.org/10.1145/2452376.2452456\"&gt;here&lt;/a&gt;. * 所使用的算法是基于streamlib在实践中的“HyperLogLog”的实现: * “一种艺术基数估计算法状态的算法工程”， * * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017. * 相对精度。 * 较小的值创建需要更多空间的计数器。 * 它必须大于0.000017。 */ def countApproxDistinct(relativeSD: Double): Long = rdd.countApproxDistinct(relativeSD) countAsync12345678/** * The asynchronous version of `count`, which returns a * future for counting the number of elements in this RDD. * “count”的异步版本，它为计算这个RDD中元素的数量返回一个未来。 */def countAsync(): JavaFutureAction[jl.Long] = &#123;new JavaFutureActionWrapper[Long, jl.Long](rdd.countAsync(), jl.Long.valueOf)&#125; collectAsync12345678910111213/** * The asynchronous version of `collect`, which returns a future for * retrieving an array containing all of the elements in this RDD. * “collect”的异步版本， * 它返回一个用于检索包含该RDD中所有元素的数组的未来。 * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 尽量应用于小数量数组。 */def collectAsync(): JavaFutureAction[JList[T]] = &#123;new JavaFutureActionWrapper(rdd.collectAsync(), (x: Seq[T]) =&gt; x.asJava)&#125; takeAsync123456789101112/** * The asynchronous version of the `take` action, which returns a * future for retrieving the first `num` elements of this RDD. * “take”操作的异步版本， * 它将返回用于检索此RDD的第一个“num”元素的未来。 * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. */def takeAsync(num: Int): JavaFutureAction[JList[T]] = &#123;new JavaFutureActionWrapper(rdd.takeAsync(num), (x: Seq[T]) =&gt; x.asJava)&#125; foreachAsync1234567891011/** * The asynchronous version of the `foreach` action, which * applies a function f to all the elements of this RDD. * “foreach”操作的异步版本， * 它将函数f应用于这个RDD的所有元素。 * */def foreachAsync(f: VoidFunction[T]): JavaFutureAction[Void] = &#123;new JavaFutureActionWrapper[Unit, Void](rdd.foreachAsync(x =&gt; f.call(x)),&#123; x =&gt; null.asInstanceOf[Void] &#125;)&#125; foreachPartitionAsync1234567891011/** * The asynchronous version of the `foreachPartition` action, which * applies a function f to each partition of this RDD. * “foreachPartition”操作的异步版本， * 它将函数f应用于该RDD的每个分区。 */def foreachPartitionAsync(f: VoidFunction[JIterator[T]]): JavaFutureAction[Void] = &#123;new JavaFutureActionWrapper[Unit, Void](rdd.foreachPartitionAsync(x =&gt; f.call(x.asJava)),&#123; x =&gt; null.asInstanceOf[Void] &#125;)&#125;&#125;","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"},{"name":"源码","slug":"源码","permalink":"https://stanxia.github.io/tags/源码/"}]},{"title":"spark取样函数分析","slug":"spark取样函数分析","date":"2017-11-08T09:30:25.000Z","updated":"2017-12-05T03:33:08.000Z","comments":true,"path":"2017/11/08/spark取样函数分析/","link":"","permalink":"https://stanxia.github.io/2017/11/08/spark取样函数分析/","excerpt":"Spark取样操作无法获取随机样本的解决方案\n\n背景Dataset中sample函数源码如下：\n123456789101112131415161718192021222324252627282930313233343536373839404142434445/**  * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.  *  * 通过使用用户提供的种子，通过抽样的方式返回一个新的[[Dataset]]。  *  * @param withReplacement Sample with replacement or not.  *                        如果withReplacement=true的话表示有放回的抽样，采用泊松抽样算法实现.  *                        如果withReplacement=false的话表示无放回的抽样，采用伯努利抽样算法实现.  * @param fraction        Fraction of rows to generate.  *                        每一行数据被取样的概率.服从二项分布.当withReplacement=true的时候fraction&gt;=0,当withReplacement=false的时候 0 &lt; fraction &lt; 1.  * @param seed            Seed for sampling.  *                        取样种子（与随机数生成有关）  * @note This is NOT guaranteed to provide exactly the fraction of the count  *       of the given [[Dataset]].  *       不能保证准确的按照给定的分数取样。（一般结果会在概率值*总数左右）  * @group typedrel  * @since 1.6.0  */def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = &#123;  require(fraction &gt;= 0,    s\"Fraction must be nonnegative, but got $&#123;fraction&#125;\")  withTypedPlan &#123;    Sample(0.0, fraction, withReplacement, seed, logicalPlan)()  &#125;&#125;/**  * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.  *  * 通过程序随机的种子，抽样返回新的DataSet  *  * @param withReplacement Sample with replacement or not.  *                        取样结果是否放回  * @param fraction        Fraction of rows to generate.  *                        每行数据被取样的概率  * @note This is NOT guaranteed to provide exactly the fraction of the total count  *       of the given [[Dataset]].  *       不能保证准确的按照给定的分数取样。（一般结果会在概率值*总数左右）  * @group typedrel  * @since 1.6.0  */def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = &#123;  sample(withReplacement, fraction, Utils.random.nextLong)&#125;","text":"Spark取样操作无法获取随机样本的解决方案 背景Dataset中sample函数源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed. * * 通过使用用户提供的种子，通过抽样的方式返回一个新的[[Dataset]]。 * * @param withReplacement Sample with replacement or not. * 如果withReplacement=true的话表示有放回的抽样，采用泊松抽样算法实现. * 如果withReplacement=false的话表示无放回的抽样，采用伯努利抽样算法实现. * @param fraction Fraction of rows to generate. * 每一行数据被取样的概率.服从二项分布.当withReplacement=true的时候fraction&gt;=0,当withReplacement=false的时候 0 &lt; fraction &lt; 1. * @param seed Seed for sampling. * 取样种子（与随机数生成有关） * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * 不能保证准确的按照给定的分数取样。（一般结果会在概率值*总数左右） * @group typedrel * @since 1.6.0 */def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = &#123; require(fraction &gt;= 0, s\"Fraction must be nonnegative, but got $&#123;fraction&#125;\") withTypedPlan &#123; Sample(0.0, fraction, withReplacement, seed, logicalPlan)() &#125;&#125;/** * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed. * * 通过程序随机的种子，抽样返回新的DataSet * * @param withReplacement Sample with replacement or not. * 取样结果是否放回 * @param fraction Fraction of rows to generate. * 每行数据被取样的概率 * @note This is NOT guaranteed to provide exactly the fraction of the total count * of the given [[Dataset]]. * 不能保证准确的按照给定的分数取样。（一般结果会在概率值*总数左右） * @group typedrel * @since 1.6.0 */def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = &#123; sample(withReplacement, fraction, Utils.random.nextLong)&#125; 问题结果数据的行数一般在（fraction*总数）左右。没有一个固定的值，如果需要得到固定行数的随机数据的话不建议采用该方法。 办法获取随机取样的替代方法： 123456df.createOrReplaceTempView(\"test_sample\"); // 生成临时表df.sqlContext() // 添加随机数列，并根据其进行排序 .sql(\"select * ,rand() as random from test_sample order by random\") .limit(2) // 根据参数的fraction计算需要获取的取样结果 .drop(\"random\") // 删除掉添加的随机列 .show();","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"}]},{"title":"spark源码注释翻译","slug":"spark源码注释翻译","date":"2017-11-06T08:57:05.000Z","updated":"2017-12-05T03:32:01.000Z","comments":true,"path":"2017/11/06/spark源码注释翻译/","link":"","permalink":"https://stanxia.github.io/2017/11/06/spark源码注释翻译/","excerpt":"版本：spark2.1.1目的：方便中文用户阅读源码，把时间花在理解而不是翻译上\n\n初衷开始立项进行翻译，一方面方便日后阅读源码，另一方面先粗粒度的熟悉下spark框架和组件。优化完之后希望能帮助更多的中文用户，节省翻译时间。","text":"版本：spark2.1.1目的：方便中文用户阅读源码，把时间花在理解而不是翻译上 初衷开始立项进行翻译，一方面方便日后阅读源码，另一方面先粗粒度的熟悉下spark框架和组件。优化完之后希望能帮助更多的中文用户，节省翻译时间。 进度已完成： 正在作：spark core模块 模块名 模块介绍 完成度 api broadcast deploy executor 执行器：用于启动线程池，是真正负责执行task的部件 已完成 input internal io launcher mapred memory metrics network partial rdd rpc scheduler 调度器：spark应用程序的任务调度器 正在作 security serializer shuffle status.api.v1 storage util","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"}]},{"title":"spark关于parquet的优化","slug":"spark关于parquet的优化","date":"2017-11-01T06:53:13.000Z","updated":"2017-11-30T08:18:06.000Z","comments":true,"path":"2017/11/01/spark关于parquet的优化/","link":"","permalink":"https://stanxia.github.io/2017/11/01/spark关于parquet的优化/","excerpt":"parquet是一种列式存储。可以提供面向列的存储和查询。\nParquet的优势在sparkSQL程序中使用parquet格式存储文件，在存储空间和查询性能方面都有很高的效率。\n存储方面因为是面向列的存储，同一列的类型相同，因而在存储的过程中可以使用更高效的压缩方案，可以节省大量的存储空间。\n查询方面在执行查询任务时，只会扫描需要的列，而不是全部，高度灵活性使查询变得非常高效。","text":"parquet是一种列式存储。可以提供面向列的存储和查询。 Parquet的优势在sparkSQL程序中使用parquet格式存储文件，在存储空间和查询性能方面都有很高的效率。 存储方面因为是面向列的存储，同一列的类型相同，因而在存储的过程中可以使用更高效的压缩方案，可以节省大量的存储空间。 查询方面在执行查询任务时，只会扫描需要的列，而不是全部，高度灵活性使查询变得非常高效。 实例测试 测试数据大小 存储类型 存储所占空间 查询性能 1T TEXTFILE 897.9G 698s 1T Parquet 231.4G 21s Parquet的使用使用parquet的简单demo： 12345678910111213141516171819202122// Encoders for most common types are automatically provided by importing spark.implicits._import spark.implicits._val peopleDF = spark.read.json(\"examples/src/main/resources/people.json\")// DataFrames can be saved as Parquet files, maintaining the schema informationpeopleDF.write.parquet(\"people.parquet\")// Read in the parquet file created above// Parquet files are self-describing so the schema is preserved// The result of loading a Parquet file is also a DataFrameval parquetFileDF = spark.read.parquet(\"people.parquet\")// Parquet files can also be used to create a temporary view and then used in SQL statementsparquetFileDF.createOrReplaceTempView(\"parquetFile\")val namesDF = spark.sql(\"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19\")namesDF.map(attributes =&gt; \"Name: \" + attributes(0)).show()// +------------+// | value|// +------------+// |Name: Justin|// +------------+ Parquet 的问题 spark 写入数据到 hive 中，使用 Parquet 存储格式，查询该表时报错如下： 1Error: java.io.IOException: org.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file 当时设置的字段属性为： 经过比对，发现是 decimal 类型出了问题，查询 decimal 的字段时候就会报错，而查询其他的并不会报错。（这应该是 spark 引起的，因为在 hive 客户端执行 decimal 类型的操作时并不会出错。） 查阅网上，也有些朋友遇到了类似的事情，应该是官方的 bug ，暂时的解决办法是: 121. 将 Parquet 的存储格式转换为 ORC 2. 或将 decimal 换为 double 类型存储字段","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"},{"name":"parquet","slug":"parquet","permalink":"https://stanxia.github.io/tags/parquet/"}]},{"title":"三步走战略","slug":"三步走战略","date":"2017-11-01T02:45:09.000Z","updated":"2017-11-30T08:18:20.000Z","comments":true,"path":"2017/11/01/三步走战略/","link":"","permalink":"https://stanxia.github.io/2017/11/01/三步走战略/","excerpt":"设定中长期规划稳扎稳打，逐个击破，实现技术上的重大突破","text":"设定中长期规划稳扎稳打，逐个击破，实现技术上的重大突破 第一步深刻了解spark运行机制第二步深度剖析sparkSQL和sparkStreaming第三步实现对spark机器学习的深度掌握","raw":null,"content":null,"categories":[{"name":"规划","slug":"规划","permalink":"https://stanxia.github.io/categories/规划/"}],"tags":[{"name":"规划","slug":"规划","permalink":"https://stanxia.github.io/tags/规划/"}]},{"title":"手把手搭建vps和shadowsocks","slug":"手把手搭建vps和shadowsocks","date":"2017-10-30T16:16:22.000Z","updated":"2017-11-30T08:18:43.000Z","comments":true,"path":"2017/10/31/手把手搭建vps和shadowsocks/","link":"","permalink":"https://stanxia.github.io/2017/10/31/手把手搭建vps和shadowsocks/","excerpt":"记性不好，做个记录，日后有需要时难得费神。\n名词解释了解一些原理，熟悉一些名词，也方便理解接下来安装过程中的操作。\nvpsVPS(Virtual private server) 译作虚拟专用伺服器。你可以把它简单地理解为一台在远端的强劲电脑。当你租用了它以后，可以给它安装操作系统、软件，并通过一些工具连接和远程操控它。\nvultrVultr 是一家 VPS 服务器提供商，有美国、亚洲、欧洲等多地的 VPS。它家的服务器以性价比高闻名，按时间计费，最低的资费为每月 $2.5。\nlinuxLinux 是免费开源的操作系统，大概被世界上过半服务器所采用。有大量优秀的开源软件可以安装，上述 Shadowsocks 就是其一。你可以通过命令行来直接给 Linux 操作系统「下命令」，比如 $ cd ~/Desktop 就是进入你根目录下的 Desktop 文件夹。\nssh SSH 是一种网络协议，作为每一台 Linux 电脑的标准配置，用于计算机之间的加密登录。当你为租用的 VPS 安装 Linux 系统后，只要借助一些工具，就可以用 SSH 在你自己的 Mac/PC 电脑上远程登录该 VPS 了。\nshadowsocksShadowsocks(ss) 是由 Clowwindy 开发的一款软件，其作用本来是加密传输资料。当然，也正因为它加密传输资料的特性，使得 GFW 没法将由它传输的资料和其他普通资料区分开来，也就不能干扰我们访问那些「不存在」的网站了。","text":"记性不好，做个记录，日后有需要时难得费神。 名词解释了解一些原理，熟悉一些名词，也方便理解接下来安装过程中的操作。 vpsVPS(Virtual private server) 译作虚拟专用伺服器。你可以把它简单地理解为一台在远端的强劲电脑。当你租用了它以后，可以给它安装操作系统、软件，并通过一些工具连接和远程操控它。 vultrVultr 是一家 VPS 服务器提供商，有美国、亚洲、欧洲等多地的 VPS。它家的服务器以性价比高闻名，按时间计费，最低的资费为每月 $2.5。 linuxLinux 是免费开源的操作系统，大概被世界上过半服务器所采用。有大量优秀的开源软件可以安装，上述 Shadowsocks 就是其一。你可以通过命令行来直接给 Linux 操作系统「下命令」，比如 $ cd ~/Desktop 就是进入你根目录下的 Desktop 文件夹。 ssh SSH 是一种网络协议，作为每一台 Linux 电脑的标准配置，用于计算机之间的加密登录。当你为租用的 VPS 安装 Linux 系统后，只要借助一些工具，就可以用 SSH 在你自己的 Mac/PC 电脑上远程登录该 VPS 了。 shadowsocksShadowsocks(ss) 是由 Clowwindy 开发的一款软件，其作用本来是加密传输资料。当然，也正因为它加密传输资料的特性，使得 GFW 没法将由它传输的资料和其他普通资料区分开来，也就不能干扰我们访问那些「不存在」的网站了。 搭建vps目的就是搭建梯子。无建站的需求。推荐vultr，最便宜的有2.5美元一个月。500g流量完全够用了。且现在支持支付宝付款，颇为方便。现阶段的优惠活动是新注册的用户完成指定的任务会获得3美元的奖励。（详细情况可依参见官网。） 注册首先点击右侧注册链接：https://www.vultr.com/2017Promo，然后会来到下图所示的注册页面。 第一个框中填写注册邮箱，第二个框中填写注册密码（至少包含1个小写字母、1个大写字母和1个数字），最后点击Create Account创建账户。 创建账户后注册邮箱会收到一封验证邮件，我们需要点击Verify Your E-mail来验证邮箱。 如果注册邮箱收不到验证邮件请更换注册邮箱后重复第一步。 验证邮箱后我们会来到下图所示的登录界面，按下图中指示填写信息，然后点击Login登录。 登陆后我们会来到充值界面。Vultr要求新账户充值后才可以正常创建服务器。Vultr已经支持支付宝了，在这里推荐大家使用支付宝充值，最低金额为10美元。 购买充值完毕后点击右上角的蓝色加号按钮进入创建服务器界面。 首先需要选择Server Location即机房位置，从左到右、从上到下依次为东京、新加坡、伦敦、法兰克福、巴黎、阿姆斯特丹、迈阿密、亚特兰大、芝加哥、硅谷、达拉斯、洛杉矶、纽约、西雅图、悉尼。 然后需要选择Server Type即服务类型，这里大家需要选择安装Debian 7 x64系统，因为这个系统折腾起来比较容易，搭建东西也简单便捷。 然后需要选择Server Size即方案类型，这里大家可以按照需要自行选择，如果只是普通使用那么选择第二个5美元方案即可。 然后Additional Features、Startup Script、SSH Keys以及Server Hostname &amp; Label等四部分大家保持默认即可，最后点击右下方的蓝色Deploy Now按钮确认创建服务器。 创建服务器后我们会看到下图所示界面。 上图中我们需要耐心等待3~4分钟，等红色Installing字变为绿色Running字后，点击Cloud Instance即可进入服务器详细信息界面，如下图所示。 左侧红框内四行信息依次为机房位置、IP地址、登录用户名、登录密码。IP地址后面的按钮为复制IP地址，登录密码后面的按钮为复制密码及显示/隐藏密码。右上角红框内后面四个按钮分别是关闭服务器、重启服务器、重装系统、删除服务器。 远程登录安装远程登录软件。这里以windos端的xshell为例。使用mac的同学可以下载iTerm。 下载安装后打开软件。根据下图中的指示，我们点击会话框中的新建按钮。 点击新建按钮后会弹出下图所示界面。根据图中指示，我们首先填写IP地址，然后点击确定按钮。 点击确定按钮后我们会回到下图所示界面。根据图中指示，我们双击打开新建会话或者点击下方连接按钮打开新建会话。 开新建会话后会弹出下图所示界面。根据图中指示，我们点击接受并保存按钮。 点击接受并保存按钮会弹出下图所示界面。根据图中指示，我们首先填写SSH连接密码，然后打钩记住密码，最后点击确定按钮。 如果提示需要输入用户名（登录名），那么请输入root！ 点击确定按钮后服务器会自动连接，连接完毕后我们会来到下图所示界面 部署shadowsocks这里采用网上整理的一键部署的方案。简单方便操作。 首先复制以下内容： 1wget -N --no-check-certificate https://0123.cool/download/55r.sh &amp;&amp; chmod +x 55r.sh &amp;&amp; ./55r.sh 然后回到Xshell软件，右击选择粘贴，粘贴完毕后回车继续。 回车后系统会自行下载脚本文件并运行。根据下图图中指示，我们依次输入SSR的各项连接信息，最后回车继续。 安装完成后会出现下图所示界面。根据图中指示，我们将红框圈中的信息保存到记事本内。 配置锐意加速根据下图图中指示，我们继续复制下列信息： 1wget -N --no-check-certificate https://0123.cool/download/rs.sh &amp;&amp; bash rs.sh install 然后回到Xshell软件，右击选择粘贴，粘贴完毕后回车继续。 回车后系统会自行下载脚本文件并运行。根据下图图中指示，我们依次输入锐速的各项配置信息，最后回车继续。 回车后，系统自动执行命令完成破解版锐速安装，如下图所示。 我们首先输入： 1reboot 然后回车，Xshell会断开连接，系统会在1分钟后重启完毕，此时可以关闭Xshell软件了。 搭建教程到此结束，亲测成功。如果不能连接的，请检查自己的每一步操作。","raw":null,"content":null,"categories":[{"name":"vps","slug":"vps","permalink":"https://stanxia.github.io/categories/vps/"}],"tags":[{"name":"vps","slug":"vps","permalink":"https://stanxia.github.io/tags/vps/"}]},{"title":"spark报错集","slug":"spark报错集","date":"2017-10-30T05:58:58.000Z","updated":"2017-11-30T08:18:55.000Z","comments":true,"path":"2017/10/30/spark报错集/","link":"","permalink":"https://stanxia.github.io/2017/10/30/spark报错集/","excerpt":"有话要说针对一个老毛病：有些错误屡犯屡改，屡改屡犯，没有引起根本上的注意，或者没有从源头理解错误发生的底层原理，导致做很多无用功。\n总结历史，并从中吸取教训，减少无用功造成的时间浪费。特此将从目前遇到的spark问题全部记录在这里，搞清楚问题，自信向前。","text":"有话要说针对一个老毛病：有些错误屡犯屡改，屡改屡犯，没有引起根本上的注意，或者没有从源头理解错误发生的底层原理，导致做很多无用功。 总结历史，并从中吸取教训，减少无用功造成的时间浪费。特此将从目前遇到的spark问题全部记录在这里，搞清楚问题，自信向前。 问题汇总关键词：spark-hive概述：1Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Unable to instantiate SparkSession with Hive support because Hive classes are not found. 场景：1在本地调试spark程序，连接虚拟机上的集群，尝试执行sparkSQL时，启动任务就报错。 原理：1缺少sparkSQL连接hive的必要和依赖jar包 办法：123456789在项目／模块的pom.xml中添加相关的spark-hive依赖jar包。&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive_2.11 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;重新编译项目／模块即可。","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"}]},{"title":"life","slug":"life","date":"2017-10-29T03:00:42.000Z","updated":"2017-11-23T03:19:35.000Z","comments":true,"path":"2017/10/29/life/","link":"","permalink":"https://stanxia.github.io/2017/10/29/life/","excerpt":"\n\nvar dplayer0 = new DPlayer({\"element\":document.getElementById(\"dplayer0\"),\"autoplay\":false,\"theme\":\"#FADFA3\",\"loop\":true,\"video\":{\"url\":\"http://oliji9s3j.bkt.clouddn.com/Unbroken%20-%20Motivational%20Video.mp4\",\"pic\":\"/images/pic/life.jpeg\"}});\nLife is simple &amp;&amp; funny.","text":"var dplayer0 = new DPlayer({\"element\":document.getElementById(\"dplayer0\"),\"autoplay\":false,\"theme\":\"#FADFA3\",\"loop\":true,\"video\":{\"url\":\"http://oliji9s3j.bkt.clouddn.com/Unbroken%20-%20Motivational%20Video.mp4\",\"pic\":\"/images/pic/life.jpeg\"}}); Life is simple &amp;&amp; funny.","raw":null,"content":null,"categories":[{"name":"movie","slug":"movie","permalink":"https://stanxia.github.io/categories/movie/"}],"tags":[{"name":"movie","slug":"movie","permalink":"https://stanxia.github.io/tags/movie/"}]},{"title":"杂乱无章","slug":"杂乱无章","date":"2017-10-28T17:31:31.000Z","updated":"2017-11-30T08:25:18.000Z","comments":true,"path":"2017/10/29/杂乱无章/","link":"","permalink":"https://stanxia.github.io/2017/10/29/杂乱无章/","excerpt":"时光的机器，加足马力冲回过去\n历史的长河，丝丝涟漪涌向未来","text":"时光的机器，加足马力冲回过去 历史的长河，丝丝涟漪涌向未来 道不清楚，说不明白，夜深人静的时候，说一些想到的废话。窗外隆隆作响，不知疲倦的机器不知疲倦的执行着不知疲倦的动作。窗内屏幕暗淡，双眼干涩，思索着宇宙外的回想。 小时候，望向星空，那时的天空群星闪烁，哪像现在，嘿，享受了大城市的霓虹，哪里再给你无垠的星空，贪。 躺在草地，微风轻拂脸颊，初秋的夜晚，有点微凉。 仰望星河，也想着外面的世界，多精彩。 揣摩着无垠的宇宙，翻过地球，越过银河，驶向无限拓展的星际，身上的烦恼，微风一吹，全散了。 风轻拂，静静望着天空，思考着外面的朋友或许也在渴望着远方的我，伸手触摸这天空，抓一把星辰贪婪的放入梦。 深邃的夜空，望不尽的远方，是光明中的无尽黑暗，也似黑暗道路的一束亮光，洒向我，思绪跟着遨游，呵，世界与我万千美好，我与世界却念念叨叨，琐琐碎碎，麻麻烦烦。心里是想放飞的。 夜深，车水呼啸，诉说着城市的不眠，可我困，关窗，闷。开窗，嘿，不知疲倦的机器又开始不知疲倦的执行不知疲倦的动作。这样的夜晚，眠难。 深夜思考，写作。夜使我宁静，内心的宁静，这白天的大城市给予不了。感谢夜的馈赠，接收这无上的加冕，驰骋在思绪的星空，痛快，精彩，精彩。 杂乱无章，呵，可以。","raw":null,"content":null,"categories":[{"name":"think","slug":"think","permalink":"https://stanxia.github.io/categories/think/"}],"tags":[{"name":"think","slug":"think","permalink":"https://stanxia.github.io/tags/think/"}]},{"title":"闲谈","slug":"闲谈","date":"2017-10-28T03:07:14.000Z","updated":"2017-11-30T08:25:38.000Z","comments":true,"path":"2017/10/28/闲谈/","link":"","permalink":"https://stanxia.github.io/2017/10/28/闲谈/","excerpt":"九九登高忆重阳","text":"九九登高忆重阳","raw":null,"content":null,"categories":[{"name":"think","slug":"think","permalink":"https://stanxia.github.io/categories/think/"}],"tags":[{"name":"think","slug":"think","permalink":"https://stanxia.github.io/tags/think/"}]},{"title":"这个杀手不太冷","slug":"这个杀手不太冷","date":"2017-10-27T17:18:04.000Z","updated":"2017-11-30T08:26:06.000Z","comments":true,"path":"2017/10/28/这个杀手不太冷/","link":"","permalink":"https://stanxia.github.io/2017/10/28/这个杀手不太冷/","excerpt":"Is life always this hard,or is it just when you’re a kid?Always like this.\n","text":"Is life always this hard,or is it just when you’re a kid?Always like this.","raw":null,"content":null,"categories":[{"name":"movie","slug":"movie","permalink":"https://stanxia.github.io/categories/movie/"}],"tags":[{"name":"movie","slug":"movie","permalink":"https://stanxia.github.io/tags/movie/"}]},{"title":"mac使用小技巧","slug":"mac使用小技巧","date":"2017-10-27T17:13:44.000Z","updated":"2017-11-30T08:26:19.000Z","comments":true,"path":"2017/10/28/mac使用小技巧/","link":"","permalink":"https://stanxia.github.io/2017/10/28/mac使用小技巧/","excerpt":"记录mac使用的小技巧持续更新ing \n\n开启充电提示音（类似于iphone充电提示音，默认关闭）终端输入（开启）：\n1defaults write com.apple.PowerChime ChimeOnAllHardware -bool true; open /System/Library/CoreServices/PowerChime.app &amp;\n关闭：\n1defaults write com.apple.PowerChime ChimeOnAllHardware -bool false;killall PowerChime","text":"记录mac使用的小技巧持续更新ing 开启充电提示音（类似于iphone充电提示音，默认关闭）终端输入（开启）： 1defaults write com.apple.PowerChime ChimeOnAllHardware -bool true; open /System/Library/CoreServices/PowerChime.app &amp; 关闭： 1defaults write com.apple.PowerChime ChimeOnAllHardware -bool false;killall PowerChime 隐藏文件夹更好的保护学习资料，有时候需要设置隐藏文件夹： 1mv foldername .foldername 查看隐藏文件夹mac最新版本： 1⌘⇧.(Command + Shift + .) #隐藏 和显示 Macbook Pro 用外接显示器时，如何关闭笔记本屏幕，同时开盖使用12sudo nvram boot-args=&quot;iog=0x0&quot; #(10.10以前版本)sudo nvram boot-args=&quot;niog=1&quot; #(10.10及以后版本)这个命令的意思就是外接显示器时关闭自身屏幕，重启生效 开机流程：连上电源和外接显示器，按开机键，立即合盖，等外接显示器有信号时开盖即可如果报错 (已知 10.11/10.12 会报错)nvram: Error setting variable - ‘boot-args’: (iokit/common) general error 重启，按住command + r 进入恢复界面 左上角菜单里面找到终端，输入nvram boot-args=”niog=1”，回车问题解决。重启生效","raw":null,"content":null,"categories":[{"name":"mac","slug":"mac","permalink":"https://stanxia.github.io/categories/mac/"}],"tags":[{"name":"mac","slug":"mac","permalink":"https://stanxia.github.io/tags/mac/"}]},{"title":"photo","slug":"photo","date":"1999-10-26T16:54:07.000Z","updated":"2017-12-01T06:47:44.000Z","comments":false,"path":"1999/10/27/photo/","link":"","permalink":"https://stanxia.github.io/1999/10/27/photo/","excerpt":"","text":"","raw":null,"content":null,"categories":[{"name":"photo","slug":"photo","permalink":"https://stanxia.github.io/categories/photo/"}],"tags":[{"name":"photo","slug":"photo","permalink":"https://stanxia.github.io/tags/photo/"}]}]}