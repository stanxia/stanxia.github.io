{"meta":{"title":"东篱下","subtitle":"斯坦@森","description":"采菊东篱下，悠然见南山","author":"森","url":"https://stanxia.github.io"},"posts":[{"title":"火之意志","slug":"火之意志","date":"2022-10-26T16:54:07.000Z","updated":"2017-11-23T02:15:43.000Z","comments":false,"path":"2022/10/27/火之意志/","link":"","permalink":"https://stanxia.github.io/2022/10/27/火之意志/","excerpt":"","text":"","raw":null,"content":null,"categories":[{"name":"火影","slug":"火影","permalink":"https://stanxia.github.io/categories/火影/"}],"tags":[{"name":"动漫","slug":"动漫","permalink":"https://stanxia.github.io/tags/动漫/"}]},{"title":"spark分层取样","slug":"spark分层取样","date":"2017-11-27T08:57:10.000Z","updated":"2017-11-28T07:41:08.000Z","comments":true,"path":"2017/11/27/spark分层取样/","link":"","permalink":"https://stanxia.github.io/2017/11/27/spark分层取样/","excerpt":"先将总体的单位按某种特征分为若干次级总体（层），然后再从每一层内进行单纯随机抽样，组成一个样本的统计学计算方法叫做分层抽样。在spark.mllib中，用key来分层。\n与存在于spark.mllib中的其它统计函数不同，分层采样方法sampleByKey和sampleByKeyExact可以在key-value对的RDD上执行。在分层采样中，可以认为key是一个标签，value是特定的属性。例如，key可以是男人或者女人或者文档id,它相应的value可能是一组年龄或者是文档中的词。sampleByKey方法通过掷硬币的方式决定是否采样一个观察数据，因此它需要我们传递（pass over）数据并且提供期望的数据大小(size)。sampleByKeyExact比每层使用sampleByKey随机抽样需要更多的有意义的资源，但是它能使样本大小的准确性达到了99.99%。","text":"先将总体的单位按某种特征分为若干次级总体（层），然后再从每一层内进行单纯随机抽样，组成一个样本的统计学计算方法叫做分层抽样。在spark.mllib中，用key来分层。 与存在于spark.mllib中的其它统计函数不同，分层采样方法sampleByKey和sampleByKeyExact可以在key-value对的RDD上执行。在分层采样中，可以认为key是一个标签，value是特定的属性。例如，key可以是男人或者女人或者文档id,它相应的value可能是一组年龄或者是文档中的词。sampleByKey方法通过掷硬币的方式决定是否采样一个观察数据，因此它需要我们传递（pass over）数据并且提供期望的数据大小(size)。sampleByKeyExact比每层使用sampleByKey随机抽样需要更多的有意义的资源，但是它能使样本大小的准确性达到了99.99%。sampleByKeyExact()允许用户准确抽取f_k * n_k个样本，这里f_k表示期望获取键为k的样本的比例，n_k表示键为k的键值对的数量。下面是一个使用的例子： 123456789import org.apache.spark.SparkContextimport org.apache.spark.SparkContext._import org.apache.spark.rdd.PairRDDFunctionsval sc: SparkContext = ...val data = ... // an RDD[(K, V)] of any key value pairsval fractions: Map[K, Double] = ... // specify the exact fraction desired from each key// Get an exact sample from each stratumval approxSample = data.sampleByKey(withReplacement = false, fractions)val exactSample = data.sampleByKeyExact(withReplacement = false, fractions) 当withReplacement为true时，采用PoissonSampler取样器，当withReplacement为false使，采用BernoulliSampler取样器。 123456789101112131415161718192021def sampleByKey(withReplacement: Boolean, fractions: Map[K, Double], seed: Long = Utils.random.nextLong): RDD[(K, V)] = self.withScope &#123; val samplingFunc = if (withReplacement) &#123; StratifiedSamplingUtils.getPoissonSamplingFunction(self, fractions, false, seed) &#125; else &#123; StratifiedSamplingUtils.getBernoulliSamplingFunction(self, fractions, false, seed) &#125; self.mapPartitionsWithIndex(samplingFunc, preservesPartitioning = true) &#125;def sampleByKeyExact( withReplacement: Boolean, fractions: Map[K, Double], seed: Long = Utils.random.nextLong): RDD[(K, V)] = self.withScope &#123; val samplingFunc = if (withReplacement) &#123; StratifiedSamplingUtils.getPoissonSamplingFunction(self, fractions, true, seed) &#125; else &#123; StratifiedSamplingUtils.getBernoulliSamplingFunction(self, fractions, true, seed) &#125; self.mapPartitionsWithIndex(samplingFunc, preservesPartitioning = true) &#125; 下面我们分别来看sampleByKey和sampleByKeyExact的实现。 sampleByKey的实现当我们需要不重复抽样时，我们需要用泊松抽样器来抽样。当需要重复抽样时，用伯努利抽样器抽样。sampleByKey的实现比较简单，它就是统一的随机抽样。 泊松抽样器我们首先看泊松抽样器的实现。 12345678910111213141516171819def getPoissonSamplingFunction[K: ClassTag, V: ClassTag](rdd: RDD[(K, V)], fractions: Map[K, Double], exact: Boolean, seed: Long): (Int, Iterator[(K, V)]) =&gt; Iterator[(K, V)] = &#123; (idx: Int, iter: Iterator[(K, V)]) =&gt; &#123; //初始化随机生成器 val rng = new RandomDataGenerator() rng.reSeed(seed + idx) iter.flatMap &#123; item =&gt; //获得下一个泊松值 val count = rng.nextPoisson(fractions(item._1)) if (count == 0) &#123; Iterator.empty &#125; else &#123; Iterator.fill(count)(item) &#125; &#125; &#125;&#125; getPoissonSamplingFunction返回的是一个函数，传递给mapPartitionsWithIndex处理每个分区的数据。这里RandomDataGenerator是一个随机生成器，它用于同时生成均匀值(uniform values)和泊松值(Poisson values)。 伯努利抽样器1234567891011121314def getBernoulliSamplingFunction[K, V](rdd: RDD[(K, V)], fractions: Map[K, Double], exact: Boolean, seed: Long): (Int, Iterator[(K, V)]) =&gt; Iterator[(K, V)] = &#123; var samplingRateByKey = fractions (idx: Int, iter: Iterator[(K, V)]) =&gt; &#123; //初始化随机生成器 val rng = new RandomDataGenerator() rng.reSeed(seed + idx) // Must use the same invoke pattern on the rng as in getSeqOp for without replacement // in order to generate the same sequence of random numbers when creating the sample iter.filter(t =&gt; rng.nextUniform() &lt; samplingRateByKey(t._1)) &#125; &#125; sampleByKeyExact的实现sampleByKeyExact获取更准确的抽样结果，它的实现也分为两种情况，重复抽样和不重复抽样。前者使用泊松抽样器，后者使用伯努利抽样器。 泊松抽样器12345678910111213141516171819202122232425val counts = Some(rdd.countByKey())//计算立即接受的样本数量，并且为每层生成候选名单val finalResult = getAcceptanceResults(rdd, true, fractions, counts, seed)//决定接受样本的阈值，生成准确的样本大小val thresholdByKey = computeThresholdByKey(finalResult, fractions)(idx: Int, iter: Iterator[(K, V)]) =&gt; &#123; val rng = new RandomDataGenerator() rng.reSeed(seed + idx) iter.flatMap &#123; item =&gt; val key = item._1 val acceptBound = finalResult(key).acceptBound // Must use the same invoke pattern on the rng as in getSeqOp for with replacement // in order to generate the same sequence of random numbers when creating the sample val copiesAccepted = if (acceptBound == 0) 0L else rng.nextPoisson(acceptBound) //候选名单 val copiesWaitlisted = rng.nextPoisson(finalResult(key).waitListBound) val copiesInSample = copiesAccepted + (0 until copiesWaitlisted).count(i =&gt; rng.nextUniform() &lt; thresholdByKey(key)) if (copiesInSample &gt; 0) &#123; Iterator.fill(copiesInSample.toInt)(item) &#125; else &#123; Iterator.empty &#125; &#125;&#125; 伯努利抽样1234567891011121314151617def getBernoulliSamplingFunction[K, V](rdd: RDD[(K, V)], fractions: Map[K, Double], exact: Boolean, seed: Long): (Int, Iterator[(K, V)]) =&gt; Iterator[(K, V)] = &#123; var samplingRateByKey = fractions //计算立即接受的样本数量，并且为每层生成候选名单 val finalResult = getAcceptanceResults(rdd, false, fractions, None, seed) //决定接受样本的阈值，生成准确的样本大小 samplingRateByKey = computeThresholdByKey(finalResult, fractions) (idx: Int, iter: Iterator[(K, V)]) =&gt; &#123; val rng = new RandomDataGenerator() rng.reSeed(seed + idx) // Must use the same invoke pattern on the rng as in getSeqOp for without replacement // in order to generate the same sequence of random numbers when creating the sample iter.filter(t =&gt; rng.nextUniform() &lt; samplingRateByKey(t._1)) &#125; &#125;","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"},{"name":"源码","slug":"源码","permalink":"https://stanxia.github.io/tags/源码/"},{"name":"取样","slug":"取样","permalink":"https://stanxia.github.io/tags/取样/"}]},{"title":"Dataset.scala","slug":"Dataset-scala","date":"2017-11-23T01:34:44.000Z","updated":"2017-11-27T02:51:26.000Z","comments":true,"path":"2017/11/23/Dataset-scala/","link":"","permalink":"https://stanxia.github.io/2017/11/23/Dataset-scala/","excerpt":"\n前言Dataset 是一种强类型的领域特定对象集合，可以在使用功能或关系操作的同时进行转换。每个 Dataset 也有一个名为 “DataFrame” 的无类型视图，它是 [[Row]] 的 Dataset。Dataset 上可用的操作分为转换和动作:\n\n转换：产生新的 Dataset ；包括 map, filter, select, and aggregate (groupBy).动作：触发计算并返回结果 ；包括 count, show, or 写数据到文件系统。\n\nDataset是懒加载的，例如：只有提交动作的时候才会触发计算。在内部，Datasets表示一个逻辑计划，它描述生成数据所需的计算。当提交动作时，Spark的查询优化器会优化逻辑计划，并以并行和分布式的方式生成有效执行的物理计划。请使用explain 功能，探索逻辑计划和优化的物理计划。\n为了有效地支持特定于领域的对象，需要[[Encoder]]。编码器将特定类型的“T”映射到Spark的内部类型系统。例如：给一个 Person 类，并带有两个属性：name (string) and age (int),编码器告诉Spark在运行时生成代码，序列化 Person 对象为二进制结构。\n通常有两种创建Dataset的方法:\n\n使用 SparkSession 上可用的 read 方法读取 Spark 指向的存储系统上的文件。用现存的 Datasets 转换而来。\n\nDataset操作也可以是无类型的，通过多种领域专用语言（DSL）方法定义：这些操作非常类似于 R或Python语言中的 数据框架抽象中可用的操作。","text":"前言Dataset 是一种强类型的领域特定对象集合，可以在使用功能或关系操作的同时进行转换。每个 Dataset 也有一个名为 “DataFrame” 的无类型视图，它是 [[Row]] 的 Dataset。Dataset 上可用的操作分为转换和动作: 转换：产生新的 Dataset ；包括 map, filter, select, and aggregate (groupBy).动作：触发计算并返回结果 ；包括 count, show, or 写数据到文件系统。 Dataset是懒加载的，例如：只有提交动作的时候才会触发计算。在内部，Datasets表示一个逻辑计划，它描述生成数据所需的计算。当提交动作时，Spark的查询优化器会优化逻辑计划，并以并行和分布式的方式生成有效执行的物理计划。请使用explain 功能，探索逻辑计划和优化的物理计划。 为了有效地支持特定于领域的对象，需要[[Encoder]]。编码器将特定类型的“T”映射到Spark的内部类型系统。例如：给一个 Person 类，并带有两个属性：name (string) and age (int),编码器告诉Spark在运行时生成代码，序列化 Person 对象为二进制结构。 通常有两种创建Dataset的方法: 使用 SparkSession 上可用的 read 方法读取 Spark 指向的存储系统上的文件。用现存的 Datasets 转换而来。 Dataset操作也可以是无类型的，通过多种领域专用语言（DSL）方法定义：这些操作非常类似于 R或Python语言中的 数据框架抽象中可用的操作。 basic-基础方法toDF123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * Converts this strongly typed collection of data to generic Dataframe. In contrast to the * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]] * objects that allow fields to be accessed by ordinal or name. * 将这种强类型的数据集合转换为一般的Dataframe。 * 与Dataset操作所使用的强类型对象相反， * Dataframe返回泛型[[Row]]对象，这些对象允许通过序号或名称访问字段 * * @group basic * @since 1.6.0 */// This is declared with parentheses to prevent the Scala compiler from treating// `ds.toDF(\"1\")` as invoking this toDF and then apply on the returned DataFrame.// 这是用括号声明的，以防止Scala编译器处理ds.toDF(“1”)调用这个toDF，然后在返回的DataFrame上应用。def toDF(): DataFrame = new Dataset[Row](sparkSession, queryExecution, RowEncoder(schema)) /** * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed. * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with * meaningful names. For example: * * 将这种强类型的数据集合转换为通用的“DataFrame”，并将列重命名。 * 在将tuple的RDD转换为富有含义的名称的“DataFrame”时，这是非常方便的，如： * * &#123;&#123;&#123; * val rdd: RDD[(Int, String)] = ... * rdd.toDF() // 隐式转换创建了 DataFrame ，列名为： `_1` and `_2` * rdd.toDF(\"id\", \"name\") // 创建了 DataFrame ，列名为： \"id\" and \"name\" * &#125;&#125;&#125; * * @group basic * @since 2.0.0 */@scala.annotation.varargsdef toDF(colNames: String*): DataFrame = &#123; require(schema.size == colNames.size, \"The number of columns doesn't match.\\n\" + s\"Old column names ($&#123;schema.size&#125;): \" + schema.fields.map(_.name).mkString(\", \") + \"\\n\" + s\"New column names ($&#123;colNames.size&#125;): \" + colNames.mkString(\", \")) val newCols = logicalPlan.output.zip(colNames).map &#123; case (oldAttribute, newName) =&gt; Column(oldAttribute).as(newName) &#125; select(newCols: _*)&#125; as123456789101112131415161718192021222324252627282930313233/** * :: Experimental :: * Returns a new Dataset where each record has been mapped on to the specified type. The * method used to map columns depend on the type of `U`: * * 返回一个新的Dataset，其中每个记录都被映射到指定的类型。用于映射列的方法取决于“U”的类型: * * - When `U` is a class, fields for the class will be mapped to columns of the same name * (case sensitivity is determined by `spark.sql.caseSensitive`). * * 当“U”是类时：类的属性将映射到相同名称的列 * * - When `U` is a tuple, the columns will be be mapped by ordinal (i.e. the first column will * be assigned to `_1`). * * 当“U”是元组时：列将由序数映射 （例如，第一列将为 \"_1\"） * * - When `U` is a primitive type (i.e. String, Int, etc), then the first column of the * `DataFrame` will be used. * * 当“U”是 基本类型（如 String，Int等）：然后将使用“DataFrame”的第一列。 * * If the schema of the Dataset does not match the desired `U` type, you can use `select` * along with `alias` or `as` to rearrange or rename as required. * * 如果数据集的模式与所需的“U”类型不匹配，您可以使用“select”和“alias”或“as”来重新排列或重命名。 * * @group basic * @since 1.6.0 */ @Experimental @InterfaceStability.Evolving def as[U: Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan) schema12345678/** * Returns the schema of this Dataset. * 返回该Dataset的模版 * * @group basic * @since 1.6.0 */def schema: StructType = queryExecution.analyzed.schema printSchema12345678910/** * Prints the schema to the console in a nice tree format. * * 以一种漂亮的树格式将模式打印到控制台。 * * @group basic * @since 1.6.0 */// scalastyle:off printlndef printSchema(): Unit = println(schema.treeString) explain1234567891011121314151617181920212223242526/** * Prints the plans (logical and physical) to the console for debugging purposes. * * 将计划(逻辑和物理)打印到控制台以进行调试。 * 参数：extended = false 为物理计划 * * @group basic * @since 1.6.0 */def explain(extended: Boolean): Unit = &#123; val explain = ExplainCommand(queryExecution.logical, extended = extended) sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach &#123; // scalastyle:off println r =&gt; println(r.getString(0)) // scalastyle:on println &#125;&#125; /** * Prints the physical plan to the console for debugging purposes. * 将物理计划打印到控制台以进行调试。 * * @group basic * @since 1.6.0 */def explain(): Unit = explain(extended = false) dtypes12345678910/** * Returns all column names and their data types as an array. * 以数组的形式返回所有列名称和它们的数据类型 * * @group basic * @since 1.6.0 */def dtypes: Array[(String, String)] = schema.fields.map &#123; field =&gt; (field.name, field.dataType.toString)&#125; columns12345678/** * Returns all column names as an array. * 以数组的形式返回 所有列名 * * @group basic * @since 1.6.0 */def columns: Array[String] = schema.fields.map(_.name) isLocal123456789/** * Returns true if the `collect` and `take` methods can be run locally * (without any Spark executors). * 如果`collect` and `take` 方法能在本地运行，则返回true * * @group basic * @since 1.6.0 */def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation] checkpoint12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/** * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * 急切地检查一个数据集并返回新的数据集。 * 检查点能用来清除Dataset的逻辑计划，尤其是在可能生成指数级别的迭代算法中尤其有用。 * 将会在检查点目录中保存检查文件。可以在`SparkContext#setCheckpointDir`中设置。 * * @group basic * @since 2.1.0 */@Experimental@InterfaceStability.Evolvingdef checkpoint(): Dataset[T] = checkpoint(eager = true) /** * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the * logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * 返回Dataset 之前检查过的版本。 * 检查点能用来清除Dataset的逻辑计划，尤其是在可能生成指数级别的迭代算法中尤其有用。 * 将会在检查点目录中保存检查文件。可以在`SparkContext#setCheckpointDir`中设置。 * * @group basic * @since 2.1.0 */@Experimental@InterfaceStability.Evolvingdef checkpoint(eager: Boolean): Dataset[T] = &#123; val internalRdd = queryExecution.toRdd.map(_.copy()) internalRdd.checkpoint() if (eager) &#123; internalRdd.count() &#125; val physicalPlan = queryExecution.executedPlan // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the // size of `PartitioningCollection` may grow exponentially for queries involving deep inner // joins. // 每当我们看到“PartitioningCollection”时，就采用第一个叶子分区 // 否则，用于涉及深度内连接的查询，“PartitioningCollection”的大小可能会以指数形式增长。 def firstLeafPartitioning(partitioning: Partitioning): Partitioning = &#123; partitioning match &#123; case p: PartitioningCollection =&gt; firstLeafPartitioning(p.partitionings.head) case p =&gt; p &#125; &#125; val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning) Dataset.ofRows( sparkSession, LogicalRDD( logicalPlan.output, internalRdd, outputPartitioning, physicalPlan.outputOrdering )(sparkSession)).as[T]&#125; persist1234567891011121314151617181920212223242526272829303132333435/** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * 持久化。 * 根据默认的 存储级别 (`MEMORY_AND_DISK`) 持久化Dataset。 * * @group basic * @since 1.6.0 */def persist(): this.type = &#123; sparkSession.sharedState.cacheManager.cacheQuery(this) this&#125; /** * Persist this Dataset with the given storage level. * * 根据指定的 存储级别 持久化 Dataset。 * * @param newLevel One of: * `MEMORY_ONLY`, * `MEMORY_AND_DISK`, * `MEMORY_ONLY_SER`, * `MEMORY_AND_DISK_SER`, * `DISK_ONLY`, * `MEMORY_ONLY_2`, 与MEMORY_ONLY的区别是会备份数据到其他节点上 * `MEMORY_AND_DISK_2`, 与MEMORY_AND_DISK的区别是会备份数据到其他节点上 * etc. * @group basic * @since 1.6.0 */def persist(newLevel: StorageLevel): this.type = &#123; sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel) this&#125; cache1234567891011/** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * 持久化。 * 根据默认的 存储级别 (`MEMORY_AND_DISK`) 持久化Dataset。 * 和 persist 一致。 * * @group basic * @since 1.6.0 */def cache(): this.type = persist() storageLevel12345678910111213/** * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted. * * 获取当前Dataset的当前存储级别。如果没有缓存则 StorageLevel.NONE。 * * @group basic * @since 2.1.0 */def storageLevel: StorageLevel = &#123; sparkSession.sharedState.cacheManager.lookupCachedData(this).map &#123; cachedData =&gt; cachedData.cachedRepresentation.storageLevel &#125;.getOrElse(StorageLevel.NONE)&#125; unpersist1234567891011121314151617181920212223242526/** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * * 解除持久化。 * 将Dataset标记为非持久化，并从内存和磁盘中移除所有的块。 * * @param blocking Whether to block until all blocks are deleted. * 是否阻塞，直到删除所有的块。 * @group basic * @since 1.6.0 */def unpersist(blocking: Boolean): this.type = &#123; sparkSession.sharedState.cacheManager.uncacheQuery(this, blocking) this&#125; /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * * 解除持久化。 * 将Dataset标记为非持久化，并从内存和磁盘中移除所有的块。 * * @group basic * @since 1.6.0 */def unpersist(): this.type = unpersist(blocking = false) rdd123456789101112131415/** * Represents the content of the Dataset as an `RDD` of [[T]]. * * 转换为[[T]]的“RDD”，表示Dataset的内容 * * @group basic * @since 1.6.0 */lazy val rdd: RDD[T] = &#123; val objectType = exprEnc.deserializer.dataType val deserialized = CatalystSerde.deserialize[T](logicalPlan) sparkSession.sessionState.executePlan(deserialized).toRdd.mapPartitions &#123; rows =&gt; rows.map(_.get(0, objectType).asInstanceOf[T]) &#125;&#125; toJavaRDD12345678910111213141516171819/** * Returns the content of the Dataset as a `JavaRDD` of [[T]]s. * * 转换为JavaRDD * * @group basic * @since 1.6.0 */def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD() /** * Returns the content of the Dataset as a `JavaRDD` of [[T]]s. * * 转换为JavaRDD * * @group basic * @since 1.6.0 */def javaRDD: JavaRDD[T] = toJavaRDD registerTempTable1234567891011121314/** * Registers this Dataset as a temporary table using the given name. The lifetime of this * temporary table is tied to the [[SparkSession]] that was used to create this Dataset. * * 根据指定的表名，注册临时表。 * 生命周期为[[SparkSession]]的生命周期。 * * @group basic * @since 1.6.0 */@deprecated(\"Use createOrReplaceTempView(viewName) instead.\", \"2.0.0\")def registerTempTable(tableName: String): Unit = &#123; createOrReplaceTempView(tableName)&#125; createTempView123456789101112131415161718192021/** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * 用指定的名字创建本地临时表。 * 与[[SparkSession]] 同生命周期。 * * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that * created it, i.e. it will be automatically dropped when the session terminates. It's not * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view. * * 本地临时表是 session范围内的。当创建它的session停止的时候，该表也随之停止。 * * @throws AnalysisException if the view name already exists * @group basic * @since 2.0.0 */@throws[AnalysisException]def createTempView(viewName: String): Unit = withPlan &#123; createTempViewCommand(viewName, replace = false, global = false)&#125; createOrReplaceTempView123456789101112/** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * 用指定的名字创建本地临时表。如果已经有了则替换。 * * @group basic * @since 2.0.0 */def createOrReplaceTempView(viewName: String): Unit = withPlan &#123; createTempViewCommand(viewName, replace = true, global = false)&#125; createGlobalTempView1234567891011121314151617181920212223/** * Creates a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * 创建全局临时表。 * 生命周期为整个Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `_global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM _global_temp.view1`. * * 全局临时表是跨session的。属于 _global_temp 数据库。e.g. `SELECT * FROM _global_temp.view1`. * * @throws AnalysisException if the view name already exists * 如果表已经存在，则报错。 * @group basic * @since 2.1.0 */@throws[AnalysisException]def createGlobalTempView(viewName: String): Unit = withPlan &#123; createTempViewCommand(viewName, replace = false, global = true)&#125; write123456789101112131415/** * Interface for saving the content of the non-streaming Dataset out into external storage. * * 将非流Dataset的内容保存到外部存储中的接口。 * * @group basic * @since 1.6.0 */def write: DataFrameWriter[T] = &#123; if (isStreaming) &#123; logicalPlan.failAnalysis( \"'write' can not be called on streaming Dataset/DataFrame\") &#125; new DataFrameWriter[T](this)&#125; writeStream123456789101112131415161718/** * :: Experimental :: * Interface for saving the content of the streaming Dataset out into external storage. * * 将流Dataset保存在外部存储。 * * @group basic * @since 2.0.0 */@Experimental@InterfaceStability.Evolvingdef writeStream: DataStreamWriter[T] = &#123; if (!isStreaming) &#123; logicalPlan.failAnalysis( \"'writeStream' can be called only on streaming Dataset/DataFrame\") &#125; new DataStreamWriter[T](this)&#125; toJSON1234567891011121314151617181920212223242526272829303132333435/** * Returns the content of the Dataset as a Dataset of JSON strings. * * 将Dataset转换为JSON。 * * @since 2.0.0 */def toJSON: Dataset[String] = &#123; val rowSchema = this.schema val rdd: RDD[String] = queryExecution.toRdd.mapPartitions &#123; iter =&gt; val writer = new CharArrayWriter() // create the Generator without separator inserted between 2 records val gen = new JacksonGenerator(rowSchema, writer) new Iterator[String] &#123; override def hasNext: Boolean = iter.hasNext override def next(): String = &#123; gen.write(iter.next()) gen.flush() val json = writer.toString if (hasNext) &#123; writer.reset() &#125; else &#123; gen.close() &#125; json &#125; &#125; &#125; import sparkSession.implicits.newStringEncoder sparkSession.createDataset(rdd)&#125; inputFiles12345678910111213141516171819202122/** * Returns a best-effort snapshot of the files that compose this Dataset. This method simply * asks each constituent BaseRelation for its respective files and takes the union of all results. * Depending on the source relations, this may not find all input files. Duplicates are removed. * * 返回组成这个Dataset的所有文件的最佳快照。 * 该方法简单地要求每个组件BaseRelation对其各自的文件进行处理，并联合所有结果。 * 基于源关系，应该可以找到所有的输入文件。 * 重复的也会被移除。 * * @group basic * @since 2.0.0 */def inputFiles: Array[String] = &#123; val files: Seq[String] = queryExecution.optimizedPlan.collect &#123; case LogicalRelation(fsBasedRelation: FileRelation, _, _) =&gt; fsBasedRelation.inputFiles case fr: FileRelation =&gt; fr.inputFiles &#125;.flatten files.toSet.toArray&#125; streamingisStreaming123456789101112131415161718/** * Returns true if this Dataset contains one or more sources that continuously * return data as it arrives. A Dataset that reads data from a streaming source * must be executed as a `StreamingQuery` using the `start()` method in * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or * `collect()`, will throw an [[AnalysisException]] when there is a streaming * source present. * * 如果Dataset包含一个或多个持续返回数据的源，则返回true； * 如果Dataset从streaming源读取数据，则必须像 `StreamingQuery` 一样执行：使用 `DataStreamWriter` 中的 `start()`方法。 * 返回单个值的方法，例如： `count()` or `collect()`，当存在streaming源时，将会抛出[[AnalysisException]]。 * * @group streaming * @since 2.0.0 */@Experimental@InterfaceStability.Evolvingdef isStreaming: Boolean = logicalPlan.isStreaming withWatermark123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * :: Experimental :: 实验性的 * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time * before which we assume no more late data is going to arrive. * * 为这个[[Dataset]]定义事件时间水印。 * 我们假设没有更多的晚期数据将到达之前，一个水印跟踪一个时间点。 * * Spark will use this watermark for several purposes: * Spark用水印有几个目的： * - To know when a given time window aggregation can be finalized and thus can be emitted when * using output modes that do not allow updates. * * 可以知道何时完成给定的时间窗口聚合能够完成，因此当使用不允许更新的输出模式时能够被放出。 * - To minimize the amount of state that we need to keep for on-going aggregations. * 为了最小化我们需要持续不断的聚合的状态数量。 * * * The current watermark is computed by looking at the `MAX(eventTime)` seen across * all of the partitions in the query minus a user specified `delayThreshold`. Due to the cost * of coordinating this value across partitions, the actual watermark used is only guaranteed * to be at least `delayThreshold` behind the actual event time. In some cases we may still * process records that arrive more than `delayThreshold` late. * * 当前的水印 = 查看查询中所有分区上看到的`MAX(eventTime)` - 用户指定的`delayThreshold` * 由于在分区之间协调这个值的花销，实际使用的水印只保证在实际事件时间后至少是“delayThreshold”。 * 在某些情况下，我们可能还会处理比“delayThreshold”晚些时候到达的记录。 * * @param eventTime the name of the column that contains the event time of the row. * 包含行的事件时间的列名 * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest * record that has been processed in the form of an interval * (e.g. \"1 minute\" or \"5 hours\"). * 等待晚到数据的最少延迟，相对于以间隔形式处理的最新记录 * @group streaming * @since 2.1.0 */@Experimental@InterfaceStability.Evolving// We only accept an existing column name, not a derived column here as a watermark that is// defined on a derived column cannot referenced elsewhere in the plan.// 我们只接受一个现有的列名，而不是作为一个在派生列上定义的水印的派生列，而不能在该计划的其他地方引用。def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan &#123; val parsedDelay = Option(CalendarInterval.fromString(\"interval \" + delayThreshold)) .getOrElse(throw new AnalysisException(s\"Unable to parse time delay '$delayThreshold'\")) EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan)&#125; actionshow123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990/** * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated, * and all cells will be aligned right. For example: * * 以表格形式显示数据集。 * 字符串超过20个字符将被截断， * 所有单元格将被对齐。 * &#123;&#123;&#123; * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * &#125;&#125;&#125; * * @param numRows Number of rows to show 要显示的行数 * @group action * @since 1.6.0 */def show(numRows: Int): Unit = show(numRows, truncate = true) /** * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters * will be truncated, and all cells will be aligned right. * 显示头20行 * * @group action * @since 1.6.0 */def show(): Unit = show(20)/** * Displays the top 20 rows of Dataset in a tabular form. * 显示头20行 * * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * 是否截断长字符串。如果 true：超过20个字符就会被截断 * @group action * @since 1.6.0 */def show(truncate: Boolean): Unit = show(20, truncate)/** * Displays the Dataset in a tabular form. For example: * &#123;&#123;&#123; * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * &#125;&#125;&#125; * * @param numRows Number of rows to show 显示的行数 * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * 是否截断长字符串 * @group action * @since 1.6.0 */// scalastyle:off printlndef show(numRows: Int, truncate: Boolean): Unit = if (truncate) &#123; println(showString(numRows, truncate = 20))&#125; else &#123; println(showString(numRows, truncate = 0))&#125;// scalastyle:on println/** * Displays the Dataset in a tabular form. For example: * &#123;&#123;&#123; * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * &#125;&#125;&#125; * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * 设置 触发截断字符串的阈值 * @group action * @since 1.6.0 */// scalastyle:off printlndef show(numRows: Int, truncate: Int): Unit = println(showString(numRows, truncate)) reduce1234567891011121314151617181920212223242526272829/** * :: Experimental :: * (Scala-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * 使用指定的二进制函数减少这个数据集的元素。给定的“func”必须是可交换的和关联的，否则结果可能是不确定性的。 * * @group action * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef reduce(func: (T, T) =&gt; T): T = rdd.reduce(func)/** * :: Experimental :: * (Java-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * 使用指定的二进制函数减少这个数据集的元素。给定的“func”必须是可交换的和关联的，否则结果可能是不确定性的。 * * @group action * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _)) describe123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * Computes statistics for numeric and string columns, including count, mean, stddev, min, and * max. If no columns are given, this function computes statistics for all numerical or string * columns. * * 计算数字和字符串列的统计数据，包括count、mean、stddev、min和max。 * 如果没有给出任何列，该函数计算所有数值或字符串列的统计信息。 * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * 这个函数用于探索性的数据分析，因为我们不能保证生成数据集的模式的向后兼容性。 * 如果您想通过编程计算汇总统计信息，可以使用“agg”函数。 * * &#123;&#123;&#123; * ds.describe(\"age\", \"height\").show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // max 92.0 192.0 * &#125;&#125;&#125; * * @group action * @since 1.6.0 */ @scala.annotation.varargs def describe(cols: String*): DataFrame = withPlan &#123; // The list of summary statistics to compute, in the form of expressions. val statistics = List[(String, Expression =&gt; Expression)]( \"count\" -&gt; ((child: Expression) =&gt; Count(child).toAggregateExpression()), \"mean\" -&gt; ((child: Expression) =&gt; Average(child).toAggregateExpression()), \"stddev\" -&gt; ((child: Expression) =&gt; StddevSamp(child).toAggregateExpression()), \"min\" -&gt; ((child: Expression) =&gt; Min(child).toAggregateExpression()), \"max\" -&gt; ((child: Expression) =&gt; Max(child).toAggregateExpression())) val outputCols = (if (cols.isEmpty) aggregatableColumns.map(usePrettyExpression(_).sql) else cols).toList val ret: Seq[Row] = if (outputCols.nonEmpty) &#123; val aggExprs = statistics.flatMap &#123; case (_, colToAgg) =&gt; outputCols.map(c =&gt; Column(Cast(colToAgg(Column(c).expr), StringType)).as(c)) &#125; val row = groupBy().agg(aggExprs.head, aggExprs.tail: _*).head().toSeq // Pivot the data so each summary is one row row.grouped(outputCols.size).toSeq.zip(statistics).map &#123; case (aggregation, (statistic, _)) =&gt; Row(statistic :: aggregation.toList: _*) &#125; &#125; else &#123; // If there are no output columns, just output a single column that contains the stats. statistics.map &#123; case (name, _) =&gt; Row(name) &#125; &#125; // All columns are string type val schema = StructType( StructField(\"summary\", StringType) :: outputCols.map(StructField(_, StringType))).toAttributes // `toArray` forces materialization to make the seq serializable LocalRelation.fromExternalRows(schema, ret.toArray.toSeq) &#125; head123456789101112131415161718192021222324/** * Returns the first `n` rows. * * 返回前n行 * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 仅适用于结果很少的时候使用，因为会将结果加载进内存中 * @group action * @since 1.6.0 */def head(n: Int): Array[T] = withTypedCallback(\"head\", limit(n)) &#123; df =&gt; df.collect(needCallback = false)&#125;/** * Returns the first row. * * 返回第一行（默认1） * * @group action * @since 1.6.0 */def head(): T = head(1).head first123456789/** * Returns the first row. Alias for head(). * * 返回第一行 ，与head()一样 * * @group action * @since 1.6.0 */def first(): T = head() foreach12345678910111213141516171819202122/** * Applies a function `f` to all rows. * * 对所有行应用函数f。 * * @group action * @since 1.6.0 */def foreach(f: T =&gt; Unit): Unit = withNewExecutionId &#123; rdd.foreach(f)&#125;/** * (Java-specific) * Runs `func` on each element of this Dataset. * * 在这个数据集的每个元素上运行“func”。 * * @group action * @since 1.6.0 */def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_)) foreachPartition1234567891011121314151617181920212223/** * Applies a function `f` to each partition of this Dataset. * * 对这个数据集的每个分区应用一个函数f。 * * @group action * @since 1.6.0 */def foreachPartition(f: Iterator[T] =&gt; Unit): Unit = withNewExecutionId &#123; rdd.foreachPartition(f)&#125;/** * (Java-specific) * Runs `func` on each partition of this Dataset. * * 对这个数据集的每个分区应用一个函数f。 * * @group action * @since 1.6.0 */def foreachPartition(func: ForeachPartitionFunction[T]): Unit = foreachPartition(it =&gt; func.call(it.asJava)) take123456789101112131415/** * Returns the first `n` rows in the Dataset. * * 返回数据集中的前“n”行。 * 同head(n) * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * take在driver端执行，n太大会造成oom * * @group action * @since 1.6.0 */def take(n: Int): Array[T] = head(n) takeAsList1234567891011121314/** * Returns the first `n` rows in the Dataset as a list. * * 以List形式返回 前n行 * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * take在driver端执行，n太大会造成oom * * @group action * @since 1.6.0 */def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n): _*) collect12345678910111213141516/** * Returns an array that contains all of [[Row]]s in this Dataset. * * 返回包含所有Row的 一个数组 * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * 会将所有数据移动到driver，所以可能会造成oom * * For Java API, use [[collectAsList]]. * * @group action * @since 1.6.0 */def collect(): Array[T] = collect(needCallback = true) collectAsList12345678910111213141516171819/** * Returns a Java list that contains all of [[Row]]s in this Dataset. * * 返回包含所有Row的一个Java List * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * 会将所有数据移动到driver，所以可能会造成oom * * @group action * @since 1.6.0 */def collectAsList(): java.util.List[T] = withCallback(\"collectAsList\", toDF()) &#123; _ =&gt; withNewExecutionId &#123; val values = queryExecution.executedPlan.executeCollect().map(boundEnc.fromRow) java.util.Arrays.asList(values: _*) &#125;&#125; toLocalIterator12345678910111213141516171819202122/** * Return an iterator that contains all of [[Row]]s in this Dataset. * * 返回包含所有Row的一个迭代器 * * The iterator will consume as much memory as the largest partition in this Dataset. * * 迭代器将消耗与此数据集中最大的分区一样多的内存。 * * @note this results in multiple Spark jobs, and if the input Dataset is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input Dataset should be cached first. * 这将导致多个Spark作业，如果输入数据集是宽依赖转换的结果(例如，与不同的分区连接)， * 那么为了避免重新计算输入数据，应该首先缓存输入数据集。 * @group action * @since 2.0.0 */def toLocalIterator(): java.util.Iterator[T] = withCallback(\"toLocalIterator\", toDF()) &#123; _ =&gt; withNewExecutionId &#123; queryExecution.executedPlan.executeToIterator().map(boundEnc.fromRow).asJava &#125;&#125; count1234567891011/** * Returns the number of rows in the Dataset. * * 返回总行数 * * @group action * @since 1.6.0 */def count(): Long = withCallback(\"count\", groupBy().count()) &#123; df =&gt; df.collect(needCallback = false).head.getLong(0)&#125; untypedrel-无类型转换na123456789101112/** * Returns a [[DataFrameNaFunctions]] for working with missing data. * 返回一个用于处理丢失数据的[[DataFrameNaFunctions]]。 * &#123;&#123;&#123; * // Dropping rows containing any null values. 删除包含任何null 值的行 * ds.na.drop() * &#125;&#125;&#125; * * @group untypedrel * @since 1.6.0 */def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF()) stat123456789101112/** * Returns a [[DataFrameStatFunctions]] for working statistic functions support. * 返回用于支持统计功能的[[DataFrameStatFunctions]]。 * &#123;&#123;&#123; * // Finding frequent items in column with name 'a'. 查询列名为\"a\"中的频繁数据。 * ds.stat.freqItems(Seq(\"a\")) * &#125;&#125;&#125; * * @group untypedrel * @since 1.6.0 */def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF()) join123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180/** * Join with another `DataFrame`. * 和 另一个 `DataFrame` jion * * Behaves as an INNER JOIN and requires a subsequent join predicate. * 作为一个内部连接，并需要一个后续的连接谓词。 * * @param right Right side of the join operation. join操作的右侧 * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_]): DataFrame = withPlan &#123; Join(logicalPlan, right.logicalPlan, joinType = Inner, None) &#125; /** * Inner equi-join with another `DataFrame` using the given column. * 给定列名的内部等值连接 * * Different from other join functions, the join column will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * &#123;&#123;&#123; * // Joining df1 and df2 using the column \"user_id\" 用\"user_id\" 连接 df1 和df2 * df1.join(df2, \"user_id\") * &#125;&#125;&#125; * * @param right Right side of the join operation. join连接右侧 * @param usingColumn Name of the column to join on. This column must exist on both sides. * 列名。必须在两边都存在 * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * 自连接的时候，请指定 表别名。不然干不了事 * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumn: String): DataFrame = &#123; join(right, Seq(usingColumn)) &#125; /** * Inner equi-join with another `DataFrame` using the given columns. * 根据指定多个列进行join * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * &#123;&#123;&#123; * // Joining df1 and df2 using the columns \"user_id\" and \"user_name\" * df1.join(df2, Seq(\"user_id\", \"user_name\")) * &#125;&#125;&#125; * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = &#123; join(right, usingColumns, \"inner\") &#125; /** * Equi-join with another `DataFrame` using the given columns. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * @param joinType One of: `inner`, `outer`, `left_outer`, `right_outer`, `leftsemi`. * 连接类型：内连接，外连接，左外连接，右外连接，左内连接 * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = &#123; // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right // by creating a new instance for one of the branch. // 自连接的时候，为其中一个分支创建一个新实例来消除左vs右的歧义。 val joined = sparkSession.sessionState.executePlan( Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None)) .analyzed.asInstanceOf[Join] withPlan &#123; Join( joined.left, joined.right, UsingJoin(JoinType(joinType), usingColumns), None) &#125; &#125; /** * Inner join with another `DataFrame`, using the given join expression. * 用给定的表达式进行join * &#123;&#123;&#123; * // The following two are equivalent: * df1.join(df2, $\"df1Key\" === $\"df2Key\") * df1.join(df2).where($\"df1Key\" === $\"df2Key\") * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, \"inner\") /** * Join with another `DataFrame`, using the given join expression. The following performs * a full outer join between `df1` and `df2`. * * &#123;&#123;&#123; * // Scala: * import org.apache.spark.sql.functions._ * df1.join(df2, $\"df1Key\" === $\"df2Key\", \"outer\") * * // Java: * import static org.apache.spark.sql.functions.*; * df1.join(df2, col(\"df1Key\").equalTo(col(\"df2Key\")), \"outer\"); * &#125;&#125;&#125; * * @param right Right side of the join. * @param joinExprs Join expression. * @param joinType One of: `inner`, `outer`, `left_outer`, `right_outer`, `leftsemi`. * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = &#123; // Note that in this function, we introduce a hack in the case of self-join to automatically // resolve ambiguous join conditions into ones that might make sense [SPARK-6231]. // Consider this case: df.join(df, df(\"key\") === df(\"key\")) // Since df(\"key\") === df(\"key\") is a trivially true condition, this actually becomes a // cartesian join. However, most likely users expect to perform a self join using \"key\". // With that assumption, this hack turns the trivially true condition into equality on join // keys that are resolved to both sides. // Trigger analysis so in the case of self-join, the analyzer will clone the plan. // After the cloning, left and right side will have distinct expression ids. // 针对自连接的优化：正常情况下，自连接如果使用 df.join(df, df(\"key\") === df(\"key\")) // 会造成 笛卡尔积 // 这种情况下，分析器会 克隆计划，克隆完成后，左右两边则有不同的 id val plan = withPlan( Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr))) .queryExecution.analyzed.asInstanceOf[Join] // If auto self join alias is disabled, return the plan. if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) &#123; return withPlan(plan) &#125; // If left/right have no output set intersection, return the plan. val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) &#123; return withPlan(plan) &#125; // Otherwise, find the trivially true predicates and automatically resolves them to both sides. // By the time we get here, since we have already run analysis, all attributes should've been // resolved and become AttributeReference. val cond = plan.condition.map &#123; _.transform &#123; case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference) if a.sameRef(b) =&gt; catalyst.expressions.EqualTo( withPlan(plan.left).resolve(a.name), withPlan(plan.right).resolve(b.name)) &#125; &#125; withPlan &#123; plan.copy(condition = cond) &#125; &#125; crossJoin12345678910111213/** * Explicit cartesian join with another `DataFrame`. * 显式笛卡尔积join * * @param right Right side of the join operation. * @note Cartesian joins are very expensive without an extra filter that can be pushed down. * 如果没有额外的过滤器，笛卡尔连接非常昂贵。 * @group untypedrel * @since 2.1.0 */def crossJoin(right: Dataset[_]): DataFrame = withPlan &#123; Join(logicalPlan, right.logicalPlan, joinType = Cross, None)&#125; apply123456789101112/** * Selects column based on the column name and return it as a [[Column]]. * * 选择基于列名的列，并将其作为[[Column]]返回。 * * @note The column name can also reference to a nested column like `a.b`. * * 列名也可以引用像“a.b”这样的嵌套列。 * @group untypedrel * @since 2.0.0 */def apply(colName: String): Column = col(colName) col123456789101112131415161718/** * Selects column based on the column name and return it as a [[Column]]. * * 选择基于列名的列，并将其作为[[Column]]返回。 * * @note The column name can also reference to a nested column like `a.b`. * * 列名也可以引用像“a.b”这样的嵌套列。 * @group untypedrel * @since 2.0.0 */def col(colName: String): Column = colName match &#123; case \"*\" =&gt; Column(ResolvedStar(queryExecution.analyzed.output)) case _ =&gt; val expr = resolve(colName) Column(expr)&#125; select12345678910111213141516171819202122232425262728293031/** * Selects a set of column based expressions. * &#123;&#123;&#123; * ds.select($\"colA\", $\"colB\" + 1) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */@scala.annotation.varargsdef select(cols: Column*): DataFrame = withPlan &#123; Project(cols.map(_.named), logicalPlan)&#125;/** * Selects a set of columns. This is a variant of `select` that can only select * existing columns using column names (i.e. cannot construct expressions). * * 只能是已经存在的列名 * * &#123;&#123;&#123; * // The following two are equivalent: * ds.select(\"colA\", \"colB\") * ds.select($\"colA\", $\"colB\") * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */@scala.annotation.varargsdef select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)): _*) selectExpr12345678910111213141516171819202122/** * Selects a set of SQL expressions. This is a variant of `select` that accepts * SQL expressions. * * 接受SQL表达式 * * &#123;&#123;&#123; * // The following are equivalent: * 以下是等价的: * ds.selectExpr(\"colA\", \"colB as newName\", \"abs(colC)\") * ds.select(expr(\"colA\"), expr(\"colB as newName\"), expr(\"abs(colC)\")) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */@scala.annotation.varargsdef selectExpr(exprs: String*): DataFrame = &#123; select(exprs.map &#123; expr =&gt; Column(sparkSession.sessionState.sqlParser.parseExpression(expr)) &#125;: _*)&#125; groupBy123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * Groups the Dataset using the specified columns, so we can run aggregation on them. See * [[RelationalGroupedDataset]] for all the available aggregate functions. * * 使用指定的列对数据集进行分组，这样我们就可以对它们进行聚合。 * 查看[[RelationalGroupedDataset]]为所有可用的聚合函数。 * * * &#123;&#123;&#123; * // Compute the average for all numeric columns grouped by department. * * 计算按部门分组的所有数字列的平均值。 * * ds.groupBy($\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -&gt; \"avg\", * \"age\" -&gt; \"max\" * )) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */@scala.annotation.varargsdef groupBy(cols: Column*): RelationalGroupedDataset = &#123; RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)&#125; /** * Groups the Dataset using the specified columns, so that we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of groupBy that can only group by existing columns using column names * (i.e. cannot construct expressions). * * &#123;&#123;&#123; * // Compute the average for all numeric columns grouped by department. * ds.groupBy(\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -&gt; \"avg\", * \"age\" -&gt; \"max\" * )) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */@scala.annotation.varargsdef groupBy(col1: String, cols: String*): RelationalGroupedDataset = &#123; val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.GroupByType)&#125; rollup1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * 使用指定的列为当前数据集创建多维的汇总，因此我们可以在它们上运行聚合。 * * * &#123;&#123;&#123; * // Compute the average for all numeric columns rolluped by department and group. * * 汇总后 求平均值 * * ds.rollup($\"department\", $\"group\").avg() * * // Compute the max age and average salary, rolluped by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -&gt; \"avg\", * \"age\" -&gt; \"max\" * )) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def rollup(cols: Column*): RelationalGroupedDataset = &#123; RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType) &#125; /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * 使用指定的列为当前数据集创建多维的rollup，因此我们可以在它们上运行聚合。 * rollup可以实现 从右到左一次递减的多级统计，显示统计某一层次结构的聚合 * 例如 rollup(a,b,c,d) =结果=&gt; (a,b,c,d),(a,b,c),(a,b),a * * This is a variant of rollup that can only group by existing columns using column names * (i.e. cannot construct expressions). * * &#123;&#123;&#123; * // Compute the average for all numeric columns rolluped by department and group. * ds.rollup(\"department\", \"group\").avg() * * // Compute the max age and average salary, rolluped by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -&gt; \"avg\", * \"age\" -&gt; \"max\" * )) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def rollup(col1: String, cols: String*): RelationalGroupedDataset = &#123; val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.RollupType) &#125; cube123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * 使用指定的列为当前数据集创建多维数据集，因此我们可以在它们上运行聚合。 * * * &#123;&#123;&#123; * // Compute the average for all numeric columns cubed by department and group. * ds.cube($\"department\", $\"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -&gt; \"avg\", * \"age\" -&gt; \"max\" * )) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def cube(cols: Column*): RelationalGroupedDataset = &#123; RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType) &#125; /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * 魔方 例如：cube(a,b,c) =结果=&gt; (a,b),(a,c),a,(b,c),b,c 结果为所有的维度 * 使用指定的列为当前数据集创建多维多维数据集，因此我们可以在它们上运行聚合。 * * This is a variant of cube that can only group by existing columns using column names * (i.e. cannot construct expressions). * * 这是一个多维数据集的变体，它只能通过使用列名的现有列来分组 * * &#123;&#123;&#123; * // Compute the average for all numeric columns cubed by department and group. * ds.cube(\"department\", \"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -&gt; \"avg\", * \"age\" -&gt; \"max\" * )) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def cube(col1: String, cols: String*): RelationalGroupedDataset = &#123; val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.CubeType) &#125; agg123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * (Scala-specific) Aggregates on the entire Dataset without groups. * 对整个数据集进行聚合，无需分组。 * &#123;&#123;&#123; * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(\"age\" -&gt; \"max\", \"salary\" -&gt; \"avg\") * ds.groupBy().agg(\"age\" -&gt; \"max\", \"salary\" -&gt; \"avg\") * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = &#123; groupBy().agg(aggExpr, aggExprs: _*) &#125; /** * (Scala-specific) Aggregates on the entire Dataset without groups. * 对整个数据集进行聚合，无需分组。 * * &#123;&#123;&#123; * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -&gt; \"max\", \"salary\" -&gt; \"avg\")) * ds.groupBy().agg(Map(\"age\" -&gt; \"max\", \"salary\" -&gt; \"avg\")) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs) /** * (Java-specific) Aggregates on the entire Dataset without groups. * * 对整个数据集进行聚合，无需分组。 * * &#123;&#123;&#123; * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -&gt; \"max\", \"salary\" -&gt; \"avg\")) * ds.groupBy().agg(Map(\"age\" -&gt; \"max\", \"salary\" -&gt; \"avg\")) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs) /** * Aggregates on the entire Dataset without groups. * * 对整个数据集进行聚合，无需分组。 * * &#123;&#123;&#123; * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(max($\"age\"), avg($\"salary\")) * ds.groupBy().agg(max($\"age\"), avg($\"salary\")) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs: _*) explode12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091/** * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of * the input row are implicitly joined with each row that is output by the function. * * 根据提供的方法，该数据集的每一行都被扩展为零个或更多的行，返回一个新的数据集。 * 这类似于HiveQL的“LATERAL VIEW”。 * 输入行的列 隐式地加入了由函数输出的每一行。 * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count * the number of books that contain a given word: * * 考虑到这已经被弃用，作为替代，您可以使用“functions.explode()”或“flatMap()”来引爆列。 * 下面的示例使用这些替代方法来计算包含给定单词的图书的数量: * * &#123;&#123;&#123; * case class Book(title: String, words: String) * val ds: Dataset[Book] * * val allWords = ds.select('title, explode(split('words, \" \")).as(\"word\")) * * val bookCountPerWord = allWords.groupBy(\"word\").agg(countDistinct(\"title\")) * &#125;&#125;&#125; * * Using `flatMap()` this can similarly be exploded as: * * &#123;&#123;&#123; * ds.flatMap(_.words.split(\" \")) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 已经过时，用 flatMap() 或 functions.explode() 代替 */@deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\")def explode[A &lt;: Product : TypeTag](input: Column*)(f: Row =&gt; TraversableOnce[A]): DataFrame = &#123; val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType] val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema) val rowFunction = f.andThen(_.map(convert(_).asInstanceOf[InternalRow])) val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr)) withPlan &#123; Generate(generator, join = true, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) &#125;&#125;/** * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All * columns of the input row are implicitly joined with each value that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()`: * * &#123;&#123;&#123; * ds.select(explode(split('words, \" \")).as(\"word\")) * &#125;&#125;&#125; * * or `flatMap()`: * * &#123;&#123;&#123; * ds.flatMap(_.words.split(\" \")) * &#125;&#125;&#125; * * @group untypedrel * @since 2.0.0 */@deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\")def explode[A, B: TypeTag](inputColumn: String, outputColumn: String)(f: A =&gt; TraversableOnce[B]): DataFrame = &#123; val dataType = ScalaReflection.schemaFor[B].dataType val attributes = AttributeReference(outputColumn, dataType)() :: Nil // TODO handle the metadata? val elementSchema = attributes.toStructType def rowFunction(row: Row): TraversableOnce[InternalRow] = &#123; val convert = CatalystTypeConverters.createToCatalystConverter(dataType) f(row(0).asInstanceOf[A]).map(o =&gt; InternalRow(convert(o))) &#125; val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil) withPlan &#123; Generate(generator, join = true, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) &#125;&#125; withColumn1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * Returns a new Dataset by adding a column or replacing the existing column that has * the same name. * 通过添加一个列或替换具有相同名称的现有列返回新的数据集。 * * @group untypedrel * @since 2.0.0 */def withColumn(colName: String, col: Column): DataFrame = &#123; val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val shouldReplace = output.exists(f =&gt; resolver(f.name, colName)) if (shouldReplace) &#123; val columns = output.map &#123; field =&gt; if (resolver(field.name, colName)) &#123; col.as(colName) &#125; else &#123; Column(field) &#125; &#125; select(columns: _*) &#125; else &#123; select(Column(\"*\"), col.as(colName)) &#125;&#125;/** * Returns a new Dataset by adding a column with metadata. * 通过添加带有元数据的列返回一个新的数据集。 */private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame = &#123; val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val shouldReplace = output.exists(f =&gt; resolver(f.name, colName)) if (shouldReplace) &#123; val columns = output.map &#123; field =&gt; if (resolver(field.name, colName)) &#123; col.as(colName, metadata) &#125; else &#123; Column(field) &#125; &#125; select(columns: _*) &#125; else &#123; select(Column(\"*\"), col.as(colName, metadata)) &#125;&#125; withColumnRenamed1234567891011121314151617181920212223242526/** * Returns a new Dataset with a column renamed. * This is a no-op if schema doesn't contain existingName. * 返回一个重命名的列的新数据集。 * 如果模式不包含存在名称，那么这是不操作的。 * * @group untypedrel * @since 2.0.0 */def withColumnRenamed(existingName: String, newName: String): DataFrame = &#123; val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val shouldRename = output.exists(f =&gt; resolver(f.name, existingName)) if (shouldRename) &#123; val columns = output.map &#123; col =&gt; if (resolver(col.name, existingName)) &#123; Column(col).as(newName) &#125; else &#123; Column(col) &#125; &#125; select(columns: _*) &#125; else &#123; toDF() &#125;&#125; drop1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain * column name. * * 返回删除指定列之后的新Dataset * * This method can only be used to drop top level columns. the colName string is treated * literally without further interpretation. * * 仅用于删除顶层的列 * * @group untypedrel * @since 2.0.0 */def drop(colName: String): DataFrame = &#123; drop(Seq(colName): _*)&#125;/** * Returns a new Dataset with columns dropped. * This is a no-op if schema doesn't contain column name(s). * * 删除指定的多个列，并返回新的dataset * * This method can only be used to drop top level columns. the colName string is treated literally * without further interpretation. * * @group untypedrel * @since 2.0.0 */@scala.annotation.varargsdef drop(colNames: String*): DataFrame = &#123; val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output val remainingCols = allColumns.filter &#123; attribute =&gt; colNames.forall(n =&gt; !resolver(attribute.name, n)) &#125;.map(attribute =&gt; Column(attribute)) if (remainingCols.size == allColumns.size) &#123; toDF() &#125; else &#123; this.select(remainingCols: _*) &#125;&#125;/** * Returns a new Dataset with a column dropped. * This version of drop accepts a [[Column]] rather than a name. * This is a no-op if the Dataset doesn't have a column * with an equivalent expression. * * 删除指定的 列（根据Column） * * @group untypedrel * @since 2.0.0 */def drop(col: Column): DataFrame = &#123; val expression = col match &#123; case Column(u: UnresolvedAttribute) =&gt; queryExecution.analyzed.resolveQuoted( u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u) case Column(expr: Expression) =&gt; expr &#125; val attrs = this.logicalPlan.output val colsAfterDrop = attrs.filter &#123; attr =&gt; attr != expression &#125;.map(attr =&gt; Column(attr)) select(colsAfterDrop: _*)&#125; typedrel-有类型的转换joinWith123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108/** * :: Experimental :: 实验的 * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to * true. * 连接这个数据集返回一个“Tuple2”对每一对的“条件”计算为true。 * * This is similar to the relation `join` function with one important difference in the * result schema. Since `joinWith` preserves objects present on either side of the join, the * result schema is similarly nested into a tuple under the column names `_1` and `_2`. * 这类似于关系“join”函数，在结果模式中有一个重要的区别。 * 由于“joinWith”保存了连接的任何一边的对象，因此结果模式类似地嵌套在列名称“_1”和“_2”下面的tuple中。 * * This type of join can be useful both for preserving type-safety with the original object * types as well as working with relational data where either side of the join has column * names in common. * 这种类型的联接既可以用于保存与原始对象类型的类型安全性， * 也可以用于处理连接的任何一端都有列名的关系数据。 * * @param other Right side of the join. * @param condition Join expression. * @param joinType One of: `inner`, `outer`, `left_outer`, `right_outer`, `leftsemi`. * @group typedrel * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = &#123; // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved, // 创建一个联接节点并首先解析它，使Join条件得到解析，self - Join解析， // etc. val joined = sparkSession.sessionState.executePlan( Join( this.logicalPlan, other.logicalPlan, JoinType(joinType), Some(condition.expr))).analyzed.asInstanceOf[Join] // For both join side, combine all outputs into a single column and alias it with \"_1\" or \"_2\", // to match the schema for the encoder of the join result. // 对于这两个连接，将所有输出合并为一个列，并将其别名为“_1”或“_2”，以匹配连接结果的编码器的模式。 // Note that we do this before joining them, to enable the join operator to return null for one // side, in cases like outer-join. // 请注意，在join它们之前，我们这样做，使join操作符在像outer - join这样的情况下返回null。 val left = &#123; val combined = if (this.exprEnc.flat) &#123; assert(joined.left.output.length == 1) Alias(joined.left.output.head, \"_1\")() &#125; else &#123; Alias(CreateStruct(joined.left.output), \"_1\")() &#125; Project(combined :: Nil, joined.left) &#125; val right = &#123; val combined = if (other.exprEnc.flat) &#123; assert(joined.right.output.length == 1) Alias(joined.right.output.head, \"_2\")() &#125; else &#123; Alias(CreateStruct(joined.right.output), \"_2\")() &#125; Project(combined :: Nil, joined.right) &#125; // Rewrites the join condition to make the attribute point to correct column/field, after we // combine the outputs of each join side. // 在将每个连接的输出组合在一起之后,重写联接条件，使属性指向正确的列/字段。 val conditionExpr = joined.condition.get transformUp &#123; case a: Attribute if joined.left.outputSet.contains(a) =&gt; if (this.exprEnc.flat) &#123; left.output.head &#125; else &#123; val index = joined.left.output.indexWhere(_.exprId == a.exprId) GetStructField(left.output.head, index) &#125; case a: Attribute if joined.right.outputSet.contains(a) =&gt; if (other.exprEnc.flat) &#123; right.output.head &#125; else &#123; val index = joined.right.output.indexWhere(_.exprId == a.exprId) GetStructField(right.output.head, index) &#125; &#125; implicit val tuple2Encoder: Encoder[(T, U)] = ExpressionEncoder.tuple(this.exprEnc, other.exprEnc) withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))&#125;/** * :: Experimental :: * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair * where `condition` evaluates to true. * * 使用内部的等连接加入这个数据集，为每一对返回一个“Tuple2”，其中“条件”的计算结果为true。 * * @param other Right side of the join. * @param condition Join expression. * @group typedrel * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = &#123; joinWith(other, condition, \"inner\")&#125; sortWithinPartitions123456789101112131415161718192021222324252627282930313233/** * Returns a new Dataset with each partition sorted by the given expressions. * * 返回一个新的数据集，每个分区按照给定的表达式排序。 * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * 这与SQL(Hive QL)中“SORT BY”的操作相同。 * * @group typedrel * @since 2.0.0 */@scala.annotation.varargsdef sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = &#123; sortWithinPartitions((sortCol +: sortCols).map(Column(_)): _*)&#125;/** * Returns a new Dataset with each partition sorted by the given expressions. * * 返回一个新的数据集，每个分区按照给定的表达式排序。 * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * 这与SQL(Hive QL)中“SORT BY”的操作相同。 * * @group typedrel * @since 2.0.0 */@scala.annotation.varargsdef sortWithinPartitions(sortExprs: Column*): Dataset[T] = &#123; sortInternal(global = false, sortExprs)&#125; sort1234567891011121314151617181920212223242526272829303132333435/** * Returns a new Dataset sorted by the specified column, all in ascending order. * 排序 升序 * &#123;&#123;&#123; * // The following 3 are equivalent * 下面3个是等价的 * ds.sort(\"sortcol\") * ds.sort($\"sortcol\") * ds.sort($\"sortcol\".asc) * &#125;&#125;&#125; * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sort(sortCol: String, sortCols: String*): Dataset[T] = &#123; sort((sortCol +: sortCols).map(apply): _*) &#125; /** * Returns a new Dataset sorted by the given expressions. For example: * * 返回一个由给定表达式排序的新数据集。例如: * * &#123;&#123;&#123; * ds.sort($\"col1\", $\"col2\".desc) * &#125;&#125;&#125; * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sort(sortExprs: Column*): Dataset[T] = &#123; sortInternal(global = true, sortExprs) &#125; orderBy123456789101112131415161718192021/** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * 这是“sort”函数的别名。 * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols: _*) /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * 这是“sort”函数的别名。 * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs: _*) as12345678910111213141516171819/** * Returns a new Dataset with an alias set. * * 返回一个具有别名集的新数据集。 * * @group typedrel * @since 1.6.0 */ def as(alias: String): Dataset[T] = withTypedPlan &#123; SubqueryAlias(alias, logicalPlan, None) &#125; /** * (Scala-specific) Returns a new Dataset with an alias set. * * @group typedrel * @since 2.0.0 */ def as(alias: Symbol): Dataset[T] = as(alias.name) alias12345678910111213141516/** * Returns a new Dataset with an alias set. Same as `as`. * 返回一个具有别名集的新数据集。与“as”相同。 * * @group typedrel * @since 2.0.0 */ def alias(alias: String): Dataset[T] = as(alias) /** * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0 */ def alias(alias: Symbol): Dataset[T] = as(alias) select1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/** * :: Experimental :: * Returns a new Dataset by computing the given [[Column]] expression for each element. * * 通过计算每个元素的给定[[列]]表达式返回一个新的数据集。 * * &#123;&#123;&#123; * val ds = Seq(1, 2, 3).toDS() * val newDS = ds.select(expr(\"value + 1\").as[Int]) * &#125;&#125;&#125; * * @group typedrel * @since 1.6.0 */ @Experimental @InterfaceStability.Evolving def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = &#123; implicit val encoder = c1.encoder val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan) if (encoder.flat) &#123; new Dataset[U1](sparkSession, project, encoder) &#125; else &#123; // Flattens inner fields of U1 // 使U1的内部区域变平 new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1) &#125; &#125; /** * :: Experimental :: * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ @Experimental @InterfaceStability.Evolving def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] = selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]] /** * :: Experimental :: * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ @Experimental @InterfaceStability.Evolving def select[U1, U2, U3]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] = selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]] /** * :: Experimental :: * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ @Experimental @InterfaceStability.Evolving def select[U1, U2, U3, U4]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] = selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]] /** * :: Experimental :: * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ @Experimental @InterfaceStability.Evolving def select[U1, U2, U3, U4, U5]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4], c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] = selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]] filter12345678910111213141516171819202122232425262728293031323334/** * Filters rows using the given condition. * * 用给定的条件过滤rows * * &#123;&#123;&#123; * // The following are equivalent: * 以下是等价的： * peopleDs.filter($\"age\" &gt; 15) * peopleDs.where($\"age\" &gt; 15) * &#125;&#125;&#125; * * @group typedrel * @since 1.6.0 */ def filter(condition: Column): Dataset[T] = withTypedPlan &#123; Filter(condition.expr, logicalPlan) &#125; /** * Filters rows using the given SQL expression. * * 用给定的 SQL 表达式 过滤rows * * &#123;&#123;&#123; * peopleDs.filter(\"age &gt; 15\") * &#125;&#125;&#125; * * @group typedrel * @since 1.6.0 */ def filter(conditionExpr: String): Dataset[T] = &#123; filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) &#125; where1234567891011121314151617181920212223242526272829303132/** * Filters rows using the given condition. This is an alias for `filter`. * * 使用给定条件过滤行。 * 这是“filter”的别名。 * * &#123;&#123;&#123; * // The following are equivalent: * peopleDs.filter($\"age\" &gt; 15) * peopleDs.where($\"age\" &gt; 15) * &#125;&#125;&#125; * * @group typedrel * @since 1.6.0 */ def where(condition: Column): Dataset[T] = filter(condition) /** * Filters rows using the given SQL expression. * * 使用给定的 SQL 表达式 过滤 rows * * &#123;&#123;&#123; * peopleDs.where(\"age &gt; 15\") * &#125;&#125;&#125; * * @group typedrel * @since 1.6.0 */ def where(conditionExpr: String): Dataset[T] = &#123; filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) &#125; groupByKey12345678910111213141516171819202122232425262728293031323334353637/** * :: Experimental :: * (Scala-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * 返回一个[[KeyValueGroupedDataset]]，数据由给定键' func '分组。 * * @group typedrel * @since 2.0.0 */@Experimental@InterfaceStability.Evolvingdef groupByKey[K: Encoder](func: T =&gt; K): KeyValueGroupedDataset[K, T] = &#123; val inputPlan = logicalPlan val withGroupingKey = AppendColumns(func, inputPlan) val executed = sparkSession.sessionState.executePlan(withGroupingKey) new KeyValueGroupedDataset( encoderFor[K], encoderFor[T], executed, inputPlan.output, withGroupingKey.newColumns)&#125;/** * :: Experimental :: * (Java-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * 返回一个[[KeyValueGroupedDataset]]，数据由给定键' func '分组。 * * @group typedrel * @since 2.0.0 */@Experimental@InterfaceStability.Evolvingdef groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] = groupByKey(func.call(_))(encoder) limit123456789101112131415/** * Returns a new Dataset by taking the first `n` rows. The difference between this function * and `head` is that `head` is an action and returns an array (by triggering query execution) * while `limit` returns a new Dataset. * * 通过使用第一个“n”行返回一个新的数据集。 * 这个函数和“head”的区别在于“head”是一个动作， * 并返回一个数组(通过触发查询执行)，而“limit”则返回一个新的数据集。 * * @group typedrel * @since 2.0.0 */def limit(n: Int): Dataset[T] = withTypedPlan &#123; Limit(Literal(n), logicalPlan)&#125; unionAll-已过时1234567891011121314151617/** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * This is equivalent to `UNION ALL` in SQL. * * 返回一个新的数据集，该数据集包含该数据集中的行和另一个数据集。 * 这相当于SQL中的“UNION ALL”。 * * To do a SQL-style set union (that does deduplication of elements), use this function followed * by a [[distinct]]. * * 如果需要去重的话，在该方法后继续直接 [[distinct]] * * @group typedrel * @since 2.0.0 已经过时 */@deprecated(\"use union()\", \"2.0.0\")def unionAll(other: Dataset[T]): Dataset[T] = union(other) union123456789101112131415161718192021/** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * This is equivalent to `UNION ALL` in SQL. * * 返回一个新的数据集，该数据集包含该数据集中的行和另一个数据集。 * 这相当于SQL中的“UNION ALL”。 * * To do a SQL-style set union (that does deduplication of elements), use this function followed * by a [[distinct]]. * * 如果需要去重的话，在该方法后继续直接 [[distinct]] * * @group typedrel * @since 2.0.0 */def union(other: Dataset[T]): Dataset[T] = withSetOperator &#123; // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. // 这打破了缓存，但通常是可以的，因为它解决了一个非常具体的用例:使用union来联合许多文件或分区。 CombineUnions(Union(logicalPlan, other.logicalPlan))&#125; intersect-交集123456789101112131415161718/** * Returns a new Dataset containing rows only in both this Dataset and another Dataset. * This is equivalent to `INTERSECT` in SQL. * * 返回一个新的数据集，只包含该数据集和另一个数据集相同的行. * 这相当于在SQL中“INTERSECT”。 * 会去重. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * 等式检查直接执行数据的编码表示，因此不受定义为“T”的自定义“equals”函数的影响。 * @group typedrel * @since 1.6.0 */def intersect(other: Dataset[T]): Dataset[T] = withSetOperator &#123; Intersect(logicalPlan, other.logicalPlan)&#125; except-只显示另个Dataset中没有的值12345678910111213141516/** * Returns a new Dataset containing rows in this Dataset but not in another Dataset. * This is equivalent to `EXCEPT` in SQL. * * 返回一个新的数据集，该数据集包含该数据集中的行，而不是在另一个数据集。 * 这等价于SQL中的“EXCEPT”。 * 会去重. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * @group typedrel * @since 2.0.0 */def except(other: Dataset[T]): Dataset[T] = withSetOperator &#123; Except(logicalPlan, other.logicalPlan)&#125; sample-随机抽样1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed. * * 通过使用用户提供的种子，通过抽样的方式返回一个新的[[Dataset]]。 * * @param withReplacement Sample with replacement or not. * 样本已经取过的值是否放回 * @param fraction Fraction of rows to generate. * 每一行数据被取样的概率 * @param seed Seed for sampling. * 取样种子（与随机数生成有关） * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * 不能保证准确的按照给定的分数取样。（一般结果会在概率值*总数左右） * @group typedrel * @since 1.6.0 */def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = &#123; require(fraction &gt;= 0, s\"Fraction must be nonnegative, but got $&#123;fraction&#125;\") withTypedPlan &#123; Sample(0.0, fraction, withReplacement, seed, logicalPlan)() &#125;&#125;/** * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed. * * 通过程序随机的种子，抽样返回新的DataSet * * @param withReplacement Sample with replacement or not. * 取样结果是否放回 * @param fraction Fraction of rows to generate. * 每行数据被取样的概率 * @note This is NOT guaranteed to provide exactly the fraction of the total count * of the given [[Dataset]]. * 不能保证准确的按照给定的分数取样。（一般结果会在概率值*总数左右） * @group typedrel * @since 1.6.0 */def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = &#123; sample(withReplacement, fraction, Utils.random.nextLong)&#125; randomSplit-按照权重分割1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Randomly splits this Dataset with the provided weights. * * 随机将此数据集按照所提供的权重进行分割。 * * @param weights weights for splits, will be normalized if they don't sum to 1. * 切分的权重。如果和不为1就会被标准化。 * @param seed Seed for sampling. * 取样的种子（影响随机数生成器） * * For Java API, use [[randomSplitAsList]]. * Java API 使用 [[randomSplitAsList]]. * @group typedrel * @since 2.0.0 */ def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = &#123; require(weights.forall(_ &gt;= 0), s\"Weights must be nonnegative, but got $&#123;weights.mkString(\"[\", \",\", \"]\")&#125;\") require(weights.sum &gt; 0, s\"Sum of weights must be positive, but got $&#123;weights.mkString(\"[\", \",\", \"]\")&#125;\") // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its // constituent partitions each time a split is materialized which could result in // overlapping splits. To prevent this, we explicitly sort each input partition to make the // ordering deterministic. // MapType cannot be sorted. val sorted = Sort(logicalPlan.output.filterNot(_.dataType.isInstanceOf[MapType]) .map(SortOrder(_, Ascending)), global = false, logicalPlan) val sum = weights.sum // scanLeft 从右到右依次累计算 scanLeft(0.0d)(_+_): (0.0,(0.0+0.2),(0.0+0.2+0.8)) val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) // sliding(n) 每次取n个值，以步长为1向右滑动，如：(0.0,0.2,0.8).sliding(2)=(0.0,0.2),(0.2,0.8) normalizedCumWeights.sliding(2).map &#123; x =&gt; new Dataset[T]( sparkSession, Sample(x(0), x(1), withReplacement = false, seed, sorted)(), encoder) &#125;.toArray &#125; /** * Randomly splits this Dataset with the provided weights. * * 程序自动生成随机数种子，随机将此数据集按照所提供的权重进行分割。 * * @param weights weights for splits, will be normalized if they don't sum to 1. * 切分的权重。如果和不为1就会被标准化。 * @group typedrel * @since 2.0.0 */ def randomSplit(weights: Array[Double]): Array[Dataset[T]] = &#123; randomSplit(weights, Utils.random.nextLong) &#125; /** * Randomly splits this Dataset with the provided weights. Provided for the Python Api. * Python 使用该方法 * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. */ private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = &#123; randomSplit(weights.toArray, seed) &#125; randomSplitAsList12345678910111213141516/** * Returns a Java list that contains randomly split Dataset with the provided weights. * * 根据提供的权重分割DataFrames，返回Java list * * @param weights weights for splits, will be normalized if they don't sum to 1. * 切分的权重。如果和不为1就会被标准化。 * @param seed Seed for sampling. * 取样的种子（影响随机数生成器） * @group typedrel * @since 2.0.0 */def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = &#123; val values = randomSplit(weights, seed) java.util.Arrays.asList(values: _*)&#125; dropDuplicates-去重123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `distinct`. * * 删除重复的row数据，是distinct的别名 * * @group typedrel * @since 2.0.0 */def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)/** * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * 只删除指定列的重复数据 * * @group typedrel * @since 2.0.0 */def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan &#123; val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output val groupCols = colNames.flatMap &#123; colName =&gt; // It is possibly there are more than one columns with the same name, // so we call filter instead of find. val cols = allColumns.filter(col =&gt; resolver(col.name, colName)) if (cols.isEmpty) &#123; throw new AnalysisException( s\"\"\"Cannot resolve column name \"$colName\" among ($&#123;schema.fieldNames.mkString(\", \")&#125;)\"\"\") &#125; cols &#125; val groupColExprIds = groupCols.map(_.exprId) val aggCols = logicalPlan.output.map &#123; attr =&gt; if (groupColExprIds.contains(attr.exprId)) &#123; attr &#125; else &#123; // Removing duplicate rows should not change output attributes. We should keep // the original exprId of the attribute. Otherwise, to select a column in original // dataset will cause analysis exception due to unresolved attribute. // 删除重复行不应该更改输出属性。 // 我们应该保留这个属性的原始属性。 // 否则，在原始数据集中选择一个列将导致分析异常，原因是未解析的属性。 Alias(new First(attr).toAggregateExpression(), attr.name)(exprId = attr.exprId) &#125; &#125; Aggregate(groupCols, aggCols, logicalPlan)&#125;/** * Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * 只针对特定列做去重 * * @group typedrel * @since 2.0.0 */def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)/** * Returns a new [[Dataset]] with duplicate rows removed, considering only * the subset of columns. * * 只针对特定多列做去重 * * @group typedrel * @since 2.0.0 */@scala.annotation.varargsdef dropDuplicates(col1: String, cols: String*): Dataset[T] = &#123; val colNames: Seq[String] = col1 +: cols dropDuplicates(colNames)&#125; transform-自定义转换1234567891011121314151617/** * Concise syntax for chaining custom transformations. * * 用于链接自定义转换的简明语法。 * * &#123;&#123;&#123; * def featurize(ds: Dataset[T]): Dataset[U] = ... * * ds * .transform(featurize) * .transform(...) * &#125;&#125;&#125; * * @group typedrel * @since 1.6.0 */def transform[U](t: Dataset[T] =&gt; Dataset[U]): Dataset[U] = t(this) filter-过滤12345678910111213141516171819202122232425262728293031/** * :: Experimental :: * (Scala-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * 该数据集只包含“func”返回“true”的元素。 * * @group typedrel * @since 1.6.0 */ @Experimental @InterfaceStability.Evolving def filter(func: T =&gt; Boolean): Dataset[T] = &#123; withTypedPlan(TypedFilter(func, logicalPlan)) &#125; /** * :: Experimental :: * (Java-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * 返回一个新数据集，该数据集只包含“func”返回“true”的元素。 * * @group typedrel * @since 1.6.0 */ @Experimental @InterfaceStability.Evolving def filter(func: FilterFunction[T]): Dataset[T] = &#123; withTypedPlan(TypedFilter(func, logicalPlan)) &#125; map1234567891011121314151617181920212223242526272829303132/** * :: Experimental :: * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * 返回一个新的数据集，该数据集包含对每个元素应用“func”的结果。 * * @group typedrel * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef map[U: Encoder](func: T =&gt; U): Dataset[U] = withTypedPlan &#123; MapElements[T, U](func, logicalPlan)&#125;/** * :: Experimental :: * (Java-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * 返回一个新的数据集，该数据集包含对每个元素应用“func”的结果。 * * @group typedrel * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = &#123; implicit val uEnc = encoder withTypedPlan(MapElements[T, U](func, logicalPlan))&#125; mapPartitions1234567891011121314151617181920212223242526272829303132333435/** * :: Experimental :: * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each partition. * * 返回一个新的数据集，该数据集包含对每个分区应用“func”的结果。 * * @group typedrel * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef mapPartitions[U: Encoder](func: Iterator[T] =&gt; Iterator[U]): Dataset[U] = &#123; new Dataset[U]( sparkSession, MapPartitions[T, U](func, logicalPlan), implicitly[Encoder[U]])&#125;/** * :: Experimental :: * (Java-specific) * Returns a new Dataset that contains the result of applying `f` to each partition. * * 返回一个新的数据集，该数据集包含对每个分区应用“f”的结果。 * * @group typedrel * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = &#123; val func: (Iterator[T]) =&gt; Iterator[U] = x =&gt; f.call(x.asJava).asScala mapPartitions(func)(encoder)&#125; flatMap-将map结果flat扁平化123456789101112131415161718192021222324252627282930313233/** * :: Experimental :: * (Scala-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * 返回一个新的数据集，首先对该数据集的所有元素应用一个函数，然后将结果扁平化。 * * @group typedrel * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef flatMap[U: Encoder](func: T =&gt; TraversableOnce[U]): Dataset[U] = mapPartitions(_.flatMap(func))/** * :: Experimental :: * (Java-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * 返回一个新的数据集，首先对该数据集的所有元素应用一个函数，然后将结果扁平化。 * * @group typedrel * @since 1.6.0 */@Experimental@InterfaceStability.Evolvingdef flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = &#123; val func: (T) =&gt; Iterator[U] = x =&gt; f.call(x).asScala flatMap(func)(encoder)&#125; repartition-重分区123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * Returns a new Dataset that has exactly `numPartitions` partitions. * * 返回一个 给定分区数量的新DataSet * * @group typedrel * @since 1.6.0 */ def repartition(numPartitions: Int): Dataset[T] = withTypedPlan &#123; Repartition(numPartitions, shuffle = true, logicalPlan) &#125; /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is hash partitioned. * * 返回一个由给定的分区表达式划分为“num分区”的新数据集。 * 生成的Dataset是哈希分区的。 * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * 和 SQL (Hive QL) 中的 \"DISTRIBUTE BY\" 作用相同 * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = withTypedPlan &#123; RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, Some(numPartitions)) &#125; /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is hash partitioned. * * 根据指定的分区表达式进行重分区。 * 分区数量由`spark.sql.shuffle.partitions` 获得。 * 结果Dataset 是哈希分区的。 * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * 和 SQL (Hive QL) 中的 \"DISTRIBUTE BY\" 作用相同 * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def repartition(partitionExprs: Column*): Dataset[T] = withTypedPlan &#123; RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions = None) &#125; coalesce-合并分区1234567891011121314151617/** * Returns a new Dataset that has exactly `numPartitions` partitions. * Similar to coalesce defined on an `RDD`, this operation results in a narrow dependency, e.g. * if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of * the 100 new partitions will claim 10 of the current partitions. * * 合并。 * 返回确定分区数量的Dataset。 * 和RDD中的合并方法类似，这个操作导致了一个窄依赖。 * 例如：将1000个分区合并为100个分区，这个过程没有shuffle，而是100个新分区中的每个分区将声明当前的10个分区。 * * @group typedrel * @since 1.6.0 */ def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan &#123; Repartition(numPartitions, shuffle = false, logicalPlan) &#125; distinct-去重1234567891011121314/** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `dropDuplicates`. * * 去重。 * 返回去重后的Dataset。 * 和 `dropDuplicates` 方法一致。 * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * @group typedrel * @since 2.0.0 */ def distinct(): Dataset[T] = dropDuplicates()","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"},{"name":"源码","slug":"源码","permalink":"https://stanxia.github.io/tags/源码/"}]},{"title":"JavaRDDLike.scala","slug":"JavaRDDLike-scala","date":"2017-11-21T06:21:23.000Z","updated":"2017-11-23T02:01:42.000Z","comments":true,"path":"2017/11/21/JavaRDDLike-scala/","link":"","permalink":"https://stanxia.github.io/2017/11/21/JavaRDDLike-scala/","excerpt":"使用Java开发Spark程序，JavaRDD的功能算子中英文注释JavaRDDLike的实现应该扩展这个虚拟抽象类，而不是直接继承这个特性。","text":"使用Java开发Spark程序，JavaRDD的功能算子中英文注释JavaRDDLike的实现应该扩展这个虚拟抽象类，而不是直接继承这个特性。 JavaRDD1234567891011121314151617181920package org.apache.spark.api.javaprivate[spark] abstract class AbstractJavaRDDLike[T, This &lt;: JavaRDDLike[T, This]] extends JavaRDDLike[T, This]/** * Defines operations common to several Java RDD implementations. * * 定义几个Java RDD实现的常见操作。 * * @note This trait is not intended to be implemented by user code. * * 该特性不打算由用户代码实现。 */trait JavaRDDLike[T, This &lt;: JavaRDDLike[T, This]] extends Serializable &#123; def wrapRDD(rdd: RDD[T]): This implicit val classTag: ClassTag[T] def rdd: RDD[T] partitions1234/** Set of partitions in this RDD. * 在这个RDD中设置的分区。 * */def partitions: JList[Partition] = rdd.partitions.toSeq.asJava getNumPartitions12345/** Return the number of partitions in this RDD. * 返回该RDD中的分区数。 * */@Since(\"1.6.0\")def getNumPartitions: Int = rdd.getNumPartitions partitioner1234/** The partitioner of this RDD. * 这个RDD的分区。 * */def partitioner: Optional[Partitioner] = JavaUtils.optionToOptional(rdd.partitioner) context12345/** The [[org.apache.spark.SparkContext]] that this RDD was created on. * * 这个RDD是在[[org.apache.spark.SparkContext]]上面创建的。 * */def context: SparkContext = rdd.context id1234/** A unique ID for this RDD (within its SparkContext). * 这个RDD的惟一ID(在它的SparkContext内)。 * */def id: Int = rdd.id name1def name(): String = rdd.name getStorageLevel1234/** Get the RDD's current storage level, or StorageLevel.NONE if none is set. * 获取RDD的当前存储级别，或StorageLevel。如果没有设置就没有。 * */def getStorageLevel: StorageLevel = rdd.getStorageLevel iterator12345678910/** * Internal method to this RDD; will read from cache if applicable, or otherwise compute it. * This should ''not'' be called by users directly, but is available for implementors of custom * subclasses of RDD. * 内部方法的RDD;将从缓存读取，如果适用的话，或者计算它。 * 这应该“不是”直接由用户调用，而是用于RDD的自定义子类的实现者 * */def iterator(split: Partition, taskContext: TaskContext): JIterator[T] = rdd.iterator(split, taskContext).asJavs Transformations (return a new RDD)map1234567/** * Return a new RDD by applying a function to all elements of this RDD. * 将一个函数应用于这个RDD的所有元素，返回一个新的RDD。 * */def map[R](f: JFunction[T, R]): JavaRDD[R] = new JavaRDD(rdd.map(f)(fakeClassTag))(fakeClassTag) mapPartitionsWithIndex1234567891011/** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * 通过在RDD的每个分区上应用一个函数来返回一个新的RDD，同时跟踪原始分区的索引。 * */def mapPartitionsWithIndex[R]( f: JFunction2[jl.Integer, JIterator[T], JIterator[R]], preservesPartitioning: Boolean = false): JavaRDD[R] = new JavaRDD(rdd.mapPartitionsWithIndex((a, b) =&gt; f.call(a, b.asJava).asScala, preservesPartitioning)(fakeClassTag))(fakeClassTag) mapToDouble1234567/** * Return a new RDD by applying a function to all elements of this RDD. * 将一个函数应用于这个RDD的所有元素，返回一个新的RDD。 */def mapToDouble[R](f: DoubleFunction[T]): JavaDoubleRDD = &#123; new JavaDoubleRDD(rdd.map(f.call(_).doubleValue()))&#125; mapToPair123456789/** * Return a new RDD by applying a function to all elements of this RDD. * 将一个函数应用于这个RDD的所有元素，返回一个新的RDD。 * */def mapToPair[K2, V2](f: PairFunction[T, K2, V2]): JavaPairRDD[K2, V2] = &#123; def cm: ClassTag[(K2, V2)] = implicitly[ClassTag[(K2, V2)]] new JavaPairRDD(rdd.map[(K2, V2)](f)(cm))(fakeClassTag[K2], fakeClassTag[V2])&#125; flatMap12345678910/** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. * 返回一个新的RDD，首先将一个函数应用于这个RDD的所有元素，然后将结果扁平化。 * */def flatMap[U](f: FlatMapFunction[T, U]): JavaRDD[U] = &#123; def fn: (T) =&gt; Iterator[U] = (x: T) =&gt; f.call(x).asScala JavaRDD.fromRDD(rdd.flatMap(fn)(fakeClassTag[U]))(fakeClassTag[U])&#125; flatMapToDouble12345678910/** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. * 返回一个新的RDD，首先将一个函数应用于这个RDD的所有元素，然后将结果扁平化。 * */def flatMapToDouble(f: DoubleFlatMapFunction[T]): JavaDoubleRDD = &#123; def fn: (T) =&gt; Iterator[jl.Double] = (x: T) =&gt; f.call(x).asScala new JavaDoubleRDD(rdd.flatMap(fn).map(_.doubleValue()))&#125; flatMapToPair1234567891011/** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. * 返回一个新的RDD，首先将一个函数应用于这个RDD的所有元素，然后将结果扁平化。 * */def flatMapToPair[K2, V2](f: PairFlatMapFunction[T, K2, V2]): JavaPairRDD[K2, V2] = &#123; def fn: (T) =&gt; Iterator[(K2, V2)] = (x: T) =&gt; f.call(x).asScala def cm: ClassTag[(K2, V2)] = implicitly[ClassTag[(K2, V2)]] JavaPairRDD.fromRDD(rdd.flatMap(fn)(cm))(fakeClassTag[K2], fakeClassTag[V2])&#125; mapPartitions12345678910111213141516171819202122232425/** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */def mapPartitions[U](f: FlatMapFunction[JIterator[T], U]): JavaRDD[U] = &#123; def fn: (Iterator[T]) =&gt; Iterator[U] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; JavaRDD.fromRDD(rdd.mapPartitions(fn)(fakeClassTag[U]))(fakeClassTag[U])&#125;/** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */def mapPartitions[U](f: FlatMapFunction[JIterator[T], U], preservesPartitioning: Boolean): JavaRDD[U] = &#123; def fn: (Iterator[T]) =&gt; Iterator[U] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; JavaRDD.fromRDD( rdd.mapPartitions(fn, preservesPartitioning)(fakeClassTag[U]))(fakeClassTag[U])&#125; mapPartitionsToDouble12345678910111213141516171819202122232425/** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */def mapPartitionsToDouble(f: DoubleFlatMapFunction[JIterator[T]]): JavaDoubleRDD = &#123; def fn: (Iterator[T]) =&gt; Iterator[jl.Double] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; new JavaDoubleRDD(rdd.mapPartitions(fn).map(_.doubleValue()))&#125;/** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */def mapPartitionsToDouble(f: DoubleFlatMapFunction[JIterator[T]], preservesPartitioning: Boolean): JavaDoubleRDD = &#123; def fn: (Iterator[T]) =&gt; Iterator[jl.Double] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; new JavaDoubleRDD(rdd.mapPartitions(fn, preservesPartitioning) .map(_.doubleValue()))&#125; mapPartitionsToPair1234567891011121314151617181920212223242526/** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */def mapPartitionsToPair[K2, V2](f: PairFlatMapFunction[JIterator[T], K2, V2]):JavaPairRDD[K2, V2] = &#123; def fn: (Iterator[T]) =&gt; Iterator[(K2, V2)] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; JavaPairRDD.fromRDD(rdd.mapPartitions(fn))(fakeClassTag[K2], fakeClassTag[V2])&#125;/** * Return a new RDD by applying a function to each partition of this RDD. * 通过将一个函数应用于这个RDD的每个分区，返回一个新的RDD。 * */def mapPartitionsToPair[K2, V2](f: PairFlatMapFunction[JIterator[T], K2, V2], preservesPartitioning: Boolean): JavaPairRDD[K2, V2] = &#123; def fn: (Iterator[T]) =&gt; Iterator[(K2, V2)] = &#123; (x: Iterator[T]) =&gt; f.call(x.asJava).asScala &#125; JavaPairRDD.fromRDD( rdd.mapPartitions(fn, preservesPartitioning))(fakeClassTag[K2], fakeClassTag[V2])&#125; foreachPartition12345678/** * Applies a function f to each partition of this RDD. * 将函数f应用于该RDD的每个分区。 * */def foreachPartition(f: VoidFunction[JIterator[T]]): Unit = &#123; rdd.foreachPartition(x =&gt; f.call(x.asJava))&#125; glom1234567/** * Return an RDD created by coalescing all elements within each partition into an array. * 返回一个RDD，它将每个分区中的所有元素合并到一个数组中。 * */def glom(): JavaRDD[JList[T]] = new JavaRDD(rdd.glom().map(_.toSeq.asJava)) cartesian12345678/** * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of * elements (a, b) where a is in `this` and b is in `other`. * 返回这个RDD和另一个的笛卡尔乘积，即所有元素对的RDD(a,b) ：a在该RDD中，b在另一个RDD中 * */def cartesian[U](other: JavaRDDLike[U, _]): JavaPairRDD[T, U] = JavaPairRDD.fromRDD(rdd.cartesian(other.rdd)(other.classTag))(classTag, other.classTag) groupBy123456789101112131415161718192021222324252627/** * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements * mapping to that key. * 返回分组元素的RDD。 * 每个组由一个键和一个映射到该键的元素序列组成。 * */def groupBy[U](f: JFunction[T, U]): JavaPairRDD[U, JIterable[T]] = &#123; // The type parameter is U instead of K in order to work around a compiler bug; see SPARK-4459 // 类型参数是U而不是K，是为了绕过编译器错误 implicit val ctagK: ClassTag[U] = fakeClassTag implicit val ctagV: ClassTag[JList[T]] = fakeClassTag JavaPairRDD.fromRDD(groupByResultToJava(rdd.groupBy(f)(fakeClassTag)))&#125;/** * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements * mapping to that key. * 返回分组元素的RDD。 * 每个组由一个键和一个映射到该键的元素序列组成。 */def groupBy[U](f: JFunction[T, U], numPartitions: Int): JavaPairRDD[U, JIterable[T]] = &#123; // The type parameter is U instead of K in order to work around a compiler bug; see SPARK-4459 implicit val ctagK: ClassTag[U] = fakeClassTag implicit val ctagV: ClassTag[JList[T]] = fakeClassTag JavaPairRDD.fromRDD(groupByResultToJava(rdd.groupBy(f, numPartitions)(fakeClassTag[U])))&#125; pipe123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * Return an RDD created by piping elements to a forked external process. * 返回由管道元素调用外部程序返回新的RDD * */def pipe(command: String): JavaRDD[String] = &#123; rdd.pipe(command)&#125;/** * Return an RDD created by piping elements to a forked external process. * 返回由管道元素调用外部程序返回新的RDD * */def pipe(command: JList[String]): JavaRDD[String] = &#123; rdd.pipe(command.asScala)&#125;/** * Return an RDD created by piping elements to a forked external process. * 返回由管道元素调用外部程序返回新的RDD * */def pipe(command: JList[String], env: JMap[String, String]): JavaRDD[String] = &#123; rdd.pipe(command.asScala, env.asScala)&#125;/** * Return an RDD created by piping elements to a forked external process. * 返回由管道元素调用外部程序返回新的RDD * */def pipe(command: JList[String], env: JMap[String, String], separateWorkingDir: Boolean, bufferSize: Int): JavaRDD[String] = &#123; rdd.pipe(command.asScala, env.asScala, null, null, separateWorkingDir, bufferSize)&#125;/** * Return an RDD created by piping elements to a forked external process. * 返回由管道元素调用外部程序返回新的RDD * */def pipe(command: JList[String], env: JMap[String, String], separateWorkingDir: Boolean, bufferSize: Int, encoding: String): JavaRDD[String] = &#123; rdd.pipe(command.asScala, env.asScala, null, null, separateWorkingDir, bufferSize, encoding)&#125; zip123456789101112/** * Zips this RDD with another one, returning key-value pairs with the first element in each RDD, * second element in each RDD, etc. Assumes that the two RDDs have the *same number of * partitions* and the *same number of elements in each partition* (e.g. one was made through * a map on the other). * 将此RDD与另一个RDD进行Zips，返回键值对，每个RDD中的第一个元素，每个RDD中的第二个元素，等等。 * 假设两个RDDs拥有相同数量的分区和每个分区中相同数量的元素 * (例如，一个是通过另一个map的)。 */def zip[U](other: JavaRDDLike[U, _]): JavaPairRDD[T, U] = &#123; JavaPairRDD.fromRDD(rdd.zip(other.rdd)(other.classTag))(classTag, other.classTag)&#125; zipPartitions1234567891011121314151617/** * Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by * applying a function to the zipped partitions. Assumes that all the RDDs have the * *same number of partitions*, but does *not* require them to have the same number * of elements in each partition. * 用一个(或多个)RDD(或多个)来压缩这个RDD的分区，并返回一个新的RDD将函数应用于压缩分区。 * 假设所有RDDs拥有相同数量的分区，但不要求它们在每个分区中拥有相同数量的元素。 */def zipPartitions[U, V]( other: JavaRDDLike[U, _], f: FlatMapFunction2[JIterator[T], JIterator[U], V]): JavaRDD[V] = &#123; def fn: (Iterator[T], Iterator[U]) =&gt; Iterator[V] = &#123; (x: Iterator[T], y: Iterator[U]) =&gt; f.call(x.asJava, y.asJava).asScala &#125; JavaRDD.fromRDD( rdd.zipPartitions(other.rdd)(fn)(other.classTag, fakeClassTag[V]))(fakeClassTag[V])&#125; zipWithUniqueId1234567891011/** * Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k, * 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method * won't trigger a spark job, which is different from [[org.apache.spark.rdd.RDD#zipWithIndex]]. * 用生成的唯一长的id来压缩这个RDD。 * 第k个分区的项将得到id k,n + k,2 *n+ k，…，其中n是分区数。 * 因此，可能存在差距，但这种方法不会触发spark作业，它与[org .apache.spark. spark.rdd. rdd. rdd # zipWithIndex]不同。 */def zipWithUniqueId(): JavaPairRDD[T, jl.Long] = &#123; JavaPairRDD.fromRDD(rdd.zipWithUniqueId()).asInstanceOf[JavaPairRDD[T, jl.Long]]&#125; zipWithIndex12345678910111213141516/** * Zips this RDD with its element indices. The ordering is first based on the partition index * and then the ordering of items within each partition. So the first item in the first * partition gets index 0, and the last item in the last partition receives the largest index. * This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type. * This method needs to trigger a spark job when this RDD contains more than one partitions. * * 用它的元素索引来压缩这个RDD。 * 排序首先基于分区索引，然后是每个分区中的条目的排序。 * 因此，第一个分区中的第一个项的索引值为0，最后一个分区中的最后一个项得到最大的索引。 * 这类似于Scala的zipWithIndex，但它使用的是Long而不是Int作为索引类型。 * 当这个RDD包含多个分区时，这个方法需要触发一个spark作业。 */def zipWithIndex(): JavaPairRDD[T, jl.Long] = &#123; JavaPairRDD.fromRDD(rdd.zipWithIndex()).asInstanceOf[JavaPairRDD[T, jl.Long]]&#125; Actions (launch a job to return a value to the user program)foreach1234567/** * Applies a function f to all elements of this RDD. * 将函数f应用于该RDD的所有元素。 */def foreach(f: VoidFunction[T]) &#123; rdd.foreach(x =&gt; f.call(x))&#125; collect12345678910/** * Return an array that contains all of the elements in this RDD. * 返回包含该RDD中所有元素的数组。 * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。 */def collect(): JList[T] = rdd.collect().toSeq.asJava toLocalIterator123456789/** * Return an iterator that contains all of the elements in this RDD. * 返回包含该RDD中所有元素的迭代器。 * * The iterator will consume as much memory as the largest partition in this RDD. * 迭代器将消耗与此RDD中最大的分区一样多的内存。 */def toLocalIterator(): JIterator[T] = asJavaIteratorConverter(rdd.toLocalIterator).asJava collectPartitions1234567891011 /** * Return an array that contains all of the elements in a specific partition of this RDD. * 返回包含该RDD的特定分区中的所有元素的数组。 */def collectPartitions(partitionIds: Array[Int]): Array[JList[T]] = &#123; // This is useful for implementing `take` from other language frontends // like Python where the data is serialized. // 这有助于从其他语言的前沿实现“take”，如Python，数据被序列化。 val res = context.runJob(rdd, (it: Iterator[T]) =&gt; it.toArray, partitionIds) res.map(_.toSeq.asJava)&#125; reduce123456/** * Reduces the elements of this RDD using the specified commutative and associative binary * operator. * 使用指定的交换和关联二元运算符来减少该RDD的元素。 */def reduce(f: JFunction2[T, T, T]): T = rdd.reduce(f) treeReduce12345678910111213/** * Reduces the elements of this RDD in a multi-level tree pattern. * 将此RDD的元素简化为多层树模式。 * * @param depth suggested depth of the tree 建议树的深度 * @see [[org.apache.spark.api.java.JavaRDDLike#reduce]] */def treeReduce(f: JFunction2[T, T, T], depth: Int): T = rdd.treeReduce(f, depth)/** * [[org.apache.spark.api.java.JavaRDDLike#treeReduce]] 建议深度 2 . */def treeReduce(f: JFunction2[T, T, T]): T = treeReduce(f, 2) fold123456789101112131415161718192021 /** * Aggregate the elements of each partition, and then the results for all the partitions, using a * given associative function and a neutral \"zero value\". The function * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object * allocation; however, it should not modify t2. * 对每个分区的元素进行聚合，然后使用给定的关联函数和中立的“零值”，对所有分区进行结果。 * 函数op(t1,t2)被允许修改t1，并将其作为其结果值返回，以避免对象分配;但是，它不应该修改t2。 * * This behaves somewhat differently from fold operations implemented for non-distributed * collections in functional languages like Scala. This fold operation may be applied to * partitions individually, and then fold those results into the final result, rather than * apply the fold to each element sequentially in some defined ordering. For functions * that are not commutative, the result may differ from that of a fold applied to a * non-distributed collection. * 这与在Scala等函数式语言中实现非分布式集合的折叠操作有一定的不同。 * 这个折叠操作可以单独应用于分区，然后将这些结果折叠到最终结果中，而不是在某些定义的排序中顺序地对每个元素进行折叠。 * 对于非交换的函数，结果可能与应用于非分布式集合的函数不同。 * */def fold(zeroValue: T)(f: JFunction2[T, T, T]): T = rdd.fold(zeroValue)(f) aggregate12345678910111213141516 /** * Aggregate the elements of each partition, and then the results for all the partitions, using * given combine functions and a neutral \"zero value\". This function can return a different result * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are * allowed to modify and return their first argument instead of creating a new U to avoid memory * allocation. * 对每个分区的元素进行聚合，然后使用给定的组合函数和一个中立的“零值”，对所有分区进行结果。 * 这个函数可以返回一个不同的结果类型U，而不是这个RDD的类型。 * 因此，我们需要一个操作来将一个T合并到一个U和一个合并两个U的操作，就像在scala . traversableonce中那样。 * 这两个函数都可以修改和返回第一个参数，而不是创建一个新的U，以避免内存分配。 * */def aggregate[U](zeroValue: U)(seqOp: JFunction2[U, T, U], combOp: JFunction2[U, U, U]): U = rdd.aggregate(zeroValue)(seqOp, combOp)(fakeClassTag[U]) treeAggregate12345678910111213141516171819202122232425 /** * Aggregates the elements of this RDD in a multi-level tree pattern. * 将此RDD的元素聚集在多层树模式中。 * * @param depth suggested depth of the tree 建议的树的深度 * @see [[org.apache.spark.api.java.JavaRDDLike#aggregate]] */def treeAggregate[U]( zeroValue: U, seqOp: JFunction2[U, T, U], combOp: JFunction2[U, U, U], depth: Int): U = &#123; rdd.treeAggregate(zeroValue)(seqOp, combOp, depth)(fakeClassTag[U])&#125;/** * [[org.apache.spark.api.java.JavaRDDLike#treeAggregate]] with suggested depth 2. * 建议的树的深度为 2 */def treeAggregate[U]( zeroValue: U, seqOp: JFunction2[U, T, U], combOp: JFunction2[U, U, U]): U = &#123; treeAggregate(zeroValue, seqOp, combOp, 2)&#125; count123456/** * Return the number of elements in the RDD. * 返回RDD中元素的数量。 * */def count(): Long = rdd.count() countApprox12345678910111213141516171819202122232425262728293031323334353637 /** * Approximate version of count() that returns a potentially incomplete result * within a timeout, even if not all tasks have finished. * 近似版本的count()，即使不是所有的任务都完成了，也会在一个超时中返回一个潜在的不完整的结果。 * * * The confidence is the probability that the error bounds of the result will * contain the true value. That is, if countApprox were called repeatedly * with confidence 0.9, we would expect 90% of the results to contain the * true count. The confidence must be in the range [0,1] or an exception will * be thrown. * 置信值是结果的误差边界包含真实值的概率。 * 也就是说，如果countApprox被反复调用，confidence 0.9，我们将期望90%的结果包含真实的计数。 * confidence必须在范围[0,1]中，否则将抛出异常。 * * * @param timeout maximum time to wait for the job, in milliseconds * 等待工作的最大时间，以毫秒为单位 * @param confidence the desired statistical confidence in the result * 对结果的期望的统计信心 * @return a potentially incomplete result, with error bounds * 一个可能不完整的结果，有错误界限 */def countApprox(timeout: Long, confidence: Double): PartialResult[BoundedDouble] = rdd.countApprox(timeout, confidence)/** * Approximate version of count() that returns a potentially incomplete result * within a timeout, even if not all tasks have finished. * 近似版本的count()，即使不是所有的任务都完成了，也会在一个超时中返回一个潜在的不完整的结果。 * * * @param timeout maximum time to wait for the job, in milliseconds * 等待工作的最大时间，以毫秒为单位 */def countApprox(timeout: Long): PartialResult[BoundedDouble] = rdd.countApprox(timeout) countByValue123456789/** * Return the count of each unique value in this RDD as a map of (value, count) pairs. The final * combine step happens locally on the master, equivalent to running a single reduce task. * 将此RDD中的每个惟一值的计数作为(值、计数)对的映射。 * 最后的联合步骤在master的本地发生，相当于运行一个reduce任务。 * */def countByValue(): JMap[T, jl.Long] = mapAsSerializableJavaMap(rdd.countByValue()).asInstanceOf[JMap[T, jl.Long]] countByValueApprox1234567891011121314151617181920212223242526272829303132333435363738 /** * Approximate version of countByValue(). * countByValue()近似的版本。 * * The confidence is the probability that the error bounds of the result will * contain the true value. That is, if countApprox were called repeatedly * with confidence 0.9, we would expect 90% of the results to contain the * true count. The confidence must be in the range [0,1] or an exception will * be thrown. * 置信值是结果的误差边界包含真实值的概率。 * 也就是说，如果countApprox被反复调用，confidence 0.9，我们将期望90%的结果包含真实的计数。 * confidence必须在范围[0,1]中，否则将抛出异常。 * * * @param timeout maximum time to wait for the job, in milliseconds * 等待工作的最大时间，毫秒为单位。 * @param confidence the desired statistical confidence in the result * 对结果的期望的统计信心 * @return a potentially incomplete result, with error bounds * 一个可能不完整的结果，有错误界限 */def countByValueApprox( timeout: Long, confidence: Double ): PartialResult[JMap[T, BoundedDouble]] = rdd.countByValueApprox(timeout, confidence).map(mapAsSerializableJavaMap)/** * Approximate version of countByValue(). * countByValue().的近似版本. * * @param timeout maximum time to wait for the job, in milliseconds * 等待工作的最大时间，毫秒为单位。 * @return a potentially incomplete result, with error bounds * 一个可能不完整的结果，有错误界限 */def countByValueApprox(timeout: Long): PartialResult[JMap[T, BoundedDouble]] = rdd.countByValueApprox(timeout).map(mapAsSerializableJavaMap) take12345678910111213141516 /** * Take the first num elements of the RDD. This currently scans the partitions *one by one*, so * it will be slow if a lot of partitions are required. In that case, use collect() to get the * whole RDD instead. * 获取RDD的第一个num元素。 * 这将会一次一个地扫描分区，所以如果需要很多分区，它将会很慢。 * 在这种情况下，使用collect()来获得整个RDD。 * * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。 * */def take(num: Int): JList[T] = rdd.take(num).toSeq.asJava takeSample12345def takeSample(withReplacement: Boolean, num: Int): JList[T] = takeSample(withReplacement, num, Utils.random.nextLong)def takeSample(withReplacement: Boolean, num: Int, seed: Long): JList[T] = rdd.takeSample(withReplacement, num, seed).toSeq.asJava first12345 /** * Return the first element in this RDD. * 返回这个RDD中的第一个元素。 */def first(): T = rdd.first() isEmpty1234567/** * @return true if and only if the RDD contains no elements at all. Note that an RDD * may be empty even when it has at least 1 partition. * 当且仅当RDD不包含任何元素，则为真。 * 请注意，即使在至少有一个分区的情况下，RDD也可能是空的。 */def isEmpty(): Boolean = rdd.isEmpty() saveAsTextFile123456789101112131415/** * Save this RDD as a text file, using string representations of elements. * 将此RDD保存为文本文件，使用元素的字符串表示形式。 */def saveAsTextFile(path: String): Unit = &#123; rdd.saveAsTextFile(path)&#125;/** * Save this RDD as a compressed text file, using string representations of elements. * 将此RDD保存为一个压缩文本文件，使用元素的字符串表示形式。 */def saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec]): Unit = &#123; rdd.saveAsTextFile(path, codec)&#125; saveAsObjectFile1234567/** * Save this RDD as a SequenceFile of serialized objects. * 将此RDD保存为序列化对象的序列文件。 */def saveAsObjectFile(path: String): Unit = &#123; rdd.saveAsObjectFile(path)&#125; keyBy12345678910/** * Creates tuples of the elements in this RDD by applying `f`. * 通过应用“f”创建这个RDD中元素的元组。 */def keyBy[U](f: JFunction[T, U]): JavaPairRDD[U, T] = &#123; // The type parameter is U instead of K in order to work around a compiler bug; see SPARK-4459 // 类型参数用U替代K，为了绕过编译器错误; implicit val ctag: ClassTag[U] = fakeClassTag JavaPairRDD.fromRDD(rdd.keyBy(f))&#125; checkpoint12345678910111213141516/** * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint * directory set with SparkContext.setCheckpointDir() and all references to its parent * RDDs will be removed. This function must be called before any job has been * executed on this RDD. It is strongly recommended that this RDD is persisted in * memory, otherwise saving it on a file will require recomputation. * 将此RDD标记为检查点。 * 它将被保存到由SparkContext.setCheckpointDir()设置的检查点目录下的文件中。 * 所有对其父RDDs的引用将被删除。 * 在此RDD上执行任何作业之前，必须调用此函数。 * 强烈建议将此RDD保存在内存中，否则将其保存在文件中需要重新计算。 * */def checkpoint(): Unit = &#123; rdd.checkpoint()&#125; isCheckpointed12345/** * Return whether this RDD has been checkpointed or not * 返回 RDD是否已被检查过 */def isCheckpointed: Boolean = rdd.isCheckpointed getCheckpointFile1234567/** * Gets the name of the file to which this RDD was checkpointed * 获取该RDD所指向的checkpointed文件的名称 */def getCheckpointFile(): Optional[String] = &#123; JavaUtils.optionToOptional(rdd.getCheckpointFile)&#125; toDebugString123456/** A description of this RDD and its recursive dependencies for debugging. * 对该RDD及其对调试的递归依赖的描述。 * */def toDebugString(): String = &#123; rdd.toDebugString&#125; top12345678910111213141516171819202122232425262728293031323334/** * Returns the top k (largest) elements from this RDD as defined by * the specified Comparator[T] and maintains the order. * 根据指定的比较器[T]，从这个RDD中返回最大的k(最大)元素，并维护顺序。 * * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。 * * @param num k, the number of top elements to return 返回的元素数量 * @param comp the comparator that defines the order 定义排序的比较器 * @return an array of top elements 返回最大元素的数组 */def top(num: Int, comp: Comparator[T]): JList[T] = &#123; rdd.top(num)(Ordering.comparatorToOrdering(comp)).toSeq.asJava&#125;/** * Returns the top k (largest) elements from this RDD using the * natural ordering for T and maintains the order. * 使用T的自然顺序，从这个RDD中返回最大的k(最大)元素，并维护顺序。 * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。 * * @param num k, the number of top elements to return 返回的元素数量 * @return an array of top elements 最大元素的数组 */def top(num: Int): JList[T] = &#123; val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[T]] top(num, comp)&#125; takeOrdered123456789101112131415161718192021222324252627282930313233/** * Returns the first k (smallest) elements from this RDD as defined by * the specified Comparator[T] and maintains the order. * 从这个RDD中返回第一个k(最小)元素，由指定的Comparator[T]定义，并维护该顺序。 * * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 该方法只在预期的数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。 * * @param num k, the number of elements to return 返回的元素数量 * @param comp the comparator that defines the order 排序比较器 * @return an array of top elements 元素数组 */def takeOrdered(num: Int, comp: Comparator[T]): JList[T] = &#123; rdd.takeOrdered(num)(Ordering.comparatorToOrdering(comp)).toSeq.asJava&#125;/** * Returns the first k (smallest) elements from this RDD using the * natural ordering for T while maintain the order. * 使用原生的 T排序比较器，返回 k个 最小值，并维护这个顺序 * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 尽量应用于小的数组，因为会加载到driver内存中。 * @param num k, the number of top elements to return * @return an array of top elements */def takeOrdered(num: Int): JList[T] = &#123; val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[T]] takeOrdered(num, comp)&#125; max123456789101112/** * Returns the maximum element from this RDD as defined by the specified * Comparator[T]. * 按照指定比较器[T]定义的RDD， * 返回最大元素。 * * @param comp the comparator that defines ordering 指定的比较器 * @return the maximum of the RDD 最大值 */def max(comp: Comparator[T]): T = &#123; rdd.max()(Ordering.comparatorToOrdering(comp))&#125; min123456789101112/** * Returns the minimum element from this RDD as defined by the specified * Comparator[T]. * 按照指定比较器[T]定义的RDD， * 返回最小元素。 * * @param comp the comparator that defines ordering 指定的比较器 * @return the minimum of the RDD 最小值 */def min(comp: Comparator[T]): T = &#123; rdd.min()(Ordering.comparatorToOrdering(comp))&#125; countApproxDistinct123456789101112131415161718/*** Return approximate number of distinct elements in the RDD.* 返回RDD中不重复元素的数量近似数。** The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice:* Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * &lt;a href=\"http://dx.doi.org/10.1145/2452376.2452456\"&gt;here&lt;/a&gt;. * 所使用的算法是基于streamlib在实践中的“HyperLogLog”的实现: * “一种艺术基数估计算法状态的算法工程”， * * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017. * 相对精度。 * 较小的值创建需要更多空间的计数器。 * 它必须大于0.000017。 */ def countApproxDistinct(relativeSD: Double): Long = rdd.countApproxDistinct(relativeSD) countAsync12345678/** * The asynchronous version of `count`, which returns a * future for counting the number of elements in this RDD. * “count”的异步版本，它为计算这个RDD中元素的数量返回一个未来。 */def countAsync(): JavaFutureAction[jl.Long] = &#123;new JavaFutureActionWrapper[Long, jl.Long](rdd.countAsync(), jl.Long.valueOf)&#125; collectAsync12345678910111213/** * The asynchronous version of `collect`, which returns a future for * retrieving an array containing all of the elements in this RDD. * “collect”的异步版本， * 它返回一个用于检索包含该RDD中所有元素的数组的未来。 * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * 尽量应用于小数量数组。 */def collectAsync(): JavaFutureAction[JList[T]] = &#123;new JavaFutureActionWrapper(rdd.collectAsync(), (x: Seq[T]) =&gt; x.asJava)&#125; takeAsync123456789101112/** * The asynchronous version of the `take` action, which returns a * future for retrieving the first `num` elements of this RDD. * “take”操作的异步版本， * 它将返回用于检索此RDD的第一个“num”元素的未来。 * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. */def takeAsync(num: Int): JavaFutureAction[JList[T]] = &#123;new JavaFutureActionWrapper(rdd.takeAsync(num), (x: Seq[T]) =&gt; x.asJava)&#125; foreachAsync1234567891011/** * The asynchronous version of the `foreach` action, which * applies a function f to all the elements of this RDD. * “foreach”操作的异步版本， * 它将函数f应用于这个RDD的所有元素。 * */def foreachAsync(f: VoidFunction[T]): JavaFutureAction[Void] = &#123;new JavaFutureActionWrapper[Unit, Void](rdd.foreachAsync(x =&gt; f.call(x)),&#123; x =&gt; null.asInstanceOf[Void] &#125;)&#125; foreachPartitionAsync1234567891011/** * The asynchronous version of the `foreachPartition` action, which * applies a function f to each partition of this RDD. * “foreachPartition”操作的异步版本， * 它将函数f应用于该RDD的每个分区。 */def foreachPartitionAsync(f: VoidFunction[JIterator[T]]): JavaFutureAction[Void] = &#123;new JavaFutureActionWrapper[Unit, Void](rdd.foreachPartitionAsync(x =&gt; f.call(x.asJava)),&#123; x =&gt; null.asInstanceOf[Void] &#125;)&#125;&#125;","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"},{"name":"源码","slug":"源码","permalink":"https://stanxia.github.io/tags/源码/"}]},{"title":"spark取样函数分析","slug":"spark取样函数分析","date":"2017-11-08T09:30:25.000Z","updated":"2017-11-27T09:08:28.000Z","comments":true,"path":"2017/11/08/spark取样函数分析/","link":"","permalink":"https://stanxia.github.io/2017/11/08/spark取样函数分析/","excerpt":"Spark取样操作无法获取随机样本的解决方案","text":"Spark取样操作无法获取随机样本的解决方案 背景Dataset中sample函数源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed. * * 通过使用用户提供的种子，通过抽样的方式返回一个新的[[Dataset]]。 * * @param withReplacement Sample with replacement or not. * 如果withReplacement=true的话表示有放回的抽样，采用泊松抽样算法实现. * 如果withReplacement=false的话表示无放回的抽样，采用伯努利抽样算法实现. * @param fraction Fraction of rows to generate. * 每一行数据被取样的概率.服从二项分布.当withReplacement=true的时候fraction&gt;=0,当withReplacement=false的时候 0 &lt; fraction &lt; 1. * @param seed Seed for sampling. * 取样种子（与随机数生成有关） * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * 不能保证准确的按照给定的分数取样。（一般结果会在概率值*总数左右） * @group typedrel * @since 1.6.0 */def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = &#123; require(fraction &gt;= 0, s\"Fraction must be nonnegative, but got $&#123;fraction&#125;\") withTypedPlan &#123; Sample(0.0, fraction, withReplacement, seed, logicalPlan)() &#125;&#125;/** * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed. * * 通过程序随机的种子，抽样返回新的DataSet * * @param withReplacement Sample with replacement or not. * 取样结果是否放回 * @param fraction Fraction of rows to generate. * 每行数据被取样的概率 * @note This is NOT guaranteed to provide exactly the fraction of the total count * of the given [[Dataset]]. * 不能保证准确的按照给定的分数取样。（一般结果会在概率值*总数左右） * @group typedrel * @since 1.6.0 */def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = &#123; sample(withReplacement, fraction, Utils.random.nextLong)&#125; 问题结果数据的行数一般在（fraction*总数）左右。没有一个固定的值，如果需要得到固定行数的随机数据的话不建议采用该方法。 办法获取随机取样的替代方法： 123456df.createOrReplaceTempView(\"test_sample\"); // 生成临时表df.sqlContext() // 添加随机数列，并根据其进行排序 .sql(\"select * ,rand() as random from test_sample order by random\") .limit(2) // 根据参数的fraction计算需要获取的取样结果 .drop(\"random\") // 删除掉添加的随机列 .show();","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"}]},{"title":"spark源码注释翻译","slug":"spark源码注释翻译","date":"2017-11-06T08:57:05.000Z","updated":"2017-11-23T02:01:40.000Z","comments":true,"path":"2017/11/06/spark源码注释翻译/","link":"","permalink":"https://stanxia.github.io/2017/11/06/spark源码注释翻译/","excerpt":"版本：spark2.1.1目的：方便中文用户阅读源码，把时间花在理解而不是翻译上","text":"版本：spark2.1.1目的：方便中文用户阅读源码，把时间花在理解而不是翻译上 初衷开始立项进行翻译，一方面方便日后阅读源码，另一方面先粗粒度的熟悉下spark框架和组件。优化完之后希望能帮助更多的中文用户，节省翻译时间。 进度已完成： 正在作：spark core模块 模块名 模块介绍 完成度 api broadcast deploy executor 执行器：用于启动线程池，是真正负责执行task的部件 已完成 input internal io launcher mapred memory metrics network partial rdd rpc scheduler 调度器：spark应用程序的任务调度器 正在作 security serializer shuffle status.api.v1 storage util","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"}]},{"title":"spark关于parquet的优化","slug":"spark关于parquet的优化","date":"2017-11-01T06:53:13.000Z","updated":"2017-11-23T08:00:07.000Z","comments":true,"path":"2017/11/01/spark关于parquet的优化/","link":"","permalink":"https://stanxia.github.io/2017/11/01/spark关于parquet的优化/","excerpt":"parquet是一种列式存储。可以提供面向列的存储和查询。\nParquet的优势在sparkSQL程序中使用parquet格式存储文件，在存储空间和查询性能方面都有很高的效率。\n存储方面因为是面向列的存储，同一列的类型相同，因而在存储的过程中可以使用更高效的压缩方案，可以节省大量的存储空间。\n查询方面在执行查询任务时，只会扫描需要的列，而不是全部，高度灵活性使查询变得非常高效。","text":"parquet是一种列式存储。可以提供面向列的存储和查询。 Parquet的优势在sparkSQL程序中使用parquet格式存储文件，在存储空间和查询性能方面都有很高的效率。 存储方面因为是面向列的存储，同一列的类型相同，因而在存储的过程中可以使用更高效的压缩方案，可以节省大量的存储空间。 查询方面在执行查询任务时，只会扫描需要的列，而不是全部，高度灵活性使查询变得非常高效。 实例测试 测试数据大小 存储类型 存储所占空间 查询性能 1T TEXTFILE 897.9G 698s 1T Parquet 231.4G 21s Parquet的使用使用parquet的简单demo： 12345678910111213141516171819202122// Encoders for most common types are automatically provided by importing spark.implicits._import spark.implicits._val peopleDF = spark.read.json(\"examples/src/main/resources/people.json\")// DataFrames can be saved as Parquet files, maintaining the schema informationpeopleDF.write.parquet(\"people.parquet\")// Read in the parquet file created above// Parquet files are self-describing so the schema is preserved// The result of loading a Parquet file is also a DataFrameval parquetFileDF = spark.read.parquet(\"people.parquet\")// Parquet files can also be used to create a temporary view and then used in SQL statementsparquetFileDF.createOrReplaceTempView(\"parquetFile\")val namesDF = spark.sql(\"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19\")namesDF.map(attributes =&gt; \"Name: \" + attributes(0)).show()// +------------+// | value|// +------------+// |Name: Justin|// +------------+","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"},{"name":"parquet","slug":"parquet","permalink":"https://stanxia.github.io/tags/parquet/"}]},{"title":"三步走战略","slug":"三步走战略","date":"2017-11-01T02:45:09.000Z","updated":"2017-11-23T02:03:57.000Z","comments":true,"path":"2017/11/01/三步走战略/","link":"","permalink":"https://stanxia.github.io/2017/11/01/三步走战略/","excerpt":"设定中长期规划稳扎稳打，逐个击破，实现技术上的重大突破","text":"设定中长期规划稳扎稳打，逐个击破，实现技术上的重大突破 第一步深刻了解spark运行机制第二步深度剖析sparkSQL和sparkStreaming第三步实现对spark机器学习的深度掌握","raw":null,"content":null,"categories":[{"name":"规划","slug":"规划","permalink":"https://stanxia.github.io/categories/规划/"}],"tags":[{"name":"规划","slug":"规划","permalink":"https://stanxia.github.io/tags/规划/"}]},{"title":"手把手搭建vps和shadowsocks","slug":"手把手搭建vps和shadowsocks","date":"2017-10-30T16:16:22.000Z","updated":"2017-11-23T08:01:00.000Z","comments":true,"path":"2017/10/31/手把手搭建vps和shadowsocks/","link":"","permalink":"https://stanxia.github.io/2017/10/31/手把手搭建vps和shadowsocks/","excerpt":"记性不好，做个记录，日后有需要时难得费神。\n名词解释了解一些原理，熟悉一些名词，也方便理解接下来安装过程中的操作。\nvpsVPS(Virtual private server) 译作虚拟专用伺服器。你可以把它简单地理解为一台在远端的强劲电脑。当你租用了它以后，可以给它安装操作系统、软件，并通过一些工具连接和远程操控它。\nvultrVultr 是一家 VPS 服务器提供商，有美国、亚洲、欧洲等多地的 VPS。它家的服务器以性价比高闻名，按时间计费，最低的资费为每月 $2.5。\nlinuxLinux 是免费开源的操作系统，大概被世界上过半服务器所采用。有大量优秀的开源软件可以安装，上述 Shadowsocks 就是其一。你可以通过命令行来直接给 Linux 操作系统「下命令」，比如 $ cd ~/Desktop 就是进入你根目录下的 Desktop 文件夹。\nssh SSH 是一种网络协议，作为每一台 Linux 电脑的标准配置，用于计算机之间的加密登录。当你为租用的 VPS 安装 Linux 系统后，只要借助一些工具，就可以用 SSH 在你自己的 Mac/PC 电脑上远程登录该 VPS 了。\nshadowsocksShadowsocks(ss) 是由 Clowwindy 开发的一款软件，其作用本来是加密传输资料。当然，也正因为它加密传输资料的特性，使得 GFW 没法将由它传输的资料和其他普通资料区分开来，也就不能干扰我们访问那些「不存在」的网站了。","text":"记性不好，做个记录，日后有需要时难得费神。 名词解释了解一些原理，熟悉一些名词，也方便理解接下来安装过程中的操作。 vpsVPS(Virtual private server) 译作虚拟专用伺服器。你可以把它简单地理解为一台在远端的强劲电脑。当你租用了它以后，可以给它安装操作系统、软件，并通过一些工具连接和远程操控它。 vultrVultr 是一家 VPS 服务器提供商，有美国、亚洲、欧洲等多地的 VPS。它家的服务器以性价比高闻名，按时间计费，最低的资费为每月 $2.5。 linuxLinux 是免费开源的操作系统，大概被世界上过半服务器所采用。有大量优秀的开源软件可以安装，上述 Shadowsocks 就是其一。你可以通过命令行来直接给 Linux 操作系统「下命令」，比如 $ cd ~/Desktop 就是进入你根目录下的 Desktop 文件夹。 ssh SSH 是一种网络协议，作为每一台 Linux 电脑的标准配置，用于计算机之间的加密登录。当你为租用的 VPS 安装 Linux 系统后，只要借助一些工具，就可以用 SSH 在你自己的 Mac/PC 电脑上远程登录该 VPS 了。 shadowsocksShadowsocks(ss) 是由 Clowwindy 开发的一款软件，其作用本来是加密传输资料。当然，也正因为它加密传输资料的特性，使得 GFW 没法将由它传输的资料和其他普通资料区分开来，也就不能干扰我们访问那些「不存在」的网站了。 搭建vps目的就是搭建梯子。无建站的需求。推荐vultr，最便宜的有2.5美元一个月。500g流量完全够用了。且现在支持支付宝付款，颇为方便。现阶段的优惠活动是新注册的用户完成指定的任务会获得3美元的奖励。（详细情况可依参见官网。） 注册首先点击右侧注册链接：https://www.vultr.com/2017Promo，然后会来到下图所示的注册页面。 第一个框中填写注册邮箱，第二个框中填写注册密码（至少包含1个小写字母、1个大写字母和1个数字），最后点击Create Account创建账户。 创建账户后注册邮箱会收到一封验证邮件，我们需要点击Verify Your E-mail来验证邮箱。 如果注册邮箱收不到验证邮件请更换注册邮箱后重复第一步。 验证邮箱后我们会来到下图所示的登录界面，按下图中指示填写信息，然后点击Login登录。 登陆后我们会来到充值界面。Vultr要求新账户充值后才可以正常创建服务器。Vultr已经支持支付宝了，在这里推荐大家使用支付宝充值，最低金额为10美元。 购买充值完毕后点击右上角的蓝色加号按钮进入创建服务器界面。 首先需要选择Server Location即机房位置，从左到右、从上到下依次为东京、新加坡、伦敦、法兰克福、巴黎、阿姆斯特丹、迈阿密、亚特兰大、芝加哥、硅谷、达拉斯、洛杉矶、纽约、西雅图、悉尼。 然后需要选择Server Type即服务类型，这里大家需要选择安装Debian 7 x64系统，因为这个系统折腾起来比较容易，搭建东西也简单便捷。 然后需要选择Server Size即方案类型，这里大家可以按照需要自行选择，如果只是普通使用那么选择第二个5美元方案即可。 然后Additional Features、Startup Script、SSH Keys以及Server Hostname &amp; Label等四部分大家保持默认即可，最后点击右下方的蓝色Deploy Now按钮确认创建服务器。 创建服务器后我们会看到下图所示界面。 上图中我们需要耐心等待3~4分钟，等红色Installing字变为绿色Running字后，点击Cloud Instance即可进入服务器详细信息界面，如下图所示。 左侧红框内四行信息依次为机房位置、IP地址、登录用户名、登录密码。IP地址后面的按钮为复制IP地址，登录密码后面的按钮为复制密码及显示/隐藏密码。右上角红框内后面四个按钮分别是关闭服务器、重启服务器、重装系统、删除服务器。 远程登录安装远程登录软件。这里以windos端的xshell为例。使用mac的同学可以下载iTerm。 下载安装后打开软件。根据下图中的指示，我们点击会话框中的新建按钮。 点击新建按钮后会弹出下图所示界面。根据图中指示，我们首先填写IP地址，然后点击确定按钮。 点击确定按钮后我们会回到下图所示界面。根据图中指示，我们双击打开新建会话或者点击下方连接按钮打开新建会话。 开新建会话后会弹出下图所示界面。根据图中指示，我们点击接受并保存按钮。 点击接受并保存按钮会弹出下图所示界面。根据图中指示，我们首先填写SSH连接密码，然后打钩记住密码，最后点击确定按钮。 如果提示需要输入用户名（登录名），那么请输入root！ 点击确定按钮后服务器会自动连接，连接完毕后我们会来到下图所示界面 部署shadowsocks这里采用网上整理的一键部署的方案。简单方便操作。 首先复制以下内容： 1wget -N --no-check-certificate https://0123.cool/download/55r.sh &amp;&amp; chmod +x 55r.sh &amp;&amp; ./55r.sh 然后回到Xshell软件，右击选择粘贴，粘贴完毕后回车继续。 回车后系统会自行下载脚本文件并运行。根据下图图中指示，我们依次输入SSR的各项连接信息，最后回车继续。 安装完成后会出现下图所示界面。根据图中指示，我们将红框圈中的信息保存到记事本内。 配置锐意加速根据下图图中指示，我们继续复制下列信息： 1wget -N --no-check-certificate https://0123.cool/download/rs.sh &amp;&amp; bash rs.sh install 然后回到Xshell软件，右击选择粘贴，粘贴完毕后回车继续。 回车后系统会自行下载脚本文件并运行。根据下图图中指示，我们依次输入锐速的各项配置信息，最后回车继续。 回车后，系统自动执行命令完成破解版锐速安装，如下图所示。 我们首先输入： 1reboot 然后回车，Xshell会断开连接，系统会在1分钟后重启完毕，此时可以关闭Xshell软件了。 搭建教程到此结束，亲测成功。如果不能连接的，请检查自己的每一步操作。","raw":null,"content":null,"categories":[{"name":"vps","slug":"vps","permalink":"https://stanxia.github.io/categories/vps/"}],"tags":[{"name":"vps","slug":"vps","permalink":"https://stanxia.github.io/tags/vps/"}]},{"title":"spark报错集","slug":"spark报错集","date":"2017-10-30T05:58:58.000Z","updated":"2017-11-23T03:29:59.000Z","comments":true,"path":"2017/10/30/spark报错集/","link":"","permalink":"https://stanxia.github.io/2017/10/30/spark报错集/","excerpt":"\n有话要说针对一个老毛病：有些错误屡犯屡改，屡改屡犯，没有引起根本上的注意，或者没有从源头理解错误发生的底层原理，导致做很多无用功。\n总结历史，并从中吸取教训，减少无用功造成的时间浪费。特此将从目前遇到的spark问题全部记录在这里，搞清楚问题，自信向前。","text":"有话要说针对一个老毛病：有些错误屡犯屡改，屡改屡犯，没有引起根本上的注意，或者没有从源头理解错误发生的底层原理，导致做很多无用功。 总结历史，并从中吸取教训，减少无用功造成的时间浪费。特此将从目前遇到的spark问题全部记录在这里，搞清楚问题，自信向前。 问题汇总关键词：spark-hive概述：1Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Unable to instantiate SparkSession with Hive support because Hive classes are not found. 场景：1在本地调试spark程序，连接虚拟机上的集群，尝试执行sparkSQL时，启动任务就报错。 原理：1缺少sparkSQL连接hive的必要和依赖jar包 办法：123456789在项目／模块的pom.xml中添加相关的spark-hive依赖jar包。&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive_2.11 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;重新编译项目／模块即可。","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://stanxia.github.io/tags/spark/"}]},{"title":"life","slug":"life","date":"2017-10-29T03:00:42.000Z","updated":"2017-11-23T03:19:35.000Z","comments":true,"path":"2017/10/29/life/","link":"","permalink":"https://stanxia.github.io/2017/10/29/life/","excerpt":"\n\nvar dplayer0 = new DPlayer({\"element\":document.getElementById(\"dplayer0\"),\"autoplay\":false,\"theme\":\"#FADFA3\",\"loop\":true,\"video\":{\"url\":\"http://oliji9s3j.bkt.clouddn.com/Unbroken%20-%20Motivational%20Video.mp4\",\"pic\":\"/images/pic/life.jpeg\"}});\nLife is simple &amp;&amp; funny.","text":"var dplayer0 = new DPlayer({\"element\":document.getElementById(\"dplayer0\"),\"autoplay\":false,\"theme\":\"#FADFA3\",\"loop\":true,\"video\":{\"url\":\"http://oliji9s3j.bkt.clouddn.com/Unbroken%20-%20Motivational%20Video.mp4\",\"pic\":\"/images/pic/life.jpeg\"}}); Life is simple &amp;&amp; funny.","raw":null,"content":null,"categories":[{"name":"movie","slug":"movie","permalink":"https://stanxia.github.io/categories/movie/"}],"tags":[{"name":"movie","slug":"movie","permalink":"https://stanxia.github.io/tags/movie/"}]},{"title":"杂乱无章","slug":"杂乱无章","date":"2017-10-28T17:31:31.000Z","updated":"2017-11-05T04:24:21.000Z","comments":true,"path":"2017/10/29/杂乱无章/","link":"","permalink":"https://stanxia.github.io/2017/10/29/杂乱无章/","excerpt":"\n时光的机器，加足马力冲回过去\n历史的长河，丝丝涟漪涌向未来","text":"时光的机器，加足马力冲回过去 历史的长河，丝丝涟漪涌向未来 道不清楚，说不明白，夜深人静的时候，说一些想到的废话。窗外隆隆作响，不知疲倦的机器不知疲倦的执行着不知疲倦的动作。窗内屏幕暗淡，双眼干涩，思索着宇宙外的回想。 小时候，望向星空，那时的天空群星闪烁，哪像现在，嘿，享受了大城市的霓虹，哪里再给你无垠的星空，贪。 躺在草地，微风轻拂脸颊，初秋的夜晚，有点微凉。 仰望星河，也想着外面的世界，多精彩。 揣摩着无垠的宇宙，翻过地球，越过银河，驶向无限拓展的星际，身上的烦恼，微风一吹，全散了。 风轻拂，静静望着天空，思考着外面的朋友或许也在渴望着远方的我，伸手触摸这天空，抓一把星辰贪婪的放入梦。 深邃的夜空，望不尽的远方，是光明中的无尽黑暗，也似黑暗道路的一束亮光，洒向我，思绪跟着遨游，呵，世界与我万千美好，我与世界却念念叨叨，琐琐碎碎，麻麻烦烦。心里是想放飞的。 夜深，车水呼啸，诉说着城市的不眠，可我困，关窗，闷。开窗，嘿，不知疲倦的机器又开始不知疲倦的执行不知疲倦的动作。这样的夜晚，眠难。 深夜思考，写作。夜使我宁静，内心的宁静，这白天的大城市给予不了。感谢夜的馈赠，接收这无上的加冕，驰骋在思绪的星空，痛快，精彩，精彩。 杂乱无章，呵，可以。","raw":null,"content":null,"categories":[{"name":"think","slug":"think","permalink":"https://stanxia.github.io/categories/think/"}],"tags":[{"name":"think","slug":"think","permalink":"https://stanxia.github.io/tags/think/"}]},{"title":"闲谈","slug":"闲谈","date":"2017-10-28T03:07:14.000Z","updated":"2017-11-05T04:23:25.000Z","comments":true,"path":"2017/10/28/闲谈/","link":"","permalink":"https://stanxia.github.io/2017/10/28/闲谈/","excerpt":"\n九九登高忆重阳","text":"九九登高忆重阳","raw":null,"content":null,"categories":[{"name":"think","slug":"think","permalink":"https://stanxia.github.io/categories/think/"}],"tags":[{"name":"think","slug":"think","permalink":"https://stanxia.github.io/tags/think/"}]},{"title":"这个杀手不太冷","slug":"这个杀手不太冷","date":"2017-10-27T17:18:04.000Z","updated":"2017-11-23T05:42:34.000Z","comments":true,"path":"2017/10/28/这个杀手不太冷/","link":"","permalink":"https://stanxia.github.io/2017/10/28/这个杀手不太冷/","excerpt":"Is life always this hard,or is it just when you’re a kid?Always like this.\n","text":"Is life always this hard,or is it just when you’re a kid?Always like this.","raw":null,"content":null,"categories":[{"name":"movie","slug":"movie","permalink":"https://stanxia.github.io/categories/movie/"}],"tags":[{"name":"movie","slug":"movie","permalink":"https://stanxia.github.io/tags/movie/"}]},{"title":"mac使用小技巧","slug":"mac使用小技巧","date":"2017-10-27T17:13:44.000Z","updated":"2017-11-23T02:14:40.000Z","comments":true,"path":"2017/10/28/mac使用小技巧/","link":"","permalink":"https://stanxia.github.io/2017/10/28/mac使用小技巧/","excerpt":"记录mac使用的小技巧持续更新ing \n\n开启充电提示音（类似于iphone充电提示音，默认关闭）终端输入（开启）：\n1defaults write com.apple.PowerChime ChimeOnAllHardware -bool true; open /System/Library/CoreServices/PowerChime.app &amp;\n关闭：\n1defaults write com.apple.PowerChime ChimeOnAllHardware -bool false;killall PowerChime","text":"记录mac使用的小技巧持续更新ing 开启充电提示音（类似于iphone充电提示音，默认关闭）终端输入（开启）： 1defaults write com.apple.PowerChime ChimeOnAllHardware -bool true; open /System/Library/CoreServices/PowerChime.app &amp; 关闭： 1defaults write com.apple.PowerChime ChimeOnAllHardware -bool false;killall PowerChime 隐藏文件夹更好的保护学习资料，有时候需要设置隐藏文件夹： 1mv foldername .foldername 查看隐藏文件夹mac最新版本： 1⌘⇧.(Command + Shift + .) #隐藏 和显示 Macbook Pro 用外接显示器时，如何关闭笔记本屏幕，同时开盖使用12sudo nvram boot-args=&quot;iog=0x0&quot; #(10.10以前版本)sudo nvram boot-args=&quot;niog=1&quot; #(10.10及以后版本)这个命令的意思就是外接显示器时关闭自身屏幕，重启生效 开机流程：连上电源和外接显示器，按开机键，立即合盖，等外接显示器有信号时开盖即可如果报错 (已知 10.11/10.12 会报错)nvram: Error setting variable - ‘boot-args’: (iokit/common) general error 重启，按住command + r 进入恢复界面 左上角菜单里面找到终端，输入nvram boot-args=”niog=1”，回车问题解决。重启生效","raw":null,"content":null,"categories":[{"name":"mac","slug":"mac","permalink":"https://stanxia.github.io/categories/mac/"}],"tags":[{"name":"mac","slug":"mac","permalink":"https://stanxia.github.io/tags/mac/"}]}]}