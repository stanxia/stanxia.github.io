<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="东篱下" type="application/atom+xml" />






<meta name="description" content="采菊东篱下，悠然见南山">
<meta property="og:type" content="website">
<meta property="og:title" content="东篱下">
<meta property="og:url" content="https://stanxia.github.io/page/2/index.html">
<meta property="og:site_name" content="东篱下">
<meta property="og:description" content="采菊东篱下，悠然见南山">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="东篱下">
<meta name="twitter:description" content="采菊东篱下，悠然见南山">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://stanxia.github.io/page/2/"/>





  <title>东篱下</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">东篱下</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">斯坦@森</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://stanxia.github.io/2017/04/25/企业大数据平台下数仓建设思路/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="森">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="东篱下">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/04/25/企业大数据平台下数仓建设思路/" itemprop="url">企业大数据平台下数仓建设思路</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-25T23:06:53+08:00">
                2017-04-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script src="/assets/js/DPlayer.min.js"> </script><p>大牛解读</p>
<blockquote>
<p>转载自：<a href="https://yq.aliyun.com/articles/67020" target="_blank" rel="external">https://yq.aliyun.com/articles/67020</a></p>
</blockquote>
<h2 id="数仓上遇到的挑战：数据质量保障、稳定和重复性"><a href="#数仓上遇到的挑战：数据质量保障、稳定和重复性" class="headerlink" title="数仓上遇到的挑战：数据质量保障、稳定和重复性"></a>数仓上遇到的挑战：数据质量保障、稳定和重复性</h2><p>在数据魔方、淘宝指数和阿里大数据数仓解决方案设计中，介然遇到了不少有挑战性的技术问题，主要集中在以下三点：</p>
<p>1.数据质量保障：随着业务的复杂度增加，数据源头的类型和数据量也会越来越多，经常会碰到某些数据源因为一些偶发的原因同步过来的数据质量出现问题。比如日志出现乱码、数据库因为切库造成数据同步量变少等等。这就要求在整个数仓体系的搭建过程中不只要完成数据业务逻辑的处理，还需要增加数据质量的监控。“我们在核心的数据处理流程中，增加数据质量监控代码，如果碰到数据量的突变或者核心指标的突变，会将数据处理流程暂停并预警，让数据运维人员处理数据质量问题后再进行后续数据流程的运行，保障有质量问题的数据不流到下游应用中。”</p>
<p>2.数据产出稳定性保障：随着数据量的增加、计算资源的逐渐饱和，业务数据最终产出的时间开始延迟，并有可能不能按照业务要求的时间点产出。“这个时候我们会分析数据产出的关键路径，找出关键路径下消耗时间最多的运行JOB，通过数据模型优化、计算任务拆解或者计算任务代码优化的手段减少任务产出的时间，同时保障整体产出时间满足预期。”</p>
<p>3.重复的数据处理代码：由于业务的特殊性，会对某种类型的数据加工操作需求非常多。比如计算交易中,TOP N的商家、TOP N 的品牌、TOP N的商品，商家中TOP N的商品、品牌中TOP N的商家等等。 这类代码都是非常类似的，如果每个计算都独立任务，会造成计算资源的大量浪费。“我们通过特殊的代码框架，让一份基础数据中多种TOPN的数据可以在一次计算过程中产出，大大减少资源消耗，保障数据产出稳定。”</p>
<h2 id="优秀数仓的三要素：清晰、保障和扩展性好"><a href="#优秀数仓的三要素：清晰、保障和扩展性好" class="headerlink" title="优秀数仓的三要素：清晰、保障和扩展性好"></a>优秀数仓的三要素：清晰、保障和扩展性好</h2><p>介然认为，优秀的数据仓库应该包含以下要素：</p>
<p>1.结构、分层清晰：不一定需要多少个分层和主题，但是一定要清晰。用数据的人能够很快找到需要数据的位置。</p>
<p>2.数据质量和产出时间有保障；</p>
<p>3.扩展性好：不会因为业务的些许变化造成模型的大面积重构。</p>
<p>而从系统架构、数据架构两个纬度来看，要想设计好大数据应用下的数据仓库，还应做到以下两点。</p>
<p>1.系统架构上：足够的容错性，减少不必要的系统间的强耦合。因为你会碰到各种问题，不要因为一个不必要的依赖造成数据无法产出。</p>
<p>2.数据架构上：简单、清晰、强质量控制。数据架构上扁平化的数据处理流程会对数据质量的控制和数据产出的稳定性提供非常好的基础。</p>
<h2 id="互联网人转型做大数据数仓需要注意哪几个点？"><a href="#互联网人转型做大数据数仓需要注意哪几个点？" class="headerlink" title="互联网人转型做大数据数仓需要注意哪几个点？"></a>互联网人转型做大数据数仓需要注意哪几个点？</h2><p>对于之前做互联网数据仓库，现在想转型做大数据仓库的人，介然也提了一些建议，主要是四点：</p>
<p>1.不必再苛刻的精打细算：基于传统平台构建数仓时，为了照顾平台的处理能力，我们经常会构建多层数据结构，预先对不同粒度的数据做预先汇总，以方便使用者在使用数据时能够已最小的计算代价获得计算结果。这也造成了整个数据处理流程较长，步骤很多，问题追溯困难。 新的大数据仓库基于分布式计算平台，平台的计算能力通常都比传统的平台强大很多。 所以有时候需要时再计算数据，或者基于明细进行各粒度的数据汇总已经能够满足需求，并能够大大减少整体数据处理流程步骤，用计算的代价减少人工的成本，更划算，数据体系也更健壮。</p>
<p>2.不是模型层次越多越好：在传统的数仓架构中，大家都喜欢多数据模型进行分层设计，不同的模型层次拥有不同的数据域和作用域。这样设计固然看起来更清晰，但实际情况时多层之间可能存在重复数据，或者数据使用者在上层找不到完全切合的数据时，更愿意从底层的明细数据上自己去加工。一方面造成了数据使用上的混乱，一方面也会让数据整个处理流程长度增加，对于数据的运维带来较大的成本消耗。合理的层次设计，及在计算成本和人力成本间的平衡，是一个好的数仓架构的表现。</p>
<p>3.质量是生命线：不再是你拿到的数据都是正确的，新的环境下的数据什么情况都会发生，而好的数仓架构需要有足够的容错性和质量保障。不要因为一条日志的乱码造成整个数据流程无法走通，也不要说一份日志50%的乱码你的程序还发现不了。在数据质量上投入再多的资源都不是浪费。</p>
<p>4.数据变成生产资料：传统的数据应用绝大部分都是以报表和BI分析的形式支持业务。也许你的报表晚出来会被老板骂一通，但是对业务的影响并不大。 但是在新的数据应用场景下，数据已经变成生产资料，数据会服务化直接应用到业务系统中，也许一份数据的质量出现问题或者产出延迟，都可能对你的业务系统产生致命的影响。所以数仓开始承担新的使命。</p>
<p><img src="https://yqfile.alicdn.com/b179703ed70608ead1d5f1c2ee360c9944cf65fd.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/d45d71f1e7150b1db613d34bff12b325d013ffbf.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/76327d7ec53932d8c16f6d1f02b0c5026eee96d1.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/ae9f20459c5838978a9d163767982b9e403cbd6e.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/ab1f9275d27c6cdbe39b1e9a46d746041c169e1a.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/487586d50b0fee659990f3915138c06b7fb523e6.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/8f085a4291ece66e7d1b8da28fddb4ea89825a3d.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/a45d8e4c431c951cda6690d0dbc4a48f9cad6eb3.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/2aa2cf31c8657b7e5722f40fd1921ad6bad21ff3.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/b6d3c753639d2a157f022cd84bf39990228c5a5e.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/fe10046f04fe1b57302aeaab49bb822aa0ba5b84.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/917add91d80ecc9e1cac080acac091a8d795ec83.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/caebb3f89a33489bb6250889c221556caf9e2a93.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/4d6f959ed86b8c47ae8e8b0ea3dcc800dfc92694.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/3ba23945c5ff03bdd679cb461a2f56b99baf2f94.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/42a5fd36b3020ccbed8e4f6cc1f34fdbe08ce239.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/a22871be93640b55ce022aba9648752bd1ebe73b.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/6eb136254008467abfd7401c8b81106fdc5460fd.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/7e962808351228f0e450e2c3c80f2eeffe058624.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/1ad7466dffee8b318c5aaadea9a50a7a90e424a6.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/349986d12df4c5d8e926fcc8bf6b6474a63b91a3.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/ab2e7bf1bb5321ca33fdbc9b62065466a9b10030.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/6202ca10ff3850da8a077bdacf2351b15400be2f.png" alt=""></p>
<p><img src="https://yqfile.alicdn.com/4c17dbdcc180832226b19ed8181e87be01bc3e6e.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://stanxia.github.io/2017/04/25/dplayer-test/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="森">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="东篱下">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/04/25/dplayer-test/" itemprop="url">使用 Hexo 插件插入音乐/视频</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-25T20:55:12+08:00">
                2017-04-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/测试功能/" itemprop="url" rel="index">
                    <span itemprop="name">测试功能</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script src="/assets/js/DPlayer.min.js"> </script><h1 id="使用-Hexo-插件插入音乐-视频"><a href="#使用-Hexo-插件插入音乐-视频" class="headerlink" title="使用 Hexo 插件插入音乐/视频"></a>使用 Hexo 插件插入音乐/视频</h1><p>用于播放视频和音乐的的hexo插件：</p>
<p><strong>hexo-tag-aplayer：<a href="https://github.com/grzhan/hexo-tag-aplayer" target="_blank" rel="external">https://github.com/grzhan/hexo-tag-aplayer</a></strong></p>
<p><strong>hexo-tag-dplayer： <a href="https://github.com/NextMoe/hexo-tag-dplayer" target="_blank" rel="external">https://github.com/NextMoe/hexo-tag-dplayer</a></strong></p>
<h2 id="播放音乐的aplayer"><a href="#播放音乐的aplayer" class="headerlink" title="播放音乐的aplayer"></a>播放音乐的aplayer</h2><p>在cmd页面内，使用npm安装：<br><code>npm install hexo-tag-aplayer</code></p>
<p>在markdown内添加以下代码：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;% aplayer "她的睫毛" "周杰伦" "http://home.ustc.edu.cn/~mmmwhy/%d6%dc%bd%dc%c2%d7%20-%20%cb%fd%b5%c4%bd%de%c3%ab.mp3"  "http://home.ustc.edu.cn/~mmmwhy/jay.jpg" "autoplay=false" %&#125;</div></pre></td></tr></table></figure>
<h2 id="播放视频的dplayer"><a href="#播放视频的dplayer" class="headerlink" title="播放视频的dplayer"></a>播放视频的dplayer</h2><p>在cmd页面内，使用npm安装：<br><code>npm install hexo-tag-dplayer</code></p>
<p>在markdown内添加以下代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;% dplayer &quot;url=http://home.ustc.edu.cn/~mmmwhy/GEM.mp4&quot;  &quot;pic=http://home.ustc.edu.cn/~mmmwhy/GEM.jpg&quot; &quot;loop=yes&quot; &quot;theme=#FADFA3&quot; &quot;autoplay=false&quot; &quot;token=tokendemo&quot; %&#125;</div></pre></td></tr></table></figure>
<div id="dplayer7" class="dplayer" style="margin-bottom: 20px;"></div><script>var dplayer7 = new DPlayer({"element":document.getElementById("dplayer7"),"autoplay":false,"theme":"#FADFA3","loop":true,"video":{"url":"http://home.ustc.edu.cn/~mmmwhy/GEM.mp4","pic":"http://home.ustc.edu.cn/~mmmwhy/GEM.jpg"}});</script>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://stanxia.github.io/2017/04/24/Apache Kylin在唯品会大数据的应用/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="森">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="东篱下">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/04/24/Apache Kylin在唯品会大数据的应用/" itemprop="url">Apache Kylin在唯品会大数据的应用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-24T20:54:56+08:00">
                2017-04-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script src="/assets/js/DPlayer.min.js"> </script><blockquote>
<p>转载自：<a href="http://www.infoq.com/cn/articles/application-of-apache-kylin-in-vip-big-data" target="_blank" rel="external">http://www.infoq.com/cn/articles/application-of-apache-kylin-in-vip-big-data</a></p>
</blockquote>
<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>引子：随着传统基于RDBMS的EDW往大数据的演进的过程中，Batch可处理的数据量越来越大，时间越来越快，但是Ad-hoc的响应速度却始终是大数据的瓶颈。</p>
<p>在2015年 唯品会的数据分析碰到了以下两个瓶颈：第一是数据准备的流程长，第二是缺少合适数据提取和分析工具。</p>
<p><strong>首先</strong>，从数据准备流程来看，常见的流程是业务人员提出需求，BI同事定角度、找数据， 如果数据不完善，还得继续找数据开发。这就导致同一个需求，需要和不同的人反复沟通，在沟通过程中参与的人越多，信息衰减也就越厉害。再加上排期的等待，最终的结果一方面可能与初衷有所偏差，另一方面时间一长也失去了对热点关注度，分析变得非常滞后，不能及时的反应线上业务并加以改进。</p>
<p><strong>其次</strong>，对于有分析能力的业务侧同学，没有趁手的工具就导致即使有能力准备撩袖子大干一场了也发现巧妇难为无米之炊，大家只能感慨大数据的门槛太高了，又回到了第一点的长时间等待的恶性循环里去了。</p>
<p>我们总结下来，在唯品会这样规模的公司里，数据分析有两个痛点：</p>
<ol>
<li>需要一个可以自由组合的维度和指标的平台，业务人员可以根据自己的视角自给自足的完成数据提取和分析；</li>
<li>这个平台，不仅数据要够丰富，即使大数据量响应速度也要快。</li>
</ol>
<p>针对这两个痛点，本着“让大数据成为唯品会的增长引擎”这个目标，我们大数据部门的提供了一套完整的解决方案：自助多维分析平台。我们通过有较高可扩展性的维度建模准备数据，在此之上搭建一套数据查询引擎，并配上操作简单的数据可视化前端，为业务人员搭了数据分析的台子。随着大家数据分析技能的提升，人人都是数据分析师的这个理念就逐渐在公司内部扩展开来了。</p>
<p><img src="http://cdn1.infoqstatic.com/statics_s2_20170411-0445/resource/articles/application-of-apache-kylin-in-vip-big-data/zh/resources/10.jpg" alt="img"></p>
<h2 id="唯品会如何使用Kylin"><a href="#唯品会如何使用Kylin" class="headerlink" title="唯品会如何使用Kylin"></a>唯品会如何使用Kylin</h2><p>数据和前端是皮和肉，需要通过好的数据引擎才能支撑起来。在数据引擎角度，我们通过一段时间的积累和演进，从基于Presto的ROLAP模型进化到了基于Kylin和Presto的双计算引擎。往超大数据集也要快速ad-hoc响应的方向走近了一步。</p>
<p>第一阶段，我们的目标是在Ad-hoc响应时间&lt;= 10秒的前提条件下，支持：</p>
<ol>
<li>平均每次查询10亿+明细数据做汇总；</li>
<li>平均每个查询0-15个维度；</li>
<li>平均每个查询1-5个指标。</li>
</ol>
<p>根据这个目标，我们选择使用Presto作为计算引擎，Presto MPP的架构 + 通过Hive Connector直接访问HDFS上的数据，为我们提供良好的Ad-hoc响应速度和相对较低的维护成本。为了满足高Ad-hoc响应速度的需求，常见的做法是把HDFS上处理完的数据同步到Ad-hoc响应友好的数据库中，比如GreenPlum或Hbase等，但这样的缺点是虽然速度上去了，但数据模型在Hive和Ad-hoc库中需要维护两份并保持一致，维护的成本非常高。Presto的Connector机制很好的解决了这个问题，同时他的计算能力也满足了我们第一阶段的需求。</p>
<p>然后我们通过SQL Parser，将前端拖拽或事件描述的对象转化为SQL，同时完成SQL的变形和性能优化，把计算引擎和用户操作连接在一起，完成了第一阶段的目标。</p>
<p><img src="http://cdn1.infoqstatic.com/statics_s2_20170411-0445/resource/articles/application-of-apache-kylin-in-vip-big-data/zh/resources/11.jpg" alt="img"></p>
<p>自助多维分析平台一阶段逻辑架构</p>
<p>随着业务的不断增长，在自助多维分析平台上逐渐出现了很多维度和指标组合类似、频率较高的查询，这些查询有着明显的模式，且通过分析我们了解到这些维度和指标的组合是业务部门常用的核心数据。这些查询反复的在Presto上执行，显然不是最佳选择，也达不到业务部门提出的新目标，核心数据查询响应时间&lt;=3秒。<strong>此时，Kylin就成了我们的首选</strong>。我们的数据引擎的架构，也从单纯的操纵SQL扩展到计算引擎的路由。通过读取Metadata并根据规则，在Kylin和Presto两个计算引擎之间路由，我们可以在不显著提高数据模型维护成本的前提条件下，通过Kylin对关键数据做预计算，提高核心数据的响应速度。</p>
<p><img src="http://cdn1.infoqstatic.com/statics_s2_20170411-0445/resource/articles/application-of-apache-kylin-in-vip-big-data/zh/resources/12.jpg" alt="img"></p>
<h2 id="为什么选择Kylin"><a href="#为什么选择Kylin" class="headerlink" title="为什么选择Kylin"></a>为什么选择Kylin</h2><p>首先，Kylin利用空间换时间，从原理上已经确保了Ad-hoc响应速度达标，和Oracle CUBE/物化视图的原理相同易于理解。</p>
<p>第二，Kylin支持SQL，这对于数据分析而言至关重要，同时满足我们一个SQL在不同计算引擎之间路由的需求。</p>
<p>另外，Kylin的SQL on Hbase的实现也很好的解决了Hbase不易查询的问题。第三是支持Dimension-Fact的join，这极大的解耦了数据模型和计算引擎之间的关系，不像ES或Pinot只支持单表，还有为他们专门处理数据的额外工作。第四是对数据开发来说，创建和管理CUBE比较简单，且透明化了MR和HBASE同步。第五是可以很方便的在调度系统中调用Kylin API定时刷新CUBE。综上所述，Kylin对于一个数据分析系统来说是一个好的解决方案。</p>
<p>经过一段时间的测试和线上运行，我们在之前把Kylin覆盖到核心指标的查询基础上还扩展到了在Presto上查询需要30秒以上的指标和维度组合上。因为这类查询往往需要扫描大量的基础数据，在Kylin上预计算可以有效的较低资源使用。另一方面，基于自助多维分析平台的业务场景，我们也在以下两个场景中不启用Kylin。第一是维度的基数大于1亿的场景，主要是由于大基数的维度加载的Kylin Server的内存中容易引起OOM。第二是数据模型经常变化的主题，在Kylin中维护CUBE的成本就很高了，每次变化都需要重建CUBE，重刷数据，这显然与我们提高复用降低重复开发的初衷不符。对于这两个场景，由Presto完成计算也可以很好的满足需求。</p>
<p>基于以上的原则，目前我们累计有20+个CUBE，10+T存储，最大CUBE记录数上千亿，覆盖了23%的查询。同时，Ad-hoc的响应速度也令人满意。Kylin的平均响应速度是Presto的10.5倍，中位数响应速度是Presto的4.5倍。</p>
<p><img src="http://cdn1.infoqstatic.com/statics_s2_20170411-0445/resource/articles/application-of-apache-kylin-in-vip-big-data/zh/resources/13.jpg" alt="img"></p>
<h2 id="唯品会对Kylin做的改进"><a href="#唯品会对Kylin做的改进" class="headerlink" title="唯品会对Kylin做的改进"></a>唯品会对Kylin做的改进</h2><p>针对唯品会的痛点，我们也在开源框架的基础上进行了修改。基础升级方面，我们针对自助多维分析平台的需求进行了升级。比如，在查找CUBE的时候，仅当CUBE内数据包含SQL查询的时间范围才命中CUBE，避免给用户不完整的数据集。同时我们采集了Kylin运行中metadata，并给予这些数据提供SQL分析API以解析Kylin能运行的SQL子查询。另外一些BUG修复也提交到了社区。</p>
<p>除此之外，我们基于Presto+Kylin双引擎的架构，开发了Presto on Kylin这个功能。通过在Presto侧增加Kylin Connector，我们支持了Kylin与Hive数据源的跨源Join，支持Raw data汇总后的数据和Kylin Cube 数据Join。为了支持以上两个功能，我们在Kylin增加了Explain功能简化了Cube命中探查的复杂度。同时，为了进一步降低数据开发寻找查询组合的复杂度，我们开发了Cube Advisor，通过统计分析Presto SQL获得所有维度和指标的组合频次，根据最常使用和响应时间长两个条件，推荐合适的Cube定义建议，数据开发可以直接根据推荐的建议创建Cube。</p>
<p>下一步，我们会改造Kylin维表的Cache机制，解决大基数维表不能创建CUBE的问题，同时进一步扩展CUBE Advisor支持一键生成CUBE的功能并能够支持自动刷新历史数据，降低人工维护成本。同时，将Kylin的应用推广到报表类数据产品。</p>
<p>在提高大数据分析Ad-hoc响应速度的路上，可谓八仙过海各显神通，我们通过Presto和Kylin的结合满足了当前的需求，后面我们也会继续探索更多解决方案，寻找下一代的多维分析引擎，在此过程中，欢迎大家与我们一起讨论。</p>
<h2 id="讲师介绍"><a href="#讲师介绍" class="headerlink" title="讲师介绍"></a>讲师介绍</h2><p><strong>谢麟炯，</strong>唯品会大数据平台高级技术架构经理，主要负责大数据自助多维分析平台，离线数据开发平台及分析引擎团队的开发和管理工作 ，加入唯品会以来还曾负责流量基础数据的 采集和数据仓库建设以及移动流量分析等数据产品的工作。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://stanxia.github.io/2017/03/21/EasyHadoop集群部署入门/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="森">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="东篱下">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/03/21/EasyHadoop集群部署入门/" itemprop="url">EasyHadoop集群部署入门</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-03-21T10:13:32+08:00">
                2017-03-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script src="/assets/js/DPlayer.min.js"> </script><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<h1 id="EasyHadoop-让你的Hadoop应用飞起来"><a href="#EasyHadoop-让你的Hadoop应用飞起来" class="headerlink" title="EasyHadoop 让你的Hadoop应用飞起来!"></a>EasyHadoop 让你的Hadoop应用飞起来!</h1><h1 id=""><a href="#" class="headerlink" title=""></a><img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image004.png" alt="img"></h1><h1 id="EasyHadoop集群部署入门"><a href="#EasyHadoop集群部署入门" class="headerlink" title="EasyHadoop集群部署入门"></a><a href="undefined">EasyHadoop</a>集群部署入门</h1><p><a href="undefined">目录</a></p>
<p><a href="#_Toc17781">EasyHadoop集群部署入门文档……………………………………………………………………………………………………………………………………………………………………….. 2</a></p>
<p><a href="#_Toc20988">目录……………………………………………………………………………………………………………………………………………………………………………………………………………………….. 2</a></p>
<p><a href="#_Toc4660">1. 文档概述……………………………………………………………………………………………………………………………………………………………………………………………….. 3</a></p>
<p><a href="#_Toc2456">2. 背景………………………………………………………………………………………………………………………………………………………………………………………………………… 3</a></p>
<p><a href="#_Toc32501">3. 名词解释……………………………………………………………………………………………………………………………………………………………………………………………….. 4</a></p>
<p><a href="#_Toc16693">4. 服务器结构…………………………………………………………………………………………………………………………………………………………………………………………… 4</a></p>
<p><a href="#_Toc13472">#Hadoop试验集群的部署结构……………………………………………………………………………………………………………………………………………………… 4</a></p>
<p><a href="#_Toc2122">#系统和组建的依赖关系……………………………………………………………………………………………………………………………………………………………….. 5</a></p>
<p><a href="#_Toc11834">#生产环境的部署结构……………………………………………………………………………………………………………………………………………………………………. 6</a></p>
<p><a href="#_Toc6542">5. Red hat Linux基础环境搭建…………………………………………………………………………………………………………………………………………………………………. 6</a></p>
<p><a href="#_Toc16475">#linux 安装 (vm虚拟机)………………………………………………………………………………………………………………………………………………………………… 6</a></p>
<p><a href="#_Toc20745">#配置机器时间同步………………………………………………………………………………………………………………………………………………………………………… 6</a></p>
<p><a href="#_Toc4466">#配置机器网络环境………………………………………………………………………………………………………………………………………………………………………… 7</a></p>
<p><a href="#_Toc13264">#配置集群hosts列表…………………………………………………………………………………………………………………………………………………………………….. 10</a></p>
<p><a href="#_Toc12566">#下载并安装 JAVA JDK系统软件……………………………………………………………………………………………………………………………………………….. 10</a></p>
<p><a href="#_Toc7823">#生成登陆密钥………………………………………………………………………………………………………………………………………………………………………………. 11</a></p>
<p><a href="#_Toc12023">#创建用户账号和Hadoop部署目录和数据目录…………………………………………………………………………………………………………………… 11</a></p>
<p><a href="#_Toc6460">#检查基础环境………………………………………………………………………………………………………………………………………………………………………………. 12</a></p>
<p><a href="#_Toc22052">6. Hadoop 单机系统 安装配置…………………………………………………………………………………………………………………………………………………………… 13</a></p>
<p><a href="#_Toc440">#Hadoop 文件下载和解压…………………………………………………………………………………………………………………………………………………………… 13</a></p>
<p><a href="#_Toc6086">#配置 hadoop-env.sh 环境变量………………………………………………………………………………………………………………………………………………….. 13</a></p>
<p><a href="#_Toc3113">#Hadoop Common组件 配置core-site.xml………………………………………………………………………………………………………………………………… 13</a></p>
<p><a href="#_Toc18568">#HDFS NameNode,DataNode组建配置hdfs-site.xml…………………………………………………………………………………………………………………… 14</a></p>
<p><a href="#_Toc20292">#配置MapReduce - JobTracker TaskTracker 启动配置………………………………………………………………………………………………………………… 15</a></p>
<p><a href="#_Toc11828">#Hadoop单机系统,启动执行和异常检查………………………………………………………………………………………………………………………………… 17</a></p>
<p><a href="#_Toc1006">#通过界面查看集群部署部署成功…………………………………………………………………………………………………………………………………………… 18</a></p>
<p><a href="#_Toc1875">#通过执行 Hadoop pi 运行样例检查集群是否成功…………………………………………………………………………………………………………….. 19</a></p>
<p><a href="#_Toc6872">#安装部署 常见错误……………………………………………………………………………………………………………………………………………………………………. 20</a></p>
<p><a href="#_Toc17628">7. Hadoop 集群系统 配置安装配置………………………………………………………………………………………………………………………………………………….. 20</a></p>
<p><a href="#_Toc8194">#检查node节点linux 基础环境是否正常,参考 [ linux 基础环境搭建]一节。…………………………………………………………….. 20</a></p>
<p><a href="#_Toc11821">#配置从master 机器到 node 节点无密钥登陆…………………………………………………………………………………………………………………….. 20</a></p>
<p><a href="#_Toc25415">#检查master到每个node节点在hadoop用户下使用密钥登陆是否正常………………………………………………………………………. 21</a></p>
<p><a href="#_Toc10716">#配置master 集群服务器地址 stop-all.sh start-all.sh 的时候调用……………………………………………………………………………………….. 21</a></p>
<p><a href="#_Toc22114">#通过界面查看集群部署部署成功…………………………………………………………………………………………………………………………………………… 22</a></p>
<p><a href="#_Toc31281">#通过执行 Hadoop pi 运行样例检查集群是否成功…………………………………………………………………………………………………………….. 24</a></p>
<p><a href="#_Toc21520">8. 自动化安装脚本………………………………………………………………………………………………………………………………………………………………………………… 25</a></p>
<p><a href="#_Toc29249">#master 服务器自动安装脚本…………………………………………………………………………………………………………………………………………………….. 25</a></p>
<p><a href="#_Toc24883">Hive仓库集群部署入门文档…………………………………………………………………………………………………………………………………………………………………………. 27</a></p>
<p><a href="#_Toc4350">1. 名词解释……………………………………………………………………………………………………………………………………………………………………………………………… 27</a></p>
<p><a href="#_Toc16424">2. Hive的作用和原理说明…………………………………………………………………………………………………………………………………………………………………….. 27</a></p>
<p><a href="#_Toc6591">#数据仓库结构图………………………………………………………………………………………………………………………………………………………………………….. 27</a></p>
<p><a href="#_Toc12201">#Hive仓库流程图…………………………………………………………………………………………………………………………………………………………………………… 27</a></p>
<p><a href="#_Toc15882">#hive内部结构图……………………………………………………………………………………………………………………………………………………………………………. 27</a></p>
<p><a href="#_Toc14841">3. Hive 部署和安装…………………………………………………………………………………………………………………………………………………………………………………. 27</a></p>
<p><a href="#_Toc19110">#安装Hadoop集群,看EasyHadoop安装文档。……………………………………………………………………………………………………………………….. 27</a></p>
<p><a href="#_Toc24103">#安装Mysql,启动Mysql,检查gc++包。………………………………………………………………………………………………………………………………………. 27</a></p>
<p><a href="#_Toc6714">#解压Hive包并配置JDBC连接地址。……………………………………………………………………………………………………………………………………… 27</a></p>
<p><a href="#_Toc27818">#启动Hive thrift Server。………………………………………………………………………………………………………………………………………………………………… 27</a></p>
<p><a href="#_Toc25105">#启动内置的Hive UI。………………………………………………………………………………………………………………………………………………………………….. 27</a></p>
<p><a href="#_Toc31144">4. Hive Cli 的基本用法……………………………………………………………………………………………………………………………………………………………………………. 28</a></p>
<p><a href="#_Toc20912">#登陆查询……………………………………………………………………………………………………………………………………………………………………………………….. 28</a></p>
<p><a href="#_Toc6817">#查询文件方式………………………………………………………………………………………………………………………………………………………………………………. 28</a></p>
<p><a href="#_Toc1120">#命令行模式…………………………………………………………………………………………………………………………………………………………………………………… 28</a></p>
<p><a href="#_Toc4701">5. HQL基本语法 (创建表,加载表,分析查询,删除表)……………………………………………………………………………………………………………………… 28</a></p>
<p><a href="#_Toc31717">#创建表……………………………………………………………………………………………………………………………………………………………………………………………. 28</a></p>
<p><a href="#_Toc25926">6. 使用Mysql构建简单数据集市……………………………………………………………………………………………………………………………………………………….. 29</a></p>
<p><a href="#_Toc21993">#Mysql的两种引擎介绍……………………………………………………………………………………………………………………………………………………………….. 29</a></p>
<p><a href="#_Toc24983">#创建一个数据表使用Hive cli 进行数据分析………………………………………………………………………………………………………………………… 29</a></p>
<p><a href="#_Toc14839">#使用shell 编写Hsql 并使用HiveCli导出数据,使用Mysql命令加载到数据库中。……………………………………………………… 29</a></p>
<p><a href="#_Toc16906">#使用crontab 新增每日运行任务定时器……………………………………………………………………………………………………………………………….. 29</a></p>
<p><a href="#_Toc11243">7. 使用FineReport 数据展现数据………………………………………………………………………………………………………………………………………………………. 29</a></p>
<p><a href="#_Toc1768">#安装FineReport,使用注册码!……………………………………………………………………………………………………………………………………………………… 29</a></p>
<p><a href="#_Toc3908">#使用FineReport,快速展现数据报表。…………………………………………………………………………………………………………………………………….. 29</a></p>
<p><a href="#_Toc16553">#FineReport 的问题和局限…………………………………………………………………………………………………………………………………………………………… 29</a></p>
<h2 id="1-文档概述"><a href="#1-文档概述" class="headerlink" title="1.  文档概述"></a><a href="undefined">1.  文档概述</a></h2><p>本文档是Hadoop部署文档,提供了Hadoop单机安装和Hadoop集群安装的方法和步骤,本文档希望让Hadoop安装部署更简单(Easy)。</p>
<p>本安装文档适用于 centos 5 /red hat 5.2 32位,64位版本,ubuntu 等操作系统需要做部分修改。</p>
<h2 id="2-背景"><a href="#2-背景" class="headerlink" title="2.  背景"></a><a href="undefined">2.  背景</a></h2><p>Hadoop为分布式文件系统和计算的基础框架系统，其中包含hadoop程序，hdfs系统等。</p>
<h2 id="3-名词解释"><a href="#3-名词解释" class="headerlink" title="3.  名词解释"></a><a href="undefined">3.  名词解释</a></h2><p>1.Hadoop,  Apache开源的分布式框架。</p>
<p>2.HDFS,                  hadoop的分布式文件系统</p>
<p>3.NameNode,       hadoop HDFS元数据主节点服务器，负责保存DataNode 文件存储元数据信息。</p>
<p>4.JobTracker,        hadoop的Map/Reduce调度器，负责与TackTracker通信分配计算任务并跟踪任务进度。</p>
<p>5.DataNode,         hadoop数据节点，负责存储数据。</p>
<p>6.TaskTracker,       hadoop调度程序，负责Map,Reduce 任务的具体启动和执行。</p>
<p>7.Fuse,         多文件系统内核程序，可将不同的文件系统mount成linux可读写模式</p>
<h2 id="4-服务器结构"><a href="#4-服务器结构" class="headerlink" title="4.  服务器结构"></a><a href="undefined">4.  服务器结构</a></h2><h3 id="Hadoop试验集群的部署结构"><a href="#Hadoop试验集群的部署结构" class="headerlink" title="#Hadoop试验集群的部署结构"></a><a href="undefined">#Hadoop</a>试验集群的部署结构</h3><p>部署路径：/opt/modules/hadoop/hadoop-1.0.3/</p>
<p><img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image006.png" alt="img"></p>
<h3 id="-1"><a href="#-1" class="headerlink" title=""></a><img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image008.png" alt="img"></h3><h3 id="系统和组建的依赖关系"><a href="#系统和组建的依赖关系" class="headerlink" title="#系统和组建的依赖关系"></a><a href="undefined">#</a>系统和组建的依赖关系</h3><p><img src="/Users/xialinsheng/Desktop/微博图床图片/hadoop组建依赖.jpeg" alt="hadoop组建依赖">  </p>
<h3 id="生产环境的部署结构"><a href="#生产环境的部署结构" class="headerlink" title="#生产环境的部署结构"></a><a href="undefined">#</a>生产环境的部署结构</h3><p><img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image010.png" alt="img"></p>
<h2 id="5-Red-hat-Linux基础环境搭建"><a href="#5-Red-hat-Linux基础环境搭建" class="headerlink" title="5.  Red hat Linux基础环境搭建"></a><a href="undefined">5.  Red hat Linux</a>基础环境搭建</h2><h3 id="linux-安装-vm虚拟机"><a href="#linux-安装-vm虚拟机" class="headerlink" title="#linux 安装 (vm虚拟机)"></a><a href="undefined">#linux </a>安装 (vm虚拟机)</h3><p>请参考其他 vmware 虚拟机安装文档。设置网络为 net 模式。</p>
<p>root hadoop </p>
<h3 id="配置机器时间同步"><a href="#配置机器时间同步" class="headerlink" title="#配置机器时间同步"></a><a href="undefined">#</a>配置机器时间同步</h3><p>#配置时间同步</p>
<p>crontab -e </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">0 1 * * * /usr/sbin/ntpdate cn.pool.ntp.org</div></pre></td></tr></table></figure>
<p>#手动同步时间</p>
<p>/usr/sbin/ntpdatecn.pool.ntp.org</p>
<h3 id="配置机器网络环境"><a href="#配置机器网络环境" class="headerlink" title="#配置机器网络环境"></a><a href="undefined">#</a>配置机器网络环境</h3><p>#修第一台 hostname 为 master </p>
<p>hostname master</p>
<p>#检测</p>
<p>hostname</p>
<p>   #配置主机名 (hostname)</p>
<p>   vi/etc/sysconfig/network </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">NETWORKING=yes <span class="comment">#启动网络</span></div><div class="line">NETWORKING_IPV6=no</div><div class="line">HOSTNAME=master  <span class="comment">#主机名</span></div></pre></td></tr></table></figure>
<p>#使用 setup 命令配置系统环境</p>
<p>setup </p>
<p>   <img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image012.png" alt="img"></p>
<p>   <img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image014.png" alt="img"> </p>
<pre><code>![img](file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image016.png)
</code></pre><p>   cat /etc/sysconfig/network-scripts/ifcfg-eth0  #检查ip配置 </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Advanced Micro Devices [AMD] 79c970 [PCnet32 LANCE]</span></div><div class="line">DEVICE=eth0</div><div class="line">BOOTPROTO=none</div><div class="line">HWADDR=00:0c:29:<span class="built_in">fc</span>:3a:09</div><div class="line">ONBOOT=yes</div><div class="line">NETMASK=255.255.255.0</div><div class="line">IPADDR=192.168.1.100</div><div class="line">TYPE=Ethernet</div></pre></td></tr></table></figure>
<p>/sbin/service network restart  #重新启动网络服务</p>
<p>/sbin/ifconfig  #检查网络ip配置</p>
<p>#关闭防火墙 如果不关闭 报错如下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">************************************************************/</div><div class="line">2012-07-18 02:47:26,331 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties</div><div class="line">2012-07-18 02:47:26,529 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.</div><div class="line">2012-07-18 02:47:26,533 ERROR org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Error getting localhost name. Using &apos;localhost&apos;...</div><div class="line">java.net.UnknownHostException: node1: node1</div><div class="line">        at java.net.InetAddress.getLocalHost(InetAddress.java:1354)</div></pre></td></tr></table></figure>
<p><img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image018.png" alt="img"></p>
<p>#关闭防火墙</p>
<p><img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image020.png" alt="img"></p>
<h3 id="配置集群hosts列表"><a href="#配置集群hosts列表" class="headerlink" title="#配置集群hosts列表"></a><a href="undefined">#</a>配置集群hosts列表</h3><p>vi /etc/hosts</p>
<p>#添加一下内容到 vi 中</p>
<h3 id="下载并安装-JAVA-JDK系统软件"><a href="#下载并安装-JAVA-JDK系统软件" class="headerlink" title="#下载并安装 JAVA JDK系统软件"></a><a href="undefined">#</a>下载并安装 JAVA JDK系统软件</h3><p>#下载jdk </p>
<p>wget<a href="http://60.28.110.228/source/package/jdk-6u21-linux-i586-rpm.bin" target="_blank" rel="external">http://60.28.110.228/source/package/jdk-6u21-linux-i586-rpm.bin</a></p>
<p>#安装jdk</p>
<p>chmod +x jdk-6u21-linux-i586-rpm.bin</p>
<p>./jdk-6u21-linux-i586-rpm.bin</p>
<p>#配置环境变量</p>
<p>vi /etc/profile.d/java.sh</p>
<p>#复制粘贴一下内容 到 vi 中。</p>
<p>#手动立即生效</p>
<p>source /etc/profile</p>
<p>#测试 </p>
<p>jps</p>
<h3 id="生成登陆密钥"><a href="#生成登陆密钥" class="headerlink" title="#生成登陆密钥"></a><a href="undefined">#</a>生成登陆密钥</h3><p>#切换Hadoop 用户下</p>
<p>su hadoop </p>
<p>cd /home/hadoop/</p>
<p>#生成公钥和私钥</p>
<p>ssh-keygen -q -t rsa -N “” -f/home/hadoop/.ssh/id_rsa</p>
<p>cd .ssh</p>
<p>cat id_rsa.pub &gt; authorized_keys</p>
<p>chmod go-wx authorized_keys</p>
<p>#公钥:复制文件内容 id_rsa.pub到authorized_keys</p>
<p>#集群环境 id_ras_pub 复制到  node1:/home/hadoop/.ssh/authorized_keys</p>
<p>#检查 </p>
<p>ll -a /home/hadoop/.ssh/</p>
<h3 id="创建用户账号和Hadoop部署目录和数据目录"><a href="#创建用户账号和Hadoop部署目录和数据目录" class="headerlink" title="#创建用户账号和Hadoop部署目录和数据目录"></a><a href="undefined">#</a>创建用户账号和Hadoop部署目录和数据目录</h3><p>#创建 hadoop 用户</p>
<p>/usr/sbin/groupadd hadoop</p>
<p>#分配 hadoop 到 hadoop 组中</p>
<p>/usr/sbin/useradd hadoop -g hadoop</p>
<p>#创建 hadoop 代码目录结构</p>
<p>mkdir -p /opt/modules/hadoop/</p>
<p>#创建 hadoop 数据目录结构</p>
<p>mkdir -p /opt/data/hadoop/</p>
<p>#修改 目录结构权限为为hadoop</p>
<p>chown -R hadoop:hadoop  /opt/modules/hadoop/</p>
<p>chown -R hadoop:hadoop  /opt/data/hadoop/</p>
<h3 id="检查基础环境"><a href="#检查基础环境" class="headerlink" title="#检查基础环境"></a><a href="undefined">#</a>检查基础环境</h3><p>/sbin/ifconfig</p>
<p>#测试命令</p>
<p>/sbin/ifconfig</p>
<p>ping master</p>
<p>ssh master</p>
<p>jps</p>
<p>echo $JAVA_HOME</p>
<p>echo $HADOOP_HOME</p>
<p>hadoop</p>
<h2 id="6-Hadoop-单机系统-安装配置"><a href="#6-Hadoop-单机系统-安装配置" class="headerlink" title="6.  Hadoop 单机系统 安装配置"></a><a href="undefined">6.  Hadoop </a>单机系统 安装配置</h2><h3 id="Hadoop-文件下载和解压"><a href="#Hadoop-文件下载和解压" class="headerlink" title="#Hadoop 文件下载和解压"></a><a href="undefined">#Hadoop </a>文件下载和解压</h3><p>#切到 hadoop 安装路径下</p>
<p>cd /opt/modules/hadoop/</p>
<p>#从 hadoop.apache.org 下载Hadoop 安装文件</p>
<p>wget <a href="http://labs.renren.com/apache-mirror/hadoop/common/hadoop-1.0.3/hadoop-1.0.3.tar.gz" target="_blank" rel="external">http://labs.renren.com/apache-mirror/hadoop/common/hadoop-1.0.3/hadoop-1.0.3.tar.gz</a></p>
<p>#如果已经下载请复制文件到安装hadoop 文件夹</p>
<p>cp hadoop-1.0.3.tar.gz  /opt/modules/hadoop/</p>
<p>#加压 复制或者下载的Hadoop 文件</p>
<p>cd /opt/modules/hadoop/</p>
<p>tar -xzvf hadoop-1.0.3.tar.gz</p>
<p>hadoop </p>
<h3 id="配置-hadoop-env-sh-环境变量"><a href="#配置-hadoop-env-sh-环境变量" class="headerlink" title="#配置 hadoop-env.sh 环境变量"></a><a href="undefined">#</a>配置 hadoop-env.sh 环境变量</h3><p>#配置Hadoop 最大HADOOP_HEAPSIZE大小,  默认为 1000,因为虚拟机最大内存配置512m,这里配置较小。</p>
<p>#配置 压缩类库地址</p>
<p>vi/opt/modules/hadoop/hadoop-1.0.3/conf/hadoop-env.sh</p>
<h3 id="Hadoop-Common组件-配置-core-site-xml"><a href="#Hadoop-Common组件-配置-core-site-xml" class="headerlink" title="#Hadoop Common组件 配置 core-site.xml"></a><a href="undefined">#Hadoop Common</a>组件 配置 core-site.xml</h3><p>#编辑 core-site.xml 文件</p>
<p>vi /opt/modules/hadoop/hadoop-1.0.3/conf/core-site.xml</p>
<table>
<thead>
<tr>
<th><configuration>    <property>      <name>fs.default.name</name>  <value>hdfs://master:9000</value>  <!--hadoop namenode 服务器地址和端口，以域名形式-->    </property>    <property>      <name>fs.checkpoint.dir</name>       <value>/opt/data/hadoop/hdfs/namesecondary</value>      <!--hadoop secondary  数据存储路径,可以配置成多个目录,用,号分隔。-->    </property>    <property>       <name>fs.checkpoint.period</name>  <value>1800</value>  <!-- editlog 每隔 30分钟 触发一次合并 -->    </property>    <property>       <name>fs.checkpoint.size</name>  <value>33554432</value>  <!-- editlog 达到32m的时候触发一次合并 -->    </property>    <property>       <name>io.compression.codecs</name>  <value>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec</value>  <!-- 配置 Hadoop 压缩包 -->    </property>       <property>       <name>fs.trash.interval</name>      <value>1440</value>      <description>Hadoop文件回收站,自动回收时间,单位分钟,这里设置是1天。</description>    </property>     </configuration></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<h3 id="HDFS-NameNode-DataNode组建配置hdfs-site-xml"><a href="#HDFS-NameNode-DataNode组建配置hdfs-site-xml" class="headerlink" title="#HDFS NameNode,DataNode组建配置hdfs-site.xml"></a><a href="undefined">#HDFS NameNode,DataNode</a>组建配置hdfs-site.xml</h3><p>vi /opt/modules/hadoop/hadoop-1.0.3/conf/hdfs-site.xml</p>
<h3 id="配置MapReduce-JobTrackerTaskTracker-启动配置"><a href="#配置MapReduce-JobTrackerTaskTracker-启动配置" class="headerlink" title="#配置MapReduce - JobTrackerTaskTracker 启动配置"></a><a href="undefined">#</a>配置MapReduce - JobTrackerTaskTracker 启动配置</h3><p>vi /opt/modules/hadoop/hadoop-1.0.3/conf/mapred-site.xml</p>
<h3 id="Hadoop单机系统-启动执行和异常检查"><a href="#Hadoop单机系统-启动执行和异常检查" class="headerlink" title="#Hadoop单机系统,启动执行和异常检查"></a><a href="undefined">#Hadoop</a>单机系统,启动执行和异常检查</h3><p>描述系统重启，启动，停止，升级，以及其他故障的处理方式</p>
<p>#创建Hadoop mapred 和 hdfs namenode 和 datanode 目录 在 root 下</p>
<p>mkdir -p/data/hadoop/</p>
<p>chown -Rhadoop:hadoop /data/*</p>
<p>#切换到 hadoop 用户下</p>
<p>su hadoop</p>
<p>#创建mapreduce </p>
<p>mkdir -p /opt/data/hadoop/mapred/mrlocal<br>mkdir -p /opt/data/hadoop/mapred/mrsystem</p>
<p>mkdir -p /opt/data/hadoop/hdfs/name</p>
<p>mkdir -p/opt/data/hadoop/hdfs/data</p>
<p>mkdir -p /opt/data/hadoop/hdfs/namesecondary</p>
<p>#启动 切换到hadoop用户</p>
<p>su hadoop</p>
<p>#格式化文件</p>
<p>/opt/modules/hadoop/hadoop-1.0.3/bin/hadoopnamenode -format</p>
<p>#启动 Master node ：</p>
<p>/opt/modules/hadoop/hadoop-1.0.3/bin/hadoop-daemon.shstart namenode</p>
<p>#启动 JobTracker：</p>
<p>/opt/modules/hadoop/hadoop-1.0.3/bin/hadoop-daemon.shstart jobtracker</p>
<p>#启动 secondarynamenode：</p>
<p>/opt/modules/hadoop/hadoop-1.0.3/bin/hadoop-daemon.shstart secondarynamenode</p>
<p>#启动 DataNode &amp;&amp; TaskTracker：</p>
<p>/opt/modules/hadoop/hadoop-1.0.3/bin/hadoop-daemon.shstart datanode</p>
<p>/opt/modules/hadoop/hadoop-1.0.3/bin/hadoop-daemon.shstart tasktracker</p>
<p>停止，命令相同，将start换为stop</p>
<p>#出现错误可查看日志</p>
<p>tail -f/opt/modules/hadoop/hadoop-1.0.3/logs/*</p>
<h3 id="通过界面查看集群部署部署成功"><a href="#通过界面查看集群部署部署成功" class="headerlink" title="#通过界面查看集群部署部署成功"></a><a href="undefined">#</a>通过界面查看集群部署部署成功</h3><p>#检查 namenode 和 datanode 是否正常</p>
<p><a href="http://localhost:50070/" target="_blank" rel="external">http://master:50070/</a></p>
<p><img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image022.png" alt="img"></p>
<p>#检查 jobtracker 和 tasktracker 是否正常</p>
<p><a href="http://localhost:50030/" target="_blank" rel="external">http://master:50030/</a></p>
<p><img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image024.png" alt="img"></p>
<h3 id="通过执行-Hadoop-pi-运行样例检查集群是否成功"><a href="#通过执行-Hadoop-pi-运行样例检查集群是否成功" class="headerlink" title="#通过执行 Hadoop pi 运行样例检查集群是否成功"></a><a href="undefined">#</a>通过执行 Hadoop pi 运行样例检查集群是否成功</h3><p>cd /opt/modules/hadoop/hadoop-1.0.3</p>
<p>bin/hadoop jar hadoop-examples-1.0.3.jar pi10 100</p>
<p>#集群正常效果如下</p>
<h3 id="安装调试方法"><a href="#安装调试方法" class="headerlink" title="#安装调试方法"></a><a href="undefined">#</a>安装调试方法</h3><p>#启动程序 namenodedatanode jobtracker tasktracker </p>
<p>hadoop-daemon.shstart xxxx</p>
<p>#检查进程是否存活<br>jps</p>
<p>#检查日志是否正常</p>
<p>tail-100f /opt/modules/hadoop/hadoop-1.0.3/libexec/../logs/hadoop-hadoop-xxxx-master.log</p>
<p>#检查端口是否正常</p>
<p><a href="http://master:50030" target="_blank" rel="external">http://master:50070</a></p>
<p><a href="http://master:50070" target="_blank" rel="external">http://master:50030</a></p>
<p>#检查 hdfs 是否正常</p>
<p>hadoopfs -ls /</p>
<p>hadoopfs -mkdir /data/</p>
<p>hadoopfs -put xxx.log /data/</p>
<p>#检查 mapreduce 是否正常</p>
<p>hadoopjar hadoop-examples-1.0.3.jar pi 100 100</p>
<h3 id="安装部署-常见错误"><a href="#安装部署-常见错误" class="headerlink" title="#安装部署 常见错误"></a>#安装部署 常见错误</h3><p>主机文件/etc/hosts中主机列表IP错误。mapred-site.xml中任务分配过多或过少，导致效率降低或内存溢出。物理硬盘的权限均应为hadoop:hadoop，执行启动也应su为hadoop用户。比较常见是出现权限错误导致无法启动故障。</p>
<p>如果遇到服务无法启动。请检查 $HADOOP_HOME/logs/ 目录具体日志情况。</p>
<p>tail -n 100 $HADOOP_HOME/logs/<em>namenode</em>  #检查namenode 服务日志</p>
<p>tail -n 100$HADOOP_HOME/logs/<em>datanode</em>   #检查datanode服务日志</p>
<p>Tail -n 100$HADOOP_HOME/logs/<em>jobtracker</em>   #检查jobtracker服务日志</p>
<h2 id="7-Hadoop-集群系统-配置安装配置"><a href="#7-Hadoop-集群系统-配置安装配置" class="headerlink" title="7.  Hadoop 集群系统 配置安装配置"></a><a href="undefined">7.  Hadoop </a>集群系统 配置安装配置</h2><h3 id="检查node节点linux-基础环境是否正常-参考-linux-基础环境搭建-一节。"><a href="#检查node节点linux-基础环境是否正常-参考-linux-基础环境搭建-一节。" class="headerlink" title="#检查node节点linux 基础环境是否正常,参考 [ linux 基础环境搭建]一节。"></a><a href="undefined">#</a>检查node节点linux 基础环境是否正常,参考 [ linux 基础环境搭建]一节。</h3><h3 id="配置从master-机器到-node-节点无密钥登陆"><a href="#配置从master-机器到-node-节点无密钥登陆" class="headerlink" title="#配置从master 机器到 node 节点无密钥登陆"></a><a href="undefined">#</a>配置从master 机器到 node 节点无密钥登陆</h3><p>#切换到Hadoop 用户下</p>
<p>su hadoop </p>
<p>cd /home/hadoop/</p>
<p>#生成公钥和私钥</p>
<p>ssh-keygen -q -t rsa -N “” -f/home/hadoop/.ssh/id_rsa</p>
<p>#查看密钥内容</p>
<p>cd /home/hadoop/.ssh</p>
<p>cat id_rsa.pub</p>
<p>#看到如下内容举例</p>
<p>#复制id_rsa.pub公钥到authorized_keys 目录</p>
<p>cat id_rsa.pub &gt; authorized_keys</p>
<p>#修改master 密钥权限,非常容易错误的地方。</p>
<p>chmod go-rwx /home/hadoop/.ssh/authorized_keys</p>
<p>#把 master 机器上的 authorized_keys文件 copy 到 node1 节点上。</p>
<p>scp /home/hadoop/.ssh/authorized_keys /home/hadoop/.ssh/</p>
<p>#输入 hadoop 密码</p>
<p>#修改 node1 密钥权限</p>
<p>chmod go-rwx /home/hadoop/.ssh/authorized_keys</p>
<p>#验证本机无密钥登陆,如果无需密码算成功。</p>
<p>ssh 192.168.1.100</p>
<p>exit #退出</p>
<p>ssh master</p>
<p>exit #退出</p>
<p>#验证登陆 192.168.1.101 ,如果无需密码算成功。</p>
<p>ssh 192.168.1.101</p>
<p>exit #退出</p>
<p>ssh node1</p>
<p>exit #退出</p>
<h3 id="检查master到每个node节点在hadoop用户下使用密钥登陆是否正常"><a href="#检查master到每个node节点在hadoop用户下使用密钥登陆是否正常" class="headerlink" title="#检查master到每个node节点在hadoop用户下使用密钥登陆是否正常"></a><a href="undefined">#</a>检查master到每个node节点在hadoop用户下使用密钥登陆是否正常</h3><p>su hadoop </p>
<p>#检查master 登陆master正常</p>
<p>ssh master</p>
<p>exit #退出</p>
<p>#检查 master 登陆node1 正常</p>
<p>ssh node1</p>
<p>exit #退出</p>
<h3 id="配置master-集群服务器地址stop-all-sh-start-all-sh-的时候调用"><a href="#配置master-集群服务器地址stop-all-sh-start-all-sh-的时候调用" class="headerlink" title="#配置master 集群服务器地址stop-all.sh start-all.sh 的时候调用"></a><a href="undefined">#</a>配置master 集群服务器地址stop-all.sh start-all.sh 的时候调用</h3><p>#设置 hadoop secondary node hostname批量启动的地址</p>
<p>#配置secondary的地址</p>
<p>vi /opt/modules/hadoop/hadoop-1.0.3/conf/masters</p>
<p>#配置 datanode 和 tasktracker 的地址</p>
<p>vi /opt/modules/hadoop/hadoop-1.0.3/conf/slaves</p>
<p>#复制 master hadoop到 node1 node2节点服务器上</p>
<p>#切换到 hadoop 用户下</p>
<p>su hadoop</p>
<p>scp -r /opt/modules/hadoop/hadoop-1.0.3/    node1:/opt/modules/hadoop/</p>
<p>#登陆到 node1 节点上</p>
<p>ssh node1 </p>
<p>#创建mapreduce</p>
<p>mkdir -p /data/hadoop/mapred/mrlocal<br>mkdir -p /data/hadoop/mapred/mrsystem</p>
<p>mkdir -p /data/hadoop/hdfs/name</p>
<p>mkdir -p /data/hadoop/hdfs/data</p>
<p>chmod go-w /data/hadoop/hdfs/data</p>
<p>#批量启动和关闭集群</p>
<p>#全部启动 </p>
<p>/opt/modules/hadoop/hadoop-1.0.3/bin/start-all.sh</p>
<p>#全部关闭</p>
<p>/opt/modules/hadoop/hadoop-1.0.3/bin/stop-all.sh</p>
<h3 id="通过界面查看集群部署部署成功-1"><a href="#通过界面查看集群部署部署成功-1" class="headerlink" title="#通过界面查看集群部署部署成功"></a><a href="undefined">#</a>通过界面查看集群部署部署成功</h3><p>#检查 namenode 和 datanode 是否正常</p>
<p><a href="http://localhost:50070/" target="_blank" rel="external">http://master:50070/</a></p>
<p><img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image025.png" alt="img"></p>
<p>#检查 jobtracker 和 tasktracker 是否正常</p>
<p><a href="http://localhost:50030/" target="_blank" rel="external">http://master:50030/</a></p>
<p>hadoop fs -ls /</p>
<p>hadoop fs -mkdir/data/</p>
<p><img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image026.png" alt="img"></p>
<h3 id="通过执行-Hadoop-pi-运行样例检查集群是否成功-1"><a href="#通过执行-Hadoop-pi-运行样例检查集群是否成功-1" class="headerlink" title="#通过执行 Hadoop pi 运行样例检查集群是否成功"></a><a href="undefined">#</a>通过执行 Hadoop pi 运行样例检查集群是否成功</h3><p>cd /opt/modules/hadoop/hadoop-1.0.3</p>
<p>bin/hadoop jar hadoop-examples-1.0.3.jar pi10 100</p>
<p>#集群正常效果如下</p>
<h2 id="8-自动化安装脚本"><a href="#8-自动化安装脚本" class="headerlink" title="8.  自动化安装脚本"></a><a href="undefined">8.  自动化安装脚本</a></h2><p>为加快服务器集群的安装和部署,会使用自动化安装脚本安装。以下为自动化部署脚本样例。脚本中#红色部分 具体参考以上配置做具体修改。本脚本里面的安装包用于 64位服务器安装,32位安装包需要单独下载修改。</p>
<h3 id="master-服务器自动安装脚本"><a href="#master-服务器自动安装脚本" class="headerlink" title="#master 服务器自动安装脚本"></a><a href="undefined">#master </a>服务器自动安装脚本</h3><p>#hadoop_install.sh,将以下放入shell脚本中并执行。</p>
<p>vi hadoop_install.sh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div></pre></td><td class="code"><pre><div class="line">#!/bin/sh</div><div class="line">yum -y install lrzsz gcc gcc-c++ libstdc++-devel ntp  #安装gcc 基础环境</div><div class="line">echo &quot;0 1 * * * root /usr/sbin/ntpdate cn.pool.ntp.org&quot; &gt;&gt; /etc/crontab  #配置时间同步</div><div class="line">/usr/sbin/ntpdate cn.pool.ntp.org #手动同步时间</div><div class="line">/usr/sbin/groupadd hadoop #新增hadoop 群组</div><div class="line">/usr/sbin/useradd hadoop -g hadoop #新增Hadoop 用户并绑定到hadoop 群中</div><div class="line"></div><div class="line">#安装依赖包并设置hadoop用户</div><div class="line">mkdir -p /opt/modules/hadoop/</div><div class="line">mkdir -p /opt/data/hadoop/</div><div class="line">chown hadoop:hadoop /opt/data/hadoop/</div><div class="line"></div><div class="line">#配置 /etc/hosts ip 对应主机名称</div><div class="line">echo -e &quot;127.0.0.1\tlocalhost.localdomain localhost</div><div class="line">#::1\tlocalhost6.localdomain6 localhost6</div><div class="line">#机架1</div><div class="line">192.168.1.100\thadoopmaster</div><div class="line">192.168.1.101\thadoopslave</div><div class="line">192.168.1.101\thadoop-node-101</div><div class="line">&quot; &gt; /etc/hosts</div><div class="line"></div><div class="line">#获取服务器外网IP并替换host中127.0.0.1 collect-*</div><div class="line">IP=`/sbin/ifconfig eth0 | grep &quot;inet addr&quot; | awk -F&quot;:&quot; &apos;&#123;print $2&#125;&apos; | awk -F&quot; &quot; &apos;&#123;print $1&#125;&apos;`</div><div class="line">sed -i &quot;s/^127.0.0.1\tcollect/$&#123;IP&#125;\tcollect/g&quot; /etc/hosts</div><div class="line"></div><div class="line">echo &quot;----------------env init finish and prepare su hadoop---------------&quot;</div><div class="line"></div><div class="line">HADOOP=/home/hadoop</div><div class="line"></div><div class="line">cd $HADOOP</div><div class="line"></div><div class="line">#生成密钥</div><div class="line">sudo -u hadoop mkdir .ssh</div><div class="line">ssh-keygen -q -t rsa -N &quot;&quot; -f $HADOOP/.ssh/id_rsa</div><div class="line">Cd$HADOOP/.ssh/ &amp;&amp; echo &quot;#此处需要 cat master id_rsa.pub&quot; &gt; $HADOOP/.ssh/authorized_keys</div><div class="line">chmod go-rwx $HADOOP/.ssh/authorized_keys #修改文件权限</div><div class="line">cd $HADOOP</div><div class="line"></div><div class="line">#下载已经配置好的 Hadoop 集群包</div><div class="line">wget http://60.28.110.228/source/package/hadoop/hadoop_gz.tar.gz</div><div class="line">wget http://60.28.110.228/source/package/hadoop/hadoop_rpm.tar.gz</div><div class="line"></div><div class="line">mkdir $HADOOP/hadoop</div><div class="line">mv *.tar.gz $HADOOP/hadoop</div><div class="line">cd $HADOOP/hadoop</div><div class="line">tar zxvf hadoop_rpm.tar.gz</div><div class="line">tar zxvf hadoop_gz.tar.gz</div><div class="line"></div><div class="line">rpm -ivh jdk-6u21-linux-amd64.rpm</div><div class="line">rpm -ivh lrzsz-0.12.20-19.x86_64.rpm</div><div class="line">rpm -ivh lzo-2.04-1.el5.rf.x86_64.rpm</div><div class="line">rpm -ivh hadoop-gpl-packaging-0.2.8-1.x86_64.rpm</div><div class="line"></div><div class="line">tar xzvf lzo-2.06.tar.gz</div><div class="line">cd lzo-2.06 &amp;&amp; ./configure --enable-shared &amp;&amp; make &amp;&amp; make install</div><div class="line">cp /usr/local/lib/liblzo2.* /usr/lib/</div><div class="line">cd ..</div><div class="line"></div><div class="line">tar xzvf  lzop-1.03.tar.gz</div><div class="line">cd lzop-1.03</div><div class="line">./configure &amp;&amp; make &amp;&amp; make install &amp;&amp; cd ..</div><div class="line"></div><div class="line">chown -R hadoop:hadoop  /opt/modules/hadoop/</div><div class="line"></div><div class="line">cp hadoop-node-0.20.203.0.tar.gz /opt/modules/hadoop/</div><div class="line">cd /opt/modules/hadoop/ &amp;&amp; tar -xzvf hadoop-node-0.20.203.0.tar.gz</div><div class="line"></div><div class="line">chown -R hadoop:hadoop /opt/modules/hadoop/</div><div class="line">chown -R hadoop:hadoop /home/hadoop</div></pre></td></tr></table></figure>
<h2 id="9-开启集群LZO"><a href="#9-开启集群LZO" class="headerlink" title="9.  开启集群LZO"></a>9.  开启集群LZO</h2><h3 id="下载相关-LZO-包"><a href="#下载相关-LZO-包" class="headerlink" title="#下载相关 LZO 包"></a>#下载相关 LZO 包</h3><p>wget <a href="http://113.11.199.230/resources/lzop-1.03.tar.gz" target="_blank" rel="external">http://113.11.199.230/resources/lzop-1.03.tar.gz</a></p>
<p>wget <a href="http://113.11.199.230/resources/x64/hadoop-gpl-packaging-0.5.3-1.x86_64.rpm" target="_blank" rel="external">http://113.11.199.230/resources/x64/hadoop-gpl-packaging-0.5.3-1.x86_64.rpm</a></p>
<p>wget <a href="http://113.11.199.230/resources/lzo-2.06.tar.gz" target="_blank" rel="external">http://113.11.199.230/resources/lzo-2.06.tar.gz</a></p>
<p>wget <a href="http://113.11.199.230/resources/x64/lzo-2.06-1.el5.rf.x86_64.rpm" target="_blank" rel="external">http://113.11.199.230/resources/x64/lzo-2.06-1.el5.rf.x86_64.rpm</a></p>
<p>wget <a href="http://113.11.199.230/resources/x64/lzo-devel-2.06-1.el5.rf.x86_64.rpm" target="_blank" rel="external">http://113.11.199.230/resources/x64/lzo-devel-2.06-1.el5.rf.x86_64.rpm</a></p>
<h3 id="安装-LZO-相关包"><a href="#安装-LZO-相关包" class="headerlink" title="#安装 LZO 相关包"></a>#安装 LZO 相关包</h3><p>rpm -ivhlzo-2.06-1.el5.rf.x86_64.rpm lzo-devel-2.06-1.el5.rf.x86_64.rpmhadoop-gpl-packaging-0.5.3-1.x86_64.rpm</p>
<h3 id="编译-安装-lzo-包"><a href="#编译-安装-lzo-包" class="headerlink" title="#编译 安装 lzo 包"></a>#编译 安装 lzo 包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">tar zxf lzo-2.06.tar.gz</div><div class="line"></div><div class="line">cd lzo-2.06</div><div class="line"></div><div class="line">./configure &amp;&amp; make &amp;&amp; makeinstall</div><div class="line"></div><div class="line">cd ../</div><div class="line"></div><div class="line">tar zxf lzop-1.03.tar.gz</div><div class="line"></div><div class="line">cd lzop-1.03</div><div class="line"></div><div class="line">./configure &amp;&amp; make &amp;&amp; makeinstall</div></pre></td></tr></table></figure>
<h3 id="修改hadoop配置文件core-site-xml"><a href="#修改hadoop配置文件core-site-xml" class="headerlink" title="#修改hadoop配置文件core-site.xml"></a>#修改hadoop配置文件core-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span>      <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,org.apache.hadoop.io.compress.BZip2Codec<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codec.lzo.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<h3 id="修改hadoop配置文件mapred-site-xml"><a href="#修改hadoop配置文件mapred-site-xml" class="headerlink" title="#修改hadoop配置文件mapred-site.xml"></a>#修改hadoop配置文件mapred-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.map.output.compression.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.child.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>-Djava.library.path=/opt/hadoopgpl/native/Linux-amd64-64:/opt/modules/hadoop/hadoop-1.0.3/lib/native/Linux-amd64-64<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<h2 id="10-开启任务调度器"><a href="#10-开启任务调度器" class="headerlink" title="10.          开启任务调度器"></a>10.          开启任务调度器</h2><h3 id="修改-mapred-site-xml"><a href="#修改-mapred-site-xml" class="headerlink" title="#修改 mapred-site.xml"></a>#修改 mapred-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.jobtracker.taskScheduler<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.CapacityTaskScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.queue.names<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>default,hive,streaming<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<h3 id="修改-capacity-scheduler-xml添加-hive-streaming-等队列。"><a href="#修改-capacity-scheduler-xml添加-hive-streaming-等队列。" class="headerlink" title="#修改 capacity-scheduler.xml添加 hive streaming 等队列。"></a>#修改 capacity-scheduler.xml添加 hive streaming 等队列。</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0"?&gt;  </div><div class="line">      </div><div class="line">    <span class="comment">&lt;!-- This is the configuration file for the resource manager in Hadoop. --&gt;</span>  </div><div class="line">    <span class="comment">&lt;!-- You can configure various scheduling parameters related to queues. --&gt;</span>  </div><div class="line">    <span class="comment">&lt;!-- The properties for a queue follow a naming convention,such as, --&gt;</span>  </div><div class="line">    <span class="comment">&lt;!-- mapred.capacity-scheduler.queue.&lt;queue-name&gt;.property-name. --&gt;</span>  </div><div class="line">      </div><div class="line">    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span>  </div><div class="line">      </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.maximum-system-jobs<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>3000<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Maximum number of jobs in the system which can be initialized,  </div><div class="line">         concurrently, by the CapacityScheduler.  </div><div class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span>      </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">        </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.default.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Percentage of the number of slots in the cluster that are  </div><div class="line">          to be available for jobs in this queue.  </div><div class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span>      </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">        </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.default.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>  </div><div class="line">        maximum-capacity defines a limit beyond which a queue cannot use the capacity of the cluster.  </div><div class="line">        This provides a means to limit how much excess capacity a queue can use. By default, there is no limit.  </div><div class="line">        The maximum-capacity of a queue can only be greater than or equal to its minimum capacity.  </div><div class="line">            Default value of -1 implies a queue can use complete capacity of the cluster.  </div><div class="line">      </div><div class="line">            This property could be to curtail certain jobs which are long running in nature from occupying more than a   </div><div class="line">            certain percentage of the cluster, which in the absence of pre-emption, could lead to capacity guarantees of   </div><div class="line">            other queues being affected.  </div><div class="line">              </div><div class="line">            One important thing to note is that maximum-capacity is a percentage , so based on the cluster's capacity  </div><div class="line">            the max capacity would change. So if large no of nodes or racks get added to the cluster , max Capacity in   </div><div class="line">            absolute terms would increase accordingly.  </div><div class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span>      </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">        </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.default.supports-priority<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, priorities of jobs will be taken into   </div><div class="line">          account in scheduling decisions.  </div><div class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.default.minimum-user-limit-percent<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span> Each queue enforces a limit on the percentage of resources   </div><div class="line">        allocated to a user at any given time, if there is competition for them.   </div><div class="line">        This user limit can vary between a minimum and maximum value. The former  </div><div class="line">        depends on the number of users who have submitted jobs, and the latter is  </div><div class="line">        set to this property value. For example, suppose the value of this   </div><div class="line">        property is 25. If two users have submitted jobs to a queue, no single   </div><div class="line">        user can use more than 50% of the queue resources. If a third user submits  </div><div class="line">        a job, no single user can use more than 33% of the queue resources. With 4   </div><div class="line">        or more users, no user can use more than 25% of the queue's resources. A   </div><div class="line">        value of 100 implies no user limits are imposed.   </div><div class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">        </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.default.user-limit-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The multiple of the queue capacity which can be configured to   </div><div class="line">        allow a single user to acquire more slots.   </div><div class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.default.maximum-initialized-active-tasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>200000<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum number of tasks, across all jobs in the queue,   </div><div class="line">        which can be initialized concurrently. Once the queue's jobs exceed this   </div><div class="line">        limit they will be queued on disk.    </div><div class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.default.maximum-initialized-active-tasks-per-user<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100000<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum number of tasks per-user, across all the of the   </div><div class="line">        user's jobs in the queue, which can be initialized concurrently. Once the   </div><div class="line">        user's jobs exceed this limit they will be queued on disk.    </div><div class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.default.init-accept-jobs-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The multipe of (maximum-system-jobs * queue-capacity) used to   </div><div class="line">        determine the number of jobs which are accepted by the scheduler.    </div><div class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      </div><div class="line">      <span class="comment">&lt;!-- The default configuration settings for the capacity task scheduler --&gt;</span>  </div><div class="line">      <span class="comment">&lt;!-- The default values would be applied to all the queues which don't have --&gt;</span>  </div><div class="line">      <span class="comment">&lt;!-- the appropriate property for the particular queue --&gt;</span>  </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.default-supports-priority<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, priorities of jobs will be taken into   </div><div class="line">          account in scheduling decisions by default in a job queue.  </div><div class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">        </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.default-minimum-user-limit-percent<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The percentage of the resources limited to a particular user  </div><div class="line">          for the job queue at any given point of time by default.  </div><div class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      </div><div class="line">      </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.default-user-limit-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The default multiple of queue-capacity which is used to   </div><div class="line">        determine the amount of slots a single user can consume concurrently.  </div><div class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.default-maximum-active-tasks-per-queue<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>200000<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The default maximum number of tasks, across all jobs in the   </div><div class="line">        queue, which can be initialized concurrently. Once the queue's jobs exceed   </div><div class="line">        this limit they will be queued on disk.    </div><div class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.default-maximum-active-tasks-per-user<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100000<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The default maximum number of tasks per-user, across all the of   </div><div class="line">        the user's jobs in the queue, which can be initialized concurrently. Once   </div><div class="line">        the user's jobs exceed this limit they will be queued on disk.    </div><div class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.default-init-accept-jobs-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The default multipe of (maximum-system-jobs * queue-capacity)   </div><div class="line">        used to determine the number of jobs which are accepted by the scheduler.    </div><div class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      </div><div class="line">      <span class="comment">&lt;!-- Capacity scheduler Job Initialization configuration parameters --&gt;</span>  </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.init-poll-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>5000<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The amount of time in miliseconds which is used to poll   </div><div class="line">        the job queues for jobs to initialize.  </div><div class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.init-worker-threads<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Number of worker threads which would be used by  </div><div class="line">        Initialization poller to initialize jobs in a set of queue.  </div><div class="line">        If number mentioned in property is equal to number of job queues  </div><div class="line">        then a single thread would initialize jobs in a queue. If lesser  </div><div class="line">        then a thread would get a set of queues assigned. If the number  </div><div class="line">        is greater then number of threads would be equal to number of   </div><div class="line">        job queues.  </div><div class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.hive.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.hive.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>80<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.hive.supports-priority<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.hive.minimum-user-limit-percent<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>20<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.hive.user-limit-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.hive.maximum-initialized-active-tasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>200000<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.hive.maximum-initialized-active-tasks-per-user<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100000<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.hive.init-accept-jobs-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line"></div><div class="line"><span class="comment">&lt;!--streaming--&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.streaming.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>30<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.streaming.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>90<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.streaming.supports-priority<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.streaming.minimum-user-limit-percent<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.streaming.user-limit-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.streaming.maximum-initialized-active-tasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>200000<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.streaming.maximum-initialized-active-tasks-per-user<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>100000<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.capacity-scheduler.queue.streamingh.init-accept-jobs-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        </div><div class="line">    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure>
<h2 id="11-开启机架感知"><a href="#11-开启机架感知" class="headerlink" title="11.          开启机架感知"></a>11.          开启机架感知</h2><h3 id="修改hadoop配置文件core-site-xml-添加机架感知代码"><a href="#修改hadoop配置文件core-site-xml-添加机架感知代码" class="headerlink" title="#修改hadoop配置文件core-site.xml 添加机架感知代码"></a>#修改hadoop配置文件core-site.xml 添加机架感知代码</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>topology.script.file.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/modules/hadoop/hadoop-1.0.3/conf/RackAware.py<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<h3 id="新建-RackAware-py-文件"><a href="#新建-RackAware-py-文件" class="headerlink" title="#新建 RackAware.py 文件"></a>#新建 RackAware.py 文件</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/python</span></div><div class="line"><span class="comment">#-*-coding:UTF-8 -*-</span></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"></div><div class="line">rack = &#123;<span class="string">"hadoopnode-101"</span>:<span class="string">"rack1"</span>,</div><div class="line">        <span class="string">"hadoopnode-102"</span>:<span class="string">"rack1"</span>,</div><div class="line">        <span class="string">"hadoopnode-203"</span>:<span class="string">"rack2"</span>,</div><div class="line">        <span class="string">"192.168.1.101"</span>:<span class="string">"rack1"</span>,</div><div class="line">        <span class="string">"192.168.1.102"</span>:<span class="string">"rack1"</span>,</div><div class="line">        <span class="string">"192.168.1.203"</span>:<span class="string">"rack2"</span>,</div><div class="line">        &#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</div><div class="line">    <span class="keyword">print</span> <span class="string">"/"</span> + rack.get(sys.argv[<span class="number">1</span>],<span class="string">"rack0"</span>)</div></pre></td></tr></table></figure>
<h2 id="12-配置详解"><a href="#12-配置详解" class="headerlink" title="12.          配置详解"></a><a href="undefined">12.          配置详解</a></h2><h3 id="Hadoop系统配置详解"><a href="#Hadoop系统配置详解" class="headerlink" title="#Hadoop系统配置详解"></a>#Hadoop系统配置详解</h3><p>#core-site.xml为公共配置,hdfs-site.xml mapred-site.xml 在 hdfs 和mapreduce 启动的时候加载。</p>
<table>
<thead>
<tr>
<th>hadoop-env.sh</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>name</strong></td>
<td><strong>value</strong></td>
<td><strong>含义</strong></td>
<td></td>
</tr>
<tr>
<td>JAVA_HOME</td>
<td>/usr/java/jdk1.6.0_30</td>
<td>JDK所在路径</td>
<td></td>
</tr>
<tr>
<td>JAVA_LIBRARY_PATH</td>
<td>/opt/hadoopgpl/native/Linux-amd64-64:/opt/modules/hadoop/hadoop-0.20.203.0/lib/native/Linux-amd64-64</td>
<td>Lzo,Snappy,gzip  等压缩算法库地址</td>
<td></td>
</tr>
<tr>
<td>HADOOP_HEAPSIZE</td>
<td>26000</td>
<td>最大 HEAPSIZE 大小,默认  1000M</td>
<td></td>
</tr>
<tr>
<td><strong>core-site.xml</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Name</strong></td>
<td><strong>value</strong></td>
<td><strong>含义</strong></td>
<td></td>
</tr>
<tr>
<td>fs.default.name</td>
<td>hdfs://hadoopmaster:9000</td>
<td>指定默认的文件系统，默认端口 8020。</td>
<td></td>
</tr>
<tr>
<td>fs.checkpoint.dir</td>
<td>/data1/hdfs/secondarynamenode,/data2/hdfs/secondarynamenode</td>
<td>辅助NameNode检查点存储目录，分别存储到各个目录，支持冗余备份。</td>
<td></td>
</tr>
<tr>
<td>fs.checkpoint.period</td>
<td>1800</td>
<td>editlog和fsimage,合并触发周期30分钟。</td>
<td></td>
</tr>
<tr>
<td>fs.checkpoint.size</td>
<td>33554432</td>
<td>editlog和fsimage,合并触发日志大小32M。</td>
<td></td>
</tr>
<tr>
<td>fs.trash.interval</td>
<td>1440</td>
<td>文件清理周期 24小时</td>
<td></td>
</tr>
<tr>
<td>io.compression.codecs</td>
<td>org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec</td>
<td>压缩类库</td>
<td></td>
</tr>
<tr>
<td>io.compression.codec.lzo.class</td>
<td>com.hadoop.compression.lzo.LzoCodec</td>
<td>LZO  编码类</td>
<td></td>
</tr>
<tr>
<td>io.file.buffer.size</td>
<td>65536</td>
<td>指定缓冲区的大小，默认4K太小，64k(65536)或128k(131072)更为常用</td>
<td></td>
</tr>
<tr>
<td>topology.script.file.name</td>
<td>/opt/modules/hadoop/hadoop-0.20.203.0/conf/RackAware.py</td>
<td>配置  机架感知的代码</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>hdfs-site.xml</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Name</strong></td>
<td><strong>Value</strong></td>
<td><strong>含义</strong></td>
<td></td>
</tr>
<tr>
<td>dfs.name.dir</td>
<td>/data1/hadoop/hdfs/name,/data2/hadoop/hdfs/name,/nfs/hadoop/hdfs/name</td>
<td>NameNode上持久化存储元数据和事务日志的路径。指定多个目录的话，各个目录内容完全一致。  使用NFS在加载一个远程目录,以便后续主机宕机,快速恢复。</td>
<td></td>
</tr>
<tr>
<td>dfs.data.dir</td>
<td>/data1/hadoop/hdfs/data  /data2/hadoop/hdfs/data,/data3/hadoop/hdfs/data</td>
<td>DataNode上存储数据块的地方。如果指定多个目录，则数据库被随机的存放。</td>
<td></td>
</tr>
<tr>
<td>dfs.http.address</td>
<td>hadoopmaster:50070</td>
<td>HDFS  管理界面</td>
<td></td>
</tr>
<tr>
<td>dfs.secondary.http.address</td>
<td>hadoopslave:50090</td>
<td>secondary  namenode http 地址</td>
<td></td>
</tr>
<tr>
<td>dfs.replication</td>
<td>整数</td>
<td>数据复制的份数</td>
<td></td>
</tr>
<tr>
<td>dfs.datanode.du.reserved</td>
<td>1073741824</td>
<td>预留文件数量</td>
<td></td>
</tr>
<tr>
<td>dfs.block.size</td>
<td>134217728</td>
<td>HDFS 文件块大小,默认128M</td>
<td></td>
</tr>
<tr>
<td>dfs.datanode.max.xcievers</td>
<td>4096</td>
<td>datanode同时打开的文件上限。默认256太小。</td>
<td></td>
</tr>
<tr>
<td>dfs.permissions</td>
<td>FALSE</td>
<td>默认是 true，则打开前文所述的权限系统。如果是 false，权限检查 就是关闭的</td>
<td></td>
</tr>
<tr>
<td>dfs.support.append</td>
<td>FALSE</td>
<td>支持文件append，主要是支持hbase</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>mapred-size.xml</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Name</strong></td>
<td><strong>Value</strong></td>
<td><strong>说明</strong></td>
<td></td>
</tr>
<tr>
<td>mapred.job.tracker</td>
<td>hadoopmaster:9001</td>
<td>Jobtracker的RPC服务器所在的主机名称和端口。</td>
<td></td>
</tr>
<tr>
<td>mapred.local.dir</td>
<td>/data1/hadoop/mapred/mrlocal,/data2/hadoop/mapred/mrlocal</td>
<td>存储作业中间数据的目录列表，作业结束后，数据被清楚</td>
<td></td>
</tr>
<tr>
<td>mapred.system.dir</td>
<td>/data1/hadoop/mapred/mrsystem</td>
<td>作业运行期间的存储共享目录的目录，必须是HDFS之上的目录</td>
<td></td>
</tr>
<tr>
<td>mapred.task.tracker.map.tasks.maximum</td>
<td>12</td>
<td>运行在tasktracker之上的最大map任务数</td>
<td></td>
</tr>
<tr>
<td>mapred.task.tracker.reduce.tasks.maximum</td>
<td>4</td>
<td>运行在tasktracker之上的最大reduce任务数 (MAP+RED=CPU核心*2) (Map/Red=4/1)</td>
<td></td>
</tr>
<tr>
<td>mapred.child.java.opts</td>
<td>-Xmx1536M</td>
<td>JVM选项，默认 -Xmx200m</td>
<td></td>
</tr>
<tr>
<td>mapred.compress.map.output</td>
<td>true</td>
<td>Map输出后压缩传输,可以缩短文件传输时间</td>
<td></td>
</tr>
<tr>
<td>mapred.map.output.compression.codec</td>
<td>com.hadoop.compression.lzo.LzoCodec</td>
<td>使用Lzo库作为压缩算法</td>
<td></td>
</tr>
<tr>
<td>mapred.child.java.opts</td>
<td>-Djava.library.path=/opt/hadoopgpl/native/Linux-amd64-64</td>
<td>加载Lzo 库</td>
<td></td>
</tr>
<tr>
<td>mapred.jobtracker.taskScheduler</td>
<td>org.apache.hadoop.mapred.CapacityTaskScheduler</td>
<td>使用能力调度器</td>
<td></td>
</tr>
<tr>
<td>mapred.queue.names</td>
<td>default,HIVE,ETL</td>
<td>配置能力调度器队列</td>
<td></td>
</tr>
<tr>
<td>fs.inmemory.size.mb</td>
<td>300</td>
<td>为reduce阶段合并map输出所需的内存文件系统分配更多的内存</td>
<td></td>
</tr>
<tr>
<td>io.sort.mb</td>
<td>300</td>
<td>reduce 排序时的内存上限</td>
<td></td>
</tr>
<tr>
<td>mapred.jobtracker.restart.recover</td>
<td>true</td>
<td>默认:false</td>
<td></td>
</tr>
<tr>
<td>mapred.reduce.parallel.copies</td>
<td>10</td>
<td>默认:5 ,reduce 并行  copy的线程数</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>masters</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Value</strong></td>
<td><strong>说明</strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td>hadoopslave</td>
<td></td>
<td>SecondaryNameNode HostName地址</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>slaves</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Value</strong></td>
<td><strong>说明</strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td>datanode1</td>
<td></td>
<td>DataNode  TaskTracker HostName 地址列表</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>样例</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="http://hadoop.apache.org/common/docs/r0.20.2/core-default.html" target="_blank" rel="external">http://hadoop.apache.org/common/docs/r0.20.2/core-default.html</a></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="http://hadoop.apache.org/common/docs/r0.20.2/hdfs-default.html" target="_blank" rel="external">http://hadoop.apache.org/common/docs/r0.20.2/hdfs-default.html</a></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="http://hadoop.apache.org/common/docs/r0.20.0/mapred-default.html" target="_blank" rel="external">http://hadoop.apache.org/common/docs/r0.20.0/mapred-default.html</a></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="机器配置推荐"><a href="#机器配置推荐" class="headerlink" title="#机器配置推荐"></a>#机器配置推荐</h3><table>
<thead>
<tr>
<th>编号</th>
<th>机器类型</th>
<th>系统</th>
<th>数量</th>
<th>CPU</th>
<th>内存</th>
<th>硬盘</th>
<th>网卡</th>
<th>推荐机型</th>
<th>说明</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>网络设备</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>交换机</td>
<td>千兆以太网交换机</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>华为8512</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>核心和管理节点</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>NameNode</td>
<td>CentOS  5.8</td>
<td>1</td>
<td>2*4核</td>
<td>ECC  DDR2 96G</td>
<td>SATA  1T+1T*3(RAID 10)</td>
<td>千兆以太网卡*2</td>
<td>Dell(R410),HP(DL160)</td>
<td>配置相同,故障可以快速切换</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>Secondary  NameNode</td>
<td>CentOS  5.8</td>
<td>1</td>
<td>2*4核</td>
<td>ECC  DDR2 96G</td>
<td>SATA  1T+1T*3(RAID 10)</td>
<td>千兆以太网卡*2</td>
<td>Dell(R410),HP(DL160)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>JobTracker</td>
<td>CentOS  5.8</td>
<td>1</td>
<td>2*4核</td>
<td>ECC  DDR2 32G</td>
<td>SATA  1T+1T*3(RAID 10)</td>
<td>千兆以太网卡*2</td>
<td>Dell(R410),HP(DL160)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>4</td>
<td>负载均衡(HaProxy)</td>
<td>CentOS  5.8</td>
<td>2</td>
<td>2*4核</td>
<td>ECC  DDR2 8~16G</td>
<td>SATA  500G+(RAID 10)</td>
<td>千兆以太网卡*2</td>
<td>Dell(R410),HP(DL160)</td>
<td>互备</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>6</td>
<td>监控管理(Cacti Nagios Ganglia)</td>
<td>CentOS  5.8</td>
<td>1</td>
<td>2*4核</td>
<td>ECC  DDR2 8~16G</td>
<td>SATA  500G+(RAID 10)</td>
<td>千兆以太网卡*2</td>
<td>Dell(R410),HP(DL160)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>7</td>
<td>Hive  MetaStore DB Server (Mysql)</td>
<td>CentOS  5.9</td>
<td>2</td>
<td>2*5核</td>
<td>ECC  DDR2 8~16G</td>
<td>SATA  500G+(RAID 10)</td>
<td>千兆以太网卡*2</td>
<td>Dell(R410),HP(DL160)</td>
<td>压力较小</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>存储和运算节点</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>5</td>
<td>DataNode+TaskTracker</td>
<td>CentOS  5.8</td>
<td>N</td>
<td>2*4核</td>
<td>ECC  DDR2 16~32G</td>
<td>SATA  2T+2T*(3~12)</td>
<td>千兆以太网卡*2</td>
<td>Dell(R410,R510)</td>
<td>磁盘不做 RAID</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>6</td>
<td>HiveServer</td>
<td>CentOS  5.8</td>
<td>2</td>
<td>2*4核</td>
<td>ECC  DDR2 32G</td>
<td>SATA  2T</td>
<td>千兆以太网卡*2</td>
<td>Dell(R410),HP(DL160)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="EasyHive仓库集群部署入门"><a href="#EasyHive仓库集群部署入门" class="headerlink" title="EasyHive仓库集群部署入门"></a>EasyHive仓库集群部署入门</h1><p><img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image028.png" alt="img"></p>
<h2 id="1-名词解释"><a href="#1-名词解释" class="headerlink" title="1.  名词解释"></a><a href="undefined">1.  名词解释</a></h2><p>\1. Hive,由facebook 开源的数据仓库系统,可以基于Hql语句操作Hadoop集群数据。</p>
<p>\2. Mysql,开源数据库系统,H ive 存储原数据使用。</p>
<p>\3. FineReport 一个简单易用的快速报表研发工具。</p>
<p>\4. alexa 国际知名网站排名统计</p>
<p>\5. Sogou 数据资源,<a href="http://www.sogou.com/labs/resources.html" target="_blank" rel="external">http://www.sogou.com/labs/resources.html</a></p>
<h2 id="2-Hive的作用和原理说明"><a href="#2-Hive的作用和原理说明" class="headerlink" title="2.  Hive的作用和原理说明"></a><a href="undefined">2.  Hive</a>的作用和原理说明</h2><h3 id="Hadoop仓库和传统数据仓库的协作结构图"><a href="#Hadoop仓库和传统数据仓库的协作结构图" class="headerlink" title="#Hadoop仓库和传统数据仓库的协作结构图"></a><a href="undefined">#Hadoop</a>仓库和传统数据仓库的协作结构图</h3><p><img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image030.png" alt="img"></p>
<p>属于互为补充的关系,相比传统数据仓库技术,Hadoop仓库更合适做非结构化数据分析。</p>
<h3 id="Hadoop-Hive仓库数据日志分析流向图"><a href="#Hadoop-Hive仓库数据日志分析流向图" class="headerlink" title="#Hadoop/Hive仓库数据日志分析流向图"></a><a href="undefined">#Hadoop/Hive</a>仓库数据日志分析流向图</h3><p><img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image032.png" alt="img"></p>
<p><img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image034.png" alt="img"></p>
<h3 id="hive内部结构图"><a href="#hive内部结构图" class="headerlink" title="#hive内部结构图"></a><a href="undefined">#hive</a>内部结构图</h3><p><img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image036.png" alt="img"></p>
<h2 id="3-Hive-部署和安装"><a href="#3-Hive-部署和安装" class="headerlink" title="3.  Hive 部署和安装"></a><a href="undefined">3.  Hive </a>部署和安装</h2><h3 id="安装Hadoop集群-看EasyHadoop集群部署入门章节。"><a href="#安装Hadoop集群-看EasyHadoop集群部署入门章节。" class="headerlink" title=" #安装Hadoop集群,看EasyHadoop集群部署入门章节。"></a><a href="undefined"> #</a>安装Hadoop集群,看EasyHadoop集群部署入门章节。</h3><h3 id="编译安装Mysql-启动Mysql-检查gc-包。"><a href="#编译安装Mysql-启动Mysql-检查gc-包。" class="headerlink" title="#编译安装Mysql,启动Mysql,检查gc++包。"></a><a href="undefined">#</a>编译安装Mysql,启动Mysql,检查gc++包。</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">##/bin/sh</span></div><div class="line"><span class="built_in">cd</span> /opt/modules/src</div><div class="line">wget http://ftp.gnu.org/pub/gnu/ncurses/ncurses-5.6.tar.gz</div><div class="line">tar zxvf ncurses-5.6.tar.gz</div><div class="line"><span class="built_in">cd</span> ncurses-5.6</div><div class="line">./configure -prefix=/usr -with-shared -without-debug</div><div class="line">make &amp;&amp; make install clean</div><div class="line"></div><div class="line">wget http://60.28.110.228/<span class="built_in">source</span>/package/mysql-5.1.42.tar.gz</div><div class="line">tar -xzvf mysql-5.1.42.tar.gz</div><div class="line"><span class="built_in">cd</span> mysql-5.1.42</div><div class="line">./configure  <span class="string">'--with-embedded-server'</span> <span class="string">'--with-comment=MySQL Community Server (GPL)'</span> <span class="string">'--with-mysqld-ldflags=-static'</span> <span class="string">'--with-client-ldflags=-static'</span> <span class="string">'--enable-assembler'</span> <span class="string">'--enable-local-infile'</span> <span class="string">'--with-fast-mutexes'</span> <span class="string">'--with-mysqld-user=mysql'</span> <span class="string">'--with-unix-socket-path=/var/lib/mysql/mysql.sock'</span> <span class="string">'--with-pic'</span> <span class="string">'--prefix=/opt/modules/mysql/'</span> <span class="string">'--with-extra-charsets=complex'</span> <span class="string">'--with-ssl'</span> <span class="string">'--sysconfdir=/etc'</span> <span class="string">'--datadir=/opt/data/modules/mysql/'</span> <span class="string">'--enable-thread-safe-client'</span> <span class="string">'--with-readline'</span> <span class="string">'--with-innodb'</span> <span class="string">'--with-plugin-innodb_plugin'</span> <span class="string">'--without-ndbcluster'</span> <span class="string">'--with-archive-storage-engine'</span> <span class="string">'--with-csv-storage-engine'</span> <span class="string">'--with-blackhole-storage-engine'</span> <span class="string">'--with-federated-storage-engine'</span> <span class="string">'--without-plugin-daemon_example'</span> <span class="string">'--without-plugin-ftexample'</span> <span class="string">'--with-partition'</span> <span class="string">'--with-big-tables'</span> <span class="string">'--with-zlib-dir=bundled'</span> <span class="string">'--enable-shared'</span> <span class="string">'CC=gcc'</span> <span class="string">'CFLAGS=-O2 -g -pipe'</span> <span class="string">'LDFLAGS='</span> <span class="string">'CXX=gcc'</span> <span class="string">'CXXFLAGS=-O2 -g -pipe -felide-constructors -fno-exceptions -fno-rtti '</span></div><div class="line"></div><div class="line">make &amp;&amp; make install </div><div class="line">/usr/sbin/groupadd mysql </div><div class="line">/usr/sbin/useradd -g mysql mysql</div><div class="line">mkdir -p  /opt/data/modules/mysql/logs/</div><div class="line">mkdir -p  /opt/data/modules/mysql/data/</div><div class="line">./scripts/mysql_install_db</div><div class="line">chown -R mysql:mysql /opt/data/modules/mysql/</div><div class="line">chown -R mysql:mysql /opt/modules/mysql/var</div><div class="line">/opt/modules/mysql/bin/mysqld_safe &amp;</div></pre></td></tr></table></figure>
<p> <a href="undefined">   </a> #RPM 安装Mysql server </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#http://mirrors.oss.org.cn/mysql/MySQL-6.0/?C=M;O=D 下载地址!</span></div><div class="line"></div><div class="line"><span class="comment">#mysql server</span></div><div class="line">rpm -ivh MySQL-server-community-6.0.11-0.rhel5.i386.rpm</div><div class="line"></div><div class="line"><span class="comment">#mysql client </span></div><div class="line">rpm -ivh MySQL-client-community-6.0.11-0.rhel5.i386.rpm</div></pre></td></tr></table></figure>
<pre><code>#启动mysql服务器   

/sbin/service mysql start
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master ~]# /sbin/service mysql restart</div><div class="line">Shutting down MySQL..                                      [  OK  ]</div><div class="line">Starting MySQL.                                            [  OK  ]</div></pre></td></tr></table></figure>
<h3 id="登陆Mysql-服务器"><a href="#登陆Mysql-服务器" class="headerlink" title="#登陆Mysql 服务器"></a>#登陆Mysql 服务器</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mysql -uroot -p</div></pre></td></tr></table></figure>
<h3 id="添加Hive用户名和密码"><a href="#添加Hive用户名和密码" class="headerlink" title="#添加Hive用户名和密码"></a>#添加Hive用户名和密码</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#2).授权法。例如,你想myuser使用mypassword从任何主机连接到mysql服务器的话。</span></div><div class="line">GRANT ALL PRIVILEGES ON *.* TO <span class="string">'hive'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'hive'</span> WITH GRANT OPTION ;</div><div class="line"><span class="comment">#如果你想允许用户myuser从ip为192.168.1.%的主机连接到mysql服务器，并使用mypassword作为密码 </span></div><div class="line">GRANT ALL PRIVILEGES ON *.* TO <span class="string">'hive'</span>@<span class="string">'192.168.1.%'</span> IDENTIFIED BY <span class="string">'hive'</span> WITH GRANT OPTION;</div><div class="line"><span class="comment">#我用的第一个方法,刚开始发现不行,在网上查了一下,少执行一个语句</span></div><div class="line"></div><div class="line">mysql&gt;FLUSH PRIVILEGES</div><div class="line"></div><div class="line"><span class="comment">#使修改生效，就可以了</span></div><div class="line"><span class="comment">#遗留问题</span></div><div class="line">[root@master ~]<span class="comment"># mysql -uhive -phive -h192.168.1.100</span></div><div class="line">ERROR 1130 (HY000): Host <span class="string">'::ffff:192.168.1.100'</span> is not allowed to connect to this MySQL server</div></pre></td></tr></table></figure>
<h3 id="解压Hive包并配置JDBC连接地址。"><a href="#解压Hive包并配置JDBC连接地址。" class="headerlink" title="#解压Hive包并配置JDBC连接地址。"></a>#解压Hive包并配置JDBC连接地址。</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">mkdir -p  /opt/modules/hive/</div><div class="line">cp hive-0.9.0.tar.gz /opt/modules/</div><div class="line"><span class="built_in">cd</span> /opt/modules/</div><div class="line">tar -xzvf hive-0.9.0.tar.gz</div><div class="line">cp mysql-connector-java-5.1.20-bin.jar  /opt/modules/hive/hive-0.9.0/lib/</div></pre></td></tr></table></figure>
<h4 id="编辑配置Hive默认文件"><a href="#编辑配置Hive默认文件" class="headerlink" title="#编辑配置Hive默认文件"></a>#编辑配置Hive默认文件</h4><p>cp/opt/modules/hive/hive-0.9.0/conf/hive-env.sh.template/opt/modules/hive/hive-0.9.0/conf/hive-env.sh</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> HADOOP_HEAPSIZE=64 <span class="comment">#默认1024m</span></div></pre></td></tr></table></figure>
<p>cp /opt/modules/hive/hive-0.9.0/conf/hive-default.xml.template/opt/modules/hive-0.9.0/conf/hive-site.xml</p>
<p>vi /opt/modules/hive/hive-0.9.0/conf/hive-site.xml </p>
<h4 id="修改一下hive-site-mysql-JDBC节点内容"><a href="#修改一下hive-site-mysql-JDBC节点内容" class="headerlink" title="#修改一下hive-site mysql JDBC节点内容"></a>#修改一下hive-site mysql JDBC节点内容</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://192.168.1.100:3306/hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<h3 id="添加Mysql-Hive用户名和密码-创建Hive-仓库"><a href="#添加Mysql-Hive用户名和密码-创建Hive-仓库" class="headerlink" title=" #添加Mysql Hive用户名和密码,创建Hive 仓库"></a><a href="undefined"> #</a>添加Mysql Hive用户名和密码,创建Hive 仓库</h3><h4 id="登陆mysql-服务器"><a href="#登陆mysql-服务器" class="headerlink" title="#登陆mysql 服务器"></a>#登陆mysql 服务器</h4><p>/opt/modules/mysql/bin/mysql -uroot -p</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#创建Hive metastore</span></div><div class="line">create database hive;</div></pre></td></tr></table></figure>
<h3 id="启动Hive-thrift-Server"><a href="#启动Hive-thrift-Server" class="headerlink" title="#启动Hive thrift Server"></a>#启动Hive thrift Server</h3><h4 id="启动hive-server"><a href="#启动hive-server" class="headerlink" title="#启动hive server"></a>#启动hive server</h4><p>/opt/modules/hive/hive-0.9.0/bin/hive–service hiveserver 10001</p>
<h4 id="测试"><a href="#测试" class="headerlink" title="#测试"></a>#测试</h4><p>netstat -nap |grep 10001</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@hadoop-231 bin]<span class="comment">#netstat -nap |grep 10001</span></div><div class="line">tcp        0      0 :::10001                    :::*                        LISTEN      31166/java</div></pre></td></tr></table></figure>
<h3 id="启动内置的Hive-UI。"><a href="#启动内置的Hive-UI。" class="headerlink" title="#启动内置的Hive UI。"></a><a href="undefined">#</a>启动内置的Hive UI。</h3><p>/opt/modules/hive/hive-0.9.0/bin/hive–service hwi</p>
<p><img src="file://localhost/Users/xialinsheng/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image038.png" alt="img"></p>
<h2 id="4-Hive-Cli-的基本用法"><a href="#4-Hive-Cli-的基本用法" class="headerlink" title="4.  Hive Cli 的基本用法"></a><a href="undefined">4.  Hive Cli </a>的基本用法</h2><p>  cd /opt/modules/hive/hive-0.9.0/bin/</p>
<h3 id="登陆查询"><a href="#登陆查询" class="headerlink" title="#登陆查询"></a><a href="undefined">#</a>登陆查询</h3><p>#执行hive 命令到 hive 命令行模式下面</p>
<p>./hive </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@hadoop-231 bin]<span class="comment">#./hive</span></div><div class="line">hive&gt; show databases;</div><div class="line">OK</div><div class="line">alexa</div><div class="line">default</div><div class="line"><span class="built_in">test</span></div><div class="line">Time taken: 3.103 seconds</div><div class="line">hive&gt;</div></pre></td></tr></table></figure>
<h3 id="查询文件方式"><a href="#查询文件方式" class="headerlink" title="#查询文件方式"></a><a href="undefined">#</a>查询文件方式</h3><h4 id="编辑一个HQL-文件"><a href="#编辑一个HQL-文件" class="headerlink" title="#编辑一个HQL 文件"></a>#编辑一个HQL 文件</h4><p>vi test.q</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">use <span class="built_in">test</span>;</div><div class="line">select * from test_text <span class="built_in">limit</span> 30;</div></pre></td></tr></table></figure>
<h4 id="读取查询文件查询方式"><a href="#读取查询文件查询方式" class="headerlink" title="#读取查询文件查询方式"></a>#读取查询文件查询方式</h4><p>./hive -f test.q</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[root@hadoop-231 bin]<span class="comment">#./hive -f test.q </span></div><div class="line">Hive <span class="built_in">history</span> file=/tmp/root/hive_job_log_root_201207281215_1090497680.txt</div><div class="line">OK</div><div class="line">Time taken: 3.306 seconds</div><div class="line">OK</div><div class="line">12944   28595   34502</div><div class="line">11412   27522   34974</div><div class="line">17982   27989   30923</div><div class="line">17813   26214   38065</div><div class="line">.........................................................</div></pre></td></tr></table></figure>
<h3 id="命令行模式"><a href="#命令行模式" class="headerlink" title="#命令行模式"></a><a href="undefined">#</a>命令行模式</h3><p>./hive -e “select * fromtest.test_text limit 30”</p>
<p>#效果同上</p>
<h2 id="5-HQL基本语法-创建表-加载表-分析查询-删除表"><a href="#5-HQL基本语法-创建表-加载表-分析查询-删除表" class="headerlink" title="5.  HQL基本语法 (创建表,加载表,分析查询,删除表)"></a><a href="undefined">5.  HQL</a>基本语法 (创建表,加载表,分析查询,删除表)</h2><h3 id="快速案例"><a href="#快速案例" class="headerlink" title="#快速案例"></a><a href="undefined">#</a>快速案例</h3><p>./hive #登陆hive Cli模式</p>
<p>CREATEdatabase alexa; #建库</p>
<p>usealexa;</p>
<p>create  table uid3(uid string)</p>
<p>PARTITIONED BY(dt STRING)</p>
<p>ROW FORMAT DELIMITED FIELDSTERMINATED BY ‘\t’</p>
<p>collectionitems terminated by “\n”</p>
<p>STOREDAS TEXTFILE</p>
<p>LOCATION’/data/dw/demo/uid3/‘</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE top100w (id BIGINT,domain STRING COMMENT <span class="string">'alexa domain'</span>)</div><div class="line"> COMMENT <span class="string">'alexa top100w'</span></div><div class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="string">','</span></div><div class="line">collection items terminated by <span class="string">"\n"</span> </div><div class="line">STORED AS TEXTFILE</div></pre></td></tr></table></figure>
<p>LOADDATA  LOCAL INPATH ‘/root/top-1m.csv’OVERWRITE INTO TABLE top100w; #加载</p>
<p>LOADDATA  INPATH’/data/dw/top100w/top-1m.csv’ OVERWRITE INTO TABLE top100w; #加载</p>
<p>loaddata inpath ‘/data/uid.txt’ overwrite into table uid;</p>
<p>#测试</p>
<p>SELECT* FROM alexa.top100w limit 100; </p>
<p>select* from alexa.top100w where domain=”itcast.cn” </p>
<h3 id="创建表"><a href="#创建表" class="headerlink" title="#创建表"></a>#创建表</h3><p>  a.内部表创建</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE top100w (id BIGINT,domain STRING) COMMENT <span class="string">'alexa top100w'</span></div></pre></td></tr></table></figure>
<p>  b.外部表创建</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">CREATE EXTERNAL TABLE top100w (id BIGINT,domain STRING COMMENT <span class="string">'alexa domain'</span>)</div><div class="line"> COMMENT <span class="string">'alexa top100w'</span></div><div class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="string">','</span></div><div class="line">collection items terminated by <span class="string">"\n"</span> </div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION <span class="string">'/data/dw/alexa/top100w/'</span></div></pre></td></tr></table></figure>
<h3 id="加载数据"><a href="#加载数据" class="headerlink" title="#加载数据"></a>#加载数据</h3><p>  #加载数据到外部表使用 put 或者 copyFromLocal</p>
<p>  Hadoop fs -mkdir /data/dw/alexa/top100w/</p>
<p>  hadoop fs -put /root/top-1m.csv /data/dw/alexa/top100w/</p>
<p>  #检查</p>
<p> Select * from alexa.top100w limit 100;</p>
<p>  #加载数据到内部表</p>
<p>  LOADDATA LOCAL INPATH ‘/home/hadoop/top100w.csv’ OVERWRITE INTO TABLE log;</p>
<h3 id="分析查询"><a href="#分析查询" class="headerlink" title="#分析查询"></a>#分析查询</h3><ol>
<li><p>简单查询 select * from top100w limit 10 分析背后原理</p>
</li>
<li><p>简单查询 select a from top100w limit 10  分析背后原理</p>
<p>3.where 练习</p>
<p>4.group by 练习</p>
<p>5.order by 练习</p>
<p>6.union all 练习</p>
<p>7.INSERT OVERWRITE TABLE 练习</p>
<p>8.INSERT OVERWRITE DIRECTORY ‘/user/facebook/tmp/pv_age_sum’</p>
<p>9.SELECT a.key, a.value FROM a WHERE a.key in (SELECT b.key FROM B) -&gt; SELECTa.key, a.val FROM a LEFT SEMI JOIN b on (a.key = b.key)</p>
<p>10.Hive 函数查询方法 查询地址: <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/Hive/LanguageManual</a></p>
<p>11.dfs 操作命令</p>
</li>
</ol>
<h2 id="6-使用hive到Mysql构建简单数据集市"><a href="#6-使用hive到Mysql构建简单数据集市" class="headerlink" title="6.  使用hive到Mysql构建简单数据集市"></a><a href="undefined">6.  使用hive</a>到Mysql构建简单数据集市</h2><h3 id="创建一个数据表使用Hivecli-进行数据分析"><a href="#创建一个数据表使用Hivecli-进行数据分析" class="headerlink" title="#创建一个数据表使用Hivecli 进行数据分析"></a><a href="undefined">#</a>创建一个数据表使用Hivecli 进行数据分析</h3><h3 id="使用shell-编写Hsql-并使用HiveCli导出数据-使用Mysql命令加载到数据库中。"><a href="#使用shell-编写Hsql-并使用HiveCli导出数据-使用Mysql命令加载到数据库中。" class="headerlink" title="#使用shell 编写Hsql 并使用HiveCli导出数据,使用Mysql命令加载到数据库中。"></a><a href="undefined">#</a>使用shell 编写Hsql 并使用HiveCli导出数据,使用Mysql命令加载到数据库中。</h3><h3 id="使用crontab-新增每日运行任务定时器"><a href="#使用crontab-新增每日运行任务定时器" class="headerlink" title="#使用crontab 新增每日运行任务定时器"></a><a href="undefined">#</a>使用crontab 新增每日运行任务定时器</h3><p> crontab -e</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">50 02 * * *  /opt/<span class="built_in">jobs</span>/top100w.sh</div></pre></td></tr></table></figure>
<h2 id="7-使用FineReport-数据展现数据"><a href="#7-使用FineReport-数据展现数据" class="headerlink" title="7.  使用FineReport 数据展现数据"></a><a href="undefined">7.  使用FineReport </a>数据展现数据</h2><h3 id="安装FineReport-使用注册码"><a href="#安装FineReport-使用注册码" class="headerlink" title="#安装FineReport,使用注册码"></a><a href="undefined">#</a>安装FineReport,使用注册码</h3><h3 id="使用FineReport-快速展现数据报表"><a href="#使用FineReport-快速展现数据报表" class="headerlink" title="#使用FineReport,快速展现数据报表"></a><a href="undefined">#</a>使用FineReport,快速展现数据报表</h3><h2 id="8-练习"><a href="#8-练习" class="headerlink" title="8.  练习"></a>8.  练习</h2><p>\1. 下载 <a href="http://www.sogou.com/labs/dl/q.html" target="_blank" rel="external">http://www.sogou.com/labs/dl/q.html</a>完整版(1.9GB)</p>
<p>\2. 计算出网民搜索行为曲线,并用图表展现</p>
<p>\3. 计算出关键词搜索行为次数排行榜</p>
<p>\4. 计算出域名访问排行榜</p>
<p>思考题:通过挖掘日志如何改进搜索结果,那个URL才是用户想要的?</p>
<p>\5. 使用日志计算出每个点击URL的用户行为权重</p>
<p>\6. 下载其他搜狗数据库做练习</p>
<p>搜狗行为库结构:</p>
<p>访问时间\t用户ID\t[查询词]\t该URL在返回结果中的排名\t用户点击的顺序号\t用户点击的URL</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">logtime string </div><div class="line">user bigint</div><div class="line">keyword string</div><div class="line">rank int</div><div class="line">clickrank int</div><div class="line">url string </div><div class="line"></div><div class="line">CREATE EXTERNAL TABLE sogouq (logtime string,user bigint,keyword string,rank int,clickrank int,url string )</div><div class="line"> COMMENT <span class="string">'sogou click log'</span></div><div class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="string">'\t'</span></div><div class="line">collection items terminated by <span class="string">"\n"</span> </div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION <span class="string">'/data/dw/sogouq/'</span></div><div class="line"></div><div class="line">CREATE TABLE clicklog2 (logtime string,user bigint,keyword string,rank int,clickrank int,url string )</div><div class="line"> COMMENT <span class="string">'sogou click log'</span></div><div class="line">PARTITIONED BY(dt STRING)</div><div class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="string">'\t'</span></div><div class="line">collection items terminated by <span class="string">"\n"</span> </div><div class="line">STORED AS TEXTFILE</div><div class="line"></div><div class="line">select c.hour,count(*) from (select split(logtime,<span class="string">':'</span>)[0] hour from clicklog) c group by c.hour;</div><div class="line"></div><div class="line">CREATE TABLE `click_stat` (</div><div class="line">   `stat_hour` varchar(4) DEFAULT NULL,</div><div class="line">   `click_count` int(11) DEFAULT NULL</div><div class="line"> ) ENGINE=MyISAM DEFAULT CHARSET=utf8</div><div class="line"></div><div class="line"> mysql -uhive -phive -h192.168.1.100 --local-infile <span class="_">-e</span> <span class="string">"use test;LOAD DATA LOCAL INFILE 'click_hour.csv' REPLACE INTO TABLE click_stat CHARACTER SET utf8 FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n' ( stat_hour,click_count)"</span></div><div class="line"></div><div class="line"></div><div class="line">14:00 23232323232  Hadoop 3  2  easyhadoo.cpm</div><div class="line"> </div><div class="line">Hadoop apache</div><div class="line">Hadoop china</div><div class="line">easyhadoop</div><div class="line">Csnd hadoop </div><div class="line"></div><div class="line"></div><div class="line">iconv <span class="_">-f</span> GBK -t UTF-8 access_log.20080611.decode.filter &gt; access_log.20080611</div><div class="line">iconv <span class="_">-f</span> GB18030 -t UTF-8 access_log.20080611.decode.filter &gt; access_log.20080611</div><div class="line">/opt/modules/hadoop-1.0.3/bin/hadoop fs -copyFromLocal access_log.20080611 /data/dw/sogouq/dt=20080611/</div><div class="line"></div><div class="line">alter table sogouq add  partition(dt=20080602)  location <span class="string">'/data/dw/sogouq/dt=20080602/'</span>;</div><div class="line">访问 http://hive.easyhadoop.com/ 进行在线练习</div></pre></td></tr></table></figure>
<p>==后续课程==</p>
<p>Hive 数据仓库编程和高级应用</p>
<p>1.HiveServer HA (使用haproxy提高HiveServer可用性)</p>
<p>2.使用JDBC 连接Hive进行查询和分析</p>
<p>3.使用Lzo压缩优化数据存储容量</p>
<p>3.使用正则表达式加载数据</p>
<p>4.使用Hive分区优化查询</p>
<p>5.HQL高级语法</p>
<p>6.编写UDF函数</p>
<p>7.编写Hive自定义MapReduce脚本优化查询</p>
<p>8 编写UDAF自定义函数</p>
<p>9.Hive内存优化查询性能优化</p>
<p>———————– 大数据接收 存储 分析 展现方案 ————————-</p>
<p>nginx –&gt; scribe(flume) –&gt; Hadoop–&gt; Hive–&gt;sqoop[ETL] –&gt; oozie[工作流] –&gt; inforbirght –&gt; java+Highcharts</p>
<pre><code>--&gt;haproxy--&gt; phphiveadmin--&gt; 用户

 --&gt;Hbase  --&gt; java+Highcharts
</code></pre><p>(大云,用户日志查询)</p>
<p>10.使用Sqoop进行数据分析</p>
<p>11.使用oozie配置工作流</p>
<p>12.使用Inforbright 构建大型数据集市</p>
<p>13.使用Java+Highcharts构建动态报表</p>
<p>14.Hive+phpHiveAdmin部署和安装仓库自助查询系统</p>
<p>  a.编译安装nginx+php并运行</p>
<p>  b.修改phpHiveAdmin配置文件</p>
<p>-———————–</p>
<p>15.构建基于Hive的OLAP应用</p>
<h1 id="EasyPig数据流分析集群部署入门"><a href="#EasyPig数据流分析集群部署入门" class="headerlink" title="EasyPig数据流分析集群部署入门"></a>EasyPig数据流分析集群部署入门</h1><h3 id="1-数据测试"><a href="#1-数据测试" class="headerlink" title="1.数据测试"></a>1.数据测试</h3><p>#从 top100w 中查询出 itcast.cn 的 alexa 排名</p>
<p>top100w= load ‘/data/dw/top100w/top-1m.csv’using PigStorage(‘,’) as (id:int,domain:chararray);</p>
<p>fltrd = FILTER top100w by domain==’itcast.cn’</p>
<p>itcast = FOREACH fltrd GENERATE id,domain;</p>
<p>store itcast into ‘/data/itcast.out’</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://stanxia.github.io/2017/03/20/Hadoop-HA配置/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="森">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="东篱下">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/03/20/Hadoop-HA配置/" itemprop="url">Hadoop HA配置</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-03-20T22:40:01+08:00">
                2017-03-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script src="/assets/js/DPlayer.min.js"> </script><p>   在这里我们选用4台机器进行示范，各台机器的职责如下表格所示</p>
<table>
<thead>
<tr>
<th></th>
<th>hadoop0</th>
<th>hadoop1</th>
<th>hadoop2</th>
<th>hadoop3</th>
</tr>
</thead>
<tbody>
<tr>
<td>是NameNode吗?</td>
<td>是，属集群cluster1</td>
<td>是，属集群cluster1</td>
<td>是，属集群cluster2</td>
<td>是，属集群cluster2</td>
</tr>
<tr>
<td>是DataNode吗？</td>
<td>否</td>
<td>是</td>
<td>是</td>
<td>是</td>
</tr>
<tr>
<td>是JournalNode吗？</td>
<td>是</td>
<td>是</td>
<td>是</td>
<td>否</td>
</tr>
<tr>
<td>是ZooKeeper吗？</td>
<td>是</td>
<td>是</td>
<td>是</td>
<td>否</td>
</tr>
<tr>
<td>是ZKFC吗?</td>
<td>是</td>
<td>是</td>
<td>是</td>
<td>是</td>
</tr>
</tbody>
</table>
<h2 id="1-搭建自动HA"><a href="#1-搭建自动HA" class="headerlink" title="1.     搭建自动HA"></a>1.     搭建自动HA</h2><h3 id="1-1-复制编译后的hadoop项目到-usr-local目录下"><a href="#1-1-复制编译后的hadoop项目到-usr-local目录下" class="headerlink" title="1.1.    复制编译后的hadoop项目到/usr/local目录下"></a>1.1.    复制编译后的hadoop项目到/usr/local目录下</h3><h3 id="1-2-修改位于etc-hadoop目录下的配置文件"><a href="#1-2-修改位于etc-hadoop目录下的配置文件" class="headerlink" title="1.2.    修改位于etc/hadoop目录下的配置文件"></a>1.2.    修改位于etc/hadoop目录下的配置文件</h3><h4 id="1-2-1-hadoop-env-sh"><a href="#1-2-1-hadoop-env-sh" class="headerlink" title="1.2.1.     hadoop-env.sh"></a>1.2.1.     hadoop-env.sh</h4><p>export JAVA_HOME=/usr/local/jdk</p>
<h4 id="1-2-2-core-site-xml"><a href="#1-2-2-core-site-xml" class="headerlink" title="1.2.2.     core-site.xml"></a>1.2.2.     core-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://cluster1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  </div><div class="line">【这里的值指的是默认的HDFS路径。当有多个HDFS集群同时工作时，用户如果不写集群名称，那么默认使用哪个哪？在这里指定！该值来自于hdfs-site.xml中的配置。在节点hadoop0和hadoop1中使用cluster1，在节点hadoop2和hadoop3中使用cluster2】</div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">【这里的路径默认是NameNode、DataNode、JournalNode等存放数据的公共目录。用户也可以自己单独指定这三类节点的目录。】</div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop0:2181,hadoop1:2181,hadoop2:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">【这里是ZooKeeper集群的地址和端口。注意，数量一定是奇数，且不少于三个节点】</div><div class="line"></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure>
<h4 id="1-2-3-hdfs-site-xml"><a href="#1-2-3-hdfs-site-xml" class="headerlink" title="1.2.3.     hdfs-site.xml"></a>1.2.3.     hdfs-site.xml</h4><p>该文件只配置在hadoop0和hadoop1上。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">【指定DataNode存储block的副本数量。默认值是3个，我们现在有4个DataNode，该值不大于4即可。】</div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster1,cluster2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">【使用federation时，使用了2个HDFS集群。这里抽象出两个NameService实际上就是给这2个HDFS集群起了个别名。名字可以随便起，相互不重复即可】</div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.cluster1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop0,hadoop1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">【指定NameService是cluster1时的namenode有哪些，这里的值也是逻辑名称，名字随便起，相互不重复即可】</div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.cluster1.hadoop0<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop0:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">【指定hadoop0的RPC地址】</div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.cluster1.hadoop0<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop0:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">【指定hadoop0的http地址】</div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.cluster1.hadoop1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">【指定hadoop1的RPC地址】</div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.cluster1.hadoop1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">【指定hadoop1的http地址】</div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop0:8485;hadoop1:8485;hadoop2:8485/cluster1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">【指定cluster1的两个NameNode共享edits文件目录时，使用的JournalNode集群信息】</div><div class="line"></div><div class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled.cluster1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">【指定cluster1是否启动自动故障恢复，即当NameNode出故障时，是否自动切换到另一台NameNode】</div><div class="line"></div><div class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.cluster1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">【指定cluster1出故障时，哪个实现类负责执行故障切换】</div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.cluster2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop2,hadoop3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">【指定NameService是cluster2时，两个NameNode是谁，这里是逻辑名称，不重复即可。以下配置与cluster1几乎全部相似，不再添加注释】</div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.cluster2.hadoop2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop2:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.cluster2.hadoop2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop2:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.cluster2.hadoop3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop3:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.cluster2.hadoop3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop3:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="comment">&lt;!--</span></div><div class="line"></div><div class="line">    &lt;property&gt;</div><div class="line"></div><div class="line">		&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</div><div class="line"></div><div class="line">		&lt;value&gt;qjournal://hadoop0:8485;hadoop1:8485;hadoop2:8485/cluster2&lt;/value&gt;</div><div class="line"></div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">【这段代码是注释掉的，不要打开】</div><div class="line"></div><div class="line">    --&gt;</div><div class="line"></div><div class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled.cluster2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.cluster2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line"> </div><div class="line"></div><div class="line"> </div><div class="line"></div><div class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop/tmp/journal<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">【指定JournalNode集群在对NameNode的目录进行共享时，自己存储数据的磁盘路径】</div><div class="line"></div><div class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">【一旦需要NameNode切换，使用ssh方式进行操作】</div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">【如果使用ssh进行故障切换，使用ssh通信时用的密钥存储的位置】</div><div class="line"></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure>
<h4 id="1-2-4-slaves"><a href="#1-2-4-slaves" class="headerlink" title="1.2.4.     slaves"></a>1.2.4.     slaves</h4><p>hadoop1</p>
<p>hadoop2</p>
<p>hadoop3</p>
<h3 id="1-3-把以上配置的内容复制到hadoop1、hadoop2、hadoop3节点上"><a href="#1-3-把以上配置的内容复制到hadoop1、hadoop2、hadoop3节点上" class="headerlink" title="1.3.    把以上配置的内容复制到hadoop1、hadoop2、hadoop3节点上"></a>1.3.    把以上配置的内容复制到hadoop1、hadoop2、hadoop3节点上</h3><h3 id="1-4-修改hadoop1、hadoop2、hadoop3上的配置文件内容"><a href="#1-4-修改hadoop1、hadoop2、hadoop3上的配置文件内容" class="headerlink" title="1.4.    修改hadoop1、hadoop2、hadoop3上的配置文件内容"></a>1.4.    修改hadoop1、hadoop2、hadoop3上的配置文件内容</h3><h4 id="1-4-1-修改hadoop2上的core-site-xml内容"><a href="#1-4-1-修改hadoop2上的core-site-xml内容" class="headerlink" title="1.4.1.     修改hadoop2上的core-site.xml内容"></a>1.4.1.     修改hadoop2上的core-site.xml内容</h4><p>fs.defaultFS的值改为hdfs://cluster2</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://cluster2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<h4 id="1-4-2-修改hadoop2上的hdfs-site-xml内容"><a href="#1-4-2-修改hadoop2上的hdfs-site-xml内容" class="headerlink" title="1.4.2.     修改hadoop2上的hdfs-site.xml内容"></a>1.4.2.     修改hadoop2上的hdfs-site.xml内容</h4><p>把cluster1中关于journalnode的配置项删除，增加如下内容</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">   	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop0:8485;hadoop1:8485;hadoop2:8485/cluster2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<h4 id="1-4-3-开始启动"><a href="#1-4-3-开始启动" class="headerlink" title="1.4.3.     开始启动"></a>1.4.3.     开始启动</h4><h5 id="1-4-3-1-启动journalnode"><a href="#1-4-3-1-启动journalnode" class="headerlink" title="1.4.3.1.   启动journalnode"></a>1.4.3.1.   启动journalnode</h5><p>在hadoop0、hadoop1、hadoop2上执行sbin/hadoop-daemon.sh startjournalnode</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sbin/hadoop-daemon.sh startjournalnode</div></pre></td></tr></table></figure>
<h5 id="1-4-3-2-格式化ZooKeeper"><a href="#1-4-3-2-格式化ZooKeeper" class="headerlink" title="1.4.3.2.   格式化ZooKeeper"></a>1.4.3.2.   格式化ZooKeeper</h5><p>在hadoop0、hadoop2上执行bin/hdfs  zkfc -formatZK</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/hdfs  zkfc -formatZK</div></pre></td></tr></table></figure>
<h5 id="1-4-3-3-对hadoop0节点进行格式化和启动"><a href="#1-4-3-3-对hadoop0节点进行格式化和启动" class="headerlink" title="1.4.3.3.   对hadoop0节点进行格式化和启动"></a>1.4.3.3.   对hadoop0节点进行格式化和启动</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">bin/hdfs namenode  -format</div><div class="line"></div><div class="line">sbin/hadoop-daemon.sh  start namenode</div></pre></td></tr></table></figure>
<h5 id="1-4-3-4-对hadoop1节点进行格式化和启动"><a href="#1-4-3-4-对hadoop1节点进行格式化和启动" class="headerlink" title="1.4.3.4.   对hadoop1节点进行格式化和启动"></a>1.4.3.4.   对hadoop1节点进行格式化和启动</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">bin/hdfs namenode  -bootstrapStandby</div><div class="line"></div><div class="line">sbin/hadoop-daemon.sh  start namenode</div></pre></td></tr></table></figure>
<h5 id="1-4-3-5-在hadoop0、hadoop1上启动zkfc"><a href="#1-4-3-5-在hadoop0、hadoop1上启动zkfc" class="headerlink" title="1.4.3.5.   在hadoop0、hadoop1上启动zkfc"></a>1.4.3.5.   在hadoop0、hadoop1上启动zkfc</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sbin/hadoop-daemon.sh   start  zkfc</div></pre></td></tr></table></figure>
<p>我们的hadoop0、hadoop1有一个节点就会变为active状态。</p>
<h5 id="1-4-3-6-对于cluster2执行类似操作"><a href="#1-4-3-6-对于cluster2执行类似操作" class="headerlink" title="1.4.3.6.   对于cluster2执行类似操作"></a>1.4.3.6.   对于cluster2执行类似操作</h5><h4 id="1-4-4-启动datanode"><a href="#1-4-4-启动datanode" class="headerlink" title="1.4.4.     启动datanode"></a>1.4.4.     启动datanode</h4><p>在hadoop0上执行命令sbin/hadoop-daemons.sh   start  datanode</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sbin/hadoop-daemons.sh   start  datanode</div></pre></td></tr></table></figure>
<h3 id="1-5-配置Yarn"><a href="#1-5-配置Yarn" class="headerlink" title="1.5.    配置Yarn"></a>1.5.    配置Yarn</h3><h4 id="1-5-1-修改文件mapred-site-xml"><a href="#1-5-1-修改文件mapred-site-xml" class="headerlink" title="1.5.1.     修改文件mapred-site.xml"></a>1.5.1.     修改文件mapred-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<h4 id="1-5-2-修改文件yarn-site-xml"><a href="#1-5-2-修改文件yarn-site-xml" class="headerlink" title="1.5.2.      修改文件yarn-site.xml"></a>1.5.2.      修改文件yarn-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<p>【自定ResourceManager的地址，还是单点，这是隐患】</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"></div><div class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<h4 id="1-5-3-启动yarn"><a href="#1-5-3-启动yarn" class="headerlink" title="1.5.3.     启动yarn"></a>1.5.3.     启动yarn</h4><p>在hadoop0上执行sbin/start-yarn.sh  </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sbin/start-yarn.sh</div></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://stanxia.github.io/2017/03/13/兼顾稳定和性能，58大数据平台的技术演进与实践/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="森">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="东篱下">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/03/13/兼顾稳定和性能，58大数据平台的技术演进与实践/" itemprop="url">兼顾稳定和性能，58大数据平台的技术演进与实践</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-03-13T22:34:45+08:00">
                2017-03-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script src="/assets/js/DPlayer.min.js"> </script><p>讲师｜赵健博</p>
<p>编辑｜尚剑</p>
<p>大家好！我是赵健博，来自58赶集，今天给大家分享一下58大数据这块的经验。我先做个自我介绍，我本科和研究生分别是在北京邮电大学和中国科学院计算技术研究所先后毕业的，之前在百度和360工作，现在是58赶集高级架构师、58大数据平台负责人。我有多年的分布式系统（存储、计算）的实践和研发经验，在我工作的这些年中运营了大大小小的集群，最大单集群也达到了四五千台，在这个过程中做了大量的功能研发、系统优化，当然也淌了大量的坑，今天会给大家介绍一些我认为比较重要的。</p>
<p>接下来我会跟大家分享一下58大数据平台在最近一年半的时间内技术演进的过程。主要内容分为三方面：58大数据平台目前的整体架构是怎么样的；最近一年半的时间内我们面临的问题、挑战以及技术演进过程；以及未来的规划。</p>
<h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3gy1fdlmey6jh8j20hs0ao765.jpg" alt="img"></p>
<p>首先看一下58大数据平台架构。大的方面来说分为三层：数据基础平台层、数据应用平台层、数据应用层，还有两列监控与报警和平台管理。</p>
<p>数据基础平台层又分为四个子层：</p>
<ul>
<li>接入层，包括了Canal/Sqoop（主要解决数据库数据接入问题）、还有大量的数据采用Flume解决方案；</li>
<li>存储层，典型的系统HDFS（文件存储）、HBase（KV存储）、Kafka（消息缓存）；</li>
<li>再往上就是调度层，这个层次上我们采用了Yarn的统一调度以及Kubernetes的基于容器的管理和调度的技术；</li>
<li>再往上是计算层，包含了典型的所有计算模型的计算引擎，包含了MR、HIVE、Storm、Spark、Kylin以及深度学习平台比如Caffe、Tensorflow等等。</li>
</ul>
<p>数据应用平台主要包括以下功能：</p>
<ul>
<li>元信息管理，还有针对所有计算引擎、计算引擎job的作业管理，之后就是交互分析、多维分析以及数据可视化的功能。</li>
<li>再往上是支撑58集团的数据业务，比如说流量统计、用户行为分析、用户画像、搜索、广告等等。</li>
<li>针对业务、数据、服务、硬件要有完备的检测与报警体系。</li>
<li>平台管理方面，需要对流程、权限、配额、升级、版本、机器要有很全面的管理平台。</li>
</ul>
<p>这个就是目前58大数据平台的整体架构图。</p>
<p><img src="http://wx4.sinaimg.cn/mw1024/6aae3cf3gy1fdlmey8dbvj20hs09zjrv.jpg" alt="img"></p>
<p>这个图展示的是架构图中所包含的系统数据流动的情况。分为两个部分：</p>
<p><font color="red">首先是实时流，就是黄色箭头标识的这个路径。数据实时采集过来之后首先会进入到Kafka平台，先做缓存。实时计算引擎比如Spark streaming或storm会实时的从Kafka中取出它们想要计算的数据。经过实时的处理之后结果可能会写回到Kafka或者是形成最终的数据存到MySQL或者HBase，提供给业务系统，这是一个实时路径。</font></p>

<p><font color="red">对于离线路径，通过接入层的采集和收集，数据最后会落到HDFS上，然后经过Spark、MR批量计算引擎处理甚至是机器学习引擎的处理。其中大部分的数据要进去数据仓库中，在数据仓库这部分是要经过数据抽取、清洗、过滤、映射、合并汇总，最后聚合建模等等几部分的处理，形成数据仓库的数据。然后通过HIVE、Kylin、SparkSQL这种接口将数据提供给各个业务系统或者我们内部的数据产品，有一部分还会流向MySQL。以上是数据在大数据平台上的流动情况。</font></p>

<p>在数据流之外还有一套管理平台。包括元信息管理（云窗）、作业管理平台（58dp）、权限审批和流程自动化管理平台（NightFury）。</p>
<p><img src="http://wx4.sinaimg.cn/mw1024/6aae3cf3gy1fdlmeyoh1oj20hs07qweu.jpg" alt="img"></p>
<p>我们的规模可能不算大，跟BAT比起来有些小，但是也过了一千台，目前有1200台的机器。我们的数据规模目前有27PB，每天增量有50TB。作业规模每天大概有80000个job，核心job（产生公司核心指标的job）有20000个，每天80000个job要处理数据量是2.5PB。</p>
<h2 id="技术平台技术演进与实现"><a href="#技术平台技术演进与实现" class="headerlink" title="技术平台技术演进与实现"></a>技术平台技术演进与实现</h2><p>接下来我会重点介绍一下在最近一年半时间内我们大数据平台的技术演进过程，共分四个部分：稳定性、平台治理、性能以及异构计算。第一个部分关于稳定性的改进，稳定性是最基础的工作，我们做了比较多的工作。第二个部分是在平台治理方面的内容。第三个方面我们针对性能也做了一些优化。第四个方面，我们针对异构环境，比如说机器的异构、作业的异构，在这种环境下怎么合理地使用资源。</p>
<h3 id="稳定性改进"><a href="#稳定性改进" class="headerlink" title="稳定性改进"></a>稳定性改进</h3><p>首先看一下稳定性的改进。这块我会举一些例子进行说明。稳定性包含了几个方面，其中第一个方面就是系统的可用性，大家可以采用社区提供的HDFS  HA、Yarn  HA，Storm  HA来解决。另外一个方面是关于扩展性，例如Flume、HDFS，Yarn，Storm的扩展性。这里主要介绍下Flume和HDFS的扩展性相关的一些考虑。此外，有了可用性和扩展性，系统就稳定了吗？实际上不是这样。因为还有很多的突发问题。即使解决了可用性和扩展性，但突发问题还是可能会造成系统不可用，例如由于一些问题造成两台NameNode全部宕机。</p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3gy1fdlmeysmfqj20hs0a6jrn.jpg" alt="img"></p>
<p>首先看一下Flume的扩展性。我们人为的把它定义了两层。一个是FlumeLocal（主要解决一台机器的日志采集问题，简称Local），一个是FlumeCenter（主要从Local上收集数据，然后把数据写到HDFS上，简称Center），Local和Center之间是有一个HA的考虑的，就是Local需要在配置文件里指定两个Center去写入，一旦一个Center出现问题，数据可以马上从另一个Center流向HDFS。此外，我们还开发了一个高可靠的Agent。业务系统中会把数据产生日志写到磁盘上，Agent保证数据从磁盘上实时可靠的收集给本地的Local，其中我们采用了检查点的技术来解决数据可靠性的问题。</p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3gy1fdlmez0d2qj20hs0avmxi.jpg" alt="img"></p>
<p>这是Flume的典型架构。Local需要在配置文件里面指定死要连到哪几个Center上。如果说10台，可能还OK，100台也OK，如果一千台呢？如果发现两台Flume Center已经达到机器资源的上限，如何做紧急的扩容呢？所以从这个角度看Flume的扩展性是有问题的。</p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3gy1fdlmezg1ymj20hs0adwew.jpg" alt="img"></p>
<p>我们的解决方法是在Local和Center中间加了一个ZooKeeper，Local通过ZK动态发现Center，动态的发现下游有什么，就可以达到Center自动扩容的目标了。我们公司Local有两千多台，扩容一台Center仅需一分钟，这种架构实际上可以支持达到万台规模的，这是Flume扩展性的一些改进。</p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3gy1fdlmezrfdpj20hs0art91.jpg" alt="img"></p>
<p>接下来看一下HDFS扩展性的问题。上面这张图展示了hdfs federation的架构，左侧是一个单namespace架构，即整个目录树在一个namespace中，整个集群的文件数规模受限制于单机内存的限制。federation的思想是把目录树拆分，形成不同的namespace，不同namespace由不同namenode管理，这样就打破了单机资源限制，从而达到了可扩展的目标，如右侧图。</p>
<p>但这个方案有一些隐藏的问题，不知道大家有没有注意到，比如这里每个Datanode都会与所有的NameNode去心跳，如果DataNode数量上万台，那么就可能会出现两个问题：第一，从主节点之间的心跳、块汇报成为瓶颈，第二，如果单个部门的数据规模过大那该怎么办？</p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3gy1fdlmezrq4dj20hs0afmxl.jpg" alt="img"></p>
<p>针对从主节点之间交互的问题，我们可以进行拆分，控制一个NameNode管理的DateNode的数量，这样就可以避免主从节点交互开销过大的问题。针对单部门数据过大的话可以针对部门内数据进行进一步细拆，就OK了。或者可以考虑百度之前提供的一个方案，即把目录树和inode信息进行抽象，然后分层管理和存储。当然我们目前采用社区federation的方案。如果好好规划的话，也是可以到万台了。</p>
<p>不知道大家有没有在自己运营集群过程中遇到过一些问题，你们是怎么解决的，有些问题可能相当的棘手。突发问题是非常紧急而且重要的，需要在短时间内搞定。接下来我会分享三个例子。</p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3gy1fdlmfe5s06j20hs0b5t98.jpg" alt="img"></p>
<p>第一个例子是HDFS的Active NN会不定期异常退出，触发HA切换，这就好像一个不定时炸弹一样。这个图展示了HDFS的HA的架构图，客户端进行变更操作（如创建文件）的话会发出请求给namenode，namenode请求处理完之后会进行持久化工作，会在本地磁盘存一份，同时会在共享存储存一份，共享存储是为了active和standby之间同步状态的，standby会周期从共享存储中拉取更新的数据应用到自己的内存和目录树当中，所有的DataNode都是双汇报的，这样两个namenode都会有最新的块信息。最上面的是两个Checker，是为了仲裁究竟谁是Active的。</p>
<p>还有一个过程，Standby NameNode会定期做checkpoint工作，然后在checkpoint做完之后会回传最新的fsimage给active，最终保存在active的磁盘中，默认情况下在回传过程会造成大量的网络和磁盘的压力，导致active的本地磁盘的Util达到100%，此时用户变更请求延迟就会变高。如果磁盘的Util100%持续时间很长就会导致用户请求超时，甚至Checher的检测请求也因排队过长而超时，最终然后触发Checker仲裁HA切换。</p>
<p>切换的过程中在设计上有很重要一点考虑，不能同时有两个Active，所以要成为新Active NameNode，要把原来的Active NameNode停止掉。先会很友好地停止，什么是友好呢？就是发一个RPC，如果成功了就是友好的，如果失败了，就会ssh过去，把原来active namenode进程kill掉，这就是Active NameNode异常退的原因。</p>
<p>当这个原因了解了之后，其实要解决这个问题也非常简单。</p>
<p>第一点要把editlog与fsimage保存的本地目录分离配置，这种分离是磁盘上的分离，物理分离。</p>
<p>第二是checkpoint之后fsimage回传限速。把editlog与fsimage两个磁盘分离，fsimage回传的io压力不会对客户端请求造成影响，另外，回传限速后，也能限制io压力。这是比较棘手的问题。原因看起来很简单，但是从现象找到原因，这个过程并没有那么容易。</p>
<p><img src="http://wx4.sinaimg.cn/mw1024/6aae3cf3gy1fdlmfekc1hj20hs0ad0te.jpg" alt="img"></p>
<p>第二个案例也是一样，Active NN又出现异常退出，产生HA切换。这次和网络连接数有关，这张图是Active NameNode的所在机器的网络连接数，平时都挺正常，20000到30000之间，忽然有一个点一下打到60000多，然后就打平了，最后降下来，降下来的原因很明显，是服务进程退了。</p>
<p>为什么会出现这个情况呢？在后续分析的过程中我们发现了一个线索，在NameNode日志里报了一个空指针的异常。就顺藤摸瓜发现了一个JDK1.7的BUG，参见上面图片所示，在java select库函数调度路径过程中最终会调用这个函数（setUpdateEvents），大家可以看到，如果fd的个数超过了MAX_UPDATE_ARRAY_SIZE（65535）这个数的话，将会走到else路径，这个路径在if进行不等表达式判断时，将会出发空指针异常。</p>
<p>接下来的问题是，为什么会产生这么多的链接呢？经过分析我们发现，在问题出现的时候，存在一次大目录的DU操作，而DU会锁住整个namespace，这样就导致后续的写请求被阻塞，最终导致请求的堆积，请求的堆积导致了连接数大量堆积，连接数堆积到一定程度就触发JDK1.7的这个BUG。这个问题的解决，从两个方面看，首先我们先把JDK升级到1.8。其次，调整参数dfs.content-summary.limit，限制du操作的持锁时间。该参数默认参数是0。我们现在是设成10000了，大家可以参考。这是第二个非常棘手的问题。</p>
<p><img src="http://wx4.sinaimg.cn/mw1024/6aae3cf3gy1fdlmff4kp0j20hs0azq3m.jpg" alt="img"></p>
<p>第三个案例关于YARN主节点的，有一天中午，我们收到报警，发现Active RM异常进程退出，触发HA的切换，然而切换后一会新的Active RM节点也会异常退出，这就比较悲剧，我们先进行了恢复。之后我们从当时的日志中发现了原因：一个用户写了一万个文件到分布式缓存里，分布式缓存里数据会同步到ZK上，RM持久化作业状态到ZK时超过Znode单节点最大上限，抛出异常，最终导致ResourceManager进程的异常退出。其实问题的解决方法也非常简单，我们增加了限制逻辑，对于序列化数据量大于Znode节点大小的Job，直接抛异常触发Job的失败。另外我们还适当提升Znode节点大小。</p>
<p>以上是在稳定性方面的一些工作，这三个案例跟大家分享一下，如果有类似的问题建议大家可以尝试一下，这些方案是被我们验证OK的。</p>
<h3 id="平台治理"><a href="#平台治理" class="headerlink" title="平台治理"></a>平台治理</h3><p>接下来介绍一下平台治理这块。包含几个问题，其中第一问题是关于数据的，一方面，就是大家开发了数据之后，经常找不到，要靠喊，比如说在群里喊一下什么数据在哪，谁能告诉我一下，这个效率很低下。另外一方面是之前的管理数据是共享的，不安全，任何人都可以访问其他人的数据。</p>
<p>第二个问题是关于资源，之前是“大锅饭”模式，大家共享计算资源，相互竞争，这样“能吃的“肯定是挤兑”不能吃的“，经常出现核心任务不能按时按点完成，老板看不到数据，这点很可怕。还有是整个集群资源使用情况没有感知，这样根本不知道资源要怎么分配，是否够用。</p>
<p>第三个问题是关于作业的，开发人员开发大量的作业之后，这些作业要怎么管理，实际上他们可能都不知道。还有就是关于作业之间依赖，经常一个指标计算出来要经历多个作业，作业之间依赖是怎么考虑的，单纯靠时间上的依赖是非常脆弱的，如果前期的job延迟产生了，后续的job必然失败。最后一个问题是数据开发人员的效率不高，所需要做的步骤过多。</p>
<p><img src="http://wx4.sinaimg.cn/mw1024/6aae3cf3gy1fdlmff22ldj20hs0b3t9c.jpg" alt="img"></p>
<p>针对这四个问题我们做了一些改进，首先是数据与资源治理。数据方面要引入安全策略、元信息管理与基础数仓建设。我们自己开发了一套安全控制策略，主要增加了白名单和权限控制策略。一个HDFS的请求的流程，首先客户端会向NameNode发请求，NameNode接到请求之后首先要做连接解析，读取出请求相关内容做请求处理，再把结果反馈回来，之后客户端向相应的DataNode进行写入数据或者读取数据。从上述流程可以看出，所有HDFS操作全部要经过NameNode这一层。</p>
<p>那么安全策略只要在NameNode的两个点做下控制既可完成：在连接解析后，我们会验证请求方的IP，以及用户是不是在合法配置下面的。如果验证失败，则拒绝请求。如果验证通过，我们会进一步在请求处理过程中验证用户访问的目录和用户在否在合法的配置下。比如说用户A想访问用户B的数据，如果没在允许的情况下会把连接关掉，通过简单的策略调整就能达到灵活的数据的安全控制和数据共享的方式。接下来针对数据找不到的问题，我们开发了全公司层面的基础数据仓库以及针对全公司层面元数据管理平台。</p>
<p>这张图展示了基础数据仓库覆盖度，它覆盖了集团各个公司，又覆盖了多个平台，比如说手机、App端、PC端、微信端等等。数据层次，是数据仓库层、数据集市层还是数据应用层，所属哪个事业群，最后针对数据进行分类标签，比如说帖子数据、用户数据等等都可以通过标签的方式来找到。当想找具体一份数据的时候可以通过这个界面，点一些标签，筛选出一些数据表，甚至在搜索框里面搜数据的关键字。当查到数据表的时候可以在右侧按钮，将显示出表结构，还有表信息，表信息表明了这个表有多少列，这个表的负责人是什么，还有关于数据质量，表的数据量的变化情况等等，如果你想申请可以点击最右边的权限开通。整体开通流程也是自动化的。这是针对数据找不到的问题做的一些改进。</p>
<p>针对资源问题要避免大锅饭，必须要引入账号概念，资源按照账号预留与隔离。我们划分了不同的配额，根据预算、业务需求去申请配额，然后我们调整配额。针对队列这块我们划分多个队列，每个业务线有自己的队列，不同业务线不能跨队列提交任务，每个队列划分出不同资源，资源主要是针对业务线需求而定的。通过这些改进可以达到资源的隔离以及适度的共享。</p>
<p>有了账号的概念之后我们就可以统计每个业务线资源使用情况。我们每天都会有报表。显示了业务线的计算和存储资源的使用情况，甚至是Job的细节情况。</p>
<p>接下来我会介绍一下业务线开发效率低下问题的改进，实际上我们在易用性上也做了很多改进。首先我们开发了云窗平台，它主要解决了元信息查找、数据查询、可是化展示和多维分析这些需求。然后针对任务开发这块我们开发了58DP解决了元信息开发、作业管理与统计等。我们针对实时多维分析开发了飞流，实时作业开发全部配置化、同时支持多种统计算子、自动图表生成等等。还有NightFury，流程自动化管理平台。</p>
<p><img src="http://wx4.sinaimg.cn/mw1024/6aae3cf3gy1fdlmfffd06j20hs098dgb.jpg" alt="img"></p>
<p>这是云窗的界面，上面是一个SQL查询界面，下面是可视化产品界面，这是我们数据可视化的一个结果。</p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3gy1fdlmffnbcbj20hs0bq74i.jpg" alt="img"></p>
<p>然后关于任务开发的话，我们用58DP来做任务开发，可以支持的不同任务，涵盖目前的所有主流作业以及作业依赖等管理。这是58DP的页面，可以设置基本信息、调度及依赖等。</p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3gy1fdlmfg7udgj20hs0cet96.jpg" alt="img"></p>
<p>飞流是支持周期性的统计、全天累计性的统计，大家可以定义统计方法、定义任务的一些基本信息，设置维度、设置度量，设置完之后就展现了图形，也提供了跟昨天的对比情况。当在图里点任何一个点的时候，可以看到不同维度组合下在这个点上的数据分布，点击两个点可以看到不同维度下两个点的分布对比。针对历史数据可以进行对比，我们可以把时间拉的更长，可以查看不同周的实时统计结果，而不是一天。</p>
<p><img src="http://wx4.sinaimg.cn/mw1024/6aae3cf3gy1fdlmfgfx1dj20hs0cgdgb.jpg" alt="img"></p>
<p>这是NightFury的界面，这就是我们运维的自动化管理平台，大家可以看到有很多个流程和权限的开通申请，表单的填写、工单审批，审批之后的一些流程全部是自动化的。</p>
<h3 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h3><p>性能方面，主要分为四个方面：</p>
<p>MR作业性能、数据收集性能、SQL查询性能和多维分析的性能。针对MR作业性能，我们引用多租户功能，资源预留，核心作业执行有保障。</p>
<p>第二点小文件合并处理，可以提升任务执行效率，减少调度本身的开销。</p>
<p>第三点我们针对Shuffle阶段参数优化，可以实现并发度提升，IO消耗降低。</p>
<p>经过三个方面的改进之后，我们整体任务的运行时间实际上有一倍左右的提升。数据传输优化方面，我们经过消息合并改进数据传输性能，提升了20倍。在SQL优化方面我们引用内存执行引擎与列存储方案的结合，在同等资源情况下针对线上一百多条SQL进行测试，总体性能大概提升80%。在多维计算这块，我们引入Kylin，针对多维的查询95%以上查询能控制在2s以内。</p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3gy1fdlmfgr3zaj20hs0dawfa.jpg" alt="img"></p>
<h3 id="异构计算"><a href="#异构计算" class="headerlink" title="异构计算"></a>异构计算</h3><p>异构计算方面我们面临了两个主要问题，一个是作业的异构，我们有多种类型的作业，比如说实时作业强调低时延，而离线作业强调高吞吐，这本身就是矛盾的，怎么解决这个矛盾。第二方面是机器异构，CPU、内存、网络、磁盘配置不同，这种异构环境又要怎么办。</p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3gy1fdlmfsgkbwj20hs0cswfe.jpg" alt="img"></p>
<p>从上面图中可以看出：如果实时作业的task和批处理作业的task被调度到一台机器上了，如果批处理作业把资源占满了（例如网络带宽），则实时作业的task必将收到影响。所以，需要对实时作业和批处理作业做隔离才行。</p>
<p>做资源隔离，我们的思路是采用标签化，给每个NodeManager赋予不同标签，表示不同机器被分配了不同标签；资源队列也赋予不同标签，然后在RM调度时，保证相同标签的队列里容器资源必从相同标签的NodeManager上分配的。这样就可以通过标签的不同达到物理上的资源隔离目标。</p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3gy1fdlmfsuruej20hs0a10t0.jpg" alt="img"></p>
<p>这张图是实现图。首先可以看到NodeManager分成了两个集合，一个是实时的，一个是离线的，不同的队列也被赋予了实时或离线的标签，当用户提交一个job的时候它可以指定一个队列，提交到离线队列里就是离线任务，ResourceManager就会把这个作业所需要的资源分配到离线标签的NodeManager上，这样就可以做到物理资源隔离。</p>
<h2 id="未来规划"><a href="#未来规划" class="headerlink" title="未来规划"></a>未来规划</h2><p>以上主要是介绍了我们最近一年半做的一些工作。接下来我会介绍一下未来的规划。首先就是深度学习。这个概念今年非常火爆，甚至是要爆炸了，深度学习在58这块需求也是蛮强烈的。目前深度学习工具有这么多，caffe、theano、torch等等非常多，怎么做整合，怎么降低使用成本，这是第一个问题。第二个问题，机器是有限的，怎么高效利用资源，需要把机器分配模式变成资源分配模式。还有光有单机的机器学习或者深度学习工具还不够，因为性能太差，所以我们需要将深度学习训练分布式化。我们做了一个初步的测试，针对caffe与Tensorflow工具的分布式化训练做了比较，4卡相对于单卡模型训练性能提升100%~170%，所以分布式化的工作本身意义也是非常大的。</p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3gy1fdlmftc0xlj20hs0a0q3h.jpg" alt="img"></p>
<p>这个图展示的是工具融合方案。我们这里利用的是Kubernetes，支持主流的深度学习工具，每个工具做成镜像形成POD，用户需要的话可以直接把POD分发给他，用户在训练的时候从HDFS上直接拉取样本，并且把训练的参数回写到HDFS上，也就是说通过HDFS做数据的共享，通过这种模式可以很轻松地支持多种深度学习工具，也可以达到按所需资源量进行资源的分配目标。</p>
<p>另外我们会做一个深度学习工具分布式的改造，是针对caffe，我们用的是CaffeOnSpark，即把整个分布式的方案做成模板供用户使用。首先启动多个POD，通过POD启动一个Spark集群，然后再提一个Spark job来做训练，最后在整个训练结束之后再把集群停掉。Tensorflow也是一样的，首先启动tensorflow集群，然后提交任务，任务训练完以后再把集群停掉。其他工具分布式化我们也会采取类似的思路解决。以上是关于深度学习这块我们目前的一些工作。</p>
<p><img src="http://wx3.sinaimg.cn/mw1024/6aae3cf3gy1fdlmfth4z7j20hs0amjrx.jpg" alt="img"></p>
<p>其次，是关于空间资源利用率的。目前我们有一千多台机器，存储是很大的成本。之前也提到了，我们是属于花钱的部门，所以压力非常大。那怎么节省成本是一个很重要的问题。除了传统压缩之外，还能做什么？HDFS RAID是一个比较好的解决方案。HDFS RAID采用是RC编码，类似RAID6，比如一个文件有m个块，根据m个块生成k个校验块，然后能保证k个块丢失的情况下数据还能找回来，举个例子来说，比如文件2.5G大小，256M一个块，可以分成10个块，根据RC算法再生成4个校验块，可以保证丢了4个块情况下，数据都能找回来。在这个例子中，3副本情况下，一共需要30个块，而采用HDFS RAID，仅需要14个块。但他们的可靠性一样，空间占用情况却差了57%。</p>
<p>具体实施时，第一步对集群数据进行冷热分析，RAID毕竟有些性能问题，一旦数据有问题，你要通过计算才能恢复，势必会造成性能低下，所以针对冷数据做肯定是风险最低的。第二步就是压缩+archive+RAID，通过三方面技术结合把文件数和空间全部节省出来。归档实际上是会变换目录的，为了做适配，我们通过软连接功能，做到对用户透明。最后在数据读取时，如果是RAID数据，就要具备实时RAID修复功能才能保证在数据缺失的情况下不影响数据的访问。</p>
<p>后续我们会对计算资源利用率再做进一步提升。另外也会考虑Storm和YARN扩展性。还有Kubernetes调度优化，比如针对GPU资源管理功能。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以上就是我今天想介绍的全部内容。在结束之前请允许我再做一下总结。</p>
<p>首先我介绍了58目前的大数据平台架构是怎么样的，简单来说就是“342”，三个层次、细分为四个子层、旁边两列。所以大家要做大数据平台建设工作，这几个方面是必备的。</p>
<p>第二个方面我重点的介绍了58在一年半的时间内的技术改进。第一点是关于稳定性，主要从Flume和HDFS扩展性方面重点介绍了我们的解决方案，举了三个案例来说明突发问题，不是说有了可用性和扩展性就万事OK了，还要解决突发问题。针对平台治理首先介绍了一下数据和资源的治理方法，接着又介绍了关于易用性方面的改进，我们提供了一系列平台来提高开发人员的开发效率。</p>
<p>第三方面从性能上介绍了我们这边做的优化工作以及优化的结果是怎么样的；</p>
<p>第四方面介绍了在异构环境下如何支持不同特征的作业进行合理调度。</p>
<p>最后我介绍了58深度学习平台建设方面以及存储资源空间利用率优化方面的内容。以上就是我今天的全部内容，希望对大家有帮助。</p>
<h5 id="作者介绍"><a href="#作者介绍" class="headerlink" title="作者介绍"></a>作者介绍</h5><p>赵健博，58集团高级架构师，技术委员会委员。大数据领域专家，2009年毕业于中国科学院计算所，先后就职于百度、奇虎360、58集团，主要研究领域包括分布式计算与存储系统等。58集团大数据平台负责人，负责大数据平台在集团的研发，应用与发展。</p>
<p>Good Luck!</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://stanxia.github.io/2017/03/07/Alibaba代码规范/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="森">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="东篱下">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/03/07/Alibaba代码规范/" itemprop="url">Alibaba代码规范</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-03-07T01:24:49+08:00">
                2017-03-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/" itemprop="url" rel="index">
                    <span itemprop="name">java</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script src="/assets/js/DPlayer.min.js"> </script><h1 id=""><a href="#" class="headerlink" title=""></a><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3ly1fddnlrez7aj20m10v6wid.jpg" alt="1"></h1><p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3ly1fddnlrozi5j20m10v60xm.jpg" alt="2"></p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3ly1fddnlrxvhzj20m10v6dmk.jpg" alt="3"></p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3ly1fddnls4woqj20m10v6wjg.jpg" alt="4"></p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3ly1fddnlsevkgj20m10v6n3c.jpg" alt="5"></p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3ly1fddnlso65nj20m10v6af9.jpg" alt="6"></p>
<p><img src="http://wx3.sinaimg.cn/mw1024/6aae3cf3ly1fddnlt2v04j20m10v6wji.jpg" alt="7"></p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3ly1fddnlteijwj20m10v6jv6.jpg" alt="8"></p>
<p><img src="http://wx4.sinaimg.cn/mw1024/6aae3cf3ly1fddnltobz1j20m10v6jxd.jpg" alt="9"></p>
<p><img src="http://wx4.sinaimg.cn/mw1024/6aae3cf3ly1fddnmp0dsgj20m10v6n31.jpg" alt="10"></p>
<p><img src="http://wx3.sinaimg.cn/mw1024/6aae3cf3ly1fddnmpcafoj20m10v60wt.jpg" alt=""></p>
<p><img src="http://wx3.sinaimg.cn/mw1024/6aae3cf3ly1fddnmpkfj4j20m10v6act.jpg" alt=""></p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3ly1fddnmpsz1gj20m10v67a9.jpg" alt=""></p>
<p><img src="http://wx3.sinaimg.cn/mw1024/6aae3cf3ly1fddnmq29kaj20m10v6jvc.jpg" alt=""></p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3ly1fddnmqc2z2j20m10v6tei.jpg" alt=""></p>
<p><img src="http://wx3.sinaimg.cn/mw1024/6aae3cf3ly1fddnmquwirj20m10v6dme.jpg" alt=""></p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3ly1fddnmr8hefj20m10v6gr3.jpg" alt=""></p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3ly1fddnmrgurmj20m10v60xo.jpg" alt=""></p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3ly1fddnn3j1roj20m10v6grf.jpg" alt=""></p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3ly1fddnn3tks7j20m10v60zs.jpg" alt=""></p>
<p><img src="http://wx3.sinaimg.cn/mw1024/6aae3cf3ly1fddnn424u1j20m10v6wgi.jpg" alt=""></p>
<p><img src="http://wx3.sinaimg.cn/mw1024/6aae3cf3ly1fddnn4gxacj20m10v6gra.jpg" alt=""></p>
<p><img src="http://wx3.sinaimg.cn/mw1024/6aae3cf3ly1fddnn51icgj20m10v6gsi.jpg" alt=""></p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3ly1fddnn5ccrej20m10v6jx5.jpg" alt=""></p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3ly1fddnn5o68dj20m10v6grd.jpg" alt=""></p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3ly1fddnn5wzwdj20m10v641o.jpg" alt=""></p>
<p><img src="http://wx3.sinaimg.cn/mw1024/6aae3cf3ly1fddnn6fm1wj20m10v6n3f.jpg" alt=""></p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3ly1fddnnicr5kj20m10v6tbr.jpg" alt=""></p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3ly1fddnnjg99vj20m10v6q94.jpg" alt=""></p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3ly1fddnnkcow3j20m10v67av.jpg" alt=""></p>
<p><img src="http://wx3.sinaimg.cn/mw1024/6aae3cf3ly1fddnnkzpc6j20m10v679k.jpg" alt=""></p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3ly1fddnnllrwnj20m10v6wl6.jpg" alt=""></p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3ly1fddnnm019vj20m10v6dkg.jpg" alt=""></p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3ly1fddnnm9rsrj20m10v6djm.jpg" alt=""></p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3ly1fddnnml164j20m10v6tdv.jpg" alt=""></p>
<p><img src="http://wx4.sinaimg.cn/mw1024/6aae3cf3ly1fddnnmulirj20m10v6jvt.jpg" alt=""></p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3ly1fddnnxpv60j20m10v6q7b.jpg" alt=""></p>
<p>Good Luck!</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://stanxia.github.io/2017/02/21/经典大数据架构案例：酷狗音乐的大数据平台重构/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="森">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="东篱下">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/02/21/经典大数据架构案例：酷狗音乐的大数据平台重构/" itemprop="url">经典大数据架构案例：酷狗音乐的大数据平台重构</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-02-21T00:06:45+08:00">
                2017-02-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script src="/assets/js/DPlayer.min.js"> </script><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>编者按：本文是酷狗音乐的架构师王劲对酷狗大数据架构重构的总结。酷狗音乐的大数据架构本身很经典，而这篇讲解了对原来的架构上进行重构的工作内容，总共分为重构的原因、新一代的大数据技术架构、踩过的坑、后续持续改进四个部分来给大家谈酷狗音乐大数据平台重构的过程。</p>
<p>眨眼就新的一年了，时间过的真快，趁这段时间一直在写总结的机会，也总结下上一年的工作经验，避免重复踩坑。酷狗音乐大数据平台重构整整经历了一年时间，大头的行为流水数据迁移到新平台稳定运行，在这过程中填过坑，挖过坑，为后续业务的实时计算需求打下了很好的基础。在此感谢酷狗团队成员的不懈努力，大部分从开始只知道大数据这个概念，到现在成为团队的技术支柱，感到很欣慰。</p>
<p>从重构原因，技术架构，踩过的坑，后续持续改进四个方面来描述酷狗音乐大数据平台重构的过程，在此抛砖引玉，这次的内容与6月份在高可用架构群分享的大数据技术实践的有点不同，技术架构做了些调整。</p>
<p>其实大数据平台是一个庞大的系统工程，整个建设周期很长，涉及的生态链很长(包括：数据采集、接入，清洗、存储计算、数据挖掘，可视化等环节，每个环节都可以当做一个复杂的系统来建设)，风险也很大。</p>
<h2 id="一、重构原因"><a href="#一、重构原因" class="headerlink" title="一、重构原因"></a>一、重构原因</h2><p>在讲重构原因前，先介绍下原有的大数据平台架构，如下图：</p>
<p><img src="http://wx3.sinaimg.cn/mw1024/6aae3cf3gy1fcxe2flakmj20at07j0tr.jpg" alt="1"></p>
<p>从上图可知，主要基于Hadoop1.x+hive做离线计算(T+1)，基于大数据平台的数据采集、数据接入、数据清洗、作业调度、平台监控几个环节存在的一些问题来列举下。</p>
<p>数据采集：</p>
<ol>
<li>数据收集接口众多，且数据格式混乱，基本每个业务都有自己的上报接口</li>
<li>存在较大的重复开发成本</li>
<li>不能汇总上报，消耗客户端资源，以及网络流量</li>
<li>每个接口收集数据项和格式不统一，加大后期数据统计分析难度</li>
<li>各个接口实现质量并不高，存在被刷，泄密等风险</li>
</ol>
<p>数据接入:</p>
<ol>
<li>通过rsync同步文件，很难满足实时流计算的需求</li>
<li>接入数据出现异常后，很难排查及定位问题，需要很高的人力成本排查</li>
<li>业务系统数据通过Kettle每天全量同步到数据中心，同步时间长，导致依赖的作业经常会有延时现象</li>
</ol>
<p>数据清洗：</p>
<ol>
<li>ETL集中在作业计算前进行处理</li>
<li>存在重复清洗</li>
</ol>
<p>作业调度：</p>
<ol>
<li>大部分作业通过crontab调度，作业多了后不利于管理</li>
<li>经常出现作业调度冲突</li>
</ol>
<p>平台监控：</p>
<ol>
<li>只有硬件与操作系统级监控</li>
<li>数据平台方面的监控等于空白</li>
</ol>
<p>基于以上问题，结合在大数据中，数据的时效性越高，数据越有价值(如：实时个性化推荐系统，RTB系统，实时预警系统等)的理念，因此，开始大重构数据平台架构。</p>
<h2 id="二、新一代大数据技术架构"><a href="#二、新一代大数据技术架构" class="headerlink" title="二、新一代大数据技术架构"></a>二、新一代大数据技术架构</h2><p>在讲新一代大数据技术架构前，先讲下大数据特征与大数据技术要解决的问题。</p>
<p>1.大数据特征：“大量化(Volume)、多样化(Variety)、快速化(Velocity)、价值密度低（Value）”就是“大数据”显著的4V特征，或者说，只有具备这些特点的数据，才是大数据。</p>
<p><img src="http://wx3.sinaimg.cn/mw1024/6aae3cf3gy1fcxe2gw02gj20jp0cxwjl.jpg" alt="2"></p>
<p>2.大数据技术要解决的问题：大数据技术被设计用于在成本可承受的条件下，通过非常快速（velocity）地采集、发现和分析，从大量（volumes）、多类别（variety）的数据中提取价值（value），将是IT领域新一代的技术与架构。</p>
<p><img src="http://wx4.sinaimg.cn/mw1024/6aae3cf3gy1fcxe2iu0jpj20kb0d4tbj.jpg" alt="3"></p>
<p>介绍了大数据的特性及大数据技术要解决的问题，我们先看看新一代大数据技术架构的数据流架构图：</p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3gy1fcxe2k4hl5j20k50cldkv.jpg" alt="4"></p>
<p>从这张图中，可以了解到大数据处理过程可以分为数据源、数据接入、数据清洗、数据缓存、存储计算、数据服务、数据消费等环节，每个环节都有具有高可用性、可扩展性等特性，都为下一个节点更好的服务打下基础。整个数据流过程都被数据质量监控系统监控，数据异常自动预警、告警。</p>
<p>新一代大数据整体技术架构如图：</p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3gy1fcxe2lvooaj20kc0dmq6e.jpg" alt="5"></p>
<p>将大数据计算分为实时计算与离线计算，在整个集群中，奔着能实时计算的，一定走实时计算流处理，通过实时计算流来提高数据的时效性及数据价值，同时减轻集群的资源使用率集中现象。</p>
<p>整体架构从下往上解释下每层的作用：</p>
<p>数据实时采集：</p>
<p>主要用于数据源采集服务，从数据流架构图中，可以知道，数据源分为前端日志，服务端日志，业务系统数据。下面讲解数据是怎么采集接入的。</p>
<p>a.前端日志采集接入：</p>
<p>前端日志采集要求实时，可靠性，高可用性等特性。技术选型时，对开源的数据采集工具flume,scribe,chukwa测试对比，发现基本满足不了我们的业务场景需求。所以，选择基于kafka开发一套数据采集网关，来完成数据采集需求。数据采集网关的开发过程中走了一些弯路，最后采用nginx+lua开发，基于lua实现了kafka生产者协议。有兴趣同学可以<a href="https://github.com/doujiang24/lua-resty-kafka" target="_blank" rel="external">去Github上看看</a>，另一同事实现的，现在在github上比较活跃，被一些互联网公司应用于线上环境了。</p>
<p>b.后端日志采集接入：</p>
<p>FileCollect,考虑到很多线上环境的环境变量不能改动，为减少侵入式，目前是采用Go语言实现文件采集，年后也准备重构这块。</p>
<p>前端，服务端的数据采集整体架构如下图：</p>
<p><img src="http://wx4.sinaimg.cn/mw1024/6aae3cf3gy1fcxe2ndr1vj20ku0fugo3.jpg" alt="6"></p>
<p>c.业务数据接入</p>
<p>利用<a href="http://agapple.iteye.com/blog/1796633" target="_blank" rel="external">Canal</a>通过MySQL的binlog机制实时同步业务增量数据。</p>
<p>数据统一接入：为了后面数据流环节的处理规范，所有的数据接入数据中心，必须通过数据采集网关转换统一上报给Kafka集群，避免后端多种接入方式的处理问题。</p>
<p>数据实时清洗(ETL)：为了减轻存储计算集群的资源压力及数据可重用性角度考虑，把数据解压、解密、转义，部分简单的补全，异常数据处理等工作前移到数据流中处理，为后面环节的数据重用打下扎实的基础(实时计算与离线计算)。</p>
<p>数据缓存重用：为了避免大量数据流(400+亿条/天)写入HDFS，导致HDFS客户端不稳定现象及数据实时性考虑，把经过数据实时清洗后的数据重新写入Kafka并保留一定周期，离线计算(批处理)通过KG-Camus拉到HDFS(通过作业调度系统配置相应的作业计划)，实时计算基于Storm/JStorm直接从Kafka消费，有很完美的解决方案storm-kafka组件。</p>
<p>离线计算(批处理)：通过spark，spark SQL实现，整体性能比hive提高5—10倍，hive脚本都在转换为Spark/Spark SQL；部分复杂的作业还是通过Hive/Spark的方式实现。在离线计算中大部分公司都会涉及到数据仓库的问题，酷狗音乐也不例外，也有数据仓库的概念，只是我们在做存储分层设计时弱化了数据仓库概念。数据存储分层模型如下图：</p>
<p><img src="http://wx4.sinaimg.cn/mw1024/6aae3cf3gy1fcxe2ombcpj20ff0cl0tx.jpg" alt="7"></p>
<p>大数据平台数据存储模型分为：数据缓冲层Data Cache Layer（DCL）、数据明细层Data Detail Layer（DDL）、公共数据层（Common）、数据汇总层Data Summary Layer（DSL）、数据应用层Data Application Layer（DAL）、数据分析层（Analysis）、临时提数层（Temp）。</p>
<p>1）数据缓冲层(DCL)：存储业务系统或者客户端上报的，经过解码、清洗、转换后的原始数据，为数据过滤做准备。</p>
<p>2)数据明细层（DDL）：存储接口缓冲层数据经过过滤后的明细数据。</p>
<p>3）公共数据层（Common）：主要存储维表数据与外部业务系统数据。</p>
<p>4）数据汇总层（DSL）：存储对明细数据，按业务主题，与公共数据层数据进行管理后的用户行为主题数据、用户行为宽表数据、轻量汇总数据等。为数据应用层统计计算提供基础数据。数据汇总层的数据永久保存在集群中。</p>
<p>5）数据应用层（DAL）：存储运营分析（Operations Analysis ）、指标体系（Metrics System）、线上服务（Online Service）与用户分析（User Analysis）等。需要对外输出的数据都存储在这一层。主要基于热数据部分对外提供服务，通过一定周期的数据还需要到DSL层装载查询。</p>
<p>6）数据分析层（Analysis）：存储对数据明细层、公共数据层、数据汇总层关联后经过算法计算的、为推荐、广告、榜单等数据挖掘需求提供中间结果的数据。</p>
<p>7）临时提数层（Temp）：存储临时提数、数据质量校验等生产的临时数据。</p>
<p>实时计算：基于Storm/JStorm，<a href="http://www.drools.org/" target="_blank" rel="external">Drools</a>,<a href="http://blog.csdn.net/luonanqin/article/category/1557469" target="_blank" rel="external">Esper</a>。主要应用于实时监控系统、APM、数据实时清洗平台、实时DAU统计等。</p>
<p>HBase/MySQL：用于实时计算，离线计算结果存储服务。</p>
<p>Redis：用于中间计算结果存储或字典数据等。</p>
<p>Elasticsearch：用于明细数据实时查询及HBase的二级索引存储(这块目前在数据中心还没有大规模使用，有兴趣的同学可以加入我们一起玩ES)。</p>
<p>Druid：目前用于支持大数据集的快速即席查询(ad-hoc)。</p>
<p>数据平台监控系统：数据平台监控系统包括基础平台监控系统与数据质量监控系统，数据平台监控系统分为2大方向，宏观层面和微观层面。宏观角度的理解就是进程级别,拓扑结构级别,拿Hadoop举例，如：DataNode，NameNode，JournalNode，ResourceManager，NodeManager，主要就是这5大组件，通过分析这些节点上的监控数据，一般你能够定位到慢节点，可能某台机器的网络出问题了，或者说某台机器执行的时间总是大于正常机器等等这样类似的问题。刚刚说的另一个监控方向，就是微观层面，就是细粒度化的监控，基于user用户级别，基于单个job，单个task级别的监控，像这类监控指标就是另一大方向，这类的监控指标在实际的使用场景中特别重要，一旦你的集群资源是开放给外面的用户使用，用户本身不了解你的这套机制原理，很容易会乱申请资源，造成严重拖垮集群整体运作效率的事情，所以这类监控的指标就是为了防止这样的事情发生。目前我们主要实现了宏观层面的监控。如：数据质量监控系统实现方案如下。</p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3gy1fcxe2pdg7rj20k60cswh6.jpg" alt="8"></p>
<h2 id="三、大数据平台重构过程中踩过的坑"><a href="#三、大数据平台重构过程中踩过的坑" class="headerlink" title="三、大数据平台重构过程中踩过的坑"></a>三、大数据平台重构过程中踩过的坑</h2><p>我们在大数据平台重构过程中踩过的坑，大致可以分为操作系统、架构设计、开源组件三类，下面主要列举些比较典型的，花时间比较长的问题。</p>
<p>1、操作系统级的坑</p>
<p>Hadoop的I/O性能很大程度上依赖于Linux本地文件系统的读写性能。Linux中有多种文件系统可供选择，比如ext3和ext4，不同的文件系统性能有一定的差别。我们主要想利用ext4文件系统的特性，由于之前的操作系统都是CentOS5.9不支持ext4文件格式，所以考虑操作系统升级为CentOS6.3版本，部署Hadoop集群后，作业一启动，就出现CPU内核过高的问题。如下图</p>
<p><img src="http://wx4.sinaimg.cn/mw1024/6aae3cf3gy1fcxe2t4lrhj20hq09yaj2.jpg" alt="9"></p>
<p>经过很长时间的测试验证，发现CentOS6优化了内存申请的效率，引入了THP的特性，而Hadoop是高密集型内存运算系统，这个改动给hadoop带来了副作用。通过以下内核参数优化关闭系统THP特性，CPU内核使用率马上下降，如下图:</p>
<p>echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/enabled</p>
<p>echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag</p>
<p><img src="http://wx4.sinaimg.cn/mw1024/6aae3cf3gy1fcxe39po0yj20ip0anwlr.jpg" alt="10"></p>
<p>2、架构设计的坑</p>
<p>最初的数据流架构是数据采集网关把数据上报给Kafka，再由数据实时清洗平台(ETL)做预处理后直接实时写入HDFS，如下图：</p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3gy1fcxe3ao3htj20eb03474f.jpg" alt="11"></p>
<p>此架构，需要维持HDFS Client的长连接，由于网络等各种原因导致Storm实时写入HDFS经常不稳定，隔三差五的出现数据异常，使后面的计算结果异常不断，当时尝试过很多种手段去优化，如：保证长连接、连接断后重试机制、调整HDFS服务端参数等，都解决的不是彻底。</p>
<p>每天异常不断，旧异常没解决，新异常又来了，在压力山大的情况下，考虑从架构角度调整，不能只从具体的技术点去优化了，在做架构调整时，考虑到我们架构重构的初衷，提高数据的实时性，尽量让计算任务实时化，但重构过程中要考虑现有业务的过渡，所以架构必须支持实时与离线的需求，结合这些需求，在数据实时清洗平台(ETL)后加了一层数据缓存重用层(kafka)，也就是经过数据实时清洗平台后的数据还是写入kafka集群，由于kafka支持重复消费，所以同一份数据可以既满足实时计算也满足离线计算，从上面的整体技术架构也可以看出，如下图：</p>
<p><img src="http://wx1.sinaimg.cn/mw1024/6aae3cf3gy1fcxe3bzx3ij20j904c3z3.jpg" alt="12"></p>
<p>KG-Camus组件也是基于架构调整后，重新实现了一套离线消费Kafka集群数据的组件，此组件是参考LinkedIn的Camus实现的。此方式，使数据消费模式由原来的推方式改为拉模式了，不用维持HDFS Client的长连接等功能了，直接由作业调度系统每隔时间去拉一次数据，不同的业务可以设置不同的时间间隔，从此架构调整上线后，基本没有类似的异常出现了。</p>
<p>这个坑，是我自己给自己挖的，导致我们的重构计划延期2个月，主要原因是由最初技术预研究测试不充分所导致。</p>
<p>3、开源组件的坑</p>
<p>由于整个数据平台涉及到的开源组件很多，踩过的坑也是十个手指数不过来。</p>
<p>1）、当我们的行为数据全量接入到Kafka集群(几百亿/天)，数据采集网卡出现大量连接超时现象，但万兆网卡进出流量使用率并不是很高，只有几百Mbit/s，经过大量的测试排查后，调整以下参数，就是顺利解决了此问题。调整参数后网卡流量如下图：</p>
<p>a)、num.network.threads(网络处理线程数)值应该比cpu数略大</p>
<p>b)、num.io.threads(接收网络线程请求并处理线程数)值提高为cpu数两倍</p>
<p><img src="http://wx2.sinaimg.cn/mw1024/6aae3cf3gy1fcxe3dps6xj20de06u3zc.jpg" alt="13"></p>
<p>2）、在hive0.14 版本中，利用函数ROW_NUMBER() OVER对数据进行数据处理后，导致大量的作业出现延时很大的现象，经异常排查后，发现在数据记录数没变的情况，数据的存储容量扩大到原来的5倍左右，导致MapReduce执行很慢造成的。改为自己实现类似的函数后，解决了容量扩大为原来几倍的现象。说到这里，也在此请教读到此处的读者一个问题，在海量数据去重中采用什么算法或组件进行比较合适，既能高性能又能高准确性，有好的建议或解决方案可以加happyjim2010微信私我。</p>
<p>3）、在业务实时监控系统中，用OpenTSDB与实时计算系统（storm）结合，用于聚合并存储实时metric数据。在这种实现中，通常需要在实时计算部分使用一个时间窗口（window），用于聚合实时数据，然后将聚合结果写入tsdb。但是，由于在实际情况中，实时数据在采集、上报阶段可能会存在延时，而导致tsdb写入的数据不准确。针对这个问题，我们做了一个改进，在原有tsdb写入api的基础上，增加了一个原子加api。这样，延迟到来的数据会被叠加到之前写入的数据之上，实时的准确性由于不可避免的原因（采集、上报阶段）产生了延迟，到最终的准确性也可以得到保证。另外，添加了这个改进之后，实时计算端的时间窗口就不需要因为考虑延迟问题设置得比较大，这样既节省了内存的消耗，也提高了实时性。</p>
<h2 id="四、后续持续改进"><a href="#四、后续持续改进" class="headerlink" title="四、后续持续改进"></a>四、后续持续改进</h2><p>数据存储(分布式内存文件系统(Tachyon)、数据多介质分层存储、数据列式存储)、即席查询(OLAP)、资源隔离、数据安全、平台微观层面监控、数据对外服务等。</p>
<h2 id="作者介绍"><a href="#作者介绍" class="headerlink" title="作者介绍"></a>作者介绍</h2><p><strong>王劲</strong>，目前就职酷狗音乐，大数据架构师，负责酷狗大数据技术规划、建设、应用。 11年的IT从业经验，2年分布式应用开发，3年大数据技术实践经验，主要研究方向流式计算、大数据存储计算、分布式存储系统、NoSQL、搜索引擎等。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://stanxia.github.io/2017/01/26/CAD破解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="森">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="东篱下">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/01/26/CAD破解/" itemprop="url">CAD破解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-01-26T21:29:49+08:00">
                2017-01-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/技术/" itemprop="url" rel="index">
                    <span itemprop="name">技术</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script src="/assets/js/DPlayer.min.js"> </script><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>CAD制图软件pojie方式。</p>
<h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><p><a href="https://www.rkdot.com/autodesk-autocad-2017/" target="_blank" rel="external">点我看看原博</a></p>
<h5 id="破解"><a href="#破解" class="headerlink" title="破解"></a>破解</h5><ol>
<li><p>运行破解包中的AutoCAD_2017_English_Win_64bit_dlm_001_002.sfx.exe进行解压安装Autodesk 2017.</p>
</li>
<li><p>安装完重启</p>
</li>
<li><p>运行Autodesk 2017</p>
</li>
<li><p>提示注册，选择“Enter a Serial Number”</p>
</li>
<li><p>使用一下序列号：</p>
<pre><code>666-69696969, 667-98989898, 400-45454545或者066-66666666，产品密钥为001I1
</code></pre></li>
<li><p>断开网络，点击Next</p>
</li>
<li><p>会提示需要网络连接，选择离线激活（第二个选项）</p>
</li>
<li><p>重点步骤：</p>
<p>8.1 管理员身份运行注册机，点击Patch,此时会看到successfully patched；</p>
<p>8.2 拷贝请求码（Request cod））到注册机中，然后点击Generate，生成激活码（Activation）；</p>
<p>8.3 拷贝激活码（Activation）到软件注册窗口完成注册。</p>
</li>
</ol>
<h2 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h2><p><a href="https://pan.baidu.com/s/1c2eO9sk" target="_blank" rel="external">点我下载</a></p>
<p>密码: yuux</p>
<p>Good Luck!</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://stanxia.github.io/2016/05/22/linux防火墙开启特定端口访问/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="森">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="东篱下">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2016/05/22/linux防火墙开启特定端口访问/" itemprop="url">linux防火墙开启特定端口访问</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-05-22T14:56:40+08:00">
                2016-05-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/linux/" itemprop="url" rel="index">
                    <span itemprop="name">linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script src="/assets/js/DPlayer.min.js"> </script><p>linux开启和关闭端口外网访问的两种方式：</p>
<p>1、命令方式：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">/sbin/iptables -I INPUT -p tcp --dport 3306 -j ACCEPT <span class="comment">#开启3306端口</span></div><div class="line">/sbin/iptables -A INPUT -p tcp --dport 3306 -j DROP <span class="comment">#关闭端口</span></div></pre></td></tr></table></figure>
<p>修改完时记得保存并重启服务</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">/etc/rc.d/init.d/iptables save <span class="comment">#保存配置 </span></div><div class="line">/etc/rc.d/init.d/iptables restart <span class="comment">#重启服务 </span></div><div class="line">netstat -anp|grep 3306    <span class="comment">#查看端口是否已经开放</span></div></pre></td></tr></table></figure>
<p>2、编辑iptables文件，在22端口位置下面添加一行</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vi /etc/sysconfig/iptables</div></pre></td></tr></table></figure>
<p>添加好之后如下所示：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">###################################### </span></div><div class="line"><span class="comment"># Firewall configuration written by system-config-firewall </span></div><div class="line"><span class="comment"># Manual customization of this file is not recommended. </span></div><div class="line">*filter </div><div class="line">:INPUT ACCEPT [0:0] </div><div class="line">:FORWARD ACCEPT [0:0] </div><div class="line">:OUTPUT ACCEPT [0:0] </div><div class="line">-A INPUT -m state –state ESTABLISHED,RELATED -j ACCEPT </div><div class="line">-A INPUT -p icmp -j ACCEPT </div><div class="line">-A INPUT -i lo -j ACCEPT </div><div class="line">-A INPUT -m state –state NEW -m tcp -p tcp –dport 22 -j ACCEPT </div><div class="line">-A INPUT -m state –state NEW -m tcp -p tcp –dport 3306 -j ACCEPT </div><div class="line">-A INPUT -j REJECT –reject-with icmp-host-prohibited </div><div class="line">-A FORWARD -j REJECT –reject-with icmp-host-prohibited </div><div class="line">COMMIT</div></pre></td></tr></table></figure>
<p>最后重启防火墙使配置生效</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/etc/init.d/iptables restart</div></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">森</p>
              <p class="site-description motion-element" itemprop="description">采菊东篱下，悠然见南山</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">59</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">33</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">森</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  
  

  

  

  

</body>
</html>
